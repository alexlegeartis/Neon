%\documentclass[wcp,gray]{jmlr} % test grayscale version
 %\documentclass[wcp]{jmlr}% former name JMLR W\&CP
\documentclass[pmlr]{jmlr}% new name PMLR (Proceedings of Machine Learning)

\RequirePackage{graphicx}
 % The following packages will be automatically loaded:
 % amsmath, amssymb, natbib, graphicx, url, algorithm2e
 \usepackage{booktabs}
 %\usepackage{rotating}% for sideways figures and tables
\usepackage{longtable}% for long tables
\usepackage{arydshln}% for dashed lines in tables

\makeatletter
\def\set@curr@file#1{\def\@curr@file{#1}} %temp workaround for 2019 latex release
\makeatother
\usepackage[load-configurations=version-1]{siunitx} % newer version

% Load required packages for custom math commands
\usepackage{mathtools}
\usepackage{bm}

% Custom math commands
% Blackboard bold
\newcommand{\R}{\mathbb{R}}

% Bold matrices
\newcommand{\mA}{\bm{A}}
\newcommand{\mB}{\bm{B}}
\newcommand{\mD}{\bm{D}}
\newcommand{\mF}{\bm{F}}
\newcommand{\mG}{\bm{G}}
\newcommand{\mM}{\bm{M}}
\newcommand{\mN}{\bm{N}}
\newcommand{\mS}{\bm{S}}
\newcommand{\mU}{\bm{U}}
\newcommand{\mV}{\bm{V}}
\newcommand{\mX}{\bm{X}}
\newcommand{\mSigma}{\bm{\Sigma}}

% Vectors (note: \vec is already defined by jmlr class)
% We define shortcuts for commonly used vectors
\newcommand{\vu}{\vec{u}}
\newcommand{\vv}{\vec{v}}

% Calligraphic letters
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cS}{\mathcal{S}}

% Norms and operators
\newcommand{\norm}[1]{\lVert #1\rVert}
\DeclarePairedDelimiter{\normf}{\|}{\|_\mathrm{F}}
\DeclarePairedDelimiter{\norms}{\|}{\|_{\mathrm{2}}}
\DeclarePairedDelimiter{\normc}{\|}{\|_{\mathrm{C}}}
\DeclarePairedDelimiter{\normn}{\|}{\|_{*}}
\newcommand{\normkfk}[1]{\|#1\|_{\text{KF-}k}}
\DeclarePairedDelimiter{\normfstar}{\|}{\|_\mathrm{F*}}
\DeclarePairedDelimiter{\normftwo}{\|}{\|_\mathrm{F2}}

% Inner product notation
\def\<#1,#2>{\langle #1,#2\rangle}

% Math operators
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\diag}{diag}

% Custom shortcuts
\renewcommand{\epsilon}{\varepsilon}
\newcommand{\Rmn}{\R^{m\times n}}

% change the arguments, as appropriate, in the following:
\jmlrvolume{tbd}
\jmlryear{2025}
\jmlrworkshop{International Conference on Computational Optimization}

\title[Ky Fan Norms for Matrix Optimization]{The Ky Fan Norms and Beyond: Dual Norms and Combinations for Matrix Optimization}

\author{\Name{Alexey Kravatskiy}
       \Email{kravtskii.aiu@phystech.edu}\\
       \addr Moscow Institute of Physics and Technology (MIPT)
       \AND
       \Name{Ivan Kozyrev}
       \Email{kozyrev.in@phystech.edu}\\
       \addr Moscow Institute of Physics and Technology (MIPT)\\
       Marchuk Institute of Numerical Mathematics
       \AND
       \Name{Nikolai Kozlov}
       \Email{kozlov.na@phystech.edu}\\
       \addr Moscow Institute of Physics and Technology (MIPT)
       \AND
       \Name{Alexander Vinogradov}
       \Email{vinogradov.am@phystech.edu}\\
       \addr Moscow Institute of Physics and Technology (MIPT)
       \AND
       \Name{Daniil Merkulov}
       \Email{daniil.merkulov@phystech.edu}\\
       \addr Moscow Institute of Physics and Technology (MIPT)\\
       Skoltech, HSE, AI4Science
       \AND
       \Name{Ivan Oseledets}
       \Email{i.oseledets@skoltech.ru}\\
       \addr AIRI, Skoltech\\
       Marchuk Institute of Numerical Mathematics}

\begin{document}

\maketitle

\begin{abstract}
	In this article, we explore the use of various matrix norms for optimizing functions of weight matrices, a crucial problem in training large language models. Moving beyond the spectral norm that underlies the Muon update, we leverage the duals to the Ky Fan $k$-norms to introduce a family of Muon-like algorithms we name \emph{Fanions}, which happen to be similar to Dion. Then working with the duals of convex combinations of the Ky Fan $k$-norms and the Frobenius norm or the $l_\infty$ norm, we construct the families of \emph{F-Fanions} and \emph{S-Fanions} respectively. Their most prominent members are \emph{F-Muon} and \emph{S-Muon}. We complement our theoretical analysis with an extensive empirical study of the algorithms across a wide range of tasks and settings, from which it follows that F-Muon and S-Muon are always on par with Muon, while on fine-tuning of NanoGPT and synthetic linear least squares they are even better than vanilla Muon optimizer.
\end{abstract}

\section{Introduction}

Minimizing loss functions in unprecedentedly high-dimensional spaces has recently become an integral and crucial part in training large language models. Hence, new scalable, time- and memory-efficient algorithms have been demanded. Besides well-known Adam \citep{kingma2014adam} and AdamW \citep{loshchilov2017decoupled}, recently proposed Muon \citep{jordan2024muon} has shown promising results on training very large models \citep{liu2025muon}. Its key difference from Adam and AdamW is that it has been constructed specifically for optimizing functions of weight matrices, which are common in deep learning.

That is what can be said from a practical point of view. From a theoretical perspective, a key innovation of Muon was its principled derivation of the update rule, which emerged as the solution to an optimization problem constrained by the RMS-to-RMS norm (scaled version of the spectral norm) \citep{bernstein2025deriving}.

Motivated by the success of Muon, many generalizations and variations of it were proposed. Among the notable ones are Scion \citep{pethick2025training}, Dion \citep{ahn2025dion} and Gluon \citep{riabinin2025gluon}. Those works try to explain Muon's efficiency and establish convergence bounds. One central question, however, remains unanswered:

\textit{In deriving Muon's update step, why should one constrain by the spectral or any other operator norm? How would alternative norms affect performance and computational cost?}

In this article, we tackle this question by actually showing that there are many viable non-operator norms. We leverage the family of norms dual to Ky Fan $k$-norms to derive a new family of \textbf{Fanions}, algorithms with low-rank updates. This approach theoretically explains the backbone of Dion's update \citep{ahn2025dion} and generalizes the memory-motivated application of the nuclear norm to Sharpness-Aware Minimization \citep{pethick2025sam}. As it was done with Muon, we come up with an effective procedure for computing Fanions' updates. Lanczos algorithm, which is described and compared with its competitors in \sectionref{sec:matrix-side}, is the most operation-effective algorithm, which, however, for now lacks an effective GPU- and PyTorch-friendly implementation.

Working with duals to conic combinations of dual norms, we construct the families of \textbf{F-Fanions} and \textbf{S-Fanions}, which are hybrids of Muon and NormalizedSGD and SignSGD, respectively.

Then we compare the performances of the algorithm families on various model and real-world problems:
\begin{itemize}
	\item Synthetic least squares experiment \sectionref{subsec:lls_exps}
	\item CIFAR-10 airbench \citep{cifar2023airbench}
	\item Pre-training NanoGPT on FineWeb dataset \citep{modded_nanogpt_2024}
	\item Fine-tuning NanoGPT on Tiny Stories \citep{eldan2023tinystoriessmalllanguagemodels}
\end{itemize}

Our experiments reveal important insights into the role of matrix norms in optimization. First, we show on the example of Neon, the one-rank Fanion, that not every LMO-based algorithm is effective, despite the same asymptotics in the general bounds of \citet{kovalev2025understanding} and \citet{riabinin2025gluon}. This suggests that existing theoretical guarantees should be reworked to explain empirical performance.

Most notably, our experiments on real-world tasks demonstrate that the choice of underlying matrix norm is remarkably flexible. On CIFAR-10 airbench, properly-tuned F-Muon and S-Muon achieve $94.02 \pm 0.13\%$ and $94.03 \pm 0.13\%$ accuracy, matching Muon's $94.01 \pm 0.13\%$ performance. On NanoGPT pre-training, F-Muon achieves 3.281 cross-entropy loss, while fully-tuned Muon achieves 3.279. Finally, S-Muon matches Muon on fine-tuning of NanoGPT on Tiny Stories, while F-Muon is far more resistant to the learning rate choice than Muon. These results show that Muon-like algorithms can maintain competitive performance even when the underlying norm constraint is significantly modified, answering affirmatively the central question posed above. Moreover, the tools from \sectionref{sec:conic_combo} give the researchers an unheard-of flexibility in designing algorithms that do not have to be modifications of Muon.

\section{Preliminaries: Linear Minimization Oracle Framework}

Training a neural network is essentially an optimization of a function of several weight matrices and a few vectors. Let us start by considering the problem of minimizing a differentiable function of a \emph{single} matrix:
\begin{equation}\label{eq:matrix_problem}
	F(\cdot)\colon \Rmn \to \R\,,\qquad F(\mX) \to \min_{\mX \in \Rmn}\,
\end{equation}
We equip the matrix space $\Rmn$ with a standard dot product $\<\cdot, \cdot> \to \R$ and a norm $\norm{\cdot}\colon \Rmn \to \R_+$, which does not have to coincide with the Frobenius norm $\normf{\cdot}$. The dual norm $\norm{\cdot}^\dagger\colon \Rmn \to \R_+$ that is associated with $\norm{\cdot}$ is defined as
\begin{equation}\label{eq:dual_norm}
	\norm{\mG}^\dagger = \sup_{\mD \in \Rmn\,:\norm{\mD}\leq 1} \<\mG,\mD>\,.
\end{equation}

Such problems can be solved with an iterative algorithm based on the Linear Minimization Oracle (LMO):
\begin{equation}\label{eq:lmo}
	\mathrm{LMO}(\mM^k) \in \argmin_{\mD \in \cS} \<\mM^k, \mD>\,,
\end{equation}
where $\mM^k$ is a gradient (or a momentum) of $F$ in $\mX^k$ and $\cS \subset \Rmn$ is some set. The update of the algorithm is defined as follows:
\begin{equation}\label{eq:simple_update}
	\mX^{k + 1} = \mX^k + \gamma_k\mathrm{LMO}\left(\mM^k\right)\,.
\end{equation}

We are particularly interested in the case when $\cS$ is a unit ball in the norm $\norm{\cdot}$:

\[
\cS = \cB_{\norm{\cdot}} = \{ \mD \in \Rmn \mid \norm{\mD} \leq 1 \}\,.
\]
In this case,
\[
\argmin_{\mD \in \cS} \<\mM^k, \mD> = - \{ \mD \in \cB_1 \mid \<\mM^k, \mD> = \norm{\mM^k}^\dagger\}\,,
\]
and update for $\mX^k$ in \equationref{eq:simple_update} simplifies to
\begin{equation}\label{eq:our_update}
	\mX^{k+1} = \mX^{k} - \gamma_k \{\mD \in \cB_1 \mid \<\mM^k, \mD> = \norm{\mM^k}^\dagger\}\,.
\end{equation}

Using this formula it is easy to compute updates for algorithms induced by various norms $\norm{\cdot}$:

\paragraph{Frobenius norm and Normalized SGD}
When the norm $\norm{\cdot}$ is the Frobenius norm $\normf{\cdot}$, \equationref{eq:our_update} turns into
\begin{equation}\label{eq:nsgd_update}
	\mX^{k+1} = \mX^{k} - \eta_k \frac{\mM^k}{\normf{\mM^k}}\,,
\end{equation}
which recovers Normalized SGD (NSGD).

\paragraph{Spectral norm and Muon}
When the norm is the spectral norm $\norms{\cdot}$, we get
\begin{equation}\label{eq:muon_update}
	\mX^{k+1} = \mX^{k} - \eta_k \mU \mV^\top\,,
\end{equation}
which is Muon without the $\sqrt{m/n}$ factor.
Here, $\mM^k = \mU \mSigma \mV^\top$ is the Singular Value Decomposition (SVD) of $\mM^k$ ($\mU = [\vu_1, \vu_2, \dots, \vu_r]$, $\mSigma = \diag(\sigma_1, \sigma_2, \dots, \sigma_r)$, and $\mV = [\vv_1, \vv_2, \dots, \vv_r]$). Muon can be recovered by taking the RMS-to-RMS operator norm $\sqrt{\frac{n}{m}}\norms{\cdot}$.

\paragraph{Chebyshev norm and SignSGD}
When the norm is the Chebyshev norm $\normc{\cdot}$, we get
\begin{equation}\label{eq:signsgd_update}
	\mX^{k+1} = \mX^{k} - \eta_k \sign(\mM^k)\,,
\end{equation}
which recovers SignSGD~\citep{bernstein2018signsgd}. Here, $\sign(\mM^k)$ denotes the element-wise sign function. SignSGD is particularly notable for its communication efficiency in distributed training, as it compresses gradients to 1-bit per parameter.

\section{Duals to Ky Fan Norms Instead of the Spectral Norm}

\subsection{\texorpdfstring{$\normn{\mM^{k}}$ and Neon}{Neon}}
After considering $\normf{\mM^k}$ and $\norms{\mM^k}$, it is natural to look at the nuclear norm $\normn{\mM^k}$.
\begin{lemma}\label{lemma:neon_update}
	When $\norm{\cdot} = \normn{\cdot}$, \equationref{eq:our_update} becomes
	\begin{equation}\label{eq:neon_update}
		\mX^{k+1} = \mX^{k} - \eta_k \vu_1 \vv_1^\top\,.
	\end{equation}
\end{lemma}
\begin{proof}
Since $\norm{\cdot}^\dagger = \norms{\cdot}$, the goal is to reach $\norms{\mM^k} = \sigma_1$. Let us note that $\Delta = \vu_1 \vv_1^\top$ delivers this value. Indeed, $\normn{\Delta}=1$ and by the trace property and orthogonality of the singular vectors,
\[
\<\mM^k, \Delta> =\<\mU \mSigma \mV^\top, \vu_1 \vv_1^\top> = \tr \diag({\sigma_1, 0, \dots, 0}) = \norms{\mM^k}\,,
\]
which completes the proof.
\end{proof}

We name the derived algorithm \emph{Neon}. In \sectionref{sec:matrix-side}, we will discuss how to compute the Neon's update.

\subsection{\texorpdfstring{$\normkfk{\mM^k}^\dagger$ and Muon, Neon, and Centralized Dion without error feedback}{Muon, Neon, and Centralized Dion without error feedback}}

Neon's and Muon's updates seem to be complete opposites: one has rank one, while the other is full-rank. It would be interesting to derive algorithms with updates of intermediate ranks.

\subsubsection{Schatten norms}
\citet{cesista2025schattenp} considered Schatten-$p$ norms: 
\[\norm{\mM^k}_{S_p} = \left(\sum_{i=1}^{\min(m, n)}\sigma_i^p\right)^{1/p}\,,
\]
which produce the update
\[
\mX^{k+1} = \mX^k - \eta_k \mU \frac{\diag\left(\sigma_1^{q-1},\dots, \sigma_{\min(m,n)}^{q-1}\right)}{\left(\sum_{i=1}^{\min(m,n)}{\sigma_i^q}\right)^{\frac{q-1}{q}}}\mV^\top
\]
for $p$ and $q$ such that: $\frac{1}{p} + \frac{1}{q} = 1$. This formula recovers Neon when $p\rightarrow1$ provided that $\sigma_1 > \sigma_2$, which is true on real data; NSGD when $p=2$, and Muon when $p\rightarrow\infty$.

However, Schatten norms do not fill the gap rank: indeed, when $p > 1$, the rank of the update is full, while when $p=1$ ($p \rightarrow 1$) it is one. Moreover, it is not clear how to calculate the update for $p \neq 1, 2, \infty$: it seems one has to know all $\sigma_i$ to compute the update, which makes the problem as hard as the full SVD.

\subsubsection{Ky Fan norms}
There is another family of matrix norms, which might help us: Ky Fan norms. For $k \in \{1, \dots, \min(m, n)\}$, the Ky Fan $k$-norm $\normkfk{\cdot}$ is $\sum_{i=1}^{k}\sigma_i$, i.e. the sum of the $k$ largest singular values of the matrix. Special cases of the Ky Fan $k$-norm are the Ky Fan $1$-norm, which is the spectral norm, and the Ky Fan $\min\{m, n\}$-norm, which is the nuclear norm.

Let us derive the update for an arbitrary $k$. Since $\normkfk{\cdot}^\dagger = \max\{\frac{1}{k} \normn{\cdot}, \norms{\cdot}\}$ (see \citet{bhatia2013matrix}, p. 96), the goal is to reach $\max\{\frac{1}{k}\sum_{i=1}^{\min(m,n)}\sigma_i, \sigma_1\}$. $\Delta = \vu_1 \vv_1^\top$ from Neon delivers $\sigma_1$, while $\Delta = \frac{1}{k}\sum_{i=1}^{\min(m,n)}\vu_i \vv_i^\top$ from Muon (with an extra factor of $1/k$) delivers $\frac{1}{k}\sum_{i=1}^{\min(m,n)}\sigma_i$. Thus, the update is either
\[
\mX^{t+1} = \mX^{t} - \eta_t \vu_1 \vv_1^\top\text{ or }\mX^{t+1} = \mX^{t} - \frac{\eta_t}{k}\mU \mV^\top\,,
\]
depending on which one better minimizes the linear function $L(\mX) \equiv F(\mX^t) + \mM^t(\mX - \mX^t)$.

The rank gap is not closed by the Ky Fan norms because the obtained update is either one-rank or full-rank.

\subsubsection{Duals to Ky Fan norms}
While Schatten norms are closed under dualization and $\norm{\mM^k}_{S_p}^\dagger = \norm{\mM^k}_{S_q}$, for Ky Fan norms it is not generally the case. $k = 1$ and $k=\min(m, n)$ are exceptional: for $k=1$ the dual norm is $\normn{\cdot}$ with $k = \min(m, n)$ and for $k = \min(m, n)$ the dual norm is $\norms{\cdot}$ with $k=1$. For each other $k$, $\normkfk{\cdot} \neq \norm{\cdot}_{\text{KF-}k'}$ for any $k'$: $k'=\min(m, n)$ and $k'= 1$ correspond to the already discussed cases, while for other $k'$ one can change $\norm{\mM^k}_{\text{KF-}k'}$ by moving a small value between $\sigma_{k'}$ and $\sigma_{k'+1}$. During this change, $\norms{\mM^k}$ and $\normn{\mM^k}$ will remain constant, hence $\normkfk{\mM^k}$ will not change as well.

\begin{lemma}\label{lemma:ky_fan_update}
	When $\norm{\cdot} = \normkfk{\mM^t}^\dagger$, \equationref{eq:our_update} turns into:

	\begin{equation}\label{eq:ky_fan_update}
		\mX^{t+1} = \mX^{t} - \eta \sum_{i=1}^{k}\vu_i \vv_i^\top
	\end{equation}
\end{lemma}

\begin{proof}
Since $\norm{\cdot}^\dagger = \normkfk{\cdot}^{\dagger\dagger} = \normkfk{\cdot}$, the goal is to reach $\normkfk{\mM^t}$.

Let us note that $\Delta = \sum_{i=1}^{k} \vu_i \vv_i^\top$ delivers the value. Indeed,
\[
\<\mM^t, \Delta> =\<\mU \mSigma \mV^\top, \sum_{i=1}^{k} \vu_i \vv_i^\top> = \sum_{i,j=1}^{r, k}\<\vu_i \sigma_i \vv_i^\top, \vu_j \vv_j^\top> = \sum_{i=1}^{k}\sigma_i = \normkfk{\mM^t}\,,
\]
which completes the proof.
\end{proof}
These updates constitute the family of \emph{Fanions}, LMO-based algorithms under the $\normkfk{\mM^t}^\dagger$ norms. The algorithm for a particular $k$ we will call \emph{Fanion-$k$}. Thus, Muon is a Fanion-$\min\{m,n\}$, while Neon is a Fanion-1. Moreover, the unsharded version of the rank-$r$ Dion (Algorithm 1 from \citet{ahn2025dion}) without the error feedback and without scaling of the update is actually a Fanion-$r$ (see \citet{pethick2025understandingdion}, where Dion is written down in a notation more similar to ours).

\section{Conic Combination of LMO-algorithms is an LMO-algorithm}
\label{sec:conic_combo}

Simply applying norm dual to the Ky Fan $k$-norm however, is not enough to provide family of algorithms diverse enough for our cause. Next we consider linear combinations of algorithms from LMO framework, which happen to be LMO-algorithms too, as we show in succeeding paragraphs.

\subsection{General Case}
First, we need a well-known fact mentioned, for example, in \citet[Table 1]{yu2012arithmetic}. For the sake of completeness, we provide a proof here.

\begin{lemma}\label{lemma:dual_to_conv_comb}
	Let $\norm{\cdot}_{(1)},\dots,\norm{\cdot}_{(n)}$ be norms on a finite-dimensional Euclidean space, and let $\alpha_1,\dots,\alpha_n$ be non-negative reals. Define
	\[
		\norm{\cdot} := \sum_{i = 1}^n \alpha_i \norm{\cdot}_{(i)}.
	\]
	Then the dual unit ball of $\norm{\cdot}$ satisfies
	\[
		\cB_{\norm{\cdot}^\dagger}
		= \sum_{i = 1}^n \alpha_i \cB_{\norm{\cdot}_{(i)}^\dagger}
	\]
	where $\sum$ denotes the Minkowski sum and $\cB_{\norm{\cdot}_{(i)}^\dagger}$ is the unit ball of the dual norm $\norm{\cdot}_{(i)}^\dagger$.
\end{lemma}

\begin{proof}
	Let us first prove the lemma for the case $n = 2$. Denote $f(x) = \alpha_1 \norm{x}_{(1)}$ and $g(x) = \alpha_2 \norm{x}_{(2)}$, such that $\norm{x} = f(x) + g(x)$. Recall two standard facts:
	\begin{enumerate}
		\item For any norm $\norm{\cdot}$ and $\lambda>0$,
		      \[
			      (\lambda \norm{\cdot})^*(y) =
			      \sup_{x}\bigl( \langle y,x \rangle - \lambda \norm{x}\bigr)
			      = \delta_{\lambda \cB_{\norm{\cdot}^\dagger}}(y)\,,
		      \]
		      i.e., the indicator function of the scaled dual ball.
		\item The Fenchel conjugate of a sum satisfies
		      \[
			      (f+g)^*(y) = \inf_{u+v=y} \bigl(f^*(u) + g^*(v)\bigr)\,.
		      \]
	\end{enumerate}
	Applying these to $f$ and $g$, we have
	\[
		f^*(u) = \delta_{\alpha_1 \cB_{\norm{\cdot}_{(1)}^\dagger}}(u)\,,
		\quad
		g^*(v) = \delta_{\alpha_2 \cB_{\norm{\cdot}_{(2)}^\dagger}}(v)\,.
	\]
	Thus,
	\[
		\norm{\cdot}^*(y)
		= (f+g)^*(y)
		= \inf_{u+v=y}
		\bigl(
		\delta_{\alpha_1 \cB_{\norm{\cdot}_{(1)}^\dagger}}(u)
		+
		\delta_{\alpha_2 \cB_{\norm{\cdot}_{(2)}^\dagger}}(v)
		\bigr)
		=
		\delta_{\alpha_1 \cB_{\norm{\cdot}_{(1)}^\dagger}+\alpha_2 \cB_{\norm{\cdot}_{(2)}^\dagger}}(y).
	\]
	By definition, the conjugate of a norm is exactly the indicator of its dual unit ball:
	\[
	\norm{\cdot}^*(y) = \delta_{\cB_{\norm{\cdot}^\dagger}}(y)\,.
	\]
	Therefore, $\cB_{\norm{\cdot}^\dagger} = \alpha_1 \cB_{\norm{\cdot}_{(1)}^\dagger} + \alpha_2 \cB_{\norm{\cdot}_{(2)}^\dagger}$.

	Now we prove the general case by induction. The base case ($n=2$) is already proven. Suppose that the assumption of the lemma holds for $n = k$. Then, for $n = k + 1$,
	\[
	\norm{x} = \sum_{i = 1}^k \alpha_i \norm{x}_{(i)} + \alpha_{k + 1}\norm{x}_{(k + 1)} = \norm{x}_{(1:k)} + \alpha_{k + 1}\norm{x}_{(k + 1)}\,.
	\]
	Applying the result for $n=2$ combined with the induction assumption, we obtain
	\[
	\cB_{\norm{\cdot}^\dagger} = \cB_{\norm{\cdot}_{(1:k)}^\dagger} + \alpha_{k + 1} \cB_{\norm{\cdot}_{(k + 1)}^\dagger} = \sum_{i = 1}^{k + 1} \alpha_i \cB_{\norm{\cdot}_{(i)}^\dagger}\,,
	\]
	which proves the lemma.
\end{proof}

\begin{lemma}\label{lemma:lin_comb_lmo}
	Let $\norm{\cdot}_{(1)}, \dots, \norm{\cdot}_{(n)}$ be norms on a finite-dimensional Euclidean space, and let $\alpha_1, \dots, \alpha_n$ be non-negative reals. Consider Linear Minimization Oracles $\mathrm{LMO}_{1}, \dots, \mathrm{LMO}_{n}$, corresponding to the unit balls of these norms. Then, $\sum_{i = 1}^n \alpha_i \mathrm{LMO}_{i}$ is the LMO corresponding to the norm $\norm{\cdot}$ dual to the $\sum_{i = 1}^n \alpha_i \norm{\cdot}_{(i)}^\dagger$.
\end{lemma}

\begin{proof}
	Using \lemmaref{lemma:dual_to_conv_comb} and the fact $\norm{\cdot}^{\dagger \dagger} = \norm{\cdot}$, we can obtain general form of the unit ball in the $\norm{\cdot}$ norm: $\cB_{\norm{\cdot}} = \sum_{i = 1}^n \alpha_i \cB_{\norm{\cdot}_{(i)}}$. Thus, the linear function minimization objective on a $\norm{\cdot}$ norm ball can be transformed as follows:
	\[
	\argmin_{\mD \in \cB_{\norm{\cdot}}}\<\mM, \mD> = \argmin_{\mD_1 \in \alpha_i \cB_{\norm{\cdot}_{(1)}},\dots,\mD_n \in \alpha_n\cB_{\norm{\cdot}_{(n)}}}\<\mM, \sum_{i = 1}^n\mD_i> = \sum_{i = 1}^n \argmin_{D_i \in \cB_{\norm{\cdot}_{(i)}}}\<\mM, \mD_i>\,,
	\]
	where the last summation denotes the Minkowski sum. This immediately implies $\sum_{i = 1}^n \alpha_i \mathrm{LMO}_{i} \in \argmin_{\mD \in \cB_{\norm{\cdot}}}\<\mM, \mD>$, which proves the claim of the lemma.
\end{proof}

Applying it to optimization algorithms, we obtain the following corollary.

\begin{corollary}\label{corollary:lin_comb_alg}
	Let there be a finite family of LMO based algorithms indexed by $i = 1,\dots,n$, where the update of the $i$-th algorithm is defined by
	\[
	\mX^{k + 1} - \mX^{k} = \gamma_k \mathrm{LMO}_{i}(\mM^{k})\,,
	\]
	and $\mathrm{LMO}_i$ corresponds to the unit ball of norm $\norm{\cdot}_i$. For arbitrary non-negative $\alpha_1,\dots,\alpha_n$, the algorithm with the update given by
	\[
	\mX^{k + 1} - \mX^{k} = \gamma_k \sum_{i = 1}^n \alpha_i \mathrm{LMO}_{\norm{\cdot}_{(i)}}(\mM^k)
	\]
	is an LMO-algorithm itself, with LMO corresponding to the unit ball of the norm $\norm{\cdot}$ dual to the norm given by $\sum_{i = 1}^n \alpha_i \norm{\cdot}_{(i)}^\dagger$.
\end{corollary}

\subsection{Frobeniusize Them: F-Muon and F-Neon}
Let us construct the concrete examples of algorithms given by linear combinations of LMO-algorithms. It follows from \corollaryref{corollary:lin_comb_alg} that those algorithms can also be viewed as LMO-algorithms.

Combining Fanions for various $k$ with another excellent LMO-algorithm, NSGD, we obtain the family of algorithms with updates defined by
\begin{equation}\label{eq:nsgd_muon_update}
		\mX^{t+1} = \mX^{t} - \gamma_k \left(\alpha \sum_{i = 1}^k \vu_i \vv_i^\top + (1-\alpha)\frac{\mM^k}{\normf{\mM^k}}\right)\,.
\end{equation}
Recall that Fanion-$k$ update is governed by the norm dual to the Ky Fan-$k$ norm, and NSGD corresponds to the Frobenius norm, which is dual to itself. Thus, by \corollaryref{corollary:lin_comb_alg}, our new algorithm is an LMO-algorithm corresponding to the unit ball of the norm $\norm{\cdot}_{\mathrm{F-KF-k}}$:
\begin{equation}\label{eq:fkfk_norm}
	\norm{\cdot}_{\mathrm{F-KF-k}}^\dagger = \alpha \normkfk{\cdot} + (1-\alpha)\normf{\cdot}\,.
\end{equation}
We name the derived family of algorithms \emph{F-Fanions}.

Two edge members of this family, with $k = 1$ and $k = \min\{m, n\}$ correspondingly, \emph{F-Neon} and \emph{F-Muon}, are of particular interest to us. A bit of information and visualizations related to the $\normfstar{\cdot} = \norm{\cdot}_{\mathrm{F-KF-1}}$ norm is presented in the appendix (see \equationref{eq:normfstardual}, \figureref{fig:fstardual}).

\subsection{Add SignSGD: S-Muon and S-Neon}

Another algorithm we consider is the SignSGD. Mixing its update with that of Fanion-$k$, we obtain the update of \emph{S-Fanion-$k$}:
\begin{equation}\label{eq:sign_muon_update}
		\mX^{t+1} = \mX^{t} - \gamma_k \left(\alpha \sum_{i = 1}^k \vu_i \vv_i^\top + (1-\alpha)\eta\sign(\mM^k)\right)\,,
\end{equation}
where $\eta$ is the special SignSGD learning rate coefficient.

The norm $\norm{\cdot}_{\mathrm{C-KF-k}}$ which produces this update in the LMO framework is again given by \corollaryref{corollary:lin_comb_alg}:
\begin{equation}\label{eq:ckfk_norm}
	\norm{\cdot}_{\mathrm{C-KF-k}}^\dagger = \alpha \normkfk{\cdot} + \frac{1-\alpha}{\eta}\normc{\cdot}^\dagger\,.
\end{equation}

\section{Computing the Updates}\label{sec:matrix-side}
We employ the thick-restarted Lanczos method for the symmetric eigenvalue problem (thick-restart Lanczos, TRLan) to compute the low-rank updates of Fanions. We apply TRLan to either $\mM^{k\top}\mM^k$ or $\mM^k \mM^{k\top}$ (whichever matrix is smaller). We use the CuPy implementation of \texttt{cupy.sparse.linalg.svds} \citep{cupy_svds_ref} which internally relies on TRLan \citep{simonz1998thick}.

TRLan is specifically designed for efficiently approximating the largest singular values and corresponding singular vectors of very large matrices. The thick-restart strategy retains the most informative Ritz vectors across restarts, which dramatically accelerates convergence while keeping memory consumption moderate. TRLan is particularly attractive in our GPU setting because its dominant cost is a modest number of highly parallelizable matrix-vector multiplications (matvecs) and it avoids full reorthogonalization against the entire Krylov basis by using short recurrence relations combined with thick restarting.

The per-cycle complexity is $\mathcal{O}(mn^2 + n^2k + nk^2)$, where $m \geq n$ are the dimensions of the target matrix and $k$ is the size of the retained subspace (typically $k \ll n$).

In \tableref{tbl:matrix_methods}, we compare TRLan against randomized SVD (RSVD) and simple power iterations when computing the rank-$k$ update used in Fanion-$k$ and related algorithms. Experiments are performed on dense random matrices with i.i.d. $\mathcal{N}(0,1)$ entries and use CPU implementations for fair comparison.
We report:
\begin{itemize}
  \item $err_1$: relative error in the Frobenius norm of $\sum_i^k \vu_i \vv_i^T$,
  \item $err_2$: relative error in the Frobenius norm of $\sum_i^k \sigma_i \vu_i \vv_i^T$.
\end{itemize}

On $500\times500$ matrices, TRLan and RSVD require comparable wall-clock time, but TRLan delivers orders-of-magnitude lower error and far fewer matvecs. On larger $5000\times5000$ matrices the advantage becomes even more pronounced: TRLan is 3-4 times faster than RSVD while using $\sim$30 times fewer matvecs at comparable or better accuracy.

An interesting empirical observation is that RSVD tends to approximate the \emph{singular values} themselves reasonably well but the reconstructed low-rank matrix noticeably deviates from the truncated SVD, whereas TRLan provides an excellent approximation to the truncated SVD matrix itself (low $err_2$) at the cost of occasionally less accurate individual singular values. This makes TRLan the preferred choice for algorithms like Neon/Fanion-$k$ that only need the low-rank term $\sum \sigma_i \vu_i \vv_i^\top$, but less ideal for methods (e.g., Dion) that explicitly require accurate $\sigma_i$ for error feedback or step-size control.

A current practical limitation is the lack of a native PyTorch implementation of thick-restart Lanczos; existing PyTorch-based randomized SVD routines cannot match TRLan's accuracy/efficiency combination for the matrix reconstruction task.

For reference, \tableref{tbl:newton_schulz_reference} shows results for the Newton-Schulz polar decomposition iteration on the same matrices, $err_1$ is the relative error of $\mU\mV^T$ (29-30 iterations to converge, significantly higher matvec count than TRLan).

\begin{table}[ht]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|c|c|}
	  \hline
	  Matrix sizes & $k$  & Method           & Time (s) & Matvecs & Iterations & $err_1$         & $err_2$       \\
	  \hline
	  $500 \times 500$      & 5  & Power Iterations           & 0.062   & 2005    & 200        & 9.2$\cdot 10^{-3}$ & 9.1$\cdot10^{-3}$\\
	  $500 \times 500$      & 5  & RSVD             & 0.017   & 1170    & 38         & 9.8$\cdot 10^{-3}$ & 9.6$\cdot10^{-3}$\\
	  $500 \times 500$      & 5  & TRLan            & 0.018   & 131     & 65         & 9.6$\cdot 10^{-5}$ & 9.4$\cdot10^{-5}$\\
	  \hdashline
	  $500 \times 500$      & 50 & Power Iterations           & 0.44    & 43750   & 437      & 9.9$\cdot 10^{-3}$ & 9.0$\cdot10^{-3}$\\
	  $500 \times 500$      & 50 & RSVD             & 0.61    & 6120    & 50      & 9.9$\cdot 10^{-3}$ & 9.1$\cdot10^{-3}$\\
	  $500 \times 500$      & 50 & TRLan            & 0.16    & 462     & 231      & 3.3$\cdot 10^{-7}$ & 3.0$\cdot10^{-7}$\\
	  \hdashline
	  $5000 \times 5000$    & 5  & Power Iterations           & 9.6     & 9065    & 906      & 8.6$\cdot 10^{-3}$ & 8.6$\cdot10^{-3}$\\
	  $5000 \times 5000$    & 5  & RSVD             & 2.1     & 5640    & 187      & 9.7$\cdot 10^{-3}$ & 9.7$\cdot10^{-3}$\\
	  $5000 \times 5000$    & 5  & TRLan            & 0.70    & 205     & 102      & 7.7$\cdot 10^{-3}$ & 7.7$\cdot10^{-3}$\\
	  \hline
	\end{tabular}
  \caption{Comparison of methods for computing rank-$k$ updates on dense random matrices (CPU, double precision). Lower is better in all columns.}
  \label{tbl:matrix_methods}
  \end{table}

  \begin{table}[ht]
  \centering
  \begin{tabular}{|c|c|c|c|c|}
  \hline
  Matrix size    & Time (s) & Matvecs & Iterations & $err_1$ \\
  \hline
  $500 \times 500$     & 0.041    & 27 000  & 27         & 4.8e-3  \\
  $5000 \times 5000$   & 26.4     & 290 000 & 29         & 6.5e-3  \\
  \hline
  \end{tabular}
  \caption{Newtonâ€“Schulz iteration on the same matrices (for reference).}
  \label{tbl:newton_schulz_reference}
  \end{table}

\bibliography{icomp2024_conference}

\end{document}
