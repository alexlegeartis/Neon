%\documentclass[wcp,gray]{jmlr} % test grayscale version
 %\documentclass[wcp]{jmlr}% former name JMLR W\&CP
\documentclass[pmlr]{jmlr}% new name PMLR (Proceedings of Machine Learning)
\input{math_commands.tex}
\RequirePackage{graphicx}
 % The following packages will be automatically loaded:
 % amsmath, amssymb, natbib, graphicx, url, algorithm2e
 \usepackage{booktabs}
 %\usepackage{rotating}% for sideways figures and tables
\usepackage{longtable}% for long tables
\usepackage{xcolor}% required by colortbl
\usepackage{colortbl}% for colored table rules (required by arydshln)
\usepackage{arydshln}% for dashed lines in tables

\makeatletter
\def\set@curr@file#1{\def\@curr@file{#1}} %temp workaround for 2019 latex release
\makeatother
\usepackage[load-configurations=version-1]{siunitx} % newer version

% Load required packages for custom math commands
\usepackage{mathtools}
\usepackage{bm}





% Norms and operators
\newcommand{\norm}[1]{\lVert #1\rVert}
\DeclarePairedDelimiter{\normf}{\|}{\|_\mathrm{F}}
\DeclarePairedDelimiter{\norms}{\|}{\|_{\mathrm{2}}}
\DeclarePairedDelimiter{\normc}{\|}{\|_{\mathrm{C}}}
\DeclarePairedDelimiter{\normn}{\|}{\|_{*}}
\newcommand{\normkfk}[1]{\|#1\|_{\text{KF-}k}}
\DeclarePairedDelimiter{\normfstar}{\|}{\|_\mathrm{F*}}
\DeclarePairedDelimiter{\normftwo}{\|}{\|_\mathrm{F2}}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}

% Inner product notation
\def\<#1,#2>{\langle #1,#2\rangle}

% Math operators
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\diag}{diag}

% Custom shortcuts
\renewcommand{\epsilon}{\varepsilon}
\newcommand{\Rmn}{\R^{m\times n}}

% change the arguments, as appropriate, in the following:
\jmlrvolume{tbd}
\jmlryear{2025}
\jmlrworkshop{International Conference on Computational Optimization}

\title[Ky Fan Norms, Duals, and Norm Combinations for Matrix Optimization]{The Ky Fan Norms and Beyond: Dual Norms and Combinations for Matrix Optimization}

\author{\Name{Alexey Kravatskiy}
       \Email{kravtskii.aiu@phystech.edu}\\
       \addr MIPT
       \AND
       \Name{Ivan Kozyrev}
       \Email{kozyrev.in@phystech.edu}\\
       \addr MIPT, INM RAS
       \AND
       \Name{Nikolai Kozlov}
       \Email{kozlov.na@phystech.edu}\\
       \addr MIPT
       \AND
       \Name{Alexander Vinogradov}
       \Email{vinogradov.am@phystech.edu}\\
       \addr MIPT
       \AND
       \Name{Daniil Merkulov}
       \Email{daniil.merkulov@phystech.edu}\\
       \addr MIPT, Skoltech, HSE, AI4Science
       \AND
       \Name{Ivan Oseledets}
       \Email{i.oseledets@skoltech.ru}\\
       \addr AIRI, Skoltech, INM RAS
}

\begin{document}

\maketitle

\begin{abstract}
	In this article, we explore the use of various matrix norms for optimizing functions of weight matrices, a crucial problem in training large language models. Moving beyond the spectral norm underlying the Muon update, we leverage duals of the Ky Fan $k$-norms to introduce a family of Muon-like algorithms we name \emph{Fanions}, which are closely related to Dion. By working with duals of convex combinations of the Ky Fan $k$-norms with either the Frobenius norm or the $l_\infty$ norm, we construct the families of \emph{F-Fanions} and \emph{S-Fanions}, respectively. Their most prominent members are \emph{F-Muon} and \emph{S-Muon}. We complement our theoretical analysis with an extensive empirical study of these algorithms across a wide range of tasks and settings, demonstrating that F-Muon and S-Muon consistently match Muon's performance, while outperforming vanilla Muon on synthetic linear least squares problems.
\end{abstract}

\section{Introduction}

Minimizing loss functions in unprecedentedly high-dimensional spaces has recently become an integral and crucial part in training large language models. Hence, new scalable, time- and memory-efficient algorithms have been demanded. Besides well-known Adam \citep{kingma2014adam} and AdamW \citep{loshchilov2017decoupled}, recently proposed Muon \citep{jordan2024muon} has shown promising results on training very large models \citep{liu2025muon}. Its key difference from Adam and AdamW is that it has been constructed specifically for optimizing functions of weight matrices, which are common in deep learning.

From a theoretical perspective, a key innovation of Muon was its principled derivation of the update rule, which emerged as the solution to an optimization problem constrained by the RMS-to-RMS norm (a scaled version of the spectral norm) \citep{bernstein2025deriving}.

Motivated by the success of Muon, many generalizations and variations of it were proposed. Among the notable ones are Scion \citep{pethick2025training}, Dion \citep{ahn2025dion} and Gluon \citep{riabinin2025gluon}. Those works try to explain Muon's efficiency and establish convergence bounds. One central question, however, remains unanswered:

\emph{In deriving Muon's update step, why should one constrain by the spectral or any other operator norm? How would alternative norms affect performance and computational cost?}

In this article, we tackle this question by showing that there are many viable non-operator norms. We leverage the family of norms dual to Ky Fan $k$-norms to derive a new family of \emph{Fanions}, algorithms with low-rank updates. This approach theoretically explains the backbone of Dion's update \citep{ahn2025dion} and generalizes the memory-motivated application of the nuclear norm to Sharpness-Aware Minimization \citep{pethick2025sam}. As was done with Muon, we develop an effective procedure for computing Fanions' updates. The Lanczos algorithm, which is described and compared with its competitors in \sectionref{sec:matrix-side}, is the most operation-efficient algorithm, though it currently lacks an effective GPU- and PyTorch-friendly implementation.

Working with dual norms and various convex combinations of norms, we construct the families of \emph{F-Fanions} and \emph{S-Fanions}, which are hybrids of Muon with NormalizedSGD and SignSGD, respectively.

We then compare the performance of these algorithm families on various model and real-world problems:
\begin{itemize}
	\item Synthetic least squares experiment \sectionref{subsec:lls_exps}
	\item CIFAR-10 airbench \citep{cifar2023airbench}
	\item Pre-training NanoGPT on FineWeb dataset \citep{modded_nanogpt_2024}
	\item Fine-tuning NanoGPT on Tiny Stories \citep{eldan2023tinystoriessmalllanguagemodels}
\end{itemize}

Our experiments reveal important insights into the role of matrix norms in optimization. First, using the example of Neon (the rank-one Fanion), we show that not every LMO-based algorithm is effective, despite sharing the same asymptotics in the general bounds of \citep{kovalev2025understanding} and \citep{riabinin2025gluon}. This suggests that existing theoretical guarantees should be refined to better explain empirical performance.

Most notably, our experiments on real-world tasks demonstrate that the choice of underlying matrix norm is remarkably flexible. On CIFAR-10 airbench, properly-tuned F-Muon and S-Muon achieve $94.02 \pm 0.13\%$ and $94.03 \pm 0.13\%$ accuracy, matching Muon's $94.01 \pm 0.13\%$ performance. On NanoGPT pre-training, F-Muon achieves 3.281 cross-entropy loss, while fully-tuned Muon achieves 3.279. Finally, S-Muon matches Muon on fine-tuning NanoGPT on Tiny Stories, while F-Muon is far more resistant to learning rate choice than Muon. These results show that Muon-like algorithms can maintain competitive performance even when the underlying norm constraint is significantly modified, providing an affirmative answer to the central question posed above. Moreover, the tools from \sectionref{sec:conic_combo} give researchers considerable flexibility in designing algorithms that need not be direct modifications of Muon.

\section{Preliminaries: Linear Minimization Oracle Framework}

Training a neural network is essentially an optimization of a function of several weight matrices and a few vectors. Let us start by considering the problem of minimizing a differentiable function of a \emph{single} matrix:
\begin{equation}\label{eq:matrix_problem}
	F(\cdot)\colon \Rmn \to \R\,,\qquad F(\mX) \to \min_{\mX \in \Rmn}\,
\end{equation}
We equip the matrix space $\Rmn$ with a standard dot product $\<\cdot, \cdot> \to \R$ and a norm $\norm{\cdot}\colon \Rmn \to \R_+$, which does not have to coincide with the Frobenius norm $\normf{\cdot} = \<\cdot,\cdot>$. The dual norm $\norm{\cdot}^\dagger\colon \Rmn \to \R_+$ that is associated with $\norm{\cdot}$ is defined as
\begin{equation}\label{eq:dual_norm}
	\norm{\mG}^\dagger = \sup_{\mD \in \Rmn\,:\norm{\mD}\leq 1} \<\mG,\mD>\,.
\end{equation}

Such problems can be solved with an iterative algorithm based on the Linear Minimization Oracle (LMO):
\begin{equation}\label{eq:lmo}
	\mathrm{LMO}(\mM^t) \in \argmin_{\mD \in \cS} \<\mM^t, \mD>\,,
\end{equation}
where $\mM^t$ is the gradient (or momentum) of $F$ at $\mX^t$ and $\cS \subset \Rmn$ is some set. The update of the algorithm is defined as follows:
\begin{equation}\label{eq:simple_update}
	\mX^{t + 1} = \mX^t + \gamma_t\mathrm{LMO}\left(\mM^t\right)\,.
\end{equation}

We are particularly interested in the case when $\cS$ is a unit ball in some norm $\norm{\cdot}$:
\[
\cS = \cB_{\norm{\cdot}} = \{ \mD \in \Rmn \mid \norm{\mD} \leq 1 \}\,.
\]
In this case,
\[
\argmin_{\mD \in \cS} \<\mM^t, \mD> = - \{ \mD \in \cB_{\norm{\cdot}} \mid \<\mM^t, \mD> = \norm{\mM^t}^\dagger\}\,,
\]
and the update for $\mX^t$ in \equationref{eq:simple_update} simplifies to
\begin{equation}\label{eq:our_update}
	\mX^{t+1} = \mX^{t} - \gamma_t \{\mD \in \cB_{\norm{\cdot}} \mid \<\mM^t, \mD> = \norm{\mM^t}^\dagger\}\,.
\end{equation}

Using this formula, it is easy to compute updates for algorithms induced by various norms $\norm{\cdot}$:

\paragraph{Frobenius norm and Normalized SGD}
When the norm $\norm{\cdot}$ is the Frobenius norm $\normf{\cdot}$, \equationref{eq:our_update} turns into
\begin{equation}\label{eq:nsgd_update}
	\mX^{t+1} = \mX^{t} - \gamma_t \frac{\mM^t}{\normf{\mM^t}}\,,
\end{equation}
which recovers Normalized SGD (NSGD).

\paragraph{Spectral norm and Muon}
When the norm is the spectral norm $\norms{\cdot}$, we get
\begin{equation}\label{eq:muon_update}
	\mX^{t+1} = \mX^{t} - \gamma_t \mU \mV^\top\,,
\end{equation}
which is Muon without the $\sqrt{m/n}$ factor.
Here, $\mM^t = \mU \mSigma \mV^\top$ is the Singular Value Decomposition (SVD) of $\mM^t$ ($\mU = [\vu_1, \vu_2, \dots, \vu_r]$, $\mSigma = \diag(\sigma_1, \sigma_2, \dots, \sigma_r)$, and $\mV = [\vv_1, \vv_2, \dots, \vv_r]$). Muon can be recovered by taking the RMS-to-RMS norm: $\sqrt{n/m}\norms{\cdot}$.

\paragraph{Chebyshev norm and SignSGD}
When the norm is the Chebyshev norm $\normc{\cdot}$, we get
\begin{equation}\label{eq:signsgd_update}
	\mX^{t+1} = \mX^{t} - \gamma_t \sign(\mM^t)\,,
\end{equation}
which recovers SignSGD~\citep{bernstein2018signsgd}. Here, $\sign(\mM^t)$ denotes the element-wise sign function. SignSGD is particularly notable for its communication efficiency in distributed training, as it compresses gradients to 1-bit per parameter.

\section{Beyond the Spectral Norm: Fanions}

\subsection{The Rank Gap Problem}

Having examined the Frobenius norm (NSGD), spectral norm (Muon), and Chebyshev norm (SignSGD), all of which produce full-rank updates, we turn to the nuclear norm $\normn{\cdot}$.

\begin{lemma}\label{lemma:neon_update}
	When $\norm{\cdot} = \normn{\cdot}$, \equationref{eq:our_update} becomes
	\begin{equation}\label{eq:neon_update}
		\mX^{t+1} = \mX^{t} - \gamma_t \vu_1 \vv_1^\top\,.
	\end{equation}
\end{lemma}
\begin{proof}
Since $\normn{\cdot}^\dagger = \norms{\cdot}$, the goal is to reach $\<\mM^t, \mD> = \sigma_1$ in \equationref{eq:our_update}. Note that $\mD = \vu_1 \vv_1^\top$ delivers this value. Indeed, $\normn{\mD}=1$ and by the trace property and orthogonality of the singular vectors,
\[
\<\mM^t, \mD> =\<\mU \mSigma \mV^\top, \vu_1 \vv_1^\top> = \tr \diag({\sigma_1, 0, \dots, 0}) = \norms{\mM^t}\,,
\]
which completes the proof.
\end{proof}

We call this algorithm \emph{Neon}. The nuclear norm thus yields rank-one updates, in stark contrast to the full-rank updates of Muon, NSGD, and SignSGD. This raises a natural question: can we derive algorithms with updates of intermediate ranks?

\paragraph{Schatten norms}
\citep{cesista2025schattenp} considered Schatten-$p$ norms:
\[\norm{\mM^t}_{S_p} = \left(\sum_{i=1}^{\min(m, n)}\sigma_i^p\right)^{1/p}\,,
\]
which produce the update
\[
\mX^{t+1} = \mX^t - \gamma_t \mU \frac{\diag\left(\sigma_1^{q-1},\dots, \sigma_{\min(m,n)}^{q-1}\right)}{\left(\sum_{i=1}^{\min(m,n)}{\sigma_i^q}\right)^{\frac{q-1}{q}}}\mV^\top
\]
where $p$ and $q$ satisfy $p^{-1} + q^{-1} = 1$. This formula recovers Neon when $p\rightarrow1$ (provided $\sigma_1 > \sigma_2$, which holds on real data), NSGD when $p=2$, and Muon when $p\rightarrow\infty$.

However, Schatten norms do not fill the rank gap: when $p > 1$, the update has full rank, while when $p=1$, it has rank one. Moreover, computing the update for $p \neq 1, 2, \infty$ appears to require knowing all $\sigma_i$, making the problem as hard as computing the full SVD.

\paragraph{Ky Fan norms}
Another family of matrix norms offers a potential solution: the Ky Fan norms. For $k \in \{1, \dots, \min(m, n)\}$, the Ky Fan $k$-norm $\normkfk{\cdot}$ equals $\sum_{i=1}^{k}\sigma_i$, the sum of the $k$ largest singular values. Notable special cases include the Ky Fan $1$-norm (the spectral norm) and the Ky Fan $\min\{m, n\}$-norm (the nuclear norm).

To derive the update formula for arbitrary $k$, we use the expression for the norm dual to the Ky Fan $k$-norm (see, for instance \citep{bhatia2013matrix}, p. 96):
\[
    \normkfk{\cdot}^\dagger = \max\left\{\frac{1}{k} \normn{\cdot}, \norms{\cdot}\right\}\,.
\]
According to \equationref{eq:our_update}, we need to find $\mD$ such that $\normkfk{\mD} = 1$ and
\[
\<\mM^t, \mD> = \max\left\{\frac{1}{k}\sum_{i=1}^{\min(m,n)}\sigma_i,\ \sigma_1\right\}\,.
\]
The Neon update $\mD = \vu_1 \vv_1^\top$ achieves $\sigma_1$, while the scaled Muon update $\mD = \frac{1}{k}\sum_{i=1}^{\min(m,n)}\vu_i \vv_i^\top$ achieves $\frac{1}{k}\sum_{i=1}^{\min(m,n)}\sigma_i$. Thus, the update is either
\[
\mX^{t+1} = \mX^{t} - \gamma_t \vu_1 \vv_1^\top\text{ or }\mX^{t+1} = \mX^{t} - \frac{\gamma_t}{k}\mU \mV^\top\,,
\]
depending on which of those two expressions achieves smaller residual on the linear function $L(\mX) := F(\mX^t) + \<\mM^t, \mX - \mX^t>$. The Ky Fan norms thus fail to close the rank gap: the resulting update is either rank-one or full-rank.

\subsection{Solution: Duals to Ky Fan Norms}

Unlike Schatten norms, which satisfy $\norm{\mM^t}_{S_p}^\dagger = \norm{\mM^t}_{S_q}$ (for $p^{-1} + q^{-1} = 1$) and are thus closed under dualization, Ky Fan norms are generally not. Only two exceptional cases exist: the dual of the Ky Fan $1$-norm $\norm{\cdot}_{\text{KF-}1}$ (the spectral norm) is the Ky Fan $\min(m,n)$-norm $\norm{\cdot}_{\text{KF-}\min(m,n)}$ (the nuclear norm), and vice versa. Thus working with duals of Ky Fan norms can open up new possibilities:

\begin{lemma}\label{lemma:ky_fan_update}
	When $\norm{\cdot} = \normkfk{\mM^t}^\dagger$, \equationref{eq:our_update} turns into:
	\begin{equation}\label{eq:ky_fan_update}
		\mX^{t+1} = \mX^{t} - \gamma_t \sum_{i=1}^{k}\vu_i \vv_i^\top\,.
	\end{equation}
\end{lemma}

\begin{proof}
Since $\norm{\cdot}^\dagger = \normkfk{\cdot}^{\dagger\dagger} = \normkfk{\cdot}$, the goal is to reach $\<\mM^t, \mD> = \normkfk{\mM^t}$ in \equationref{eq:our_update}. Note that $\mD = \sum_{i=1}^{k} \vu_i \vv_i^\top$ attains this value. Indeed,
\[
\<\mM^t, \mD> =\<\mU \mSigma \mV^\top, \sum_{i=1}^{k} \vu_i \vv_i^\top> = \sum_{i,j=1}^{r, k}\<\vu_i \sigma_i \vv_i^\top, \vu_j \vv_j^\top> = \sum_{i=1}^{k}\sigma_i = \normkfk{\mM^t}\,,
\]
which completes the proof.
\end{proof}

These updates define the \emph{Fanion} family of LMO-based algorithms, each operating under a norm $\normkfk{\mM^t}^\dagger$. We denote the algorithm for a particular $k$ as \emph{Fanion-$k$}. This family elegantly bridges the rank gap, providing updates of any intermediate rank $k$.

\paragraph{Connection to existing algorithms}
The Fanion family unifies several known algorithms:
\begin{itemize}
	\item \textbf{Neon} is Fanion-1 (rank-one updates)
	\item \textbf{Muon} is Fanion-$\min\{m,n\}$ (full-rank updates)
	\item \textbf{Dion} (unsharded): The rank-$r$ Dion (Algorithm 1 from \citep{ahn2025dion}) without error feedback and without scaling of the update is actually Fanion-$r$ (see \citep{pethick2025understandingdion}, where Dion is written in a notation more similar to ours)
\end{itemize}

In \sectionref{sec:matrix-side}, we will discuss how to efficiently compute Fanion updates using the Lanczos algorithm.

\section{Conic Combination of LMO-algorithms is an LMO-algorithm}
\label{sec:conic_combo}

Simply applying norms dual to Ky Fan $k$-norms, however, does not provide sufficient algorithmic diversity for our purposes. We now consider linear combinations of LMO-based algorithms and show that these combinations are themselves LMO-algorithms.

\subsection{General Case}
We begin with a well-known result on dual norms (see, e.g., \citep[Table 1]{yu2012arithmetic}).

\begin{lemma}\label{lemma:dual_to_conv_comb}
	Let $\norm{\cdot}_{(1)},\dots,\norm{\cdot}_{(n)}$ be norms on a finite-dimensional Euclidean space, and let $\alpha_1,\dots,\alpha_n$ be non-negative reals. Define
	\[
		\norm{\cdot} := \sum_{i = 1}^n \alpha_i \norm{\cdot}_{(i)}.
	\]
	Then the dual unit ball of $\norm{\cdot}$ satisfies
	\[
		\cB_{\norm{\cdot}^\dagger}
		= \sum_{i = 1}^n \alpha_i \cB_{\norm{\cdot}_{(i)}^\dagger}
	\]
	where $\sum$ denotes the Minkowski sum and $\cB_{\norm{\cdot}_{(i)}^\dagger}$ is the unit ball of the dual norm $\norm{\cdot}_{(i)}^\dagger$.
\end{lemma}

A proof is provided in the appendix (see \sectionref{sec:proof_dual_conv_comb}).

\begin{lemma}\label{lemma:lin_comb_lmo}
	Let $\norm{\cdot}_{(1)}, \dots, \norm{\cdot}_{(n)}$ be norms on a finite-dimensional Euclidean space, and let $\alpha_1, \dots, \alpha_n$ be non-negative reals. Consider Linear Minimization Oracles $\mathrm{LMO}_{1}, \dots, \mathrm{LMO}_{n}$, corresponding to the unit balls of these norms. Then, $\sum_{i = 1}^n \alpha_i \mathrm{LMO}_{i}$ is the LMO corresponding to the norm $\norm{\cdot}$ dual to the $\sum_{i = 1}^n \alpha_i \norm{\cdot}_{(i)}^\dagger$.
\end{lemma}

\begin{proof}
	Using \lemmaref{lemma:dual_to_conv_comb} and the biduality property $\norm{\cdot}^{\dagger \dagger} = \norm{\cdot}$, we obtain the unit ball representation: $\cB_{\norm{\cdot}} = \sum_{i = 1}^n \alpha_i \cB_{\norm{\cdot}_{(i)}}$. The linear minimization problem over this ball can thus be transformed as follows:
	\[
	\argmin_{\mD \in \cB_{\norm{\cdot}}}\<\mM, \mD> = \argmin_{\mD_1 \in \alpha_i \cB_{\norm{\cdot}_{(1)}},\dots,\mD_n \in \alpha_n\cB_{\norm{\cdot}_{(n)}}}\<\mM, \sum_{i = 1}^n\mD_i> = \sum_{i = 1}^n \argmin_{D_i \in \cB_{\norm{\cdot}_{(i)}}}\<\mM, \mD_i>\,,
	\]
	where the last summation denotes the Minkowski sum. This immediately implies $\sum_{i = 1}^n \alpha_i \mathrm{LMO}_{i} \in \argmin_{\mD \in \cB_{\norm{\cdot}}}\<\mM, \mD>$, completing the proof.
\end{proof}

Applying this result to optimization algorithms yields the following corollary.

\begin{corollary}\label{corollary:lin_comb_alg}
	Let there be a finite family of LMO based algorithms indexed by $i = 1,\dots,n$, where the update of the $i$-th algorithm is defined by
	\[
	\mX^{t + 1} - \mX^{t} = \gamma_t \mathrm{LMO}_{i}(\mM^{t})\,,
	\]
	and $\mathrm{LMO}_i$ corresponds to the unit ball of norm $\norm{\cdot}_i$. For arbitrary non-negative $\alpha_1,\dots,\alpha_n$, the algorithm with the update given by
	\[
	\mX^{t + 1} - \mX^{t} = \gamma_t \sum_{i = 1}^n \alpha_i \mathrm{LMO}_{\norm{\cdot}_{(i)}}(\mM^t)
	\]
	is an LMO-algorithm itself, with LMO corresponding to the unit ball of the norm $\norm{\cdot}$ dual to the norm given by $\sum_{i = 1}^n \alpha_i \norm{\cdot}_{(i)}^\dagger$.
\end{corollary}

\subsection{Frobeniusize Them: F-Muon and F-Neon}
We now construct concrete examples of algorithms obtained via linear combinations of LMO-algorithms. By \corollaryref{corollary:lin_comb_alg}, these combinations are themselves LMO-algorithms.

Combining Fanion-$k$ with NSGD yields a family of algorithms with updates
\begin{equation}\label{eq:nsgd_muon_update}
		\mX^{t+1} = \mX^{t} - \gamma_t \left(\alpha \sum_{i = 1}^k \vu_i \vv_i^\top + (1-\alpha)\frac{\mM^t}{\normf{\mM^t}}\right)\,.
\end{equation}
Recall that Fanion-$k$ operates under the dual to the Ky Fan $k$-norm, while NSGD operates under the self-dual Frobenius norm. By \corollaryref{corollary:lin_comb_alg}, this combination defines an LMO-algorithm with norm $\norm{\cdot}_{\mathrm{F-KF-k}}^\dagger$, where
\begin{equation}\label{eq:fkfk_norm}
	\norm{\cdot}_{\mathrm{F-KF-k}} = \alpha \normkfk{\cdot} + (1-\alpha)\normf{\cdot}\,.
\end{equation}
We call this family \emph{F-Fanions}.

The extremal members of this family are \emph{F-Neon} (with $k = 1$) and \emph{F-Muon} (with $k = \min\{m, n\}$). Additional information and visualizations for the $\normfstar{\cdot} = \norm{\cdot}_{\mathrm{F-KF-1}}$ norm appear in the appendix (see \equationref{eq:normfstardual}, \figureref{fig:fstardual}).

\subsection{Add SignSGD: S-Muon and S-Neon}

Similarly, combining Fanion-$k$ with SignSGD yields \emph{S-Fanion-$k$} with update
\begin{equation}\label{eq:sign_muon_update}
		\mX^{t+1} = \mX^{t} - \gamma_t \left(\alpha \sum_{i = 1}^k \vu_i \vv_i^\top + (1-\alpha)\eta\sign(\mM^t)\right)\,,
\end{equation}
where $\eta$ is a scaling coefficient specific to SignSGD.

By \corollaryref{corollary:lin_comb_alg}, this defines an LMO-algorithm with norm $\norm{\cdot}_{\mathrm{C-KF-k}}^\dagger$, where
\begin{equation}\label{eq:ckfk_norm}
	\norm{\cdot}_{\mathrm{C-KF-k}} = \alpha \normkfk{\cdot} + \frac{1-\alpha}{\eta}\normc{\cdot}^\dagger\,.
\end{equation}

The extremal members of this family are \emph{S-Neon} (with $k = 1$) and \emph{S-Muon} (with $k = \min\{m, n\}$).

\section{Computing the Updates}\label{sec:matrix-side}
We employ the thick-restarted Lanczos method for the symmetric eigenvalue problem (thick-restart Lanczos, TRLan) to compute the low-rank updates of Fanions. We apply TRLan to either $\mM^{t\top}\mM^t$ or $\mM^t \mM^{t\top}$ (whichever matrix is smaller). We use the CuPy implementation of \texttt{cupy.sparse.linalg.svds} \citep{cupy_svds_ref} which internally relies on TRLan \citep{simonz1998thick}.

TRLan is specifically designed for efficiently approximating the largest singular values and corresponding singular vectors of very large matrices. The thick-restart strategy retains the most informative Ritz vectors across restarts, which dramatically accelerates convergence while keeping memory consumption moderate. TRLan is particularly attractive in our GPU setting because its dominant cost is a modest number of highly parallelizable matrix-vector multiplications (matvecs) and it avoids full reorthogonalization against the entire Krylov basis by using short recurrence relations combined with thick restarting.

The per-cycle complexity is $\mathcal{O}(mn^2 + n^2k + nk^2)$, where $m \geq n$ are the dimensions of the target matrix and $k$ is the size of the retained subspace (typically $k \ll n$).

In \tableref{tbl:matrix_methods}, we compare TRLan against randomized SVD (RSVD) and simple power iterations when computing the rank-$k$ update used in Fanion-$k$ and related algorithms. Experiments are performed on dense random matrices with i.i.d. $\mathcal{N}(0,1)$ entries and use CPU implementations for fair comparison.
We report:
\begin{itemize}
  \item $err_1$: relative error in the Frobenius norm of $\sum_i^k \vu_i \vv_i^T$,
  \item $err_2$: relative error in the Frobenius norm of $\sum_i^k \sigma_i \vu_i \vv_i^T$.
\end{itemize}

On $500\times500$ matrices, TRLan and RSVD require comparable wall-clock time, but TRLan delivers orders-of-magnitude lower error and far fewer matvecs. On larger $5000\times5000$ matrices the advantage becomes even more pronounced: TRLan is 3-4 times faster than RSVD while using $\sim$30 times fewer matvecs at comparable or better accuracy.

An interesting empirical observation is that RSVD tends to approximate the \emph{singular values} themselves reasonably well but the reconstructed low-rank matrix noticeably deviates from the truncated SVD, whereas TRLan provides an excellent approximation to the truncated SVD matrix itself (low $err_2$) at the cost of occasionally less accurate individual singular values. This makes TRLan the preferred choice for algorithms like Neon/Fanion-$k$ that only need the low-rank term $\sum \sigma_i \vu_i \vv_i^\top$, but less ideal for methods (e.g., Dion) that explicitly require accurate $\sigma_i$ for error feedback or step-size control.

A current practical limitation is the lack of a native PyTorch implementation of thick-restart Lanczos; existing PyTorch-based randomized SVD routines cannot match TRLan's accuracy/efficiency combination for the matrix reconstruction task.

For reference, \tableref{tbl:newton_schulz_reference} shows results for the Newton-Schulz polar decomposition iteration on the same matrices, $err_1$ is the relative error of $\mU\mV^T$ (29-30 iterations to converge, significantly higher matvec count than TRLan).

\begin{table}[ht]
\floatconts
  {tbl:matrix_methods}
  {\caption{Comparison of methods for computing rank-$k$ updates on dense random matrices (CPU, double precision). Lower is better in all columns.}}
  {%
    \begin{tabular}{|c|c|c|c|c|c|c|c|}
      \arrayrulecolor{black}\hline
      Matrix sizes & $k$  & Method           & Time (s) & Matvecs & Iterations & $err_1$         & $err_2$       \\
      \arrayrulecolor{black}\hline
      $500 \times 500$      & 5  & Power Iterations           & 0.062   & 2005    & 200        & 9.2$\cdot 10^{-3}$ & 9.1$\cdot10^{-3}$\\
      $500 \times 500$      & 5  & RSVD             & 0.017   & 1170    & 38         & 9.8$\cdot 10^{-3}$ & 9.6$\cdot10^{-3}$\\
      $500 \times 500$      & 5  & TRLan            & 0.018   & 131     & 65         & 9.6$\cdot 10^{-5}$ & 9.4$\cdot10^{-5}$\\
      \hdashline
      $500 \times 500$      & 50 & Power Iterations           & 0.44    & 43750   & 437      & 9.9$\cdot 10^{-3}$ & 9.0$\cdot10^{-3}$\\
      $500 \times 500$      & 50 & RSVD             & 0.61    & 6120    & 50      & 9.9$\cdot 10^{-3}$ & 9.1$\cdot10^{-3}$\\
      $500 \times 500$      & 50 & TRLan            & 0.16    & 462     & 231      & 3.3$\cdot 10^{-7}$ & 3.0$\cdot10^{-7}$\\
      \hdashline 
      $5000 \times 5000$    & 5  & Power Iterations           & 9.6     & 9065    & 906      & 8.6$\cdot 10^{-3}$ & 8.6$\cdot10^{-3}$\\
      $5000 \times 5000$    & 5  & RSVD             & 2.1     & 5640    & 187      & 9.7$\cdot 10^{-3}$ & 9.7$\cdot10^{-3}$\\
      $5000 \times 5000$    & 5  & TRLan            & 0.70    & 205     & 102      & 7.7$\cdot 10^{-3}$ & 7.7$\cdot10^{-3}$\\
      \arrayrulecolor{black}\hline
    \end{tabular}%
  }
\end{table}

\begin{table}[ht]
\floatconts
  {tbl:newton_schulz_reference}
  {\caption{Newton-Schulz iterations on the same matrices (for reference).}}
  {%
    \begin{tabular}{|c|c|c|c|c|}
      \arrayrulecolor{black}\hline
      Matrix size    & Time (s) & Matvecs & Iterations & $err_1$ \\
      \arrayrulecolor{black}\hline
      $500 \times 500$     & 0.041    & 27 000  & 27         & 4.8e-3  \\
      $5000 \times 5000$   & 26.4     & 290 000 & 29         & 6.5e-3  \\
      \arrayrulecolor{black}\hline
    \end{tabular}%
  }
\end{table}

\section{Experiments}\label{sec:experiments}

\subsection{Randomized Linear Least Squares}
\label{subsec:lls_exps}

First, we compare F-Fanions on the following convex $L$-smooth problem:
\begin{equation}\label{eq:lls}
  F(\mX) = \frac{1}{2} \<(\mX-\mS), \mM (\mX - \mS) \mN> \to \min_{\mX \in \Rmn}\,
\end{equation}
where $\mX \in \Rmn$, $m=500$ and $n=500$ are typical dimensions of a neural network weight matrix, $\mS \in \Rmn$, $\mM \in \mathbb{S}^m_{+}$ and $\mN \in \mathbb{S}^n_{+}$ are positive-semidefinite matrices. The spectra of $\mM$ and $\mN$ are uniformly distributed in the $(0, 1)$ interval. We set $\mS=0$, and $\mX^0 \sim 0.1 \cN(0, \mI)$.

We run different Fanions and their respective F-Fanions with $\alpha=1/2$: Neon (Fanion-1), Fanion-2, Fanion-10, Fanion-100, and Muon (Fanion-500). Also, we test S-Fanions with $\alpha=1/2$ and \texttt{sign\_lr\_coeff=0.01}. We test them against Normalized SGD and SignSGD, which are also F-Fanion and S-Fanion respectively with $\alpha = 0$ and an arbitrary $k$.

Since theoretical bounds \citep{kovalev2025understanding, riabinin2025gluon} rely on a very loose norm bound $\norm{\cdot}\le \rho \normf{\cdot}$, we do not derive learning rate or Nesterov momentum from the smoothness constants that also depend on the norm choice. Rather, we find such \texttt{(lr, momentum)} pair that the loss threshold $0.001$ is reached in a minimal number of iterations. This setting is the most realistic, and at the same time aligns with the corollaries of the convergence theorems where constant learning rate and momentum coefficients are proposed.

We perform a grid search for \texttt{momentum} $\in \{0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95\}$ and for \texttt{lr}. For Muon's, F-Muon's, S-Muon's \texttt{lr} $\in [0.005, 0.0020]$. For SignSGD, \texttt{lr} $\in 0.01 \times [0.005, 0.020]$, and for NSGD \texttt{lr} $\in [0.01, 0.10]$. The tuned parameters and the number of iterations to converge to $0.001$ are in \tableref{tbl:lls_lrs}. \texttt{lr} and \texttt{momentum}, and \texttt{sign\_lr\_coeff} for Fanions, F-Fanions, and S-Fanions are the same as for Muon, F-Muon, and S-Muon respectively due to the impossibility to tune them properly: their losses decrease too slow to reach 0.001.

The results are presented in \figureref{fig:lls} and detailed in \sectionref{sec:lls_plots_section}.

Both F-Muon and S-Muon converge faster and to a lower loss and a lower Frobenius norm of the gradient than NSGD, Muon, or SignSGD.

\begin{table}[htbp]
\floatconts
  {tbl:lls_lrs}
  {\caption{Tuning learning rates and momentum coefficients}}
  {%
    \begin{tabular}{|c|c|c|c|}
      \arrayrulecolor{black}\hline
      Algorithm & \texttt{lr}  & \texttt{momentum}  & Iterations to 0.001 loss \\
      \arrayrulecolor{black}\hline
      Muon & 0.007 & 0.5 & 1060 \\
      \hdashline
      NSGD & 0.08 & 0.95 & 1020 \\
      F-Muon & 0.015 & 0.7 & 910 \\
      \hdashline
      SignSGD & 0.016 $\times$ 0.01 & 0.95 & 2650 \\
      S-Muon & 0.011 & 0.9 & 890 \\
      \arrayrulecolor{black}\hline
    \end{tabular}%
  }
\end{table}

\begin{figure}[htbp]
\floatconts
  {fig:lls}
  {\caption{Linear least squares problem for a 500x500 matrix}}
  {%
    \subfigure[The loss]{\label{fig:lls_loss}%
      \includegraphics[width=0.8\linewidth]{figs/simple_lls/loss_long_run_16_increase.pdf}}%
    \vspace{1em}
    \subfigure[The Frobenius norm of the gradient]{\label{fig:lls_fro_grad_norm}%
      \includegraphics[width=0.8\linewidth]{figs/simple_lls/gradient_frobenius_norm_vs_iteration_long_run.pdf}}
  }
\end{figure}

\subsection{CIFAR-10 airbench}

We compare the algorithms on the CIFAR-10 airbench \citep{cifar2023airbench}. First, to test the impact of $\alpha$, we run F-Muon for different $\alpha$ with the same \texttt{lr=0.24(1 - step/total\_steps), momentum=0.65, nesterov=True} and weight normalization, as have been finetuned by Keller Jordan, 10 repetitions for each $\alpha$. We record the accuracy after 8 epochs of training (\figureref{fig:muon_alphas}).

Then we tune F-Muon with $\alpha = 0.5$. Tuned parameters are \texttt{lr=0.4(1 - step/total\_steps), momentum=0.6, nesterov = True}. Tuned F-Muon reaches $94.02 \pm 0.13\%$ after 8 epochs (averaged by 200 iterations), matching Muon's accuracy and variance. Once again we measure \texttt{val\_accuracy}($\alpha$) curve (\figureref{fig:fmuon_alphas}). We notice that even when $\alpha=0.1$, the accuracy is much higher than in the case of vanilla NSGD.

We tune S-Muon with $\alpha=0.5$ for the best \texttt{(lr, momentum, sign\_lr\_coeff)} and by setting \texttt{lr=0.42(1 - step/total\_steps), momentum=0.63, nesterov = True, sign\_lr\_coeff = 0.003} get $\mathbf{94.03} \pm 0.13\%$, which is even higher than for vanilla Muon.

\begin{figure}[htbp]
\floatconts
  {fig:diff_alphas}
  {\caption{Mean validation accuracies for F-Muon with different $\alpha$}}
  {%
    \subfigure[With parameters tuned for Muon]{\label{fig:muon_alphas}%
      \includegraphics[width=0.48\linewidth]{figs/muon_tuned_diff_alpha.pdf}}%
    \hfill
    \subfigure[With parameters tuned for F-Muon]{\label{fig:fmuon_alphas}%
      \includegraphics[width=0.48\linewidth]{figs/fmuon_tuned_diff_alpha.pdf}}
  }
\end{figure}

The Muon-level performance is surprising for the contorted geometry of F-Muon and S-Muon. F-Muon can be visualized by \figureref{fig:cifar_ball}, where the LMO balls of Muon and F-Muon for real \texttt{lr} are plotted in a 2D space of singular values. S-Muon cannot be visualized because the $l_\infty$ norm is not a function of singular values, while $\norms{\cdot}$, when plotted for the $\Rmn$ space with $m, n > 2$, requires at least a 4D space. However, it is plain that the LMO ball of S-Muon significantly differs from the ball of Muon because the $\texttt{lr}(1-\alpha)\texttt{sign\_lr\_coeff} = 6.3 \times 10^{-4}$, which is comparable to the usual \texttt{lr} of SignSGD for deep learning problems.

The most pathological case is $\alpha > 1$, which corresponds to the LMO with an area that is not a norm ball! Despite this violation, the mixture of algorithms reaches almost the same accuracy as vanilla Muon.

These observations raise the question of how much LMO-based algorithms are sensitive to the constraint area.

\begin{figure}[htbp]
\floatconts
  {fig:cifar_ball}
  {\caption{Visualization of the LMO balls for Muon and F-Muon.}}
  {\includegraphics[width=0.6\linewidth]{figs/norms/fstardual_cifar.pdf}}
\end{figure}

\subsection{NanoGPT speedrun}

We test F-Muon on the NanoGPT speedrun \citep{modded_nanogpt_2024}. For $\alpha = 0.5$, the optimal parameters are \texttt{lr=0.07, momentum=0.95}, while for Muon they were \texttt{lr=0.05, momentum=0.95}. After testing for 1750 steps we get 3.281 cross-entropy loss. For Muon, it falls below the target threshold 3.28 and reaches 3.279. However, this difference is negligible if one looks at \figureref{fig:gpt_loss}. It is even more striking, considering the fact that this F-Muon is an average of Muon and NSGD, and the latter performed quite poorly.

\begin{figure}[htbp]
\floatconts
  {fig:gpt_loss}
  {\caption{The validation loss for NanoGPT}}
  {\includegraphics[width=0.7\linewidth]{figs/nanogpt/val_loss_vs_step.pdf}}
\end{figure}

Again, as on the CIFAR airbench, if we set $\alpha=-0.1$ that corresponds to no ball, we get a 3.2818 loss, which is a small difference from the baseline.

\subsection{NanoGPT Fine-Tuning}
\label{subsec:finetune}

We fine-tuned the NanoGPT framework by Karpathy \citep{Karpathy2022} using the standard \textit{NanoGPT-medium} configuration, which corresponds to GPT-2 Medium. This model consists of 24 transformer layers, a hidden size of 1024, 16 attention heads, and approximately 345 million parameters. The training dataset was the \textit{Tiny Stories} corpus \citep{eldan2023tinystoriessmalllanguagemodels}, selected for its high entropy and structural diversity, which amplify and make more visible the differences in optimization dynamics. All training runs were initialized with the same random seed, weight initialization, and learning rate schedule to ensure that any differences in performance arose solely from the choice of optimizer.

For all one-dimensional layers, AdamW was used with a learning rate of $1\!\times\!10^{-3}$. Since momentum exhibited negligible influence on training dynamics, it was held constant across all experiments. A comparative analysis of optimization performance is provided in \figureref{fig:nanogptftexps}.

\begin{figure}[htbp]
\floatconts
  {fig:nanogptftexps}
  {\caption{Comparison of Muon, F-Muon, and S-Muon across a range of learning rates. Stars denote the best learning rate identified for each optimizer. Validation loss.}}
  {\includegraphics[width=\linewidth]{figs/nanogpt/optimizers_comparison.pdf}}
\end{figure}

\figureref{fig:nanogptftexps2} presents the train and validation loss curves at the optimal learning rate for each optimizer. While F-Muon and S-Muon maintain consistent behavior, vanilla Muon attains the lowest loss overall.

\begin{figure}[htbp]
\floatconts
  {fig:nanogptftexps2}
  {\caption{Train and validation loss at the optimal learning rate for each optimizer. The standard Muon method achieves the lowest loss, while F-Muon and S-Muon remain stable but exhibit slightly inferior performance.}}
  {\includegraphics[width=\linewidth]{figs/nanogpt/Nanogpt finetune results.pdf}}
\end{figure}

\section{Algorithms for Vectors $\rightarrow$ Algorithms for Matrices}

One cannot but notice the similarity between the updates of LMO optimizers in the vector $l_p$ and the matrix Schatten $S_p$ norms, which is illustrated by \tableref{tbl:mat_vs_vec_lmo}. The parallels are not limited to mere similarity of the updates and can be traced even in the empirical observations. SignSGD is very close to Adam, as noted in \citep{bernstein2024oldoptimizernewnorm}, and both Adam and Muon perform well in training large models \citep{zhao2025deconstructing, liu2025muon}. NSGD is the same for both matrix and vector cases. Greedy Coordinate Descent is not applied to high-dimensional problems, so from this perspective it is not surprising that one-rank Neon underperforms on such problems.

\begin{table}[t]
\floatconts
  {tbl:mat_vs_vec_lmo}
  {\caption{LMO optimizers in Schatten $S_p$ norms and in $l_p$ norms. $g$ is the gradient. When it is a matrix, $g = \mU \mSigma \mV^\top$}}
  {%
    \begingroup
    \def\arraystretch{1.2}
    \resizebox{\linewidth}{!}{%
      \begin{tabular}{|l|c|c|c|}
        \arrayrulecolor{black}\hline
        Algorithm                             & LMO constraint set $\mathcal{D}$ & LMO                                                         & Reference                            \\
        \arrayrulecolor{black}\hline
        \arrayrulecolor{black}\hline
        Normalized SGD                     & $l_2$-ball, $S_2$-ball          & $-\eta \tfrac{g}{\norm{g}_2} = -\eta \tfrac{g}{\normf{g}}$ & \citep{hazan2015beyond}              \\
        Momentum Normalized SGD            & Ball in $l_2$, or Ball in $S_2$ & $-\eta \tfrac{g}{\norm{g}_2} = -\eta\tfrac{g}{\normf{g}}$  & \citep{cutkosky2020momentum}         \\
        \arrayrulecolor{black}\hline
        SignSGD                            & Ball in Max-norm $l_\infty$     & $-\eta \sign(g)$                                            & \citep[Thm. 1]{bernstein2018signsgd} \\
        Signum                             & Ball in Max-norm $l_\infty$     & $-\eta \sign(g)$                                            & \citep[Thm. 3]{bernstein2018signsgd} \\
        \hdashline
        Muon                               & Ball in Spectral $S_\infty$     & $-\eta \mU\mV^\top$                                             & \citep{jordan2024muon}               \\
        \arrayrulecolor{black}\hline
        Gauss-Southwell Coordinate Descent & Ball in $l_1$                   & $-\eta \sum_{i \in \argmax|g_i^t|} \sign(g_i^t)e_i$                       & \citep[p. 19]{shi2016primer}         \\
        \hdashline
        Neon                               & Ball in Nuclear $S_1$           & $-\eta \vu_1 \vv_1^\top$                                        & This work                            \\
        \arrayrulecolor{black}\hline
      \end{tabular}%
    }%
    \endgroup
  }
\end{table}

\section{Related Work and Discussion}

Since Muon \citep{jordan2024muon} is a very efficient optimizer for functions of weight matrices, a lot of research has been put into, first, further improving its performance, and second, explaining its success.

\paragraph{Improvements of Muon} Regarding the first point, a large number of applications and improvements of Muon has been proposed in less than a year. \citep{liu2025muon} adapted the algorithm for training language models larger than NanoGPT. \citep{shah2025practical} organized efficient hyperparameter transfer by combining Muon with maximal update parametrization. To construct their COSMOS optimizer, \citep{chen2025cosmoshybridadaptive} applied computationally intensive updates of SOAP optimizer \citep{vyas2025soap} to a low-dimensional ``leading eigensubspace'' while using memory-efficient methods like Muon for the remaining parameters. \citep{amsel2025polar, grishina2025chebyshev} proposed more efficient alternatives to Newton-Schulz algorithm. \citep{si2025adamuon} introduced AdaMuon which combines element-wise adaptivity with orthogonal updates. We suppose that the described above or similar techniques can be applied to our optimizers as well. For example, F-Muon and S-Muon also benefit from faster alternatives to Newton-Schulz iterations, and Fanions may be a great substitute to Muon in COSMOS, because, as we have shown in \sectionref{sec:matrix-side}, Lanczos algorithm is much faster than Newton-Schulz iterations on very large matrices.

\paragraph{Theory behind Muon} Regarding the second point, there has been a prolonged gap in theory behind Muon, simplistic derivation of \citep{bernstein2025deriving} based on \citep{bernstein2024oldoptimizernewnorm} excluded. This gap, as it seems to us, is not even now completely closed. For example, \citep{kovalev2025understanding} has provided convergence guarantees of Muon and in various settings, from which, however, Muon's supremacy cannot be recovered. Indeed, although the obtained bounds depend on the norm choice, the asymptotics of the convergence remain the same as for NSGD and other optimizers, $K = \cO(\epsilon^{-4})$ in an $L$-smooth stochastic case.

A similar drawback has the article \citep{riabinin2025gluon}, where $L$-smoothness assumption is replaced with a more practical $(L_0, L_1)$-smoothness. By estimating smoothness and substituting it into their Theorem 1, the authors recovered the optimal fine-tuned stepsizes reported by \citep{pethick2025training}. However, they have not showed the optimality of the spectral or the RMS-to-RMS norm, which is observed in practice, as our comparison with NSGD highlights.

Common important drawback of the analyses is the consideration of the convergence by the norm of the gradient. As we showed in our experiments on CIFAR, the gradient norm may decrease only by a factor of ten, when the accuracy reaches 100\%.

We suppose that the reason for the recorded stark discrepancy between Neon and Muon performance, both of which are described by Stochastic Conditional Gradient \citep{pethick2025training} or Gluon frameworks, lies in the structure of the norm ball or in the preconditioner interpretation \citep{pethick2025trainingneuralnetworksscale}, which must be an object of further research.

\paragraph{The LMO and Error Feedback} We already mentioned that rank-$k$ unsharded Dion without error feedback and scaling of the update is Fanion-$k$. Since error feedback for Dion is very important as discovered by the ablation study in \citep{ahn2025dion}, F-Fanions and S-Fanions will benefit from it as well. In the context of federated learning, error feedback is effective even for compressed Muon \citep{gruntkowska2025efmuon}. The advantage of Fanions and S-Fanions is that less bits are required for transmission: $\sum_{i=1}^k \vu_i \vv_i^\top$ is easily transmitted as $\{\vu_1, \vv_1, \dots, \vu_k, \vv_k\}$ ($(m+n)\times k$ floats), while the sign part is as usual coded in $m \times n$ bits. Thus, the compression is ``built-in''. Moreover, there is an intriguing possibility to construct differentially private Fanions and S-Fanions with the specific more optimal non-Gaussian noise, as was done with DP-signSGD \citep{jang2024dplogsign}. This we leave for future research.

\paragraph{The nuclear norm in the LMO} As we found out only when transforming our results into an article, the nuclear norm has already been explored in the context of the linear minimization oracle. \citep{pethick2025sam} applied it to create $\nu$SAM, a new sharpness-aware minimization technique. It would be interesting to substitute $\normn{\cdot}$ with $\normkfk{\cdot}^\dagger$ in their approach. Since the SAM-neighborhood becomes more diverse, $k > 1$ might add to the accuracy boost, while preserving a small memory footprint and a small time overhead, if Dion-style power iterations are used.

\paragraph{Orthogonal research} Fanions, F-Fanions, and S-Fanions benefit from the general theoretical description of \citep{riabinin2025gluon}, and better learning rates can be predicted by calculating the trajectory smoothness. They could be transformed to Drop-Fanions by updating only some layers as in \citep{gruntkowska2025dropmuon}. They could be viewed as approximations of the Non-Euclidean Broximal Point Method for the corresponding norms \citep{gruntkowska2025noneuclideanbroximal}. One can clip them to produce ClippedScion-like algorithms \citep{pethick2025generalizedgradientnormclipping}. They could be made more memory-effective by the zero-order techniques that worked for Muon \citep{petrov2025zeromuon}. Finally, the results from \citep{shulgin2025inexactmuon} can be used to explain the robustness of Muon to the norm change that occurred in our experiments and to theoretically derive faster yet working approximate schemes to calculate the LMO; power iterations with a very limited number of iterations is a good algorithm to consider for the analysis.

\section{Conclusion}

In this article, we have generalized several successful algorithms, like Muon, Dion and $\nu$SAM, to the LMO-based algorithms in the $\normkfk{\cdot}^\dagger$ norm. Also we have proposed the technique of ``regularizing'' the updates with NSGD or SignSGD, a trick to increase the robustness of the algorithms, which is motivated by the consideration of the $\norm{\cdot}_{\mathrm{F-KF-k}}$ and $\norm{\cdot}_{\mathrm{C^\dagger-KF-k}}$ norms. We suggest that future works that deal with the non-Euclidean LMO explain in the corollaries to their convergence theorems the empirical superiority of Muon to other Fanions, and ideally accont for S-Fanion and F-Muon robustness as well. Without that, it is hard to believe that the bounds are relevant for practitioners, as we see by the notorious example of one-rank Neon.

\section{Author Contributions}

IO suggested using the nuclear norm in the \citep{bernstein2024oldoptimizernewnorm} framework. DM presented the problem at the MIPT optimization course. IK suggested using composite norms (though not the ones that induce F-Fanions and S-Fanions) and refactored \sectionref{sec:conic_combo}. NK suggested Lanczos algorithm as the most precise means to compute Fanions' updates, conducted experiments to prove this, and wrote \sectionref{sec:matrix-side}. AV conducted the finetuning of NanoGPT on Tiny Stories and wrote \sectionref{subsec:finetune}. All other work was done by AK: constructing Fanions, F-Fanions, S-Fanions, experimenting on the Linear Least Squares, CIFAR-airbench and NanoGPT pretrain, and writing the manuscript.

\bibliography{icomp2024_conference}

\appendix
\section{LMO for Neural Networks}

In a typical neural network, the objective function $F$ depends on a set of weight matrices $\{ \mW_1, \mW_2, \ldots \}$. The optimization framework we have described is applied in a layer-wise fashion. At each iteration $t$, a stochastic gradient $g(\mX^t, \xi^t)$ is computed using a mini-batch of data with the $\xi^t$ noise via backpropagation. This yields a separate gradient component, $\mG_i^t$, for each matrix $\mX_i$. The LMO-based update rule is then applied to each matrix $\mX_i$ using its corresponding gradient component $\mG_i^t$.

The update rule used in Muon optimizer is uSCG \citep{pethick2025training}. In the most general case, which involves momentum, it can be written as
\begin{equation}\label{eq:usgd_update}
  \begin{aligned}
    \mM^{t}     & = \alpha_{t} g(\mX^t, \xi^t) + (1 - \alpha_t)\mM^{t - 1}\,, \\
    \mX^{t + 1} & = \mX^t + \gamma_t\mathrm{LMO}(\mM^{t})\,.
  \end{aligned}
\end{equation}

\section{Norms $\normfstar{\cdot}^\dagger$ and $\normftwo{\cdot}^\dagger$}

Based on the lemma, we immediately find $\normfstar{\cdot}^\dagger$, which is related to F-Muon update. Indeed, after setting $\beta=1-\alpha$ and remembering that for smooth and bounded cases we can use $\min$ instead of $\inf$, we get
\begin{equation}\label{eq:normfstardual}
  \normfstar{\mY}^\dagger = \min_{\mZ} \min_t\{t, s.t. \norms{\mZ}\leq \alpha t, \normf{\mY-\mZ}\leq (1-\alpha) t\}
\end{equation}

If $\alpha = 1$, then $\mZ = \mY$, and we get $\normfstar{\mY}^\dagger = \norms{\mY}$. If $\alpha = 0$, then $\mZ = 0$, and we get $\normfstar{\mY}^\dagger = \normf{\mY}$.

Similarly, we find $\normftwo{\cdot}^\dagger$, which is related to F-Neon update:
\begin{equation}\label{eq:normftwodual}
  \normftwo{\mY}^\dagger = \min_{\mZ} \min_t\{t, s.t. \normn{\mZ}\leq \alpha t, \normf{\mY-\mZ}\leq (1-\alpha) t\}
\end{equation}

If $\alpha = 1$, then $\mZ = \mY$, and we get $\normftwo{\mY}^\dagger = \normn{\mY}$. If $\alpha = 0$, then $\mZ = 0$, and we get $\normftwo{\mY}^\dagger = \normf{\mY}$.

Unfortunately, $\normfstar{\cdot}^\dagger$ and $\normftwo{\cdot}^\dagger$ do not have a closed-form expression.

\section{Proof of Lemma on Dual of Convex Combinations}\label{sec:proof_dual_conv_comb}

We provide here the proof of \lemmaref{lemma:dual_to_conv_comb}.

\begin{proof}
	Let us first prove the lemma for the case $n = 2$. Denote $f(x) = \alpha_1 \norm{x}_{(1)}$ and $g(x) = \alpha_2 \norm{x}_{(2)}$, such that $\norm{x} = f(x) + g(x)$. Recall two standard facts:
	\begin{enumerate}
		\item For any norm $\norm{\cdot}$ and $\lambda>0$,
		      \[
			      (\lambda \norm{\cdot})^*(y) =
			      \sup_{x}\bigl( \langle y,x \rangle - \lambda \norm{x}\bigr)
			      = \delta_{\lambda \cB_{\norm{\cdot}^\dagger}}(y)\,,
		      \]
		      i.e., the indicator function of the scaled dual ball.
		\item The Fenchel conjugate of a sum satisfies
		      \[
			      (f+g)^*(y) = \inf_{u+v=y} \bigl(f^*(u) + g^*(v)\bigr)\,.
		      \]
	\end{enumerate}
	Applying these to $f$ and $g$, we have
	\[
		f^*(u) = \delta_{\alpha_1 \cB_{\norm{\cdot}_{(1)}^\dagger}}(u)\,,
		\quad
		g^*(v) = \delta_{\alpha_2 \cB_{\norm{\cdot}_{(2)}^\dagger}}(v)\,.
	\]
	Thus,
	\[
		\norm{\cdot}^*(y)
		= (f+g)^*(y)
		= \inf_{u+v=y}
		\bigl(
		\delta_{\alpha_1 \cB_{\norm{\cdot}_{(1)}^\dagger}}(u)
		+
		\delta_{\alpha_2 \cB_{\norm{\cdot}_{(2)}^\dagger}}(v)
		\bigr)
		=
		\delta_{\alpha_1 \cB_{\norm{\cdot}_{(1)}^\dagger}+\alpha_2 \cB_{\norm{\cdot}_{(2)}^\dagger}}(y).
	\]
	By definition, the conjugate of a norm is exactly the indicator of its dual unit ball:
	\[
	\norm{\cdot}^*(y) = \delta_{\cB_{\norm{\cdot}^\dagger}}(y)\,.
	\]
	Therefore, $\cB_{\norm{\cdot}^\dagger} = \alpha_1 \cB_{\norm{\cdot}_{(1)}^\dagger} + \alpha_2 \cB_{\norm{\cdot}_{(2)}^\dagger}$.

	Now we prove the general case by induction. The base case ($n=2$) is already proven. Suppose that the assumption of the lemma holds for $n = k$. Then, for $n = k + 1$,
	\[
	\norm{x} = \sum_{i = 1}^k \alpha_i \norm{x}_{(i)} + \alpha_{k + 1}\norm{x}_{(k + 1)} = \norm{x}_{(1:k)} + \alpha_{k + 1}\norm{x}_{(k + 1)}\,.
	\]
	Applying the result for $n=2$ combined with the induction assumption, we obtain
	\[
	\cB_{\norm{\cdot}^\dagger} = \cB_{\norm{\cdot}_{(1:k)}^\dagger} + \alpha_{k + 1} \cB_{\norm{\cdot}_{(k + 1)}^\dagger} = \sum_{i = 1}^{k + 1} \alpha_i \cB_{\norm{\cdot}_{(i)}^\dagger}\,,
	\]
	which proves the lemma.
\end{proof}

\section{Visualization of different matrix norms}
\subsection{Duals to F* and F2 norms}

It follows from \lemmaref{lemma:dual_to_conv_comb} that the norm ball in $\normfstar{\cdot}^\dagger$ is the Minkowski sum of the norm ball in $\alpha\normn{\cdot}$ and $(1-\alpha)\normf{\cdot}$ and the norm ball in $\normftwo{\cdot}^\dagger$ is the Minkowski sum of the norm ball in $\alpha\norms{\cdot}$ and $(1-\alpha)\normf{\cdot}$.

In \figureref{fig:fduals} we plot these norms. On x-axis and y-axis, there are singular values $\sigma_1$, $\sigma_2$ respectively of a matrix from $\R^{m\times n}$ with $\min\{m,n\}=2$.

\begin{figure}[htbp]
\floatconts
  {fig:fduals}
  {\caption{Balls in the duals to F* and F2 norms for different $\alpha$}}
  {%
    \subfigure[lmo balls for F-Muon for different $\alpha$]{\label{fig:fstardual}%
      \includegraphics[width=0.48\linewidth]{figs/norms/fstardualball.pdf}}%
    \hfill
    \subfigure[lmo ball for F-Neon for different $\alpha$]{\label{fig:ftwodual}%
      \includegraphics[width=0.48\linewidth]{figs/norms/ftwodualball.pdf}}
  }
\end{figure}

\subsection{The Ky Fan norm and its dual}

1-balls in $l_\infty$, $l_1$ and $l_2$ norms are well-known from textbooks. But what about the Ky Fan $k$-norm? How can it be represented?

To showcase the complex structure of the Ky Fan $k$-norm and its dual, we suggest the illustrations \figureref{fig:kyfan_combined} with the ball in the Ky Fan $2$-norm in \figureref{fig:kyfan} and its dual in \figureref{fig:kyfandual}. On x-, y-, and z-axes, there are singular values $\sigma_1$, $\sigma_2$, and $\sigma_3$ respectively of a matrix from $\R^{m\times n}$ with $\min\{m,n\}=3$. In this particular case, we do not sort the singular values. In the proposed representation, we actually plot balls in the Top-$2$ norm $\max\{\abs{x}+\abs{y}, \abs{x}+\abs{z}, \abs{y}+\abs{z}\}$ and its dual norm $\max\{\max(\abs(x),\abs{y},\abs{z}), \frac{1}{2} (\abs{x}+\abs{y}+\abs{z})\}$. The resulting balls are much more complex than balls in $l_\infty$, $l_1$ and $l_2$ norms.

In fact, those balls can be described easier if we use the results from \citep{yu2012arithmetic}. The Ky Fan $2$-norm ball is an intersection of three $l_1$ balls in $(x,y)$, $(x, z)$, and $(y,z)$ spaces. The 1-ball in the dual Ky Fan $2$-norm is an intersection of 1-ball the in $l_\infty$ norm and $\frac{1}{2}$-ball in the $l_1$ norm.

\begin{figure}[htbp]
\floatconts
  {fig:kyfan_combined}
  {\caption{Ky Fan 2-norm and its dual}}
  {%
    \subfigure[The Ky Fan $2$-norm]{\label{fig:kyfan}%
      \includegraphics[width=0.45\linewidth]{figs/norms/KyFan.pdf}}%
    \hfill
    \subfigure[Dual to the Ky Fan $2$-norm]{\label{fig:kyfandual}%
      \includegraphics[width=0.45\linewidth]{figs/norms/KyFanDual.pdf}}
  }
\end{figure}

\section{More data for Linear Least Squares}
\label{sec:lls_plots_section}

\begin{table}[htbp]
\floatconts
  {tbl:lls_lrs_app}
  {\caption{Tuning lrs and momentum coefficients}}
  {%
    \begin{tabular}{|c|c|c|c|}
      \arrayrulecolor{black}\hline
      Algorithm & \texttt{lr}  & \texttt{momentum}  & Iterations to 0.001 loss \\
      \arrayrulecolor{black}\hline
      Muon & 0.007 & 0.5 & 1060 \\
      \hdashline
      NSGD & 0.08 & 0.95 & 1020 \\
      F-Muon & 0.015 & 0.7 & 910 \\
      \hdashline
      SignSGD & 0.016 $\times$ 0.01 & 0.95 & 2650 \\
      S-Muon & 0.011 & 0.9 & 890 \\
      \arrayrulecolor{black}\hline
    \end{tabular}%
  }
\end{table}

\begin{figure}[htbp]
\floatconts
  {fig:app_lls}
  {\caption{More images for Linear least squares problem for a 500x500 matrix}}
  {%
    \subfigure[The spectral norm of the gradient]{\label{fig:lls_op_grad_norm}%
      \includegraphics[width=0.6\linewidth]{figs/simple_lls/gradient_spectral_norm_vs_iteration_long_run.pdf}}%
    \vspace{1em}
    \subfigure[The nuclear norm of the gradient]{\label{fig:lls_nuclear_grad_norm}%
      \includegraphics[width=0.6\linewidth]{figs/simple_lls/gradient_nuclear_norm_vs_iteration_long_run.pdf}}%
    \vspace{1em}
    \subfigure[The loss over time]{\label{fig:lls_loss_time}%
      \includegraphics[width=0.6\linewidth]{figs/simple_lls/loss_vs_time_long_run.pdf}}
  }
\end{figure}

\section{Under the hood of CNN on CIFAR-10}

Most bounds for Muon and other LMO-algorithms are given for the norm of the gradient \citep{kovalev2025understanding, pethick2025training, riabinin2025gluon,kovalev2025noneuclideansgdstructuredoptimization}. Accordingly, it is natural to measure those norms on a real deep learning problem.

\begin{table}[htbp]
\floatconts
  {tbl:cifar_lrs}
  {\caption{Parameters for CIFAR-airbench. \texttt{sign\_lr\_mult} for S-Muon is 0.003}}
  {%
    \begin{tabular}{|c|c|c|c|}
      \arrayrulecolor{black}\hline
      Method & \texttt{lr}  & \texttt{momentum}  & val\_accuracy, \% \\
      \arrayrulecolor{black}\hline
      NSGD & 0.5 & 0.95 & $91.6 \pm 0.52$ \\
      \hdashline
      SignSGD & 0.003 & 0.95 & $91.54 \pm 0.26$ \\
      \hdashline
      Muon & 0.24 & 0.6 & $94.01 \pm 0.10$ \\
      F-Muon & 0.40 & 0.6 & $94.01 \pm 0.13$\\
      \hdashline
      S-Muon & 0.42 & 0.63 & $94.03 \pm 0.13$ \\
      \hdashline
      Neon & 0.24 & 0.6 & $69.8 \pm 0.5$ \\
      F-Neon & 0.40 & 0.6 & $87.15 \pm 0.24$\\
      \hdashline
      Fanion-5 & 0.24 & 0.6 & $80.69 \pm 1.25$ \\
      F-Fanion-5 & 0.40 & 0.6 & $86.66 \pm 0.65$\\
      \arrayrulecolor{black}\hline
    \end{tabular}%
  }
\end{table}

We compare Muon, F-Muon, S-Muon, NSGD, SignSGD, Neon, and F-Neon, Fanion-5, and F-Fanion-5 on CIFAR-airbench. NSGD and SignSGD are not heavily tuned. The hyperparameters of Neon, F-Neon, Fanion-5, and F-Fanion-5 are taken from Muon and F-Muon. The parameters are in the table \tableref{tbl:cifar_lrs}. val\_accuracy is for the the weighted variant. However, to preserve the algorithms, we do not normalize the weights of the network each iteration.

We log train and validation accuracy, and nuclear, gradient, and spectral norms of the gradients of conv1.weight and conv2.weight for all the layers. The gradient norms decrease only by the order of ten maximum during training, while train accuracy reaches full 100\% for Muon, S-Muon, and F-Muon.

\begin{figure}[htbp]
\floatconts
  {fig:cifar_accs}
  {\caption{Different optimizers on CIFAR airbench without weight normalization}}
  {%
    \subfigure[Train accuracy]{\label{fig:train_acc_cifar}%
      \includegraphics[width=0.7\linewidth]{figs/cifar_logger/train_acc.pdf}}%
    \vspace{1em}
    \subfigure[Validation accuracy]{\label{fig:val_acc_cifar}%
      \includegraphics[width=0.7\linewidth]{figs/cifar_logger/val_acc.pdf}}
  }
\end{figure}

\begin{figure}[htbp]
\floatconts
  {fig:total_norms_cifar}
  {\caption{Matrix norms of the whole gradient of CIFAR CNN}}
  {%
    \subfigure[Total Frobenius norm]{\label{fig:total_fro_norm_cifar}%
      \includegraphics[width=0.7\linewidth]{figs/cifar_logger/total_frobenius.pdf}}%
    \vspace{1em}
    \subfigure[Total spectral norm]{\label{fig:total_spectral_norm_cifar}%
      \includegraphics[width=0.7\linewidth]{figs/cifar_logger/total_spectral.pdf}}%
    \vspace{1em}
    \subfigure[Total nuclear norm]{\label{fig:total_nuclear_norm_cifar}%
      \includegraphics[width=0.7\linewidth]{figs/cifar_logger/total_nuclear.pdf}}
  }
\end{figure}

\begin{figure}[htbp]
\floatconts
  {fig:norms_conv1_vs_conv2}
  {\caption{Matrix norms of layer2.conv1 (left) and layer2.conv2 (right) gradients of CIFAR CNN}}
  {%
    \subfigure[Frobenius norm of layer2.conv1]{\label{fig:fro_conv1}%
      \includegraphics[width=0.45\linewidth]{figs/cifar_logger/layers_2_conv1_weight_frobenius.pdf}}%
    \hfill
    \subfigure[Frobenius norm of layer2.conv2]{\label{fig:fro_conv2}%
      \includegraphics[width=0.45\linewidth]{figs/cifar_logger/layers_2_conv2_weight_frobenius.pdf}}%
    \vspace{1em}
    \subfigure[Spectral norm of layer2.conv1]{\label{fig:spec_conv1}%
      \includegraphics[width=0.45\linewidth]{figs/cifar_logger/layers_2_conv1_weight_spectral.pdf}}%
    \hfill
    \subfigure[Spectral norm of layer2.conv2]{\label{fig:spec_conv2}%
      \includegraphics[width=0.45\linewidth]{figs/cifar_logger/layers_2_conv2_weight_spectral.pdf}}%
    \vspace{1em}
    \subfigure[Nuclear norm of layer2.conv1]{\label{fig:nuc_conv1}%
      \includegraphics[width=0.45\linewidth]{figs/cifar_logger/layers_2_conv1_weight_nuclear.pdf}}%
    \hfill
    \subfigure[Nuclear norm of layer2.conv2]{\label{fig:nuc_conv2}%
      \includegraphics[width=0.45\linewidth]{figs/cifar_logger/layers_2_conv2_weight_nuclear.pdf}}
  }
\end{figure}

\section{Technical details of the experiments}


\paragraph{CIFAR airbench} We used NVIDIA RTX A4000.

\paragraph{NanoGPT pretrain} We used the standard setting of 8 $\times$ H100 as documented in \citep{modded_nanogpt_2024}. 

\paragraph{NanoGPT Fine-tuning.} Experiments were conducted on a single NVIDIA RTX 4090 (24GB) GPU using PyTorch~2.8 with CUDA~12.8 and cuDNN~9.0, running on a workstation equipped with an Intel Core i9-14900KS CPU and 128 GB of RAM. No distributed or mixed-hardware training setups were used, ensuring a strictly controlled and unbiased comparison across optimizers.

A uniform training protocol was applied to all runs, including identical batch size, warm-up and cosine decay learning rate schedule, gradient clipping strategy, and validation split. Our optimization methods were integrated into the NanoGPT training loop without modifying the model architecture or preprocessing pipeline. Model quality was primarily evaluated using validation loss tracked over 200 training steps.

\end{document}
