%\documentclass[wcp,gray]{jmlr} % test grayscale version
 %\documentclass[wcp]{jmlr}% former name JMLR W\&CP
\documentclass[pmlr]{jmlr}% new name PMLR (Proceedings of Machine Learning)

\RequirePackage{graphicx}
 % The following packages will be automatically loaded:
 % amsmath, amssymb, natbib, graphicx, url, algorithm2e
 \usepackage{booktabs}
 %\usepackage{rotating}% for sideways figures and tables
\usepackage{longtable}% for long tables

\makeatletter
\def\set@curr@file#1{\def\@curr@file{#1}} %temp workaround for 2019 latex release
\makeatother
\usepackage[load-configurations=version-1]{siunitx} % newer version

% Load required packages for custom math commands
\usepackage{mathtools}
\usepackage{bm}

% Custom math commands
% Blackboard bold
\newcommand{\R}{\mathbb{R}}

% Bold matrices
\newcommand{\mA}{\bm{A}}
\newcommand{\mB}{\bm{B}}
\newcommand{\mD}{\bm{D}}
\newcommand{\mF}{\bm{F}}
\newcommand{\mG}{\bm{G}}
\newcommand{\mM}{\bm{M}}
\newcommand{\mN}{\bm{N}}
\newcommand{\mS}{\bm{S}}
\newcommand{\mU}{\bm{U}}
\newcommand{\mV}{\bm{V}}
\newcommand{\mX}{\bm{X}}
\newcommand{\mSigma}{\bm{\Sigma}}

% Vectors (note: \vec is already defined by jmlr class)
% We define shortcuts for commonly used vectors
\newcommand{\vu}{\vec{u}}
\newcommand{\vv}{\vec{v}}

% Calligraphic letters
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cS}{\mathcal{S}}

% Norms and operators
\newcommand{\norm}[1]{\lVert #1\rVert}
\DeclarePairedDelimiter{\normf}{\|}{\|_\mathrm{F}}
\DeclarePairedDelimiter{\norms}{\|}{\|_{\mathrm{2}}}
\DeclarePairedDelimiter{\normc}{\|}{\|_{\mathrm{C}}}
\DeclarePairedDelimiter{\normn}{\|}{\|_{*}}
\newcommand{\normkfk}[1]{\|#1\|_{\text{KF-}k}}

% Inner product notation
\def\<#1,#2>{\langle #1,#2\rangle}

% Math operators
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\diag}{diag}

% Custom shortcuts
\renewcommand{\epsilon}{\varepsilon}
\newcommand{\Rmn}{\R^{m\times n}}

% change the arguments, as appropriate, in the following:
\jmlrvolume{tbd}
\jmlryear{2025}
\jmlrworkshop{International Conference on Computational Optimization}

\title[Ky Fan Norms for Matrix Optimization]{The Ky Fan Norms and Beyond: Dual Norms and Combinations for Matrix Optimization}

\author{\Name{Alexey Kravatskiy}
       \Email{kravtskii.aiu@phystech.edu}\\
       \addr Moscow Institute of Physics and Technology (MIPT)
       \AND
       \Name{Ivan Kozyrev}
       \Email{kozyrev.in@phystech.edu}\\
       \addr Moscow Institute of Physics and Technology (MIPT)\\
       Marchuk Institute of Numerical Mathematics
       \AND
       \Name{Nikolai Kozlov}
       \Email{kozlov.na@phystech.edu}\\
       \addr Moscow Institute of Physics and Technology (MIPT)
       \AND
       \Name{Alexander Vinogradov}
       \Email{vinogradov.am@phystech.edu}\\
       \addr Moscow Institute of Physics and Technology (MIPT)
       \AND
       \Name{Daniil Merkulov}
       \Email{daniil.merkulov@phystech.edu}\\
       \addr Moscow Institute of Physics and Technology (MIPT)\\
       Skoltech, HSE, AI4Science
       \AND
       \Name{Ivan Oseledets}
       \Email{i.oseledets@skoltech.ru}\\
       \addr AIRI, Skoltech\\
       Marchuk Institute of Numerical Mathematics}

\begin{document}

\maketitle

\begin{abstract}
	In this article, we explore the use of various matrix norms for optimizing functions of weight matrices, a crucial problem in training large language models. Moving beyond the spectral norm that underlies the Muon update, we leverage the duals to the Ky Fan $k$-norms to introduce a family of Muon-like algorithms we name \emph{Fanions}, which happen to be similar to Dion. Then working with the duals of convex combinations of the Ky Fan $k$-norms and the Frobenius norm or the $l_\infty$ norm, we construct the families of \emph{F-Fanions} and \emph{S-Fanions} respectively. Their most prominent members are \emph{F-Muon} and \emph{S-Muon}. We complement our theoretical analysis with an extensive empirical study of the algorithms across a wide range of tasks and settings, from which it follows that F-Muon and S-Muon are always on par with Muon, while on fine-tuning of NanoGPT and synthetic linear least squares they are even better than vanilla Muon optimizer.
\end{abstract}

\section{Introduction}

Minimizing loss functions in unprecedentedly high-dimensional spaces has recently become an integral and crucial part in training large language models. Hence, new scalable, time- and memory-efficient algorithms have been demanded. Besides well-known Adam \citep{kingma2014adam} and AdamW \citep{loshchilov2017decoupled}, recently proposed Muon \citep{jordan2024muon} has shown promising results on training very large models \citep{liu2025muon}. Its key difference from Adam and AdamW is that it has been constructed specifically for optimizing functions of weight matrices, which are common in deep learning.

That is what can be said from a practical point of view. From a theoretical perspective, a key innovation of Muon was its principled derivation of the update rule, which emerged as the solution to an optimization problem constrained by the RMS-to-RMS norm (scaled version of the spectral norm) \citep{bernstein2025deriving}.

Motivated by the success of Muon, many generalizations and variations of it were proposed. Among the notable ones are Scion \citep{pethick2025training}, Dion \citep{ahn2025dion} and Gluon \citep{riabinin2025gluon}. Those works try to explain Muon's efficiency and establish convergence bounds. One central question, however, remains unanswered:

\textit{In deriving Muon's update step, why should one constrain by the spectral or any other operator norm? How would alternative norms affect performance and computational cost?}

In this article, we tackle this question by actually showing that there are many viable non-operator norms. We leverage the family of norms dual to Ky Fan $k$-norms to derive a new family of \textbf{Fanions}, algorithms with low-rank updates. This approach theoretically explains the backbone of Dion's update \citep{ahn2025dion} and generalizes the memory-motivated application of the nuclear norm to Sharpness-Aware Minimization \citep{pethick2025sam}. As it was done with Muon, we come up with an effective procedure for computing Fanions' updates. Lanczos algorithm, which is described and compared with its competitors in \sectionref{sec:matrix-side}, is the most operation-effective algorithm, which, however, for now lacks an effective GPU- and PyTorch-friendly implementation.

Working with duals to conic combinations of dual norms, we construct the families of \textbf{F-Fanions} and \textbf{S-Fanions}, which are hybrids of Muon and NormalizedSGD and SignSGD, respectively.

Then we compare the performances of the algorithm families on various model and real-world problems:
\begin{itemize}
	\item Synthetic least squares experiment \sectionref{subsec:lls_exps}
	\item CIFAR-10 airbench \citep{cifar2023airbench}
	\item Pre-training NanoGPT on FineWeb dataset \citep{modded_nanogpt_2024}
	\item Fine-tuning NanoGPT on Tiny Stories \citep{eldan2023tinystoriessmalllanguagemodels}
\end{itemize}

Our experiments reveal important insights into the role of matrix norms in optimization. First, we show on the example of Neon, the one-rank Fanion, that not every LMO-based algorithm is effective, despite the same asymptotics in the general bounds of \citet{kovalev2025understanding} and \citet{riabinin2025gluon}. This suggests that existing theoretical guarantees should be reworked to explain empirical performance.

Most notably, our experiments on real-world tasks demonstrate that the choice of underlying matrix norm is remarkably flexible. On CIFAR-10 airbench, properly-tuned F-Muon and S-Muon achieve $94.02 \pm 0.13\%$ and $94.03 \pm 0.13\%$ accuracy, matching Muon's $94.01 \pm 0.13\%$ performance. On NanoGPT pre-training, F-Muon achieves 3.281 cross-entropy loss, while fully-tuned Muon achieves 3.279. Finally, S-Muon matches Muon on fine-tuning of NanoGPT on Tiny Stories, while F-Muon is far more resistant to the learning rate choice than Muon. These results show that Muon-like algorithms can maintain competitive performance even when the underlying norm constraint is significantly modified, answering affirmatively the central question posed above. Moreover, the tools from \sectionref{sec:conic_combo} give the researchers an unheard-of flexibility in designing algorithms that do not have to be modifications of Muon.

\section{Preliminaries: Linear Minimization Oracle Framework}

Training a neural network is essentially an optimization of a function of several weight matrices and a few vectors. Let us start by considering the problem of minimizing a differentiable function of a \emph{single} matrix:
\begin{equation}\label{eq:matrix_problem}
	F(\cdot)\colon \Rmn \to \R\,,\qquad F(\mX) \to \min_{\mX \in \Rmn}\,
\end{equation}
We equip the matrix space $\Rmn$ with a standard dot product $\<\cdot, \cdot> \to \R$ and a norm $\norm{\cdot}\colon \Rmn \to \R_+$, which does not have to coincide with the Frobenius norm $\normf{\cdot}$. The dual norm $\norm{\cdot}^\dagger\colon \Rmn \to \R_+$ that is associated with $\norm{\cdot}$ is defined as
\begin{equation}\label{eq:dual_norm}
	\norm{\mG}^\dagger = \sup_{\mD \in \Rmn\,:\norm{\mD}\leq 1} \<\mG,\mD>\,.
\end{equation}

Such problems can be solved with an iterative algorithm based on the Linear Minimization Oracle (LMO):
\begin{equation}\label{eq:lmo}
	\mathrm{LMO}(\mM^k) \in \argmin_{\mD \in \cS} \<\mM^k, \mD>\,,
\end{equation}
where $\mM^k$ is a gradient (or a momentum) of $F$ in $\mX^k$ and $\cS \subset \Rmn$ is some set. The update of the algorithm is defined as follows:
\begin{equation}\label{eq:simple_update}
	\mX^{k + 1} = \mX^k + \gamma_k\mathrm{LMO}\left(\mM^k\right)\,.
\end{equation}

We are particularly interested in the case when $\cS$ is a unit ball in the norm $\norm{\cdot}$:

\[
\cS = \cB_{\norm{\cdot}} = \{ \mD \in \Rmn \mid \norm{\mD} \leq 1 \}\,.
\]
In this case,
\[
\argmin_{\mD \in \cS} \<\mM^k, \mD> = - \{ \mD \in \cB_1 \mid \<\mM^k, \mD> = \norm{\mM^k}^\dagger\}\,,
\]
and update for $\mX^k$ in \equationref{eq:simple_update} simplifies to
\begin{equation}\label{eq:our_update}
	\mX^{k+1} = \mX^{k} - \gamma_k \{\mD \in \cB_1 \mid \<\mM^k, \mD> = \norm{\mM^k}^\dagger\}\,.
\end{equation}

Using this formula it is easy to compute updates for algorithms induced by various norms $\norm{\cdot}$:

\paragraph{Frobenius norm and Normalized SGD}
When the norm $\norm{\cdot}$ is the Frobenius norm $\normf{\cdot}$, \equationref{eq:our_update} turns into
\begin{equation}\label{eq:nsgd_update}
	\mX^{k+1} = \mX^{k} - \eta_k \frac{\mM^k}{\normf{\mM^k}}\,,
\end{equation}
which recovers Normalized SGD (NSGD).

\paragraph{Spectral norm and Muon}
When the norm is the spectral norm $\norms{\cdot}$, we get
\begin{equation}\label{eq:muon_update}
	\mX^{k+1} = \mX^{k} - \eta_k \mU \mV^\top\,,
\end{equation}
which is Muon without the $\sqrt{m/n}$ factor.
Here, $\mM^k = \mU \mSigma \mV^\top$ is the Singular Value Decomposition (SVD) of $\mM^k$ ($\mU = [\vu_1, \vu_2, \dots, \vu_r]$, $\mSigma = \diag(\sigma_1, \sigma_2, \dots, \sigma_r)$, and $\mV = [\vv_1, \vv_2, \dots, \vv_r]$). Muon can be recovered by taking the RMS-to-RMS operator norm $\sqrt{\frac{n}{m}}\norms{\cdot}$.

\paragraph{Chebyshev norm and SignSGD}
When the norm is the Chebyshev norm $\normc{\cdot}$, we get
\begin{equation}\label{eq:signsgd_update}
	\mX^{k+1} = \mX^{k} - \eta_k \sign(\mM^k)\,,
\end{equation}
which recovers SignSGD~\citep{bernstein2018signsgd}. Here, $\sign(\mM^k)$ denotes the element-wise sign function. SignSGD is particularly notable for its communication efficiency in distributed training, as it compresses gradients to 1-bit per parameter.

\section{Duals to Ky Fan Norms Instead of the Spectral Norm}

\subsection{\texorpdfstring{$\normn{\mM^{k}}$ and Neon}{Neon}}
After considering $\normf{\mM^k}$ and $\norms{\mM^k}$, it is natural to look at the nuclear norm $\normn{\mM^k}$.
\begin{lemma}\label{lemma:neon_update}
	When $\norm{\cdot} = \normn{\cdot}$, \equationref{eq:our_update} turns into:

	\begin{equation}\label{eq:neon_update}
		\mX^{k+1} = \mX^{k} - \eta_k \vu_1 \vv_1^\top
	\end{equation}
\end{lemma}
\begin{proof}
Since $\norm{\cdot}^\dagger = \norms{\cdot}$, the goal is to reach $\norms{\mM^k} = \sigma_1$.

Let us note that $\Delta = \vu_1 \vv_1^\top$ delivers this value. Indeed, $\normn{\Delta}=1$ and by the trace property and orthogonality of the singular vectors,
\[
\<\mM^k, \Delta> =\<\mU \mSigma \mV^\top, \vu_1 \vv_1^\top> = \tr \diag({\sigma_1, 0, \dots, 0}) = \norms{\mM^k}\,,
\]
which completes the proof.
\end{proof}

We name the derived algorithm \emph{Neon}. In \sectionref{sec:matrix-side}, we will discuss how to compute the Neon's update.

\subsection{\texorpdfstring{$\normkfk{\mM^k}^\dagger$ and Muon, Neon, and Centralized Dion without error feedback}{Muon, Neon, and Centralized Dion without error feedback}}

Neon's and Muon's updates seem to be complete opposites: one has rank one, while the other is full-rank. It would be interesting to derive algorithms with updates of intermediate ranks.

\subsubsection{Schatten norms}
\citet{cesista2025schattenp} considered Schatten-$p$ norms: 
\[\norm{\mM^k}_{S_p} = \left(\sum_{i=1}^{\min(m, n)}\sigma_i^p\right)^{1/p}\,,
\]
which produce the update
\[
\mX^{k+1} = \mX^k - \eta_k \mU \frac{\diag\left(\sigma_1^{q-1},\dots, \sigma_{\min(m,n)}^{q-1}\right)}{\left(\sum_{i=1}^{\min(m,n)}{\sigma_i^q}\right)^{\frac{q-1}{q}}}\mV^\top
\]
for $p$ and $q$ such that: $\frac{1}{p} + \frac{1}{q} = 1$. This formula recovers Neon when $p\rightarrow1$ provided that $\sigma_1 > \sigma_2$, which is true on real data; NSGD when $p=2$, and Muon when $p\rightarrow\infty$.

However, Schatten norms do not fill the gap rank: indeed, when $p > 1$, the rank of the update is full, while when $p=1$ ($p \rightarrow 1$) it is one. Moreover, it is not clear how to calculate the update for $p \neq 1, 2, \infty$: it seems one has to know all $\sigma_i$ to compute the update, which makes the problem as hard as the full SVD.

\subsubsection{Ky Fan norms}
There is another family of matrix norms, which might help us: Ky Fan norms. For $k \in \{1, \dots, \min(m, n)\}$, the Ky Fan $k$-norm $\normkfk{\cdot}$ is $\sum_{i=1}^{k}\sigma_i$, i.e. the sum of the $k$ largest singular values of the matrix. Special cases of the Ky Fan $k$-norm are the Ky Fan $1$-norm, which is the spectral norm, and the Ky Fan $\min\{m, n\}$-norm, which is the nuclear norm.

Let us derive the update for an arbitrary $k$. Since $\normkfk{\cdot}^\dagger = \max\{\frac{1}{k} \normn{\cdot}, \norms{\cdot}\}$ (see \citet{bhatia2013matrix}, p. 96), the goal is to reach $\max\{\frac{1}{k}\sum_{i=1}^{\min(m,n)}\sigma_i, \sigma_1\}$. $\Delta = \vu_1 \vv_1^\top$ from Neon delivers $\sigma_1$, while $\Delta = \frac{1}{k}\sum_{i=1}^{\min(m,n)}\vu_i \vv_i^\top$ from Muon (with an extra factor of $1/k$) delivers $\frac{1}{k}\sum_{i=1}^{\min(m,n)}\sigma_i$. Thus, the update is either
\[
\mX^{t+1} = \mX^{t} - \eta_t \vu_1 \vv_1^\top\text{ or }\mX^{t+1} = \mX^{t} - \frac{\eta_t}{k}\mU \mV^\top\,,
\]
depending on which one better minimizes the linear function $L(\mX) \equiv F(\mX^t) + \mM^t(\mX - \mX^t)$.

The rank gap is not closed by the Ky Fan norms because the obtained update is either one-rank or full-rank.

\subsubsection{Duals to Ky Fan norms}
While Schatten norms are closed under dualization and $\norm{\mM^k}_{S_p}^\dagger = \norm{\mM^k}_{S_q}$, for Ky Fan norms it is not generally the case. $k = 1$ and $k=\min(m, n)$ are exceptional: for $k=1$ the dual norm is $\normn{\cdot}$ with $k = \min(m, n)$ and for $k = \min(m, n)$ the dual norm is $\norms{\cdot}$ with $k=1$. For each other $k$, $\normkfk{\cdot} \neq \norm{\cdot}_{\text{KF-}k'}$ for any $k'$: $k'=\min(m, n)$ and $k'= 1$ correspond to the already discussed cases, while for other $k'$ one can change $\norm{\mM^k}_{\text{KF-}k'}$ by moving a small value between $\sigma_{k'}$ and $\sigma_{k'+1}$. During this change, $\norms{\mM^k}$ and $\normn{\mM^k}$ will remain constant, hence $\normkfk{\mM^k}$ will not change as well.

\begin{lemma}\label{lemma:ky_fan_update}
	When $\norm{\cdot} = \normkfk{\mM^t}^\dagger$, \equationref{eq:our_update} turns into:

	\begin{equation}\label{eq:ky_fan_update}
		\mX^{t+1} = \mX^{t} - \eta \sum_{i=1}^{k}\vu_i \vv_i^\top
	\end{equation}
\end{lemma}

\begin{proof}
Since $\norm{\cdot}^\dagger = \normkfk{\cdot}^{\dagger\dagger} = \normkfk{\cdot}$, the goal is to reach $\normkfk{\mM^t}$.

Let us note that $\Delta = \sum_{i=1}^{k} \vu_i \vv_i^\top$ delivers the value. Indeed,
\[
\<\mM^t, \Delta> =\<\mU \mSigma \mV^\top, \sum_{i=1}^{k} \vu_i \vv_i^\top> = \sum_{i,j=1}^{r, k}\<\vu_i \sigma_i \vv_i^\top, \vu_j \vv_j^\top> = \sum_{i=1}^{k}\sigma_i = \normkfk{\mM^t}\,,
\]
which completes the proof.
\end{proof}
These updates constitute the family of \emph{Fanions}, LMO-based algorithms under the $\normkfk{\mM^t}^\dagger$ norms. The algorithm for a particular $k$ we will call \emph{Fanion-$k$}. Thus, Muon is a Fanion-$\min\{m,n\}$, while Neon is a Fanion-1. Moreover, the unsharded version of the rank-$r$ Dion (Algorithm 1 from \citet{ahn2025dion}) without the error feedback and without scaling of the update is actually a Fanion-$r$ (see \citet{pethick2025understandingdion}, where Dion is written down in a notation more similar to ours).

\bibliography{icomp2024_conference}

\end{document}
