import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


class SMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, sgd_coeff=0, sign_lr_mult=1, scale_sgd=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, sgd_coeff=sgd_coeff, sign_lr_mult=sign_lr_mult)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        self.sgd_coeff = sgd_coeff
        self.sign_lr_mult = sign_lr_mult
        self.scale_sgd = scale_sgd

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            sgd_coeff = group["sgd_coeff"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    sign_sgd_eff_lr = (eff_lr if self.scale_sgd else group["lr"]) * group["sign_lr_mult"]

                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    # eps = 1e-12
                    # g_normalized = grad / (grad.norm() + eps)

                    if sgd_coeff != 1:
                        update_part = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                        p.add_(other=update_part, alpha=-eff_lr * (1-sgd_coeff))
                        p.add_(other=grad.sign(), alpha=-sign_sgd_eff_lr * sgd_coeff)
                    else:
                        p.add_(other=grad.sign(), alpha=-sign_sgd_eff_lr * sgd_coeff)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)
                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_reduce_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 27.5
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos : pos + max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()
    starts = []
    for i in range(1, len(boundary_positions)):
        end = boundary_positions[i].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == dist.get_world_size():
                return starts, end - pos
            start = end
    assert False # increase max_batch_span if necessary

def distributed_data_generator(filename_pattern: str, batch_size: int, align_to_bos: bool):
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    max_batch_span = 2 * batch_size if align_to_bos else batch_size # provide buffer to handle samples up to length local_batch_size
    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            batch_span = batch_size
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1750 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
from datetime import datetime
if master_process:
    # run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_id = str(int(time.time() * 1000))
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0)
# optimizer2 = SMuon(hidden_matrix_params, lr=0.003, momentum=0.95, weight_decay=0.00, sgd_coeff=1, sign_lr_mult=0.1, scale_sgd=False) # purse SignSGD, eff lr = 1e-3; you may drop WD

optimizer2 = SMuon(hidden_matrix_params, lr=0.07, momentum=0.95, weight_decay=0.0, sgd_coeff=0.5, sign_lr_mult=0.0003 / 0.07, scale_sgd=False) # S-Muon
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.18 | packaged by conda-forge | (main, Jun  4 2025, 14:45:41) [GCC 13.3.0]
Running PyTorch 2.10.0.dev20251124+cu126 compiled for CUDA 12.6
Sat Nov 29 11:55:32 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.216.03             Driver Version: 535.216.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   33C    P0             114W / 700W |   5823MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:2D:00.0 Off |                    0 |
| N/A   36C    P0             119W / 700W |   1513MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:3F:00.0 Off |                    0 |
| N/A   35C    P0             117W / 700W |   1513MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:66:00.0 Off |                    0 |
| N/A   32C    P0             118W / 700W |   1513MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   31C    P0             115W / 700W |   1513MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:AE:00.0 Off |                    0 |
| N/A   35C    P0             121W / 700W |   1513MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:BF:00.0 Off |                    0 |
| N/A   34C    P0             116W / 700W |   1513MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:E4:00.0 Off |                    0 |
| N/A   31C    P0             116W / 700W |   1513MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1750 train_time:136ms step_avg:136.15ms
step:2/1750 train_time:162ms step_avg:81.25ms
step:3/1750 train_time:232ms step_avg:77.37ms
step:4/1750 train_time:323ms step_avg:80.84ms
step:5/1750 train_time:415ms step_avg:83.05ms
step:6/1750 train_time:507ms step_avg:84.57ms
step:7/1750 train_time:598ms step_avg:85.45ms
step:8/1750 train_time:690ms step_avg:86.23ms
step:9/1750 train_time:782ms step_avg:86.92ms
step:10/1750 train_time:874ms step_avg:87.40ms
step:11/1750 train_time:966ms step_avg:87.81ms
step:12/1750 train_time:1060ms step_avg:88.30ms
step:13/1750 train_time:1155ms step_avg:88.87ms
step:14/1750 train_time:1250ms step_avg:89.29ms
step:15/1750 train_time:1343ms step_avg:89.55ms
step:16/1750 train_time:1436ms step_avg:89.74ms
step:17/1750 train_time:1528ms step_avg:89.88ms
step:18/1750 train_time:1620ms step_avg:90.03ms
step:19/1750 train_time:1713ms step_avg:90.14ms
step:20/1750 train_time:1804ms step_avg:90.21ms
step:21/1750 train_time:1897ms step_avg:90.31ms
step:22/1750 train_time:1989ms step_avg:90.39ms
step:23/1750 train_time:2082ms step_avg:90.52ms
step:24/1750 train_time:2176ms step_avg:90.67ms
step:25/1750 train_time:2269ms step_avg:90.77ms
step:26/1750 train_time:2363ms step_avg:90.87ms
step:27/1750 train_time:2455ms step_avg:90.94ms
step:28/1750 train_time:2548ms step_avg:91.00ms
step:29/1750 train_time:2640ms step_avg:91.04ms
step:30/1750 train_time:2733ms step_avg:91.09ms
step:31/1750 train_time:2825ms step_avg:91.12ms
step:32/1750 train_time:2917ms step_avg:91.15ms
step:33/1750 train_time:3009ms step_avg:91.18ms
step:34/1750 train_time:3102ms step_avg:91.25ms
step:35/1750 train_time:3196ms step_avg:91.32ms
step:36/1750 train_time:3288ms step_avg:91.34ms
step:37/1750 train_time:3382ms step_avg:91.39ms
step:38/1750 train_time:3474ms step_avg:91.43ms
step:39/1750 train_time:3567ms step_avg:91.46ms
step:40/1750 train_time:3660ms step_avg:91.49ms
step:41/1750 train_time:3753ms step_avg:91.53ms
step:42/1750 train_time:3844ms step_avg:91.53ms
step:43/1750 train_time:3938ms step_avg:91.58ms
step:44/1750 train_time:4030ms step_avg:91.59ms
step:45/1750 train_time:4123ms step_avg:91.61ms
step:46/1750 train_time:4216ms step_avg:91.64ms
step:47/1750 train_time:4308ms step_avg:91.66ms
step:48/1750 train_time:4400ms step_avg:91.67ms
step:49/1750 train_time:4492ms step_avg:91.68ms
step:50/1750 train_time:4585ms step_avg:91.69ms
step:51/1750 train_time:4677ms step_avg:91.71ms
step:52/1750 train_time:4769ms step_avg:91.71ms
step:53/1750 train_time:4861ms step_avg:91.73ms
step:54/1750 train_time:4953ms step_avg:91.73ms
step:55/1750 train_time:5046ms step_avg:91.74ms
step:56/1750 train_time:5137ms step_avg:91.74ms
step:57/1750 train_time:5230ms step_avg:91.76ms
step:58/1750 train_time:5323ms step_avg:91.77ms
step:59/1750 train_time:5415ms step_avg:91.78ms
step:60/1750 train_time:5507ms step_avg:91.78ms
step:61/1750 train_time:5600ms step_avg:91.80ms
step:62/1750 train_time:5692ms step_avg:91.81ms
step:63/1750 train_time:5788ms step_avg:91.87ms
step:64/1750 train_time:5878ms step_avg:91.85ms
step:65/1750 train_time:5970ms step_avg:91.85ms
step:66/1750 train_time:6063ms step_avg:91.86ms
step:67/1750 train_time:6156ms step_avg:91.87ms
step:68/1750 train_time:6248ms step_avg:91.88ms
step:69/1750 train_time:6340ms step_avg:91.88ms
step:70/1750 train_time:6432ms step_avg:91.89ms
step:71/1750 train_time:6524ms step_avg:91.89ms
step:72/1750 train_time:6617ms step_avg:91.90ms
step:73/1750 train_time:6710ms step_avg:91.92ms
step:74/1750 train_time:6802ms step_avg:91.92ms
step:75/1750 train_time:6895ms step_avg:91.93ms
step:76/1750 train_time:6987ms step_avg:91.93ms
step:77/1750 train_time:7080ms step_avg:91.95ms
step:78/1750 train_time:7173ms step_avg:91.96ms
step:79/1750 train_time:7265ms step_avg:91.96ms
step:80/1750 train_time:7361ms step_avg:92.01ms
step:81/1750 train_time:7450ms step_avg:91.98ms
step:82/1750 train_time:7542ms step_avg:91.98ms
step:83/1750 train_time:7635ms step_avg:91.99ms
step:84/1750 train_time:7728ms step_avg:92.00ms
step:85/1750 train_time:7820ms step_avg:92.00ms
step:86/1750 train_time:7914ms step_avg:92.02ms
step:87/1750 train_time:8006ms step_avg:92.03ms
step:88/1750 train_time:8099ms step_avg:92.04ms
step:89/1750 train_time:8192ms step_avg:92.04ms
step:90/1750 train_time:8285ms step_avg:92.06ms
step:91/1750 train_time:8378ms step_avg:92.06ms
step:92/1750 train_time:8470ms step_avg:92.07ms
step:93/1750 train_time:8563ms step_avg:92.08ms
step:94/1750 train_time:8657ms step_avg:92.09ms
step:95/1750 train_time:8749ms step_avg:92.10ms
step:96/1750 train_time:8841ms step_avg:92.10ms
step:97/1750 train_time:8934ms step_avg:92.10ms
step:98/1750 train_time:9026ms step_avg:92.10ms
step:99/1750 train_time:9119ms step_avg:92.11ms
step:100/1750 train_time:9212ms step_avg:92.12ms
step:101/1750 train_time:9305ms step_avg:92.13ms
step:102/1750 train_time:9397ms step_avg:92.13ms
step:103/1750 train_time:9489ms step_avg:92.13ms
step:104/1750 train_time:9583ms step_avg:92.14ms
step:105/1750 train_time:9675ms step_avg:92.14ms
step:106/1750 train_time:9767ms step_avg:92.15ms
step:107/1750 train_time:9860ms step_avg:92.15ms
step:108/1750 train_time:9953ms step_avg:92.16ms
step:109/1750 train_time:10045ms step_avg:92.16ms
step:110/1750 train_time:10138ms step_avg:92.16ms
step:111/1750 train_time:10231ms step_avg:92.17ms
step:112/1750 train_time:10324ms step_avg:92.18ms
step:113/1750 train_time:10416ms step_avg:92.18ms
step:114/1750 train_time:10509ms step_avg:92.18ms
step:115/1750 train_time:10602ms step_avg:92.19ms
step:116/1750 train_time:10696ms step_avg:92.20ms
step:117/1750 train_time:10788ms step_avg:92.21ms
step:118/1750 train_time:10882ms step_avg:92.22ms
step:119/1750 train_time:10974ms step_avg:92.22ms
step:120/1750 train_time:11066ms step_avg:92.22ms
step:121/1750 train_time:11159ms step_avg:92.22ms
step:122/1750 train_time:11252ms step_avg:92.23ms
step:123/1750 train_time:11344ms step_avg:92.23ms
step:124/1750 train_time:11436ms step_avg:92.23ms
step:125/1750 train_time:11528ms step_avg:92.23ms
step:125/1750 val_loss:4.6893 train_time:11616ms step_avg:92.93ms
step:126/1750 train_time:11641ms step_avg:92.39ms
step:127/1750 train_time:11720ms step_avg:92.29ms
step:128/1750 train_time:11816ms step_avg:92.31ms
step:129/1750 train_time:11910ms step_avg:92.33ms
step:130/1750 train_time:12002ms step_avg:92.32ms
step:131/1750 train_time:12094ms step_avg:92.32ms
step:132/1750 train_time:12188ms step_avg:92.33ms
step:133/1750 train_time:12279ms step_avg:92.33ms
step:134/1750 train_time:12372ms step_avg:92.33ms
step:135/1750 train_time:12464ms step_avg:92.33ms
step:136/1750 train_time:12557ms step_avg:92.33ms
step:137/1750 train_time:12650ms step_avg:92.34ms
step:138/1750 train_time:12745ms step_avg:92.35ms
step:139/1750 train_time:12839ms step_avg:92.37ms
step:140/1750 train_time:12933ms step_avg:92.38ms
step:141/1750 train_time:13026ms step_avg:92.38ms
step:142/1750 train_time:13119ms step_avg:92.39ms
step:143/1750 train_time:13212ms step_avg:92.39ms
step:144/1750 train_time:13304ms step_avg:92.39ms
step:145/1750 train_time:13396ms step_avg:92.39ms
step:146/1750 train_time:13489ms step_avg:92.39ms
step:147/1750 train_time:13582ms step_avg:92.39ms
step:148/1750 train_time:13675ms step_avg:92.40ms
step:149/1750 train_time:13768ms step_avg:92.40ms
step:150/1750 train_time:13862ms step_avg:92.41ms
step:151/1750 train_time:13956ms step_avg:92.42ms
step:152/1750 train_time:14050ms step_avg:92.43ms
step:153/1750 train_time:14142ms step_avg:92.43ms
step:154/1750 train_time:14234ms step_avg:92.43ms
step:155/1750 train_time:14327ms step_avg:92.43ms
step:156/1750 train_time:14419ms step_avg:92.43ms
step:157/1750 train_time:14512ms step_avg:92.43ms
step:158/1750 train_time:14605ms step_avg:92.44ms
step:159/1750 train_time:14698ms step_avg:92.44ms
step:160/1750 train_time:14792ms step_avg:92.45ms
step:161/1750 train_time:14886ms step_avg:92.46ms
step:162/1750 train_time:14980ms step_avg:92.47ms
step:163/1750 train_time:15073ms step_avg:92.47ms
step:164/1750 train_time:15166ms step_avg:92.47ms
step:165/1750 train_time:15259ms step_avg:92.48ms
step:166/1750 train_time:15351ms step_avg:92.48ms
step:167/1750 train_time:15444ms step_avg:92.48ms
step:168/1750 train_time:15536ms step_avg:92.48ms
step:169/1750 train_time:15630ms step_avg:92.48ms
step:170/1750 train_time:15723ms step_avg:92.49ms
step:171/1750 train_time:15816ms step_avg:92.49ms
step:172/1750 train_time:15910ms step_avg:92.50ms
step:173/1750 train_time:16003ms step_avg:92.50ms
step:174/1750 train_time:16096ms step_avg:92.50ms
step:175/1750 train_time:16192ms step_avg:92.53ms
step:176/1750 train_time:16286ms step_avg:92.53ms
step:177/1750 train_time:16374ms step_avg:92.51ms
step:178/1750 train_time:16467ms step_avg:92.51ms
step:179/1750 train_time:16560ms step_avg:92.52ms
step:180/1750 train_time:16653ms step_avg:92.52ms
step:181/1750 train_time:16746ms step_avg:92.52ms
step:182/1750 train_time:16840ms step_avg:92.53ms
step:183/1750 train_time:16938ms step_avg:92.55ms
step:184/1750 train_time:17027ms step_avg:92.54ms
step:185/1750 train_time:17120ms step_avg:92.54ms
step:186/1750 train_time:17213ms step_avg:92.54ms
step:187/1750 train_time:17306ms step_avg:92.55ms
step:188/1750 train_time:17399ms step_avg:92.55ms
step:189/1750 train_time:17496ms step_avg:92.57ms
step:190/1750 train_time:17586ms step_avg:92.56ms
step:191/1750 train_time:17679ms step_avg:92.56ms
step:192/1750 train_time:17771ms step_avg:92.56ms
step:193/1750 train_time:17864ms step_avg:92.56ms
step:194/1750 train_time:17958ms step_avg:92.57ms
step:195/1750 train_time:18051ms step_avg:92.57ms
step:196/1750 train_time:18143ms step_avg:92.57ms
step:197/1750 train_time:18237ms step_avg:92.57ms
step:198/1750 train_time:18330ms step_avg:92.58ms
step:199/1750 train_time:18423ms step_avg:92.58ms
step:200/1750 train_time:18516ms step_avg:92.58ms
step:201/1750 train_time:18609ms step_avg:92.58ms
step:202/1750 train_time:18703ms step_avg:92.59ms
step:203/1750 train_time:18796ms step_avg:92.59ms
step:204/1750 train_time:18889ms step_avg:92.60ms
step:205/1750 train_time:18982ms step_avg:92.60ms
step:206/1750 train_time:19075ms step_avg:92.60ms
step:207/1750 train_time:19169ms step_avg:92.60ms
step:208/1750 train_time:19262ms step_avg:92.60ms
step:209/1750 train_time:19355ms step_avg:92.61ms
step:210/1750 train_time:19453ms step_avg:92.63ms
step:211/1750 train_time:19543ms step_avg:92.62ms
step:212/1750 train_time:19636ms step_avg:92.62ms
step:213/1750 train_time:19730ms step_avg:92.63ms
step:214/1750 train_time:19824ms step_avg:92.63ms
step:215/1750 train_time:19915ms step_avg:92.63ms
step:216/1750 train_time:20008ms step_avg:92.63ms
step:217/1750 train_time:20102ms step_avg:92.63ms
step:218/1750 train_time:20194ms step_avg:92.63ms
step:219/1750 train_time:20287ms step_avg:92.64ms
step:220/1750 train_time:20384ms step_avg:92.66ms
step:221/1750 train_time:20476ms step_avg:92.65ms
step:222/1750 train_time:20568ms step_avg:92.65ms
step:223/1750 train_time:20661ms step_avg:92.65ms
step:224/1750 train_time:20756ms step_avg:92.66ms
step:225/1750 train_time:20847ms step_avg:92.66ms
step:226/1750 train_time:20951ms step_avg:92.70ms
step:227/1750 train_time:21033ms step_avg:92.66ms
step:228/1750 train_time:21126ms step_avg:92.66ms
step:229/1750 train_time:21219ms step_avg:92.66ms
step:230/1750 train_time:21312ms step_avg:92.66ms
step:231/1750 train_time:21406ms step_avg:92.67ms
step:232/1750 train_time:21499ms step_avg:92.67ms
step:233/1750 train_time:21592ms step_avg:92.67ms
step:234/1750 train_time:21685ms step_avg:92.67ms
step:235/1750 train_time:21779ms step_avg:92.68ms
step:236/1750 train_time:21872ms step_avg:92.68ms
step:237/1750 train_time:21968ms step_avg:92.69ms
step:238/1750 train_time:22058ms step_avg:92.68ms
step:239/1750 train_time:22152ms step_avg:92.68ms
step:240/1750 train_time:22245ms step_avg:92.69ms
step:241/1750 train_time:22337ms step_avg:92.69ms
step:242/1750 train_time:22431ms step_avg:92.69ms
step:243/1750 train_time:22524ms step_avg:92.69ms
step:244/1750 train_time:22617ms step_avg:92.69ms
step:245/1750 train_time:22711ms step_avg:92.70ms
step:246/1750 train_time:22804ms step_avg:92.70ms
step:247/1750 train_time:22897ms step_avg:92.70ms
step:248/1750 train_time:22991ms step_avg:92.70ms
step:249/1750 train_time:23084ms step_avg:92.70ms
step:250/1750 train_time:23176ms step_avg:92.71ms
step:250/1750 val_loss:4.1257 train_time:23264ms step_avg:93.06ms
step:251/1750 train_time:23290ms step_avg:92.79ms
step:252/1750 train_time:23368ms step_avg:92.73ms
step:253/1750 train_time:23461ms step_avg:92.73ms
step:254/1750 train_time:23555ms step_avg:92.74ms
step:255/1750 train_time:23647ms step_avg:92.73ms
step:256/1750 train_time:23739ms step_avg:92.73ms
step:257/1750 train_time:23831ms step_avg:92.73ms
step:258/1750 train_time:23924ms step_avg:92.73ms
step:259/1750 train_time:24017ms step_avg:92.73ms
step:260/1750 train_time:24111ms step_avg:92.73ms
step:261/1750 train_time:24204ms step_avg:92.73ms
step:262/1750 train_time:24299ms step_avg:92.74ms
step:263/1750 train_time:24393ms step_avg:92.75ms
step:264/1750 train_time:24487ms step_avg:92.75ms
step:265/1750 train_time:24581ms step_avg:92.76ms
step:266/1750 train_time:24676ms step_avg:92.77ms
step:267/1750 train_time:24770ms step_avg:92.77ms
step:268/1750 train_time:24860ms step_avg:92.76ms
step:269/1750 train_time:24954ms step_avg:92.76ms
step:270/1750 train_time:25046ms step_avg:92.76ms
step:271/1750 train_time:25139ms step_avg:92.77ms
step:272/1750 train_time:25234ms step_avg:92.77ms
step:273/1750 train_time:25328ms step_avg:92.77ms
step:274/1750 train_time:25422ms step_avg:92.78ms
step:275/1750 train_time:25516ms step_avg:92.79ms
step:276/1750 train_time:25611ms step_avg:92.79ms
step:277/1750 train_time:25703ms step_avg:92.79ms
step:278/1750 train_time:25797ms step_avg:92.79ms
step:279/1750 train_time:25889ms step_avg:92.79ms
step:280/1750 train_time:25982ms step_avg:92.79ms
step:281/1750 train_time:26075ms step_avg:92.80ms
step:282/1750 train_time:26169ms step_avg:92.80ms
step:283/1750 train_time:26262ms step_avg:92.80ms
step:284/1750 train_time:26356ms step_avg:92.80ms
step:285/1750 train_time:26450ms step_avg:92.81ms
step:286/1750 train_time:26545ms step_avg:92.81ms
step:287/1750 train_time:26638ms step_avg:92.81ms
step:288/1750 train_time:26731ms step_avg:92.82ms
step:289/1750 train_time:26825ms step_avg:92.82ms
step:290/1750 train_time:26919ms step_avg:92.82ms
step:291/1750 train_time:27012ms step_avg:92.82ms
step:292/1750 train_time:27105ms step_avg:92.83ms
step:293/1750 train_time:27201ms step_avg:92.83ms
step:294/1750 train_time:27293ms step_avg:92.83ms
step:295/1750 train_time:27387ms step_avg:92.84ms
step:296/1750 train_time:27480ms step_avg:92.84ms
step:297/1750 train_time:27574ms step_avg:92.84ms
step:298/1750 train_time:27667ms step_avg:92.84ms
step:299/1750 train_time:27761ms step_avg:92.85ms
step:300/1750 train_time:27859ms step_avg:92.86ms
step:301/1750 train_time:27948ms step_avg:92.85ms
step:302/1750 train_time:28041ms step_avg:92.85ms
step:303/1750 train_time:28135ms step_avg:92.86ms
step:304/1750 train_time:28228ms step_avg:92.86ms
step:305/1750 train_time:28322ms step_avg:92.86ms
step:306/1750 train_time:28416ms step_avg:92.86ms
step:307/1750 train_time:28509ms step_avg:92.86ms
step:308/1750 train_time:28602ms step_avg:92.86ms
step:309/1750 train_time:28696ms step_avg:92.87ms
step:310/1750 train_time:28789ms step_avg:92.87ms
step:311/1750 train_time:28886ms step_avg:92.88ms
step:312/1750 train_time:28977ms step_avg:92.87ms
step:313/1750 train_time:29070ms step_avg:92.87ms
step:314/1750 train_time:29164ms step_avg:92.88ms
step:315/1750 train_time:29258ms step_avg:92.88ms
step:316/1750 train_time:29351ms step_avg:92.88ms
step:317/1750 train_time:29445ms step_avg:92.89ms
step:318/1750 train_time:29542ms step_avg:92.90ms
step:319/1750 train_time:29633ms step_avg:92.89ms
step:320/1750 train_time:29726ms step_avg:92.89ms
step:321/1750 train_time:29819ms step_avg:92.90ms
step:322/1750 train_time:29913ms step_avg:92.90ms
step:323/1750 train_time:30006ms step_avg:92.90ms
step:324/1750 train_time:30100ms step_avg:92.90ms
step:325/1750 train_time:30194ms step_avg:92.91ms
step:326/1750 train_time:30288ms step_avg:92.91ms
step:327/1750 train_time:30381ms step_avg:92.91ms
step:328/1750 train_time:30475ms step_avg:92.91ms
step:329/1750 train_time:30568ms step_avg:92.91ms
step:330/1750 train_time:30663ms step_avg:92.92ms
step:331/1750 train_time:30756ms step_avg:92.92ms
step:332/1750 train_time:30854ms step_avg:92.93ms
step:333/1750 train_time:30944ms step_avg:92.92ms
step:334/1750 train_time:31040ms step_avg:92.93ms
step:335/1750 train_time:31131ms step_avg:92.93ms
step:336/1750 train_time:31225ms step_avg:92.93ms
step:337/1750 train_time:31318ms step_avg:92.93ms
step:338/1750 train_time:31411ms step_avg:92.93ms
step:339/1750 train_time:31505ms step_avg:92.93ms
step:340/1750 train_time:31598ms step_avg:92.94ms
step:341/1750 train_time:31692ms step_avg:92.94ms
step:342/1750 train_time:31786ms step_avg:92.94ms
step:343/1750 train_time:31880ms step_avg:92.94ms
step:344/1750 train_time:31973ms step_avg:92.95ms
step:345/1750 train_time:32067ms step_avg:92.95ms
step:346/1750 train_time:32161ms step_avg:92.95ms
step:347/1750 train_time:32255ms step_avg:92.95ms
step:348/1750 train_time:32352ms step_avg:92.97ms
step:349/1750 train_time:32443ms step_avg:92.96ms
step:350/1750 train_time:32536ms step_avg:92.96ms
step:351/1750 train_time:32629ms step_avg:92.96ms
step:352/1750 train_time:32723ms step_avg:92.96ms
step:353/1750 train_time:32817ms step_avg:92.97ms
step:354/1750 train_time:32910ms step_avg:92.97ms
step:355/1750 train_time:33003ms step_avg:92.97ms
step:356/1750 train_time:33098ms step_avg:92.97ms
step:357/1750 train_time:33191ms step_avg:92.97ms
step:358/1750 train_time:33285ms step_avg:92.97ms
step:359/1750 train_time:33379ms step_avg:92.98ms
step:360/1750 train_time:33472ms step_avg:92.98ms
step:361/1750 train_time:33566ms step_avg:92.98ms
step:362/1750 train_time:33661ms step_avg:92.99ms
step:363/1750 train_time:33754ms step_avg:92.99ms
step:364/1750 train_time:33849ms step_avg:92.99ms
step:365/1750 train_time:33942ms step_avg:92.99ms
step:366/1750 train_time:34036ms step_avg:93.00ms
step:367/1750 train_time:34130ms step_avg:93.00ms
step:368/1750 train_time:34226ms step_avg:93.01ms
step:369/1750 train_time:34317ms step_avg:93.00ms
step:370/1750 train_time:34413ms step_avg:93.01ms
step:371/1750 train_time:34505ms step_avg:93.00ms
step:372/1750 train_time:34598ms step_avg:93.00ms
step:373/1750 train_time:34691ms step_avg:93.01ms
step:374/1750 train_time:34785ms step_avg:93.01ms
step:375/1750 train_time:34878ms step_avg:93.01ms
step:375/1750 val_loss:3.9100 train_time:34967ms step_avg:93.24ms
step:376/1750 train_time:34992ms step_avg:93.06ms
step:377/1750 train_time:35070ms step_avg:93.02ms
step:378/1750 train_time:35165ms step_avg:93.03ms
step:379/1750 train_time:35258ms step_avg:93.03ms
step:380/1750 train_time:35351ms step_avg:93.03ms
step:381/1750 train_time:35444ms step_avg:93.03ms
step:382/1750 train_time:35537ms step_avg:93.03ms
step:383/1750 train_time:35630ms step_avg:93.03ms
step:384/1750 train_time:35727ms step_avg:93.04ms
step:385/1750 train_time:35817ms step_avg:93.03ms
step:386/1750 train_time:35911ms step_avg:93.03ms
step:387/1750 train_time:36008ms step_avg:93.05ms
step:388/1750 train_time:36101ms step_avg:93.04ms
step:389/1750 train_time:36202ms step_avg:93.07ms
step:390/1750 train_time:36294ms step_avg:93.06ms
step:391/1750 train_time:36386ms step_avg:93.06ms
step:392/1750 train_time:36479ms step_avg:93.06ms
step:393/1750 train_time:36575ms step_avg:93.07ms
step:394/1750 train_time:36670ms step_avg:93.07ms
step:395/1750 train_time:36765ms step_avg:93.08ms
step:396/1750 train_time:36860ms step_avg:93.08ms
step:397/1750 train_time:36955ms step_avg:93.09ms
step:398/1750 train_time:37051ms step_avg:93.09ms
step:399/1750 train_time:37147ms step_avg:93.10ms
step:400/1750 train_time:37243ms step_avg:93.11ms
step:401/1750 train_time:37338ms step_avg:93.11ms
step:402/1750 train_time:37433ms step_avg:93.12ms
step:403/1750 train_time:37528ms step_avg:93.12ms
step:404/1750 train_time:37623ms step_avg:93.13ms
step:405/1750 train_time:37718ms step_avg:93.13ms
step:406/1750 train_time:37815ms step_avg:93.14ms
step:407/1750 train_time:37910ms step_avg:93.15ms
step:408/1750 train_time:38006ms step_avg:93.15ms
step:409/1750 train_time:38101ms step_avg:93.16ms
step:410/1750 train_time:38197ms step_avg:93.16ms
step:411/1750 train_time:38292ms step_avg:93.17ms
step:412/1750 train_time:38388ms step_avg:93.17ms
step:413/1750 train_time:38483ms step_avg:93.18ms
step:414/1750 train_time:38578ms step_avg:93.18ms
step:415/1750 train_time:38673ms step_avg:93.19ms
step:416/1750 train_time:38769ms step_avg:93.19ms
step:417/1750 train_time:38864ms step_avg:93.20ms
step:418/1750 train_time:38960ms step_avg:93.21ms
step:419/1750 train_time:39056ms step_avg:93.21ms
step:420/1750 train_time:39151ms step_avg:93.22ms
step:421/1750 train_time:39248ms step_avg:93.23ms
step:422/1750 train_time:39343ms step_avg:93.23ms
step:423/1750 train_time:39438ms step_avg:93.23ms
step:424/1750 train_time:39533ms step_avg:93.24ms
step:425/1750 train_time:39629ms step_avg:93.25ms
step:426/1750 train_time:39725ms step_avg:93.25ms
step:427/1750 train_time:39820ms step_avg:93.26ms
step:428/1750 train_time:39916ms step_avg:93.26ms
step:429/1750 train_time:40012ms step_avg:93.27ms
step:430/1750 train_time:40107ms step_avg:93.27ms
step:431/1750 train_time:40202ms step_avg:93.28ms
step:432/1750 train_time:40297ms step_avg:93.28ms
step:433/1750 train_time:40393ms step_avg:93.29ms
step:434/1750 train_time:40489ms step_avg:93.29ms
step:435/1750 train_time:40584ms step_avg:93.30ms
step:436/1750 train_time:40680ms step_avg:93.30ms
step:437/1750 train_time:40776ms step_avg:93.31ms
step:438/1750 train_time:40872ms step_avg:93.32ms
step:439/1750 train_time:40968ms step_avg:93.32ms
step:440/1750 train_time:41064ms step_avg:93.33ms
step:441/1750 train_time:41158ms step_avg:93.33ms
step:442/1750 train_time:41256ms step_avg:93.34ms
step:443/1750 train_time:41349ms step_avg:93.34ms
step:444/1750 train_time:41444ms step_avg:93.34ms
step:445/1750 train_time:41539ms step_avg:93.35ms
step:446/1750 train_time:41635ms step_avg:93.35ms
step:447/1750 train_time:41731ms step_avg:93.36ms
step:448/1750 train_time:41827ms step_avg:93.36ms
step:449/1750 train_time:41923ms step_avg:93.37ms
step:450/1750 train_time:42019ms step_avg:93.38ms
step:451/1750 train_time:42114ms step_avg:93.38ms
step:452/1750 train_time:42209ms step_avg:93.38ms
step:453/1750 train_time:42305ms step_avg:93.39ms
step:454/1750 train_time:42399ms step_avg:93.39ms
step:455/1750 train_time:42495ms step_avg:93.40ms
step:456/1750 train_time:42590ms step_avg:93.40ms
step:457/1750 train_time:42688ms step_avg:93.41ms
step:458/1750 train_time:42781ms step_avg:93.41ms
step:459/1750 train_time:42880ms step_avg:93.42ms
step:460/1750 train_time:42973ms step_avg:93.42ms
step:461/1750 train_time:43068ms step_avg:93.42ms
step:462/1750 train_time:43167ms step_avg:93.44ms
step:463/1750 train_time:43259ms step_avg:93.43ms
step:464/1750 train_time:43355ms step_avg:93.44ms
step:465/1750 train_time:43450ms step_avg:93.44ms
step:466/1750 train_time:43545ms step_avg:93.44ms
step:467/1750 train_time:43641ms step_avg:93.45ms
step:468/1750 train_time:43742ms step_avg:93.47ms
step:469/1750 train_time:43831ms step_avg:93.46ms
step:470/1750 train_time:43927ms step_avg:93.46ms
step:471/1750 train_time:44023ms step_avg:93.47ms
step:472/1750 train_time:44119ms step_avg:93.47ms
step:473/1750 train_time:44215ms step_avg:93.48ms
step:474/1750 train_time:44310ms step_avg:93.48ms
step:475/1750 train_time:44406ms step_avg:93.49ms
step:476/1750 train_time:44501ms step_avg:93.49ms
step:477/1750 train_time:44596ms step_avg:93.49ms
step:478/1750 train_time:44693ms step_avg:93.50ms
step:479/1750 train_time:44788ms step_avg:93.50ms
step:480/1750 train_time:44883ms step_avg:93.51ms
step:481/1750 train_time:44978ms step_avg:93.51ms
step:482/1750 train_time:45074ms step_avg:93.51ms
step:483/1750 train_time:45170ms step_avg:93.52ms
step:484/1750 train_time:45266ms step_avg:93.52ms
step:485/1750 train_time:45361ms step_avg:93.53ms
step:486/1750 train_time:45457ms step_avg:93.53ms
step:487/1750 train_time:45553ms step_avg:93.54ms
step:488/1750 train_time:45648ms step_avg:93.54ms
step:489/1750 train_time:45743ms step_avg:93.54ms
step:490/1750 train_time:45839ms step_avg:93.55ms
step:491/1750 train_time:45935ms step_avg:93.55ms
step:492/1750 train_time:46031ms step_avg:93.56ms
step:493/1750 train_time:46126ms step_avg:93.56ms
step:494/1750 train_time:46222ms step_avg:93.57ms
step:495/1750 train_time:46318ms step_avg:93.57ms
step:496/1750 train_time:46414ms step_avg:93.58ms
step:497/1750 train_time:46509ms step_avg:93.58ms
step:498/1750 train_time:46604ms step_avg:93.58ms
step:499/1750 train_time:46699ms step_avg:93.59ms
step:500/1750 train_time:46796ms step_avg:93.59ms
step:500/1750 val_loss:3.7553 train_time:46886ms step_avg:93.77ms
step:501/1750 train_time:46911ms step_avg:93.64ms
step:502/1750 train_time:46996ms step_avg:93.62ms
step:503/1750 train_time:47093ms step_avg:93.62ms
step:504/1750 train_time:47189ms step_avg:93.63ms
step:505/1750 train_time:47284ms step_avg:93.63ms
step:506/1750 train_time:47379ms step_avg:93.63ms
step:507/1750 train_time:47477ms step_avg:93.64ms
step:508/1750 train_time:47568ms step_avg:93.64ms
step:509/1750 train_time:47663ms step_avg:93.64ms
step:510/1750 train_time:47757ms step_avg:93.64ms
step:511/1750 train_time:47854ms step_avg:93.65ms
step:512/1750 train_time:47950ms step_avg:93.65ms
step:513/1750 train_time:48047ms step_avg:93.66ms
step:514/1750 train_time:48144ms step_avg:93.67ms
step:515/1750 train_time:48240ms step_avg:93.67ms
step:516/1750 train_time:48334ms step_avg:93.67ms
step:517/1750 train_time:48430ms step_avg:93.68ms
step:518/1750 train_time:48525ms step_avg:93.68ms
step:519/1750 train_time:48620ms step_avg:93.68ms
step:520/1750 train_time:48716ms step_avg:93.68ms
step:521/1750 train_time:48814ms step_avg:93.69ms
step:522/1750 train_time:48908ms step_avg:93.69ms
step:523/1750 train_time:49005ms step_avg:93.70ms
step:524/1750 train_time:49101ms step_avg:93.70ms
step:525/1750 train_time:49197ms step_avg:93.71ms
step:526/1750 train_time:49293ms step_avg:93.71ms
step:527/1750 train_time:49388ms step_avg:93.72ms
step:528/1750 train_time:49485ms step_avg:93.72ms
step:529/1750 train_time:49579ms step_avg:93.72ms
step:530/1750 train_time:49675ms step_avg:93.73ms
step:531/1750 train_time:49770ms step_avg:93.73ms
step:532/1750 train_time:49866ms step_avg:93.73ms
step:533/1750 train_time:49962ms step_avg:93.74ms
step:534/1750 train_time:50057ms step_avg:93.74ms
step:535/1750 train_time:50154ms step_avg:93.74ms
step:536/1750 train_time:50249ms step_avg:93.75ms
step:537/1750 train_time:50345ms step_avg:93.75ms
step:538/1750 train_time:50441ms step_avg:93.76ms
step:539/1750 train_time:50536ms step_avg:93.76ms
step:540/1750 train_time:50634ms step_avg:93.77ms
step:541/1750 train_time:50727ms step_avg:93.77ms
step:542/1750 train_time:50826ms step_avg:93.77ms
step:543/1750 train_time:50919ms step_avg:93.77ms
step:544/1750 train_time:51016ms step_avg:93.78ms
step:545/1750 train_time:51112ms step_avg:93.78ms
step:546/1750 train_time:51208ms step_avg:93.79ms
step:547/1750 train_time:51303ms step_avg:93.79ms
step:548/1750 train_time:51399ms step_avg:93.79ms
step:549/1750 train_time:51495ms step_avg:93.80ms
step:550/1750 train_time:51590ms step_avg:93.80ms
step:551/1750 train_time:51686ms step_avg:93.80ms
step:552/1750 train_time:51783ms step_avg:93.81ms
step:553/1750 train_time:51877ms step_avg:93.81ms
step:554/1750 train_time:51973ms step_avg:93.81ms
step:555/1750 train_time:52068ms step_avg:93.82ms
step:556/1750 train_time:52164ms step_avg:93.82ms
step:557/1750 train_time:52260ms step_avg:93.82ms
step:558/1750 train_time:52355ms step_avg:93.83ms
step:559/1750 train_time:52451ms step_avg:93.83ms
step:560/1750 train_time:52547ms step_avg:93.83ms
step:561/1750 train_time:52643ms step_avg:93.84ms
step:562/1750 train_time:52738ms step_avg:93.84ms
step:563/1750 train_time:52834ms step_avg:93.84ms
step:564/1750 train_time:52930ms step_avg:93.85ms
step:565/1750 train_time:53026ms step_avg:93.85ms
step:566/1750 train_time:53121ms step_avg:93.85ms
step:567/1750 train_time:53216ms step_avg:93.86ms
step:568/1750 train_time:53312ms step_avg:93.86ms
step:569/1750 train_time:53408ms step_avg:93.86ms
step:570/1750 train_time:53504ms step_avg:93.87ms
step:571/1750 train_time:53599ms step_avg:93.87ms
step:572/1750 train_time:53695ms step_avg:93.87ms
step:573/1750 train_time:53790ms step_avg:93.87ms
step:574/1750 train_time:53886ms step_avg:93.88ms
step:575/1750 train_time:53982ms step_avg:93.88ms
step:576/1750 train_time:54077ms step_avg:93.88ms
step:577/1750 train_time:54174ms step_avg:93.89ms
step:578/1750 train_time:54270ms step_avg:93.89ms
step:579/1750 train_time:54366ms step_avg:93.90ms
step:580/1750 train_time:54462ms step_avg:93.90ms
step:581/1750 train_time:54557ms step_avg:93.90ms
step:582/1750 train_time:54653ms step_avg:93.91ms
step:583/1750 train_time:54749ms step_avg:93.91ms
step:584/1750 train_time:54846ms step_avg:93.91ms
step:585/1750 train_time:54941ms step_avg:93.92ms
step:586/1750 train_time:55037ms step_avg:93.92ms
step:587/1750 train_time:55133ms step_avg:93.92ms
step:588/1750 train_time:55229ms step_avg:93.93ms
step:589/1750 train_time:55325ms step_avg:93.93ms
step:590/1750 train_time:55420ms step_avg:93.93ms
step:591/1750 train_time:55516ms step_avg:93.94ms
step:592/1750 train_time:55612ms step_avg:93.94ms
step:593/1750 train_time:55707ms step_avg:93.94ms
step:594/1750 train_time:55803ms step_avg:93.94ms
step:595/1750 train_time:55899ms step_avg:93.95ms
step:596/1750 train_time:55994ms step_avg:93.95ms
step:597/1750 train_time:56089ms step_avg:93.95ms
step:598/1750 train_time:56185ms step_avg:93.96ms
step:599/1750 train_time:56281ms step_avg:93.96ms
step:600/1750 train_time:56376ms step_avg:93.96ms
step:601/1750 train_time:56472ms step_avg:93.96ms
step:602/1750 train_time:56567ms step_avg:93.97ms
step:603/1750 train_time:56663ms step_avg:93.97ms
step:604/1750 train_time:56758ms step_avg:93.97ms
step:605/1750 train_time:56854ms step_avg:93.97ms
step:606/1750 train_time:56953ms step_avg:93.98ms
step:607/1750 train_time:57046ms step_avg:93.98ms
step:608/1750 train_time:57142ms step_avg:93.98ms
step:609/1750 train_time:57238ms step_avg:93.99ms
step:610/1750 train_time:57334ms step_avg:93.99ms
step:611/1750 train_time:57429ms step_avg:93.99ms
step:612/1750 train_time:57524ms step_avg:93.99ms
step:613/1750 train_time:57620ms step_avg:94.00ms
step:614/1750 train_time:57715ms step_avg:94.00ms
step:615/1750 train_time:57812ms step_avg:94.00ms
step:616/1750 train_time:57907ms step_avg:94.00ms
step:617/1750 train_time:58002ms step_avg:94.01ms
step:618/1750 train_time:58098ms step_avg:94.01ms
step:619/1750 train_time:58195ms step_avg:94.01ms
step:620/1750 train_time:58291ms step_avg:94.02ms
step:621/1750 train_time:58387ms step_avg:94.02ms
step:622/1750 train_time:58482ms step_avg:94.02ms
step:623/1750 train_time:58577ms step_avg:94.02ms
step:624/1750 train_time:58673ms step_avg:94.03ms
step:625/1750 train_time:58769ms step_avg:94.03ms
step:625/1750 val_loss:3.6686 train_time:58859ms step_avg:94.17ms
step:626/1750 train_time:58884ms step_avg:94.06ms
step:627/1750 train_time:58966ms step_avg:94.04ms
step:628/1750 train_time:59063ms step_avg:94.05ms
step:629/1750 train_time:59159ms step_avg:94.05ms
step:630/1750 train_time:59254ms step_avg:94.05ms
step:631/1750 train_time:59349ms step_avg:94.06ms
step:632/1750 train_time:59444ms step_avg:94.06ms
step:633/1750 train_time:59539ms step_avg:94.06ms
step:634/1750 train_time:59634ms step_avg:94.06ms
step:635/1750 train_time:59730ms step_avg:94.06ms
step:636/1750 train_time:59826ms step_avg:94.07ms
step:637/1750 train_time:59924ms step_avg:94.07ms
step:638/1750 train_time:60020ms step_avg:94.08ms
step:639/1750 train_time:60116ms step_avg:94.08ms
step:640/1750 train_time:60212ms step_avg:94.08ms
step:641/1750 train_time:60307ms step_avg:94.08ms
step:642/1750 train_time:60402ms step_avg:94.08ms
step:643/1750 train_time:60499ms step_avg:94.09ms
step:644/1750 train_time:60594ms step_avg:94.09ms
step:645/1750 train_time:60690ms step_avg:94.09ms
step:646/1750 train_time:60786ms step_avg:94.10ms
step:647/1750 train_time:60882ms step_avg:94.10ms
step:648/1750 train_time:60978ms step_avg:94.10ms
step:649/1750 train_time:61074ms step_avg:94.11ms
step:650/1750 train_time:61171ms step_avg:94.11ms
step:651/1750 train_time:61268ms step_avg:94.11ms
step:652/1750 train_time:61365ms step_avg:94.12ms
step:653/1750 train_time:61462ms step_avg:94.12ms
step:654/1750 train_time:61559ms step_avg:94.13ms
step:655/1750 train_time:61656ms step_avg:94.13ms
step:656/1750 train_time:61753ms step_avg:94.14ms
step:657/1750 train_time:61850ms step_avg:94.14ms
step:658/1750 train_time:61948ms step_avg:94.15ms
step:659/1750 train_time:62045ms step_avg:94.15ms
step:660/1750 train_time:62142ms step_avg:94.16ms
step:661/1750 train_time:62241ms step_avg:94.16ms
step:662/1750 train_time:62338ms step_avg:94.17ms
step:663/1750 train_time:62435ms step_avg:94.17ms
step:664/1750 train_time:62532ms step_avg:94.17ms
step:665/1750 train_time:62629ms step_avg:94.18ms
step:666/1750 train_time:62727ms step_avg:94.18ms
step:667/1750 train_time:62825ms step_avg:94.19ms
step:668/1750 train_time:62922ms step_avg:94.19ms
step:669/1750 train_time:63019ms step_avg:94.20ms
step:670/1750 train_time:63118ms step_avg:94.21ms
step:671/1750 train_time:63215ms step_avg:94.21ms
step:672/1750 train_time:63311ms step_avg:94.21ms
step:673/1750 train_time:63409ms step_avg:94.22ms
step:674/1750 train_time:63506ms step_avg:94.22ms
step:675/1750 train_time:63604ms step_avg:94.23ms
step:676/1750 train_time:63701ms step_avg:94.23ms
step:677/1750 train_time:63798ms step_avg:94.24ms
step:678/1750 train_time:63895ms step_avg:94.24ms
step:679/1750 train_time:63993ms step_avg:94.25ms
step:680/1750 train_time:64091ms step_avg:94.25ms
step:681/1750 train_time:64190ms step_avg:94.26ms
step:682/1750 train_time:64288ms step_avg:94.26ms
step:683/1750 train_time:64385ms step_avg:94.27ms
step:684/1750 train_time:64482ms step_avg:94.27ms
step:685/1750 train_time:64580ms step_avg:94.28ms
step:686/1750 train_time:64676ms step_avg:94.28ms
step:687/1750 train_time:64773ms step_avg:94.28ms
step:688/1750 train_time:64870ms step_avg:94.29ms
step:689/1750 train_time:64967ms step_avg:94.29ms
step:690/1750 train_time:65064ms step_avg:94.30ms
step:691/1750 train_time:65161ms step_avg:94.30ms
step:692/1750 train_time:65259ms step_avg:94.30ms
step:693/1750 train_time:65355ms step_avg:94.31ms
step:694/1750 train_time:65452ms step_avg:94.31ms
step:695/1750 train_time:65550ms step_avg:94.32ms
step:696/1750 train_time:65647ms step_avg:94.32ms
step:697/1750 train_time:65744ms step_avg:94.32ms
step:698/1750 train_time:65842ms step_avg:94.33ms
step:699/1750 train_time:65940ms step_avg:94.33ms
step:700/1750 train_time:66036ms step_avg:94.34ms
step:701/1750 train_time:66133ms step_avg:94.34ms
step:702/1750 train_time:66230ms step_avg:94.35ms
step:703/1750 train_time:66330ms step_avg:94.35ms
step:704/1750 train_time:66428ms step_avg:94.36ms
step:705/1750 train_time:66525ms step_avg:94.36ms
step:706/1750 train_time:66622ms step_avg:94.37ms
step:707/1750 train_time:66719ms step_avg:94.37ms
step:708/1750 train_time:66816ms step_avg:94.37ms
step:709/1750 train_time:66913ms step_avg:94.38ms
step:710/1750 train_time:67011ms step_avg:94.38ms
step:711/1750 train_time:67108ms step_avg:94.39ms
step:712/1750 train_time:67205ms step_avg:94.39ms
step:713/1750 train_time:67303ms step_avg:94.39ms
step:714/1750 train_time:67400ms step_avg:94.40ms
step:715/1750 train_time:67497ms step_avg:94.40ms
step:716/1750 train_time:67594ms step_avg:94.41ms
step:717/1750 train_time:67691ms step_avg:94.41ms
step:718/1750 train_time:67789ms step_avg:94.41ms
step:719/1750 train_time:67887ms step_avg:94.42ms
step:720/1750 train_time:67986ms step_avg:94.42ms
step:721/1750 train_time:68084ms step_avg:94.43ms
step:722/1750 train_time:68182ms step_avg:94.43ms
step:723/1750 train_time:68279ms step_avg:94.44ms
step:724/1750 train_time:68375ms step_avg:94.44ms
step:725/1750 train_time:68472ms step_avg:94.44ms
step:726/1750 train_time:68570ms step_avg:94.45ms
step:727/1750 train_time:68668ms step_avg:94.45ms
step:728/1750 train_time:68766ms step_avg:94.46ms
step:729/1750 train_time:68864ms step_avg:94.46ms
step:730/1750 train_time:68961ms step_avg:94.47ms
step:731/1750 train_time:69059ms step_avg:94.47ms
step:732/1750 train_time:69155ms step_avg:94.47ms
step:733/1750 train_time:69252ms step_avg:94.48ms
step:734/1750 train_time:69350ms step_avg:94.48ms
step:735/1750 train_time:69447ms step_avg:94.49ms
step:736/1750 train_time:69544ms step_avg:94.49ms
step:737/1750 train_time:69641ms step_avg:94.49ms
step:738/1750 train_time:69739ms step_avg:94.50ms
step:739/1750 train_time:69836ms step_avg:94.50ms
step:740/1750 train_time:69933ms step_avg:94.50ms
step:741/1750 train_time:70030ms step_avg:94.51ms
step:742/1750 train_time:70129ms step_avg:94.51ms
step:743/1750 train_time:70226ms step_avg:94.52ms
step:744/1750 train_time:70323ms step_avg:94.52ms
step:745/1750 train_time:70420ms step_avg:94.52ms
step:746/1750 train_time:70518ms step_avg:94.53ms
step:747/1750 train_time:70615ms step_avg:94.53ms
step:748/1750 train_time:70712ms step_avg:94.53ms
step:749/1750 train_time:70809ms step_avg:94.54ms
step:750/1750 train_time:70906ms step_avg:94.54ms
step:750/1750 val_loss:3.6038 train_time:70999ms step_avg:94.66ms
step:751/1750 train_time:71023ms step_avg:94.57ms
step:752/1750 train_time:71106ms step_avg:94.56ms
step:753/1750 train_time:71204ms step_avg:94.56ms
step:754/1750 train_time:71301ms step_avg:94.56ms
step:755/1750 train_time:71398ms step_avg:94.57ms
step:756/1750 train_time:71495ms step_avg:94.57ms
step:757/1750 train_time:71592ms step_avg:94.57ms
step:758/1750 train_time:71689ms step_avg:94.58ms
step:759/1750 train_time:71785ms step_avg:94.58ms
step:760/1750 train_time:71882ms step_avg:94.58ms
step:761/1750 train_time:71979ms step_avg:94.58ms
step:762/1750 train_time:72076ms step_avg:94.59ms
step:763/1750 train_time:72175ms step_avg:94.59ms
step:764/1750 train_time:72273ms step_avg:94.60ms
step:765/1750 train_time:72372ms step_avg:94.60ms
step:766/1750 train_time:72469ms step_avg:94.61ms
step:767/1750 train_time:72566ms step_avg:94.61ms
step:768/1750 train_time:72663ms step_avg:94.61ms
step:769/1750 train_time:72759ms step_avg:94.62ms
step:770/1750 train_time:72858ms step_avg:94.62ms
step:771/1750 train_time:72954ms step_avg:94.62ms
step:772/1750 train_time:73052ms step_avg:94.63ms
step:773/1750 train_time:73149ms step_avg:94.63ms
step:774/1750 train_time:73249ms step_avg:94.64ms
step:775/1750 train_time:73346ms step_avg:94.64ms
step:776/1750 train_time:73446ms step_avg:94.65ms
step:777/1750 train_time:73541ms step_avg:94.65ms
step:778/1750 train_time:73637ms step_avg:94.65ms
step:779/1750 train_time:73734ms step_avg:94.65ms
step:780/1750 train_time:73833ms step_avg:94.66ms
step:781/1750 train_time:73934ms step_avg:94.67ms
step:782/1750 train_time:74028ms step_avg:94.67ms
step:783/1750 train_time:74128ms step_avg:94.67ms
step:784/1750 train_time:74224ms step_avg:94.67ms
step:785/1750 train_time:74322ms step_avg:94.68ms
step:786/1750 train_time:74419ms step_avg:94.68ms
step:787/1750 train_time:74516ms step_avg:94.68ms
step:788/1750 train_time:74614ms step_avg:94.69ms
step:789/1750 train_time:74712ms step_avg:94.69ms
step:790/1750 train_time:74810ms step_avg:94.70ms
step:791/1750 train_time:74908ms step_avg:94.70ms
step:792/1750 train_time:75006ms step_avg:94.70ms
step:793/1750 train_time:75104ms step_avg:94.71ms
step:794/1750 train_time:75201ms step_avg:94.71ms
step:795/1750 train_time:75298ms step_avg:94.71ms
step:796/1750 train_time:75396ms step_avg:94.72ms
step:797/1750 train_time:75494ms step_avg:94.72ms
step:798/1750 train_time:75591ms step_avg:94.73ms
step:799/1750 train_time:75688ms step_avg:94.73ms
step:800/1750 train_time:75785ms step_avg:94.73ms
step:801/1750 train_time:75882ms step_avg:94.73ms
step:802/1750 train_time:75980ms step_avg:94.74ms
step:803/1750 train_time:76078ms step_avg:94.74ms
step:804/1750 train_time:76175ms step_avg:94.75ms
step:805/1750 train_time:76274ms step_avg:94.75ms
step:806/1750 train_time:76373ms step_avg:94.76ms
step:807/1750 train_time:76471ms step_avg:94.76ms
step:808/1750 train_time:76569ms step_avg:94.76ms
step:809/1750 train_time:76667ms step_avg:94.77ms
step:810/1750 train_time:76764ms step_avg:94.77ms
step:811/1750 train_time:76862ms step_avg:94.77ms
step:812/1750 train_time:76959ms step_avg:94.78ms
step:813/1750 train_time:77056ms step_avg:94.78ms
step:814/1750 train_time:77155ms step_avg:94.78ms
step:815/1750 train_time:77252ms step_avg:94.79ms
step:816/1750 train_time:77350ms step_avg:94.79ms
step:817/1750 train_time:77447ms step_avg:94.79ms
step:818/1750 train_time:77546ms step_avg:94.80ms
step:819/1750 train_time:77643ms step_avg:94.80ms
step:820/1750 train_time:77740ms step_avg:94.80ms
step:821/1750 train_time:77837ms step_avg:94.81ms
step:822/1750 train_time:77934ms step_avg:94.81ms
step:823/1750 train_time:78031ms step_avg:94.81ms
step:824/1750 train_time:78129ms step_avg:94.82ms
step:825/1750 train_time:78226ms step_avg:94.82ms
step:826/1750 train_time:78325ms step_avg:94.82ms
step:827/1750 train_time:78422ms step_avg:94.83ms
step:828/1750 train_time:78519ms step_avg:94.83ms
step:829/1750 train_time:78618ms step_avg:94.83ms
step:830/1750 train_time:78716ms step_avg:94.84ms
step:831/1750 train_time:78815ms step_avg:94.84ms
step:832/1750 train_time:78912ms step_avg:94.85ms
step:833/1750 train_time:79009ms step_avg:94.85ms
step:834/1750 train_time:79106ms step_avg:94.85ms
step:835/1750 train_time:79204ms step_avg:94.85ms
step:836/1750 train_time:79301ms step_avg:94.86ms
step:837/1750 train_time:79398ms step_avg:94.86ms
step:838/1750 train_time:79496ms step_avg:94.86ms
step:839/1750 train_time:79593ms step_avg:94.87ms
step:840/1750 train_time:79691ms step_avg:94.87ms
step:841/1750 train_time:79789ms step_avg:94.87ms
step:842/1750 train_time:79886ms step_avg:94.88ms
step:843/1750 train_time:79985ms step_avg:94.88ms
step:844/1750 train_time:80082ms step_avg:94.88ms
step:845/1750 train_time:80179ms step_avg:94.89ms
step:846/1750 train_time:80276ms step_avg:94.89ms
step:847/1750 train_time:80373ms step_avg:94.89ms
step:848/1750 train_time:80471ms step_avg:94.89ms
step:849/1750 train_time:80568ms step_avg:94.90ms
step:850/1750 train_time:80666ms step_avg:94.90ms
step:851/1750 train_time:80765ms step_avg:94.91ms
step:852/1750 train_time:80862ms step_avg:94.91ms
step:853/1750 train_time:80959ms step_avg:94.91ms
step:854/1750 train_time:81056ms step_avg:94.91ms
step:855/1750 train_time:81154ms step_avg:94.92ms
step:856/1750 train_time:81251ms step_avg:94.92ms
step:857/1750 train_time:81349ms step_avg:94.92ms
step:858/1750 train_time:81447ms step_avg:94.93ms
step:859/1750 train_time:81544ms step_avg:94.93ms
step:860/1750 train_time:81641ms step_avg:94.93ms
step:861/1750 train_time:81738ms step_avg:94.93ms
step:862/1750 train_time:81836ms step_avg:94.94ms
step:863/1750 train_time:81935ms step_avg:94.94ms
step:864/1750 train_time:82033ms step_avg:94.95ms
step:865/1750 train_time:82131ms step_avg:94.95ms
step:866/1750 train_time:82228ms step_avg:94.95ms
step:867/1750 train_time:82326ms step_avg:94.96ms
step:868/1750 train_time:82424ms step_avg:94.96ms
step:869/1750 train_time:82522ms step_avg:94.96ms
step:870/1750 train_time:82618ms step_avg:94.96ms
step:871/1750 train_time:82716ms step_avg:94.97ms
step:872/1750 train_time:82814ms step_avg:94.97ms
step:873/1750 train_time:82912ms step_avg:94.97ms
step:874/1750 train_time:83009ms step_avg:94.98ms
step:875/1750 train_time:83107ms step_avg:94.98ms
step:875/1750 val_loss:3.5546 train_time:83199ms step_avg:95.08ms
step:876/1750 train_time:83227ms step_avg:95.01ms
step:877/1750 train_time:83309ms step_avg:94.99ms
step:878/1750 train_time:83407ms step_avg:95.00ms
step:879/1750 train_time:83504ms step_avg:95.00ms
step:880/1750 train_time:83601ms step_avg:95.00ms
step:881/1750 train_time:83698ms step_avg:95.00ms
step:882/1750 train_time:83795ms step_avg:95.01ms
step:883/1750 train_time:83892ms step_avg:95.01ms
step:884/1750 train_time:83989ms step_avg:95.01ms
step:885/1750 train_time:84086ms step_avg:95.01ms
step:886/1750 train_time:84185ms step_avg:95.02ms
step:887/1750 train_time:84285ms step_avg:95.02ms
step:888/1750 train_time:84383ms step_avg:95.03ms
step:889/1750 train_time:84481ms step_avg:95.03ms
step:890/1750 train_time:84578ms step_avg:95.03ms
step:891/1750 train_time:84676ms step_avg:95.03ms
step:892/1750 train_time:84772ms step_avg:95.04ms
step:893/1750 train_time:84870ms step_avg:95.04ms
step:894/1750 train_time:84966ms step_avg:95.04ms
step:895/1750 train_time:85064ms step_avg:95.04ms
step:896/1750 train_time:85161ms step_avg:95.05ms
step:897/1750 train_time:85259ms step_avg:95.05ms
step:898/1750 train_time:85356ms step_avg:95.05ms
step:899/1750 train_time:85455ms step_avg:95.06ms
step:900/1750 train_time:85552ms step_avg:95.06ms
step:901/1750 train_time:85650ms step_avg:95.06ms
step:902/1750 train_time:85748ms step_avg:95.06ms
step:903/1750 train_time:85845ms step_avg:95.07ms
step:904/1750 train_time:85942ms step_avg:95.07ms
step:905/1750 train_time:86039ms step_avg:95.07ms
step:906/1750 train_time:86136ms step_avg:95.07ms
step:907/1750 train_time:86235ms step_avg:95.08ms
step:908/1750 train_time:86333ms step_avg:95.08ms
step:909/1750 train_time:86430ms step_avg:95.08ms
step:910/1750 train_time:86530ms step_avg:95.09ms
step:911/1750 train_time:86628ms step_avg:95.09ms
step:912/1750 train_time:86727ms step_avg:95.10ms
step:913/1750 train_time:86826ms step_avg:95.10ms
step:914/1750 train_time:86924ms step_avg:95.10ms
step:915/1750 train_time:87022ms step_avg:95.11ms
step:916/1750 train_time:87121ms step_avg:95.11ms
step:917/1750 train_time:87219ms step_avg:95.11ms
step:918/1750 train_time:87319ms step_avg:95.12ms
step:919/1750 train_time:87418ms step_avg:95.12ms
step:920/1750 train_time:87517ms step_avg:95.13ms
step:921/1750 train_time:87615ms step_avg:95.13ms
step:922/1750 train_time:87714ms step_avg:95.13ms
step:923/1750 train_time:87814ms step_avg:95.14ms
step:924/1750 train_time:87912ms step_avg:95.14ms
step:925/1750 train_time:88010ms step_avg:95.15ms
step:926/1750 train_time:88109ms step_avg:95.15ms
step:927/1750 train_time:88208ms step_avg:95.15ms
step:928/1750 train_time:88308ms step_avg:95.16ms
step:929/1750 train_time:88408ms step_avg:95.16ms
step:930/1750 train_time:88508ms step_avg:95.17ms
step:931/1750 train_time:88607ms step_avg:95.17ms
step:932/1750 train_time:88707ms step_avg:95.18ms
step:933/1750 train_time:88806ms step_avg:95.18ms
step:934/1750 train_time:88905ms step_avg:95.19ms
step:935/1750 train_time:89004ms step_avg:95.19ms
step:936/1750 train_time:89103ms step_avg:95.20ms
step:937/1750 train_time:89203ms step_avg:95.20ms
step:938/1750 train_time:89301ms step_avg:95.20ms
step:939/1750 train_time:89399ms step_avg:95.21ms
step:940/1750 train_time:89497ms step_avg:95.21ms
step:941/1750 train_time:89595ms step_avg:95.21ms
step:942/1750 train_time:89694ms step_avg:95.22ms
step:943/1750 train_time:89793ms step_avg:95.22ms
step:944/1750 train_time:89892ms step_avg:95.22ms
step:945/1750 train_time:89991ms step_avg:95.23ms
step:946/1750 train_time:90091ms step_avg:95.23ms
step:947/1750 train_time:90191ms step_avg:95.24ms
step:948/1750 train_time:90291ms step_avg:95.24ms
step:949/1750 train_time:90390ms step_avg:95.25ms
step:950/1750 train_time:90490ms step_avg:95.25ms
step:951/1750 train_time:90589ms step_avg:95.26ms
step:952/1750 train_time:90688ms step_avg:95.26ms
step:953/1750 train_time:90788ms step_avg:95.27ms
step:954/1750 train_time:90887ms step_avg:95.27ms
step:955/1750 train_time:90986ms step_avg:95.27ms
step:956/1750 train_time:91085ms step_avg:95.28ms
step:957/1750 train_time:91185ms step_avg:95.28ms
step:958/1750 train_time:91285ms step_avg:95.29ms
step:959/1750 train_time:91384ms step_avg:95.29ms
step:960/1750 train_time:91483ms step_avg:95.30ms
step:961/1750 train_time:91582ms step_avg:95.30ms
step:962/1750 train_time:91680ms step_avg:95.30ms
step:963/1750 train_time:91778ms step_avg:95.30ms
step:964/1750 train_time:91876ms step_avg:95.31ms
step:965/1750 train_time:91976ms step_avg:95.31ms
step:966/1750 train_time:92074ms step_avg:95.31ms
step:967/1750 train_time:92173ms step_avg:95.32ms
step:968/1750 train_time:92272ms step_avg:95.32ms
step:969/1750 train_time:92372ms step_avg:95.33ms
step:970/1750 train_time:92473ms step_avg:95.33ms
step:971/1750 train_time:92572ms step_avg:95.34ms
step:972/1750 train_time:92672ms step_avg:95.34ms
step:973/1750 train_time:92771ms step_avg:95.35ms
step:974/1750 train_time:92869ms step_avg:95.35ms
step:975/1750 train_time:92968ms step_avg:95.35ms
step:976/1750 train_time:93067ms step_avg:95.36ms
step:977/1750 train_time:93167ms step_avg:95.36ms
step:978/1750 train_time:93266ms step_avg:95.36ms
step:979/1750 train_time:93365ms step_avg:95.37ms
step:980/1750 train_time:93464ms step_avg:95.37ms
step:981/1750 train_time:93563ms step_avg:95.38ms
step:982/1750 train_time:93662ms step_avg:95.38ms
step:983/1750 train_time:93761ms step_avg:95.38ms
step:984/1750 train_time:93859ms step_avg:95.39ms
step:985/1750 train_time:93958ms step_avg:95.39ms
step:986/1750 train_time:94056ms step_avg:95.39ms
step:987/1750 train_time:94155ms step_avg:95.40ms
step:988/1750 train_time:94254ms step_avg:95.40ms
step:989/1750 train_time:94353ms step_avg:95.40ms
step:990/1750 train_time:94453ms step_avg:95.41ms
step:991/1750 train_time:94553ms step_avg:95.41ms
step:992/1750 train_time:94652ms step_avg:95.42ms
step:993/1750 train_time:94752ms step_avg:95.42ms
step:994/1750 train_time:94851ms step_avg:95.42ms
step:995/1750 train_time:94950ms step_avg:95.43ms
step:996/1750 train_time:95048ms step_avg:95.43ms
step:997/1750 train_time:95147ms step_avg:95.43ms
step:998/1750 train_time:95247ms step_avg:95.44ms
step:999/1750 train_time:95346ms step_avg:95.44ms
step:1000/1750 train_time:95445ms step_avg:95.45ms
step:1000/1750 val_loss:3.5117 train_time:95539ms step_avg:95.54ms
step:1001/1750 train_time:95564ms step_avg:95.47ms
step:1002/1750 train_time:95649ms step_avg:95.46ms
step:1003/1750 train_time:95747ms step_avg:95.46ms
step:1004/1750 train_time:95847ms step_avg:95.47ms
step:1005/1750 train_time:95946ms step_avg:95.47ms
step:1006/1750 train_time:96044ms step_avg:95.47ms
step:1007/1750 train_time:96142ms step_avg:95.47ms
step:1008/1750 train_time:96240ms step_avg:95.48ms
step:1009/1750 train_time:96339ms step_avg:95.48ms
step:1010/1750 train_time:96439ms step_avg:95.48ms
step:1011/1750 train_time:96541ms step_avg:95.49ms
step:1012/1750 train_time:96641ms step_avg:95.50ms
step:1013/1750 train_time:96741ms step_avg:95.50ms
step:1014/1750 train_time:96841ms step_avg:95.50ms
step:1015/1750 train_time:96940ms step_avg:95.51ms
step:1016/1750 train_time:97039ms step_avg:95.51ms
step:1017/1750 train_time:97138ms step_avg:95.51ms
step:1018/1750 train_time:97237ms step_avg:95.52ms
step:1019/1750 train_time:97334ms step_avg:95.52ms
step:1020/1750 train_time:97432ms step_avg:95.52ms
step:1021/1750 train_time:97530ms step_avg:95.52ms
step:1022/1750 train_time:97630ms step_avg:95.53ms
step:1023/1750 train_time:97729ms step_avg:95.53ms
step:1024/1750 train_time:97829ms step_avg:95.54ms
step:1025/1750 train_time:97928ms step_avg:95.54ms
step:1026/1750 train_time:98028ms step_avg:95.54ms
step:1027/1750 train_time:98127ms step_avg:95.55ms
step:1028/1750 train_time:98227ms step_avg:95.55ms
step:1029/1750 train_time:98326ms step_avg:95.55ms
step:1030/1750 train_time:98425ms step_avg:95.56ms
step:1031/1750 train_time:98525ms step_avg:95.56ms
step:1032/1750 train_time:98624ms step_avg:95.57ms
step:1033/1750 train_time:98724ms step_avg:95.57ms
step:1034/1750 train_time:98823ms step_avg:95.57ms
step:1035/1750 train_time:98922ms step_avg:95.58ms
step:1036/1750 train_time:99021ms step_avg:95.58ms
step:1037/1750 train_time:99121ms step_avg:95.58ms
step:1038/1750 train_time:99221ms step_avg:95.59ms
step:1039/1750 train_time:99320ms step_avg:95.59ms
step:1040/1750 train_time:99420ms step_avg:95.60ms
step:1041/1750 train_time:99519ms step_avg:95.60ms
step:1042/1750 train_time:99619ms step_avg:95.60ms
step:1043/1750 train_time:99718ms step_avg:95.61ms
step:1044/1750 train_time:99818ms step_avg:95.61ms
step:1045/1750 train_time:99917ms step_avg:95.61ms
step:1046/1750 train_time:100017ms step_avg:95.62ms
step:1047/1750 train_time:100116ms step_avg:95.62ms
step:1048/1750 train_time:100215ms step_avg:95.62ms
step:1049/1750 train_time:100315ms step_avg:95.63ms
step:1050/1750 train_time:100414ms step_avg:95.63ms
step:1051/1750 train_time:100512ms step_avg:95.63ms
step:1052/1750 train_time:100611ms step_avg:95.64ms
step:1053/1750 train_time:100709ms step_avg:95.64ms
step:1054/1750 train_time:100808ms step_avg:95.64ms
step:1055/1750 train_time:100907ms step_avg:95.65ms
step:1056/1750 train_time:101008ms step_avg:95.65ms
step:1057/1750 train_time:101108ms step_avg:95.66ms
step:1058/1750 train_time:101208ms step_avg:95.66ms
step:1059/1750 train_time:101309ms step_avg:95.66ms
step:1060/1750 train_time:101408ms step_avg:95.67ms
step:1061/1750 train_time:101508ms step_avg:95.67ms
step:1062/1750 train_time:101606ms step_avg:95.67ms
step:1063/1750 train_time:101705ms step_avg:95.68ms
step:1064/1750 train_time:101805ms step_avg:95.68ms
step:1065/1750 train_time:101904ms step_avg:95.68ms
step:1066/1750 train_time:102004ms step_avg:95.69ms
step:1067/1750 train_time:102104ms step_avg:95.69ms
step:1068/1750 train_time:102204ms step_avg:95.70ms
step:1069/1750 train_time:102303ms step_avg:95.70ms
step:1070/1750 train_time:102403ms step_avg:95.70ms
step:1071/1750 train_time:102502ms step_avg:95.71ms
step:1072/1750 train_time:102601ms step_avg:95.71ms
step:1073/1750 train_time:102700ms step_avg:95.71ms
step:1074/1750 train_time:102799ms step_avg:95.72ms
step:1075/1750 train_time:102899ms step_avg:95.72ms
step:1076/1750 train_time:102999ms step_avg:95.72ms
step:1077/1750 train_time:103099ms step_avg:95.73ms
step:1078/1750 train_time:103199ms step_avg:95.73ms
step:1079/1750 train_time:103299ms step_avg:95.74ms
step:1080/1750 train_time:103399ms step_avg:95.74ms
step:1081/1750 train_time:103498ms step_avg:95.74ms
step:1082/1750 train_time:103598ms step_avg:95.75ms
step:1083/1750 train_time:103698ms step_avg:95.75ms
step:1084/1750 train_time:103796ms step_avg:95.75ms
step:1085/1750 train_time:103896ms step_avg:95.76ms
step:1086/1750 train_time:103994ms step_avg:95.76ms
step:1087/1750 train_time:104093ms step_avg:95.76ms
step:1088/1750 train_time:104192ms step_avg:95.76ms
step:1089/1750 train_time:104290ms step_avg:95.77ms
step:1090/1750 train_time:104390ms step_avg:95.77ms
step:1091/1750 train_time:104488ms step_avg:95.77ms
step:1092/1750 train_time:104588ms step_avg:95.78ms
step:1093/1750 train_time:104688ms step_avg:95.78ms
step:1094/1750 train_time:104789ms step_avg:95.79ms
step:1095/1750 train_time:104889ms step_avg:95.79ms
step:1096/1750 train_time:104989ms step_avg:95.79ms
step:1097/1750 train_time:105089ms step_avg:95.80ms
step:1098/1750 train_time:105189ms step_avg:95.80ms
step:1099/1750 train_time:105288ms step_avg:95.80ms
step:1100/1750 train_time:105388ms step_avg:95.81ms
step:1101/1750 train_time:105487ms step_avg:95.81ms
step:1102/1750 train_time:105586ms step_avg:95.81ms
step:1103/1750 train_time:105686ms step_avg:95.82ms
step:1104/1750 train_time:105786ms step_avg:95.82ms
step:1105/1750 train_time:105886ms step_avg:95.82ms
step:1106/1750 train_time:105986ms step_avg:95.83ms
step:1107/1750 train_time:106086ms step_avg:95.83ms
step:1108/1750 train_time:106185ms step_avg:95.84ms
step:1109/1750 train_time:106285ms step_avg:95.84ms
step:1110/1750 train_time:106384ms step_avg:95.84ms
step:1111/1750 train_time:106484ms step_avg:95.84ms
step:1112/1750 train_time:106583ms step_avg:95.85ms
step:1113/1750 train_time:106682ms step_avg:95.85ms
step:1114/1750 train_time:106781ms step_avg:95.85ms
step:1115/1750 train_time:106881ms step_avg:95.86ms
step:1116/1750 train_time:106981ms step_avg:95.86ms
step:1117/1750 train_time:107081ms step_avg:95.86ms
step:1118/1750 train_time:107180ms step_avg:95.87ms
step:1119/1750 train_time:107279ms step_avg:95.87ms
step:1120/1750 train_time:107378ms step_avg:95.87ms
step:1121/1750 train_time:107478ms step_avg:95.88ms
step:1122/1750 train_time:107577ms step_avg:95.88ms
step:1123/1750 train_time:107677ms step_avg:95.88ms
step:1124/1750 train_time:107776ms step_avg:95.89ms
step:1125/1750 train_time:107877ms step_avg:95.89ms
step:1125/1750 val_loss:3.4608 train_time:107972ms step_avg:95.98ms
step:1126/1750 train_time:107997ms step_avg:95.91ms
step:1127/1750 train_time:108083ms step_avg:95.90ms
step:1128/1750 train_time:108183ms step_avg:95.91ms
step:1129/1750 train_time:108282ms step_avg:95.91ms
step:1130/1750 train_time:108381ms step_avg:95.91ms
step:1131/1750 train_time:108480ms step_avg:95.92ms
step:1132/1750 train_time:108579ms step_avg:95.92ms
step:1133/1750 train_time:108678ms step_avg:95.92ms
step:1134/1750 train_time:108777ms step_avg:95.92ms
step:1135/1750 train_time:108875ms step_avg:95.92ms
step:1136/1750 train_time:108974ms step_avg:95.93ms
step:1137/1750 train_time:109074ms step_avg:95.93ms
step:1138/1750 train_time:109174ms step_avg:95.94ms
step:1139/1750 train_time:109275ms step_avg:95.94ms
step:1140/1750 train_time:109375ms step_avg:95.94ms
step:1141/1750 train_time:109474ms step_avg:95.95ms
step:1142/1750 train_time:109575ms step_avg:95.95ms
step:1143/1750 train_time:109673ms step_avg:95.95ms
step:1144/1750 train_time:109772ms step_avg:95.95ms
step:1145/1750 train_time:109872ms step_avg:95.96ms
step:1146/1750 train_time:109972ms step_avg:95.96ms
step:1147/1750 train_time:110070ms step_avg:95.96ms
step:1148/1750 train_time:110169ms step_avg:95.97ms
step:1149/1750 train_time:110269ms step_avg:95.97ms
step:1150/1750 train_time:110369ms step_avg:95.97ms
step:1151/1750 train_time:110469ms step_avg:95.98ms
step:1152/1750 train_time:110568ms step_avg:95.98ms
step:1153/1750 train_time:110667ms step_avg:95.98ms
step:1154/1750 train_time:110767ms step_avg:95.98ms
step:1155/1750 train_time:110865ms step_avg:95.99ms
step:1156/1750 train_time:110963ms step_avg:95.99ms
step:1157/1750 train_time:111062ms step_avg:95.99ms
step:1158/1750 train_time:111162ms step_avg:95.99ms
step:1159/1750 train_time:111261ms step_avg:96.00ms
step:1160/1750 train_time:111362ms step_avg:96.00ms
step:1161/1750 train_time:111462ms step_avg:96.01ms
step:1162/1750 train_time:111562ms step_avg:96.01ms
step:1163/1750 train_time:111661ms step_avg:96.01ms
step:1164/1750 train_time:111760ms step_avg:96.01ms
step:1165/1750 train_time:111858ms step_avg:96.02ms
step:1166/1750 train_time:111956ms step_avg:96.02ms
step:1167/1750 train_time:112055ms step_avg:96.02ms
step:1168/1750 train_time:112154ms step_avg:96.02ms
step:1169/1750 train_time:112254ms step_avg:96.03ms
step:1170/1750 train_time:112355ms step_avg:96.03ms
step:1171/1750 train_time:112455ms step_avg:96.03ms
step:1172/1750 train_time:112558ms step_avg:96.04ms
step:1173/1750 train_time:112659ms step_avg:96.04ms
step:1174/1750 train_time:112759ms step_avg:96.05ms
step:1175/1750 train_time:112860ms step_avg:96.05ms
step:1176/1750 train_time:112960ms step_avg:96.05ms
step:1177/1750 train_time:113060ms step_avg:96.06ms
step:1178/1750 train_time:113162ms step_avg:96.06ms
step:1179/1750 train_time:113262ms step_avg:96.07ms
step:1180/1750 train_time:113363ms step_avg:96.07ms
step:1181/1750 train_time:113462ms step_avg:96.07ms
step:1182/1750 train_time:113563ms step_avg:96.08ms
step:1183/1750 train_time:113664ms step_avg:96.08ms
step:1184/1750 train_time:113764ms step_avg:96.08ms
step:1185/1750 train_time:113865ms step_avg:96.09ms
step:1186/1750 train_time:113966ms step_avg:96.09ms
step:1187/1750 train_time:114065ms step_avg:96.10ms
step:1188/1750 train_time:114165ms step_avg:96.10ms
step:1189/1750 train_time:114265ms step_avg:96.10ms
step:1190/1750 train_time:114365ms step_avg:96.10ms
step:1191/1750 train_time:114465ms step_avg:96.11ms
step:1192/1750 train_time:114564ms step_avg:96.11ms
step:1193/1750 train_time:114663ms step_avg:96.11ms
step:1194/1750 train_time:114763ms step_avg:96.12ms
step:1195/1750 train_time:114863ms step_avg:96.12ms
step:1196/1750 train_time:114963ms step_avg:96.12ms
step:1197/1750 train_time:115063ms step_avg:96.13ms
step:1198/1750 train_time:115163ms step_avg:96.13ms
step:1199/1750 train_time:115263ms step_avg:96.13ms
step:1200/1750 train_time:115363ms step_avg:96.14ms
step:1201/1750 train_time:115461ms step_avg:96.14ms
step:1202/1750 train_time:115562ms step_avg:96.14ms
step:1203/1750 train_time:115661ms step_avg:96.14ms
step:1204/1750 train_time:115761ms step_avg:96.15ms
step:1205/1750 train_time:115861ms step_avg:96.15ms
step:1206/1750 train_time:115962ms step_avg:96.15ms
step:1207/1750 train_time:116063ms step_avg:96.16ms
step:1208/1750 train_time:116162ms step_avg:96.16ms
step:1209/1750 train_time:116262ms step_avg:96.16ms
step:1210/1750 train_time:116362ms step_avg:96.17ms
step:1211/1750 train_time:116462ms step_avg:96.17ms
step:1212/1750 train_time:116562ms step_avg:96.17ms
step:1213/1750 train_time:116662ms step_avg:96.18ms
step:1214/1750 train_time:116762ms step_avg:96.18ms
step:1215/1750 train_time:116862ms step_avg:96.18ms
step:1216/1750 train_time:116963ms step_avg:96.19ms
step:1217/1750 train_time:117064ms step_avg:96.19ms
step:1218/1750 train_time:117164ms step_avg:96.19ms
step:1219/1750 train_time:117265ms step_avg:96.20ms
step:1220/1750 train_time:117364ms step_avg:96.20ms
step:1221/1750 train_time:117464ms step_avg:96.20ms
step:1222/1750 train_time:117564ms step_avg:96.21ms
step:1223/1750 train_time:117664ms step_avg:96.21ms
step:1224/1750 train_time:117762ms step_avg:96.21ms
step:1225/1750 train_time:117862ms step_avg:96.21ms
step:1226/1750 train_time:117962ms step_avg:96.22ms
step:1227/1750 train_time:118063ms step_avg:96.22ms
step:1228/1750 train_time:118163ms step_avg:96.22ms
step:1229/1750 train_time:118264ms step_avg:96.23ms
step:1230/1750 train_time:118364ms step_avg:96.23ms
step:1231/1750 train_time:118463ms step_avg:96.23ms
step:1232/1750 train_time:118563ms step_avg:96.24ms
step:1233/1750 train_time:118662ms step_avg:96.24ms
step:1234/1750 train_time:118762ms step_avg:96.24ms
step:1235/1750 train_time:118863ms step_avg:96.25ms
step:1236/1750 train_time:118964ms step_avg:96.25ms
step:1237/1750 train_time:119064ms step_avg:96.25ms
step:1238/1750 train_time:119164ms step_avg:96.26ms
step:1239/1750 train_time:119263ms step_avg:96.26ms
step:1240/1750 train_time:119363ms step_avg:96.26ms
step:1241/1750 train_time:119465ms step_avg:96.27ms
step:1242/1750 train_time:119565ms step_avg:96.27ms
step:1243/1750 train_time:119664ms step_avg:96.27ms
step:1244/1750 train_time:119763ms step_avg:96.27ms
step:1245/1750 train_time:119863ms step_avg:96.28ms
step:1246/1750 train_time:119963ms step_avg:96.28ms
step:1247/1750 train_time:120063ms step_avg:96.28ms
step:1248/1750 train_time:120163ms step_avg:96.28ms
step:1249/1750 train_time:120264ms step_avg:96.29ms
step:1250/1750 train_time:120364ms step_avg:96.29ms
step:1250/1750 val_loss:3.4149 train_time:120459ms step_avg:96.37ms
step:1251/1750 train_time:120483ms step_avg:96.31ms
step:1252/1750 train_time:120570ms step_avg:96.30ms
step:1253/1750 train_time:120671ms step_avg:96.31ms
step:1254/1750 train_time:120770ms step_avg:96.31ms
step:1255/1750 train_time:120869ms step_avg:96.31ms
step:1256/1750 train_time:120968ms step_avg:96.31ms
step:1257/1750 train_time:121067ms step_avg:96.31ms
step:1258/1750 train_time:121167ms step_avg:96.32ms
step:1259/1750 train_time:121266ms step_avg:96.32ms
step:1260/1750 train_time:121366ms step_avg:96.32ms
step:1261/1750 train_time:121467ms step_avg:96.33ms
step:1262/1750 train_time:121569ms step_avg:96.33ms
step:1263/1750 train_time:121671ms step_avg:96.33ms
step:1264/1750 train_time:121770ms step_avg:96.34ms
step:1265/1750 train_time:121870ms step_avg:96.34ms
step:1266/1750 train_time:121969ms step_avg:96.34ms
step:1267/1750 train_time:122069ms step_avg:96.34ms
step:1268/1750 train_time:122168ms step_avg:96.35ms
step:1269/1750 train_time:122267ms step_avg:96.35ms
step:1270/1750 train_time:122367ms step_avg:96.35ms
step:1271/1750 train_time:122469ms step_avg:96.36ms
step:1272/1750 train_time:122569ms step_avg:96.36ms
step:1273/1750 train_time:122669ms step_avg:96.36ms
step:1274/1750 train_time:122772ms step_avg:96.37ms
step:1275/1750 train_time:122870ms step_avg:96.37ms
step:1276/1750 train_time:122970ms step_avg:96.37ms
step:1277/1750 train_time:123069ms step_avg:96.37ms
step:1278/1750 train_time:123169ms step_avg:96.38ms
step:1279/1750 train_time:123269ms step_avg:96.38ms
step:1280/1750 train_time:123368ms step_avg:96.38ms
step:1281/1750 train_time:123468ms step_avg:96.38ms
step:1282/1750 train_time:123569ms step_avg:96.39ms
step:1283/1750 train_time:123669ms step_avg:96.39ms
step:1284/1750 train_time:123769ms step_avg:96.39ms
step:1285/1750 train_time:123869ms step_avg:96.40ms
step:1286/1750 train_time:123968ms step_avg:96.40ms
step:1287/1750 train_time:124068ms step_avg:96.40ms
step:1288/1750 train_time:124168ms step_avg:96.40ms
step:1289/1750 train_time:124268ms step_avg:96.41ms
step:1290/1750 train_time:124368ms step_avg:96.41ms
step:1291/1750 train_time:124469ms step_avg:96.41ms
step:1292/1750 train_time:124569ms step_avg:96.42ms
step:1293/1750 train_time:124669ms step_avg:96.42ms
step:1294/1750 train_time:124769ms step_avg:96.42ms
step:1295/1750 train_time:124869ms step_avg:96.42ms
step:1296/1750 train_time:124969ms step_avg:96.43ms
step:1297/1750 train_time:125068ms step_avg:96.43ms
step:1298/1750 train_time:125168ms step_avg:96.43ms
step:1299/1750 train_time:125268ms step_avg:96.43ms
step:1300/1750 train_time:125369ms step_avg:96.44ms
step:1301/1750 train_time:125470ms step_avg:96.44ms
step:1302/1750 train_time:125570ms step_avg:96.44ms
step:1303/1750 train_time:125670ms step_avg:96.45ms
step:1304/1750 train_time:125769ms step_avg:96.45ms
step:1305/1750 train_time:125870ms step_avg:96.45ms
step:1306/1750 train_time:125970ms step_avg:96.45ms
step:1307/1750 train_time:126070ms step_avg:96.46ms
step:1308/1750 train_time:126170ms step_avg:96.46ms
step:1309/1750 train_time:126271ms step_avg:96.46ms
step:1310/1750 train_time:126371ms step_avg:96.47ms
step:1311/1750 train_time:126471ms step_avg:96.47ms
step:1312/1750 train_time:126570ms step_avg:96.47ms
step:1313/1750 train_time:126671ms step_avg:96.47ms
step:1314/1750 train_time:126770ms step_avg:96.48ms
step:1315/1750 train_time:126871ms step_avg:96.48ms
step:1316/1750 train_time:126970ms step_avg:96.48ms
step:1317/1750 train_time:127070ms step_avg:96.48ms
step:1318/1750 train_time:127170ms step_avg:96.49ms
step:1319/1750 train_time:127270ms step_avg:96.49ms
step:1320/1750 train_time:127371ms step_avg:96.49ms
step:1321/1750 train_time:127473ms step_avg:96.50ms
step:1322/1750 train_time:127572ms step_avg:96.50ms
step:1323/1750 train_time:127672ms step_avg:96.50ms
step:1324/1750 train_time:127772ms step_avg:96.50ms
step:1325/1750 train_time:127872ms step_avg:96.51ms
step:1326/1750 train_time:127972ms step_avg:96.51ms
step:1327/1750 train_time:128072ms step_avg:96.51ms
step:1328/1750 train_time:128171ms step_avg:96.51ms
step:1329/1750 train_time:128271ms step_avg:96.52ms
step:1330/1750 train_time:128370ms step_avg:96.52ms
step:1331/1750 train_time:128469ms step_avg:96.52ms
step:1332/1750 train_time:128570ms step_avg:96.52ms
step:1333/1750 train_time:128671ms step_avg:96.53ms
step:1334/1750 train_time:128771ms step_avg:96.53ms
step:1335/1750 train_time:128871ms step_avg:96.53ms
step:1336/1750 train_time:128972ms step_avg:96.54ms
step:1337/1750 train_time:129072ms step_avg:96.54ms
step:1338/1750 train_time:129171ms step_avg:96.54ms
step:1339/1750 train_time:129271ms step_avg:96.54ms
step:1340/1750 train_time:129371ms step_avg:96.55ms
step:1341/1750 train_time:129472ms step_avg:96.55ms
step:1342/1750 train_time:129571ms step_avg:96.55ms
step:1343/1750 train_time:129671ms step_avg:96.55ms
step:1344/1750 train_time:129773ms step_avg:96.56ms
step:1345/1750 train_time:129873ms step_avg:96.56ms
step:1346/1750 train_time:129974ms step_avg:96.56ms
step:1347/1750 train_time:130074ms step_avg:96.57ms
step:1348/1750 train_time:130174ms step_avg:96.57ms
step:1349/1750 train_time:130274ms step_avg:96.57ms
step:1350/1750 train_time:130376ms step_avg:96.57ms
step:1351/1750 train_time:130476ms step_avg:96.58ms
step:1352/1750 train_time:130576ms step_avg:96.58ms
step:1353/1750 train_time:130677ms step_avg:96.58ms
step:1354/1750 train_time:130778ms step_avg:96.59ms
step:1355/1750 train_time:130878ms step_avg:96.59ms
step:1356/1750 train_time:130980ms step_avg:96.59ms
step:1357/1750 train_time:131081ms step_avg:96.60ms
step:1358/1750 train_time:131181ms step_avg:96.60ms
step:1359/1750 train_time:131281ms step_avg:96.60ms
step:1360/1750 train_time:131382ms step_avg:96.60ms
step:1361/1750 train_time:131482ms step_avg:96.61ms
step:1362/1750 train_time:131582ms step_avg:96.61ms
step:1363/1750 train_time:131683ms step_avg:96.61ms
step:1364/1750 train_time:131784ms step_avg:96.62ms
step:1365/1750 train_time:131884ms step_avg:96.62ms
step:1366/1750 train_time:131988ms step_avg:96.62ms
step:1367/1750 train_time:132085ms step_avg:96.62ms
step:1368/1750 train_time:132185ms step_avg:96.63ms
step:1369/1750 train_time:132286ms step_avg:96.63ms
step:1370/1750 train_time:132386ms step_avg:96.63ms
step:1371/1750 train_time:132487ms step_avg:96.64ms
step:1372/1750 train_time:132586ms step_avg:96.64ms
step:1373/1750 train_time:132687ms step_avg:96.64ms
step:1374/1750 train_time:132788ms step_avg:96.64ms
step:1375/1750 train_time:132888ms step_avg:96.65ms
step:1375/1750 val_loss:3.3759 train_time:132983ms step_avg:96.71ms
step:1376/1750 train_time:133007ms step_avg:96.66ms
step:1377/1750 train_time:133097ms step_avg:96.66ms
step:1378/1750 train_time:133196ms step_avg:96.66ms
step:1379/1750 train_time:133294ms step_avg:96.66ms
step:1380/1750 train_time:133395ms step_avg:96.66ms
step:1381/1750 train_time:133494ms step_avg:96.66ms
step:1382/1750 train_time:133593ms step_avg:96.67ms
step:1383/1750 train_time:133692ms step_avg:96.67ms
step:1384/1750 train_time:133790ms step_avg:96.67ms
step:1385/1750 train_time:133891ms step_avg:96.67ms
step:1386/1750 train_time:133995ms step_avg:96.68ms
step:1387/1750 train_time:134096ms step_avg:96.68ms
step:1388/1750 train_time:134195ms step_avg:96.68ms
step:1389/1750 train_time:134296ms step_avg:96.69ms
step:1390/1750 train_time:134395ms step_avg:96.69ms
step:1391/1750 train_time:134495ms step_avg:96.69ms
step:1392/1750 train_time:134594ms step_avg:96.69ms
step:1393/1750 train_time:134693ms step_avg:96.69ms
step:1394/1750 train_time:134793ms step_avg:96.69ms
step:1395/1750 train_time:134895ms step_avg:96.70ms
step:1396/1750 train_time:134995ms step_avg:96.70ms
step:1397/1750 train_time:135096ms step_avg:96.70ms
step:1398/1750 train_time:135196ms step_avg:96.71ms
step:1399/1750 train_time:135296ms step_avg:96.71ms
step:1400/1750 train_time:135396ms step_avg:96.71ms
step:1401/1750 train_time:135496ms step_avg:96.71ms
step:1402/1750 train_time:135596ms step_avg:96.72ms
step:1403/1750 train_time:135697ms step_avg:96.72ms
step:1404/1750 train_time:135797ms step_avg:96.72ms
step:1405/1750 train_time:135897ms step_avg:96.72ms
step:1406/1750 train_time:135998ms step_avg:96.73ms
step:1407/1750 train_time:136100ms step_avg:96.73ms
step:1408/1750 train_time:136201ms step_avg:96.73ms
step:1409/1750 train_time:136303ms step_avg:96.74ms
step:1410/1750 train_time:136403ms step_avg:96.74ms
step:1411/1750 train_time:136503ms step_avg:96.74ms
step:1412/1750 train_time:136604ms step_avg:96.75ms
step:1413/1750 train_time:136704ms step_avg:96.75ms
step:1414/1750 train_time:136805ms step_avg:96.75ms
step:1415/1750 train_time:136907ms step_avg:96.75ms
step:1416/1750 train_time:137007ms step_avg:96.76ms
step:1417/1750 train_time:137108ms step_avg:96.76ms
step:1418/1750 train_time:137210ms step_avg:96.76ms
step:1419/1750 train_time:137310ms step_avg:96.77ms
step:1420/1750 train_time:137411ms step_avg:96.77ms
step:1421/1750 train_time:137512ms step_avg:96.77ms
step:1422/1750 train_time:137612ms step_avg:96.77ms
step:1423/1750 train_time:137712ms step_avg:96.78ms
step:1424/1750 train_time:137813ms step_avg:96.78ms
step:1425/1750 train_time:137912ms step_avg:96.78ms
step:1426/1750 train_time:138013ms step_avg:96.78ms
step:1427/1750 train_time:138112ms step_avg:96.79ms
step:1428/1750 train_time:138214ms step_avg:96.79ms
step:1429/1750 train_time:138314ms step_avg:96.79ms
step:1430/1750 train_time:138415ms step_avg:96.79ms
step:1431/1750 train_time:138516ms step_avg:96.80ms
step:1432/1750 train_time:138617ms step_avg:96.80ms
step:1433/1750 train_time:138717ms step_avg:96.80ms
step:1434/1750 train_time:138817ms step_avg:96.80ms
step:1435/1750 train_time:138920ms step_avg:96.81ms
step:1436/1750 train_time:139024ms step_avg:96.81ms
step:1437/1750 train_time:139126ms step_avg:96.82ms
step:1438/1750 train_time:139226ms step_avg:96.82ms
step:1439/1750 train_time:139327ms step_avg:96.82ms
step:1440/1750 train_time:139430ms step_avg:96.83ms
step:1441/1750 train_time:139537ms step_avg:96.83ms
step:1442/1750 train_time:139634ms step_avg:96.83ms
step:1443/1750 train_time:139735ms step_avg:96.84ms
step:1444/1750 train_time:139836ms step_avg:96.84ms
step:1445/1750 train_time:139937ms step_avg:96.84ms
step:1446/1750 train_time:140040ms step_avg:96.85ms
step:1447/1750 train_time:140138ms step_avg:96.85ms
step:1448/1750 train_time:140240ms step_avg:96.85ms
step:1449/1750 train_time:140341ms step_avg:96.85ms
step:1450/1750 train_time:140443ms step_avg:96.86ms
step:1451/1750 train_time:140545ms step_avg:96.86ms
step:1452/1750 train_time:140648ms step_avg:96.87ms
step:1453/1750 train_time:140750ms step_avg:96.87ms
step:1454/1750 train_time:140853ms step_avg:96.87ms
step:1455/1750 train_time:140954ms step_avg:96.88ms
step:1456/1750 train_time:141054ms step_avg:96.88ms
step:1457/1750 train_time:141155ms step_avg:96.88ms
step:1458/1750 train_time:141255ms step_avg:96.88ms
step:1459/1750 train_time:141355ms step_avg:96.88ms
step:1460/1750 train_time:141455ms step_avg:96.89ms
step:1461/1750 train_time:141557ms step_avg:96.89ms
step:1462/1750 train_time:141657ms step_avg:96.89ms
step:1463/1750 train_time:141759ms step_avg:96.90ms
step:1464/1750 train_time:141861ms step_avg:96.90ms
step:1465/1750 train_time:141962ms step_avg:96.90ms
step:1466/1750 train_time:142063ms step_avg:96.91ms
step:1467/1750 train_time:142165ms step_avg:96.91ms
step:1468/1750 train_time:142267ms step_avg:96.91ms
step:1469/1750 train_time:142368ms step_avg:96.92ms
step:1470/1750 train_time:142469ms step_avg:96.92ms
step:1471/1750 train_time:142570ms step_avg:96.92ms
step:1472/1750 train_time:142672ms step_avg:96.92ms
step:1473/1750 train_time:142774ms step_avg:96.93ms
step:1474/1750 train_time:142875ms step_avg:96.93ms
step:1475/1750 train_time:142975ms step_avg:96.93ms
step:1476/1750 train_time:143078ms step_avg:96.94ms
step:1477/1750 train_time:143179ms step_avg:96.94ms
step:1478/1750 train_time:143281ms step_avg:96.94ms
step:1479/1750 train_time:143382ms step_avg:96.95ms
step:1480/1750 train_time:143484ms step_avg:96.95ms
step:1481/1750 train_time:143585ms step_avg:96.95ms
step:1482/1750 train_time:143687ms step_avg:96.95ms
step:1483/1750 train_time:143790ms step_avg:96.96ms
step:1484/1750 train_time:143892ms step_avg:96.96ms
step:1485/1750 train_time:143996ms step_avg:96.97ms
step:1486/1750 train_time:144097ms step_avg:96.97ms
step:1487/1750 train_time:144197ms step_avg:96.97ms
step:1488/1750 train_time:144298ms step_avg:96.97ms
step:1489/1750 train_time:144399ms step_avg:96.98ms
step:1490/1750 train_time:144499ms step_avg:96.98ms
step:1491/1750 train_time:144602ms step_avg:96.98ms
step:1492/1750 train_time:144704ms step_avg:96.99ms
step:1493/1750 train_time:144806ms step_avg:96.99ms
step:1494/1750 train_time:144908ms step_avg:96.99ms
step:1495/1750 train_time:145008ms step_avg:97.00ms
step:1496/1750 train_time:145110ms step_avg:97.00ms
step:1497/1750 train_time:145210ms step_avg:97.00ms
step:1498/1750 train_time:145312ms step_avg:97.00ms
step:1499/1750 train_time:145414ms step_avg:97.01ms
step:1500/1750 train_time:145515ms step_avg:97.01ms
step:1500/1750 val_loss:3.3404 train_time:145610ms step_avg:97.07ms
step:1501/1750 train_time:145634ms step_avg:97.02ms
step:1502/1750 train_time:145724ms step_avg:97.02ms
step:1503/1750 train_time:145825ms step_avg:97.02ms
step:1504/1750 train_time:145929ms step_avg:97.03ms
step:1505/1750 train_time:146027ms step_avg:97.03ms
step:1506/1750 train_time:146128ms step_avg:97.03ms
step:1507/1750 train_time:146229ms step_avg:97.03ms
step:1508/1750 train_time:146327ms step_avg:97.03ms
step:1509/1750 train_time:146427ms step_avg:97.04ms
step:1510/1750 train_time:146527ms step_avg:97.04ms
step:1511/1750 train_time:146632ms step_avg:97.04ms
step:1512/1750 train_time:146732ms step_avg:97.04ms
step:1513/1750 train_time:146835ms step_avg:97.05ms
step:1514/1750 train_time:146936ms step_avg:97.05ms
step:1515/1750 train_time:147041ms step_avg:97.06ms
step:1516/1750 train_time:147146ms step_avg:97.06ms
step:1517/1750 train_time:147243ms step_avg:97.06ms
step:1518/1750 train_time:147344ms step_avg:97.06ms
step:1519/1750 train_time:147446ms step_avg:97.07ms
step:1520/1750 train_time:147548ms step_avg:97.07ms
step:1521/1750 train_time:147649ms step_avg:97.07ms
step:1522/1750 train_time:147750ms step_avg:97.08ms
step:1523/1750 train_time:147850ms step_avg:97.08ms
step:1524/1750 train_time:147953ms step_avg:97.08ms
step:1525/1750 train_time:148054ms step_avg:97.08ms
step:1526/1750 train_time:148156ms step_avg:97.09ms
step:1527/1750 train_time:148259ms step_avg:97.09ms
step:1528/1750 train_time:148362ms step_avg:97.10ms
step:1529/1750 train_time:148463ms step_avg:97.10ms
step:1530/1750 train_time:148564ms step_avg:97.10ms
step:1531/1750 train_time:148664ms step_avg:97.10ms
step:1532/1750 train_time:148766ms step_avg:97.11ms
step:1533/1750 train_time:148866ms step_avg:97.11ms
step:1534/1750 train_time:148968ms step_avg:97.11ms
step:1535/1750 train_time:149068ms step_avg:97.11ms
step:1536/1750 train_time:149168ms step_avg:97.11ms
step:1537/1750 train_time:149270ms step_avg:97.12ms
step:1538/1750 train_time:149371ms step_avg:97.12ms
step:1539/1750 train_time:149472ms step_avg:97.12ms
step:1540/1750 train_time:149574ms step_avg:97.13ms
step:1541/1750 train_time:149678ms step_avg:97.13ms
step:1542/1750 train_time:149781ms step_avg:97.13ms
step:1543/1750 train_time:149884ms step_avg:97.14ms
step:1544/1750 train_time:149984ms step_avg:97.14ms
step:1545/1750 train_time:150085ms step_avg:97.14ms
step:1546/1750 train_time:150187ms step_avg:97.15ms
step:1547/1750 train_time:150288ms step_avg:97.15ms
step:1548/1750 train_time:150390ms step_avg:97.15ms
step:1549/1750 train_time:150490ms step_avg:97.15ms
step:1550/1750 train_time:150592ms step_avg:97.16ms
step:1551/1750 train_time:150692ms step_avg:97.16ms
step:1552/1750 train_time:150796ms step_avg:97.16ms
step:1553/1750 train_time:150899ms step_avg:97.17ms
step:1554/1750 train_time:151000ms step_avg:97.17ms
step:1555/1750 train_time:151101ms step_avg:97.17ms
step:1556/1750 train_time:151203ms step_avg:97.17ms
step:1557/1750 train_time:151306ms step_avg:97.18ms
step:1558/1750 train_time:151408ms step_avg:97.18ms
step:1559/1750 train_time:151509ms step_avg:97.18ms
step:1560/1750 train_time:151609ms step_avg:97.19ms
step:1561/1750 train_time:151709ms step_avg:97.19ms
step:1562/1750 train_time:151811ms step_avg:97.19ms
step:1563/1750 train_time:151915ms step_avg:97.19ms
step:1564/1750 train_time:152016ms step_avg:97.20ms
step:1565/1750 train_time:152118ms step_avg:97.20ms
step:1566/1750 train_time:152220ms step_avg:97.20ms
step:1567/1750 train_time:152321ms step_avg:97.21ms
step:1568/1750 train_time:152423ms step_avg:97.21ms
step:1569/1750 train_time:152524ms step_avg:97.21ms
step:1570/1750 train_time:152626ms step_avg:97.21ms
step:1571/1750 train_time:152728ms step_avg:97.22ms
step:1572/1750 train_time:152829ms step_avg:97.22ms
step:1573/1750 train_time:152930ms step_avg:97.22ms
step:1574/1750 train_time:153032ms step_avg:97.22ms
step:1575/1750 train_time:153134ms step_avg:97.23ms
step:1576/1750 train_time:153237ms step_avg:97.23ms
step:1577/1750 train_time:153341ms step_avg:97.24ms
step:1578/1750 train_time:153442ms step_avg:97.24ms
step:1579/1750 train_time:153543ms step_avg:97.24ms
step:1580/1750 train_time:153645ms step_avg:97.24ms
step:1581/1750 train_time:153745ms step_avg:97.25ms
step:1582/1750 train_time:153846ms step_avg:97.25ms
step:1583/1750 train_time:153949ms step_avg:97.25ms
step:1584/1750 train_time:154051ms step_avg:97.25ms
step:1585/1750 train_time:154152ms step_avg:97.26ms
step:1586/1750 train_time:154256ms step_avg:97.26ms
step:1587/1750 train_time:154357ms step_avg:97.26ms
step:1588/1750 train_time:154459ms step_avg:97.27ms
step:1589/1750 train_time:154561ms step_avg:97.27ms
step:1590/1750 train_time:154662ms step_avg:97.27ms
step:1591/1750 train_time:154764ms step_avg:97.27ms
step:1592/1750 train_time:154865ms step_avg:97.28ms
step:1593/1750 train_time:154966ms step_avg:97.28ms
step:1594/1750 train_time:155072ms step_avg:97.28ms
step:1595/1750 train_time:155173ms step_avg:97.29ms
step:1596/1750 train_time:155274ms step_avg:97.29ms
step:1597/1750 train_time:155376ms step_avg:97.29ms
step:1598/1750 train_time:155479ms step_avg:97.30ms
step:1599/1750 train_time:155580ms step_avg:97.30ms
step:1600/1750 train_time:155682ms step_avg:97.30ms
step:1601/1750 train_time:155784ms step_avg:97.30ms
step:1602/1750 train_time:155884ms step_avg:97.31ms
step:1603/1750 train_time:155986ms step_avg:97.31ms
step:1604/1750 train_time:156087ms step_avg:97.31ms
step:1605/1750 train_time:156189ms step_avg:97.31ms
step:1606/1750 train_time:156289ms step_avg:97.32ms
step:1607/1750 train_time:156390ms step_avg:97.32ms
step:1608/1750 train_time:156491ms step_avg:97.32ms
step:1609/1750 train_time:156593ms step_avg:97.32ms
step:1610/1750 train_time:156698ms step_avg:97.33ms
step:1611/1750 train_time:156801ms step_avg:97.33ms
step:1612/1750 train_time:156902ms step_avg:97.33ms
step:1613/1750 train_time:157004ms step_avg:97.34ms
step:1614/1750 train_time:157105ms step_avg:97.34ms
step:1615/1750 train_time:157206ms step_avg:97.34ms
step:1616/1750 train_time:157307ms step_avg:97.34ms
step:1617/1750 train_time:157407ms step_avg:97.35ms
step:1618/1750 train_time:157508ms step_avg:97.35ms
step:1619/1750 train_time:157609ms step_avg:97.35ms
step:1620/1750 train_time:157710ms step_avg:97.35ms
step:1621/1750 train_time:157814ms step_avg:97.36ms
step:1622/1750 train_time:157916ms step_avg:97.36ms
step:1623/1750 train_time:158019ms step_avg:97.36ms
step:1624/1750 train_time:158121ms step_avg:97.37ms
step:1625/1750 train_time:158223ms step_avg:97.37ms
step:1625/1750 val_loss:3.3105 train_time:158319ms step_avg:97.43ms
step:1626/1750 train_time:158342ms step_avg:97.38ms
step:1627/1750 train_time:158429ms step_avg:97.38ms
step:1628/1750 train_time:158532ms step_avg:97.38ms
step:1629/1750 train_time:158631ms step_avg:97.38ms
step:1630/1750 train_time:158732ms step_avg:97.38ms
step:1631/1750 train_time:158832ms step_avg:97.38ms
step:1632/1750 train_time:158932ms step_avg:97.38ms
step:1633/1750 train_time:159033ms step_avg:97.39ms
step:1634/1750 train_time:159134ms step_avg:97.39ms
step:1635/1750 train_time:159234ms step_avg:97.39ms
step:1636/1750 train_time:159337ms step_avg:97.39ms
step:1637/1750 train_time:159442ms step_avg:97.40ms
step:1638/1750 train_time:159545ms step_avg:97.40ms
step:1639/1750 train_time:159645ms step_avg:97.40ms
step:1640/1750 train_time:159745ms step_avg:97.41ms
step:1641/1750 train_time:159848ms step_avg:97.41ms
step:1642/1750 train_time:159948ms step_avg:97.41ms
step:1643/1750 train_time:160049ms step_avg:97.41ms
step:1644/1750 train_time:160150ms step_avg:97.41ms
step:1645/1750 train_time:160250ms step_avg:97.42ms
step:1646/1750 train_time:160353ms step_avg:97.42ms
step:1647/1750 train_time:160455ms step_avg:97.42ms
step:1648/1750 train_time:160556ms step_avg:97.42ms
step:1649/1750 train_time:160657ms step_avg:97.43ms
step:1650/1750 train_time:160759ms step_avg:97.43ms
step:1651/1750 train_time:160861ms step_avg:97.43ms
step:1652/1750 train_time:160962ms step_avg:97.43ms
step:1653/1750 train_time:161064ms step_avg:97.44ms
step:1654/1750 train_time:161166ms step_avg:97.44ms
step:1655/1750 train_time:161267ms step_avg:97.44ms
step:1656/1750 train_time:161368ms step_avg:97.44ms
step:1657/1750 train_time:161469ms step_avg:97.45ms
step:1658/1750 train_time:161571ms step_avg:97.45ms
step:1659/1750 train_time:161675ms step_avg:97.45ms
step:1660/1750 train_time:161777ms step_avg:97.46ms
step:1661/1750 train_time:161880ms step_avg:97.46ms
step:1662/1750 train_time:161983ms step_avg:97.46ms
step:1663/1750 train_time:162085ms step_avg:97.47ms
step:1664/1750 train_time:162185ms step_avg:97.47ms
step:1665/1750 train_time:162287ms step_avg:97.47ms
step:1666/1750 train_time:162389ms step_avg:97.47ms
step:1667/1750 train_time:162490ms step_avg:97.47ms
step:1668/1750 train_time:162592ms step_avg:97.48ms
step:1669/1750 train_time:162694ms step_avg:97.48ms
step:1670/1750 train_time:162796ms step_avg:97.48ms
step:1671/1750 train_time:162896ms step_avg:97.48ms
step:1672/1750 train_time:162998ms step_avg:97.49ms
step:1673/1750 train_time:163100ms step_avg:97.49ms
step:1674/1750 train_time:163201ms step_avg:97.49ms
step:1675/1750 train_time:163304ms step_avg:97.49ms
step:1676/1750 train_time:163406ms step_avg:97.50ms
step:1677/1750 train_time:163508ms step_avg:97.50ms
step:1678/1750 train_time:163610ms step_avg:97.50ms
step:1679/1750 train_time:163710ms step_avg:97.50ms
step:1680/1750 train_time:163811ms step_avg:97.51ms
step:1681/1750 train_time:163912ms step_avg:97.51ms
step:1682/1750 train_time:164014ms step_avg:97.51ms
step:1683/1750 train_time:164114ms step_avg:97.51ms
step:1684/1750 train_time:164215ms step_avg:97.51ms
step:1685/1750 train_time:164316ms step_avg:97.52ms
step:1686/1750 train_time:164419ms step_avg:97.52ms
step:1687/1750 train_time:164522ms step_avg:97.52ms
step:1688/1750 train_time:164624ms step_avg:97.53ms
step:1689/1750 train_time:164726ms step_avg:97.53ms
step:1690/1750 train_time:164827ms step_avg:97.53ms
step:1691/1750 train_time:164929ms step_avg:97.53ms
step:1692/1750 train_time:165030ms step_avg:97.54ms
step:1693/1750 train_time:165133ms step_avg:97.54ms
step:1694/1750 train_time:165235ms step_avg:97.54ms
step:1695/1750 train_time:165337ms step_avg:97.54ms
step:1696/1750 train_time:165440ms step_avg:97.55ms
step:1697/1750 train_time:165545ms step_avg:97.55ms
step:1698/1750 train_time:165647ms step_avg:97.55ms
step:1699/1750 train_time:165749ms step_avg:97.56ms
step:1700/1750 train_time:165851ms step_avg:97.56ms
step:1701/1750 train_time:165952ms step_avg:97.56ms
step:1702/1750 train_time:166056ms step_avg:97.57ms
step:1703/1750 train_time:166158ms step_avg:97.57ms
step:1704/1750 train_time:166259ms step_avg:97.57ms
step:1705/1750 train_time:166360ms step_avg:97.57ms
step:1706/1750 train_time:166464ms step_avg:97.58ms
step:1707/1750 train_time:166567ms step_avg:97.58ms
step:1708/1750 train_time:166670ms step_avg:97.58ms
step:1709/1750 train_time:166770ms step_avg:97.58ms
step:1710/1750 train_time:166873ms step_avg:97.59ms
step:1711/1750 train_time:166977ms step_avg:97.59ms
step:1712/1750 train_time:167078ms step_avg:97.59ms
step:1713/1750 train_time:167183ms step_avg:97.60ms
step:1714/1750 train_time:167285ms step_avg:97.60ms
step:1715/1750 train_time:167391ms step_avg:97.60ms
step:1716/1750 train_time:167493ms step_avg:97.61ms
step:1717/1750 train_time:167594ms step_avg:97.61ms
step:1718/1750 train_time:167696ms step_avg:97.61ms
step:1719/1750 train_time:167801ms step_avg:97.62ms
step:1720/1750 train_time:167903ms step_avg:97.62ms
step:1721/1750 train_time:168005ms step_avg:97.62ms
step:1722/1750 train_time:168108ms step_avg:97.62ms
step:1723/1750 train_time:168209ms step_avg:97.63ms
step:1724/1750 train_time:168311ms step_avg:97.63ms
step:1725/1750 train_time:168416ms step_avg:97.63ms
step:1726/1750 train_time:168518ms step_avg:97.63ms
step:1727/1750 train_time:168620ms step_avg:97.64ms
step:1728/1750 train_time:168723ms step_avg:97.64ms
step:1729/1750 train_time:168825ms step_avg:97.64ms
step:1730/1750 train_time:168926ms step_avg:97.64ms
step:1731/1750 train_time:169030ms step_avg:97.65ms
step:1732/1750 train_time:169131ms step_avg:97.65ms
step:1733/1750 train_time:169232ms step_avg:97.65ms
step:1734/1750 train_time:169336ms step_avg:97.66ms
step:1735/1750 train_time:169437ms step_avg:97.66ms
step:1736/1750 train_time:169539ms step_avg:97.66ms
step:1737/1750 train_time:169643ms step_avg:97.66ms
step:1738/1750 train_time:169745ms step_avg:97.67ms
step:1739/1750 train_time:169847ms step_avg:97.67ms
step:1740/1750 train_time:169949ms step_avg:97.67ms
step:1741/1750 train_time:170055ms step_avg:97.68ms
step:1742/1750 train_time:170158ms step_avg:97.68ms
step:1743/1750 train_time:170261ms step_avg:97.68ms
step:1744/1750 train_time:170363ms step_avg:97.69ms
step:1745/1750 train_time:170466ms step_avg:97.69ms
step:1746/1750 train_time:170567ms step_avg:97.69ms
step:1747/1750 train_time:170670ms step_avg:97.69ms
step:1748/1750 train_time:170772ms step_avg:97.70ms
step:1749/1750 train_time:170873ms step_avg:97.70ms
step:1750/1750 train_time:170976ms step_avg:97.70ms
step:1750/1750 val_loss:3.2869 train_time:171074ms step_avg:97.76ms
peak memory allocated: 33278 MiB reserved: 49232 MiB
