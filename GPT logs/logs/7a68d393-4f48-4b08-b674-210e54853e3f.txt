import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


class NormMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, sgd_coeff=0):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, sgd_coeff=sgd_coeff)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        self.sgd_coeff = sgd_coeff

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            sgd_coeff = group["sgd_coeff"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    eps = 1e-12
                    g_normalized = grad / (grad.norm() + eps)
                    if sgd_coeff != 1:
                        update_part = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                        update = (1 - sgd_coeff) * update_part + sgd_coeff * g_normalized
                    else:
                        update = sgd_coeff * g_normalized
                    p.add_(other=update, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()
class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)
                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_reduce_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 27.5
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos : pos + max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()
    starts = []
    for i in range(1, len(boundary_positions)):
        end = boundary_positions[i].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == dist.get_world_size():
                return starts, end - pos
            start = end
    assert False # increase max_batch_span if necessary

def distributed_data_generator(filename_pattern: str, batch_size: int, align_to_bos: bool):
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    max_batch_span = 2 * batch_size if align_to_bos else batch_size # provide buffer to handle samples up to length local_batch_size
    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            batch_span = batch_size
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1750 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0)
optimizer2 = NormMuon(hidden_matrix_params, lr=0.05, momentum=0.95, weight_decay=0.0, sgd_coeff=0.5)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.11 | packaged by conda-forge | (main, Jun  4 2025, 14:45:31) [GCC 13.3.0]
Running PyTorch 2.9.0.dev20250824+cu126 compiled for CUDA 12.6
Sun Aug 24 16:32:34 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:04:00.0 Off |                    0 |
| N/A   30C    P0            115W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:23:00.0 Off |                    0 |
| N/A   28C    P0            112W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:43:00.0 Off |                    0 |
| N/A   30C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   27C    P0            113W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:84:00.0 Off |                    0 |
| N/A   31C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:A3:00.0 Off |                    0 |
| N/A   28C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:C3:00.0 Off |                    0 |
| N/A   30C    P0            115W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E4:00.0 Off |                    0 |
| N/A   27C    P0            110W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.05ms
step:1/1750 train_time:151ms step_avg:151.14ms
step:2/1750 train_time:172ms step_avg:86.05ms
step:3/1750 train_time:253ms step_avg:84.26ms
step:4/1750 train_time:344ms step_avg:86.07ms
step:5/1750 train_time:436ms step_avg:87.29ms
step:6/1750 train_time:529ms step_avg:88.11ms
step:7/1750 train_time:621ms step_avg:88.67ms
step:8/1750 train_time:713ms step_avg:89.11ms
step:9/1750 train_time:806ms step_avg:89.51ms
step:10/1750 train_time:898ms step_avg:89.78ms
step:11/1750 train_time:990ms step_avg:90.00ms
step:12/1750 train_time:1085ms step_avg:90.44ms
step:13/1750 train_time:1181ms step_avg:90.82ms
step:14/1750 train_time:1275ms step_avg:91.07ms
step:15/1750 train_time:1368ms step_avg:91.20ms
step:16/1750 train_time:1461ms step_avg:91.29ms
step:17/1750 train_time:1553ms step_avg:91.33ms
step:18/1750 train_time:1646ms step_avg:91.43ms
step:19/1750 train_time:1738ms step_avg:91.46ms
step:20/1750 train_time:1830ms step_avg:91.50ms
step:21/1750 train_time:1923ms step_avg:91.56ms
step:22/1750 train_time:2016ms step_avg:91.63ms
step:23/1750 train_time:2111ms step_avg:91.78ms
step:24/1750 train_time:2205ms step_avg:91.90ms
step:25/1750 train_time:2299ms step_avg:91.96ms
step:26/1750 train_time:2392ms step_avg:92.01ms
step:27/1750 train_time:2485ms step_avg:92.04ms
step:28/1750 train_time:2578ms step_avg:92.08ms
step:29/1750 train_time:2671ms step_avg:92.10ms
step:30/1750 train_time:2764ms step_avg:92.12ms
step:31/1750 train_time:2856ms step_avg:92.13ms
step:32/1750 train_time:2948ms step_avg:92.13ms
step:33/1750 train_time:3042ms step_avg:92.18ms
step:34/1750 train_time:3135ms step_avg:92.22ms
step:35/1750 train_time:3229ms step_avg:92.27ms
step:36/1750 train_time:3324ms step_avg:92.33ms
step:37/1750 train_time:3418ms step_avg:92.38ms
step:38/1750 train_time:3511ms step_avg:92.40ms
step:39/1750 train_time:3605ms step_avg:92.43ms
step:40/1750 train_time:3698ms step_avg:92.44ms
step:41/1750 train_time:3790ms step_avg:92.44ms
step:42/1750 train_time:3884ms step_avg:92.47ms
step:43/1750 train_time:3977ms step_avg:92.49ms
step:44/1750 train_time:4069ms step_avg:92.48ms
step:45/1750 train_time:4162ms step_avg:92.48ms
step:46/1750 train_time:4256ms step_avg:92.52ms
step:47/1750 train_time:4349ms step_avg:92.53ms
step:48/1750 train_time:4442ms step_avg:92.55ms
step:49/1750 train_time:4537ms step_avg:92.58ms
step:50/1750 train_time:4630ms step_avg:92.59ms
step:51/1750 train_time:4723ms step_avg:92.60ms
step:52/1750 train_time:4816ms step_avg:92.61ms
step:53/1750 train_time:4909ms step_avg:92.62ms
step:54/1750 train_time:5002ms step_avg:92.63ms
step:55/1750 train_time:5095ms step_avg:92.63ms
step:56/1750 train_time:5188ms step_avg:92.65ms
step:57/1750 train_time:5283ms step_avg:92.68ms
step:58/1750 train_time:5376ms step_avg:92.69ms
step:59/1750 train_time:5469ms step_avg:92.70ms
step:60/1750 train_time:5562ms step_avg:92.71ms
step:61/1750 train_time:5656ms step_avg:92.73ms
step:62/1750 train_time:5749ms step_avg:92.73ms
step:63/1750 train_time:5843ms step_avg:92.74ms
step:64/1750 train_time:5935ms step_avg:92.74ms
step:65/1750 train_time:6028ms step_avg:92.75ms
step:66/1750 train_time:6122ms step_avg:92.75ms
step:67/1750 train_time:6216ms step_avg:92.77ms
step:68/1750 train_time:6309ms step_avg:92.78ms
step:69/1750 train_time:6403ms step_avg:92.80ms
step:70/1750 train_time:6496ms step_avg:92.80ms
step:71/1750 train_time:6589ms step_avg:92.80ms
step:72/1750 train_time:6682ms step_avg:92.80ms
step:73/1750 train_time:6774ms step_avg:92.80ms
step:74/1750 train_time:6867ms step_avg:92.80ms
step:75/1750 train_time:6961ms step_avg:92.81ms
step:76/1750 train_time:7054ms step_avg:92.81ms
step:77/1750 train_time:7148ms step_avg:92.83ms
step:78/1750 train_time:7242ms step_avg:92.84ms
step:79/1750 train_time:7335ms step_avg:92.84ms
step:80/1750 train_time:7428ms step_avg:92.85ms
step:81/1750 train_time:7522ms step_avg:92.86ms
step:82/1750 train_time:7615ms step_avg:92.86ms
step:83/1750 train_time:7709ms step_avg:92.88ms
step:84/1750 train_time:7802ms step_avg:92.88ms
step:85/1750 train_time:7895ms step_avg:92.88ms
step:86/1750 train_time:7988ms step_avg:92.88ms
step:87/1750 train_time:8081ms step_avg:92.88ms
step:88/1750 train_time:8174ms step_avg:92.89ms
step:89/1750 train_time:8267ms step_avg:92.89ms
step:90/1750 train_time:8360ms step_avg:92.89ms
step:91/1750 train_time:8453ms step_avg:92.89ms
step:92/1750 train_time:8547ms step_avg:92.90ms
step:93/1750 train_time:8641ms step_avg:92.91ms
step:94/1750 train_time:8734ms step_avg:92.91ms
step:95/1750 train_time:8828ms step_avg:92.92ms
step:96/1750 train_time:8921ms step_avg:92.93ms
step:97/1750 train_time:9014ms step_avg:92.93ms
step:98/1750 train_time:9107ms step_avg:92.93ms
step:99/1750 train_time:9200ms step_avg:92.93ms
step:100/1750 train_time:9293ms step_avg:92.93ms
step:101/1750 train_time:9386ms step_avg:92.93ms
step:102/1750 train_time:9480ms step_avg:92.94ms
step:103/1750 train_time:9573ms step_avg:92.94ms
step:104/1750 train_time:9667ms step_avg:92.95ms
step:105/1750 train_time:9759ms step_avg:92.94ms
step:106/1750 train_time:9852ms step_avg:92.95ms
step:107/1750 train_time:9947ms step_avg:92.96ms
step:108/1750 train_time:10039ms step_avg:92.96ms
step:109/1750 train_time:10132ms step_avg:92.95ms
step:110/1750 train_time:10225ms step_avg:92.96ms
step:111/1750 train_time:10317ms step_avg:92.95ms
step:112/1750 train_time:10410ms step_avg:92.95ms
step:113/1750 train_time:10504ms step_avg:92.96ms
step:114/1750 train_time:10598ms step_avg:92.96ms
step:115/1750 train_time:10690ms step_avg:92.96ms
step:116/1750 train_time:10784ms step_avg:92.96ms
step:117/1750 train_time:10877ms step_avg:92.97ms
step:118/1750 train_time:10970ms step_avg:92.97ms
step:119/1750 train_time:11063ms step_avg:92.96ms
step:120/1750 train_time:11156ms step_avg:92.97ms
step:121/1750 train_time:11249ms step_avg:92.97ms
step:122/1750 train_time:11342ms step_avg:92.97ms
step:123/1750 train_time:11435ms step_avg:92.97ms
step:124/1750 train_time:11528ms step_avg:92.97ms
step:125/1750 train_time:11621ms step_avg:92.97ms
step:125/1750 val_loss:4.6629 train_time:11704ms step_avg:93.63ms
step:126/1750 train_time:11727ms step_avg:93.07ms
step:127/1750 train_time:11814ms step_avg:93.02ms
step:128/1750 train_time:11915ms step_avg:93.09ms
step:129/1750 train_time:12010ms step_avg:93.10ms
step:130/1750 train_time:12103ms step_avg:93.10ms
step:131/1750 train_time:12196ms step_avg:93.10ms
step:132/1750 train_time:12289ms step_avg:93.10ms
step:133/1750 train_time:12381ms step_avg:93.09ms
step:134/1750 train_time:12474ms step_avg:93.09ms
step:135/1750 train_time:12567ms step_avg:93.09ms
step:136/1750 train_time:12659ms step_avg:93.08ms
step:137/1750 train_time:12752ms step_avg:93.08ms
step:138/1750 train_time:12849ms step_avg:93.11ms
step:139/1750 train_time:12945ms step_avg:93.13ms
step:140/1750 train_time:13040ms step_avg:93.14ms
step:141/1750 train_time:13135ms step_avg:93.15ms
step:142/1750 train_time:13229ms step_avg:93.16ms
step:143/1750 train_time:13322ms step_avg:93.16ms
step:144/1750 train_time:13415ms step_avg:93.16ms
step:145/1750 train_time:13508ms step_avg:93.16ms
step:146/1750 train_time:13601ms step_avg:93.16ms
step:147/1750 train_time:13693ms step_avg:93.15ms
step:148/1750 train_time:13787ms step_avg:93.15ms
step:149/1750 train_time:13882ms step_avg:93.17ms
step:150/1750 train_time:13977ms step_avg:93.18ms
step:151/1750 train_time:14071ms step_avg:93.18ms
step:152/1750 train_time:14165ms step_avg:93.19ms
step:153/1750 train_time:14258ms step_avg:93.19ms
step:154/1750 train_time:14351ms step_avg:93.19ms
step:155/1750 train_time:14444ms step_avg:93.19ms
step:156/1750 train_time:14538ms step_avg:93.19ms
step:157/1750 train_time:14631ms step_avg:93.19ms
step:158/1750 train_time:14725ms step_avg:93.20ms
step:159/1750 train_time:14818ms step_avg:93.20ms
step:160/1750 train_time:14912ms step_avg:93.20ms
step:161/1750 train_time:15007ms step_avg:93.21ms
step:162/1750 train_time:15101ms step_avg:93.22ms
step:163/1750 train_time:15195ms step_avg:93.22ms
step:164/1750 train_time:15289ms step_avg:93.22ms
step:165/1750 train_time:15382ms step_avg:93.23ms
step:166/1750 train_time:15476ms step_avg:93.23ms
step:167/1750 train_time:15569ms step_avg:93.23ms
step:168/1750 train_time:15662ms step_avg:93.23ms
step:169/1750 train_time:15756ms step_avg:93.23ms
step:170/1750 train_time:15849ms step_avg:93.23ms
step:171/1750 train_time:15943ms step_avg:93.24ms
step:172/1750 train_time:16038ms step_avg:93.24ms
step:173/1750 train_time:16132ms step_avg:93.25ms
step:174/1750 train_time:16226ms step_avg:93.25ms
step:175/1750 train_time:16320ms step_avg:93.26ms
step:176/1750 train_time:16414ms step_avg:93.26ms
step:177/1750 train_time:16508ms step_avg:93.26ms
step:178/1750 train_time:16601ms step_avg:93.27ms
step:179/1750 train_time:16695ms step_avg:93.27ms
step:180/1750 train_time:16789ms step_avg:93.27ms
step:181/1750 train_time:16883ms step_avg:93.28ms
step:182/1750 train_time:16978ms step_avg:93.29ms
step:183/1750 train_time:17072ms step_avg:93.29ms
step:184/1750 train_time:17165ms step_avg:93.29ms
step:185/1750 train_time:17259ms step_avg:93.29ms
step:186/1750 train_time:17353ms step_avg:93.29ms
step:187/1750 train_time:17446ms step_avg:93.30ms
step:188/1750 train_time:17540ms step_avg:93.30ms
step:189/1750 train_time:17633ms step_avg:93.30ms
step:190/1750 train_time:17727ms step_avg:93.30ms
step:191/1750 train_time:17821ms step_avg:93.30ms
step:192/1750 train_time:17915ms step_avg:93.31ms
step:193/1750 train_time:18009ms step_avg:93.31ms
step:194/1750 train_time:18103ms step_avg:93.32ms
step:195/1750 train_time:18197ms step_avg:93.32ms
step:196/1750 train_time:18290ms step_avg:93.32ms
step:197/1750 train_time:18385ms step_avg:93.32ms
step:198/1750 train_time:18479ms step_avg:93.33ms
step:199/1750 train_time:18572ms step_avg:93.33ms
step:200/1750 train_time:18665ms step_avg:93.32ms
step:201/1750 train_time:18759ms step_avg:93.33ms
step:202/1750 train_time:18853ms step_avg:93.33ms
step:203/1750 train_time:18947ms step_avg:93.33ms
step:204/1750 train_time:19040ms step_avg:93.33ms
step:205/1750 train_time:19134ms step_avg:93.33ms
step:206/1750 train_time:19227ms step_avg:93.33ms
step:207/1750 train_time:19320ms step_avg:93.33ms
step:208/1750 train_time:19414ms step_avg:93.34ms
step:209/1750 train_time:19507ms step_avg:93.34ms
step:210/1750 train_time:19602ms step_avg:93.34ms
step:211/1750 train_time:19695ms step_avg:93.34ms
step:212/1750 train_time:19789ms step_avg:93.34ms
step:213/1750 train_time:19883ms step_avg:93.35ms
step:214/1750 train_time:19978ms step_avg:93.35ms
step:215/1750 train_time:20071ms step_avg:93.36ms
step:216/1750 train_time:20165ms step_avg:93.36ms
step:217/1750 train_time:20258ms step_avg:93.35ms
step:218/1750 train_time:20352ms step_avg:93.36ms
step:219/1750 train_time:20445ms step_avg:93.36ms
step:220/1750 train_time:20539ms step_avg:93.36ms
step:221/1750 train_time:20632ms step_avg:93.36ms
step:222/1750 train_time:20726ms step_avg:93.36ms
step:223/1750 train_time:20819ms step_avg:93.36ms
step:224/1750 train_time:20913ms step_avg:93.36ms
step:225/1750 train_time:21007ms step_avg:93.37ms
step:226/1750 train_time:21101ms step_avg:93.37ms
step:227/1750 train_time:21195ms step_avg:93.37ms
step:228/1750 train_time:21288ms step_avg:93.37ms
step:229/1750 train_time:21382ms step_avg:93.37ms
step:230/1750 train_time:21476ms step_avg:93.37ms
step:231/1750 train_time:21569ms step_avg:93.37ms
step:232/1750 train_time:21663ms step_avg:93.38ms
step:233/1750 train_time:21757ms step_avg:93.38ms
step:234/1750 train_time:21851ms step_avg:93.38ms
step:235/1750 train_time:21944ms step_avg:93.38ms
step:236/1750 train_time:22038ms step_avg:93.38ms
step:237/1750 train_time:22132ms step_avg:93.38ms
step:238/1750 train_time:22225ms step_avg:93.38ms
step:239/1750 train_time:22318ms step_avg:93.38ms
step:240/1750 train_time:22412ms step_avg:93.38ms
step:241/1750 train_time:22506ms step_avg:93.39ms
step:242/1750 train_time:22599ms step_avg:93.39ms
step:243/1750 train_time:22694ms step_avg:93.39ms
step:244/1750 train_time:22786ms step_avg:93.39ms
step:245/1750 train_time:22880ms step_avg:93.39ms
step:246/1750 train_time:22974ms step_avg:93.39ms
step:247/1750 train_time:23068ms step_avg:93.39ms
step:248/1750 train_time:23161ms step_avg:93.39ms
step:249/1750 train_time:23255ms step_avg:93.39ms
step:250/1750 train_time:23348ms step_avg:93.39ms
step:250/1750 val_loss:4.1037 train_time:23432ms step_avg:93.73ms
step:251/1750 train_time:23454ms step_avg:93.44ms
step:252/1750 train_time:23544ms step_avg:93.43ms
step:253/1750 train_time:23645ms step_avg:93.46ms
step:254/1750 train_time:23741ms step_avg:93.47ms
step:255/1750 train_time:23834ms step_avg:93.47ms
step:256/1750 train_time:23927ms step_avg:93.46ms
step:257/1750 train_time:24019ms step_avg:93.46ms
step:258/1750 train_time:24112ms step_avg:93.46ms
step:259/1750 train_time:24204ms step_avg:93.45ms
step:260/1750 train_time:24297ms step_avg:93.45ms
step:261/1750 train_time:24392ms step_avg:93.46ms
step:262/1750 train_time:24489ms step_avg:93.47ms
step:263/1750 train_time:24586ms step_avg:93.48ms
step:264/1750 train_time:24681ms step_avg:93.49ms
step:265/1750 train_time:24776ms step_avg:93.50ms
step:266/1750 train_time:24870ms step_avg:93.50ms
step:267/1750 train_time:24963ms step_avg:93.50ms
step:268/1750 train_time:25057ms step_avg:93.50ms
step:269/1750 train_time:25151ms step_avg:93.50ms
step:270/1750 train_time:25244ms step_avg:93.50ms
step:271/1750 train_time:25337ms step_avg:93.49ms
step:272/1750 train_time:25432ms step_avg:93.50ms
step:273/1750 train_time:25527ms step_avg:93.51ms
step:274/1750 train_time:25622ms step_avg:93.51ms
step:275/1750 train_time:25718ms step_avg:93.52ms
step:276/1750 train_time:25812ms step_avg:93.52ms
step:277/1750 train_time:25906ms step_avg:93.52ms
step:278/1750 train_time:25999ms step_avg:93.52ms
step:279/1750 train_time:26093ms step_avg:93.52ms
step:280/1750 train_time:26187ms step_avg:93.52ms
step:281/1750 train_time:26281ms step_avg:93.53ms
step:282/1750 train_time:26375ms step_avg:93.53ms
step:283/1750 train_time:26469ms step_avg:93.53ms
step:284/1750 train_time:26563ms step_avg:93.53ms
step:285/1750 train_time:26658ms step_avg:93.54ms
step:286/1750 train_time:26753ms step_avg:93.54ms
step:287/1750 train_time:26846ms step_avg:93.54ms
step:288/1750 train_time:26940ms step_avg:93.54ms
step:289/1750 train_time:27034ms step_avg:93.54ms
step:290/1750 train_time:27128ms step_avg:93.55ms
step:291/1750 train_time:27222ms step_avg:93.55ms
step:292/1750 train_time:27317ms step_avg:93.55ms
step:293/1750 train_time:27411ms step_avg:93.55ms
step:294/1750 train_time:27506ms step_avg:93.56ms
step:295/1750 train_time:27600ms step_avg:93.56ms
step:296/1750 train_time:27695ms step_avg:93.56ms
step:297/1750 train_time:27790ms step_avg:93.57ms
step:298/1750 train_time:27884ms step_avg:93.57ms
step:299/1750 train_time:27978ms step_avg:93.57ms
step:300/1750 train_time:28072ms step_avg:93.57ms
step:301/1750 train_time:28166ms step_avg:93.57ms
step:302/1750 train_time:28260ms step_avg:93.57ms
step:303/1750 train_time:28354ms step_avg:93.58ms
step:304/1750 train_time:28448ms step_avg:93.58ms
step:305/1750 train_time:28543ms step_avg:93.58ms
step:306/1750 train_time:28637ms step_avg:93.59ms
step:307/1750 train_time:28732ms step_avg:93.59ms
step:308/1750 train_time:28827ms step_avg:93.59ms
step:309/1750 train_time:28920ms step_avg:93.59ms
step:310/1750 train_time:29015ms step_avg:93.60ms
step:311/1750 train_time:29109ms step_avg:93.60ms
step:312/1750 train_time:29204ms step_avg:93.60ms
step:313/1750 train_time:29298ms step_avg:93.60ms
step:314/1750 train_time:29392ms step_avg:93.60ms
step:315/1750 train_time:29486ms step_avg:93.61ms
step:316/1750 train_time:29582ms step_avg:93.61ms
step:317/1750 train_time:29676ms step_avg:93.62ms
step:318/1750 train_time:29771ms step_avg:93.62ms
step:319/1750 train_time:29865ms step_avg:93.62ms
step:320/1750 train_time:29960ms step_avg:93.62ms
step:321/1750 train_time:30054ms step_avg:93.63ms
step:322/1750 train_time:30149ms step_avg:93.63ms
step:323/1750 train_time:30243ms step_avg:93.63ms
step:324/1750 train_time:30336ms step_avg:93.63ms
step:325/1750 train_time:30431ms step_avg:93.63ms
step:326/1750 train_time:30525ms step_avg:93.63ms
step:327/1750 train_time:30619ms step_avg:93.64ms
step:328/1750 train_time:30715ms step_avg:93.64ms
step:329/1750 train_time:30809ms step_avg:93.64ms
step:330/1750 train_time:30903ms step_avg:93.65ms
step:331/1750 train_time:30998ms step_avg:93.65ms
step:332/1750 train_time:31092ms step_avg:93.65ms
step:333/1750 train_time:31186ms step_avg:93.65ms
step:334/1750 train_time:31280ms step_avg:93.65ms
step:335/1750 train_time:31374ms step_avg:93.65ms
step:336/1750 train_time:31469ms step_avg:93.66ms
step:337/1750 train_time:31563ms step_avg:93.66ms
step:338/1750 train_time:31657ms step_avg:93.66ms
step:339/1750 train_time:31752ms step_avg:93.66ms
step:340/1750 train_time:31847ms step_avg:93.67ms
step:341/1750 train_time:31941ms step_avg:93.67ms
step:342/1750 train_time:32034ms step_avg:93.67ms
step:343/1750 train_time:32128ms step_avg:93.67ms
step:344/1750 train_time:32222ms step_avg:93.67ms
step:345/1750 train_time:32317ms step_avg:93.67ms
step:346/1750 train_time:32411ms step_avg:93.67ms
step:347/1750 train_time:32505ms step_avg:93.67ms
step:348/1750 train_time:32599ms step_avg:93.67ms
step:349/1750 train_time:32693ms step_avg:93.68ms
step:350/1750 train_time:32787ms step_avg:93.68ms
step:351/1750 train_time:32882ms step_avg:93.68ms
step:352/1750 train_time:32976ms step_avg:93.68ms
step:353/1750 train_time:33071ms step_avg:93.68ms
step:354/1750 train_time:33164ms step_avg:93.68ms
step:355/1750 train_time:33258ms step_avg:93.69ms
step:356/1750 train_time:33352ms step_avg:93.69ms
step:357/1750 train_time:33447ms step_avg:93.69ms
step:358/1750 train_time:33541ms step_avg:93.69ms
step:359/1750 train_time:33635ms step_avg:93.69ms
step:360/1750 train_time:33729ms step_avg:93.69ms
step:361/1750 train_time:33823ms step_avg:93.69ms
step:362/1750 train_time:33918ms step_avg:93.70ms
step:363/1750 train_time:34012ms step_avg:93.70ms
step:364/1750 train_time:34107ms step_avg:93.70ms
step:365/1750 train_time:34201ms step_avg:93.70ms
step:366/1750 train_time:34295ms step_avg:93.70ms
step:367/1750 train_time:34389ms step_avg:93.70ms
step:368/1750 train_time:34483ms step_avg:93.71ms
step:369/1750 train_time:34578ms step_avg:93.71ms
step:370/1750 train_time:34673ms step_avg:93.71ms
step:371/1750 train_time:34767ms step_avg:93.71ms
step:372/1750 train_time:34860ms step_avg:93.71ms
step:373/1750 train_time:34955ms step_avg:93.71ms
step:374/1750 train_time:35049ms step_avg:93.71ms
step:375/1750 train_time:35144ms step_avg:93.72ms
step:375/1750 val_loss:3.8919 train_time:35227ms step_avg:93.94ms
step:376/1750 train_time:35249ms step_avg:93.75ms
step:377/1750 train_time:35340ms step_avg:93.74ms
step:378/1750 train_time:35440ms step_avg:93.76ms
step:379/1750 train_time:35535ms step_avg:93.76ms
step:380/1750 train_time:35628ms step_avg:93.76ms
step:381/1750 train_time:35721ms step_avg:93.76ms
step:382/1750 train_time:35815ms step_avg:93.76ms
step:383/1750 train_time:35908ms step_avg:93.75ms
step:384/1750 train_time:36001ms step_avg:93.75ms
step:385/1750 train_time:36094ms step_avg:93.75ms
step:386/1750 train_time:36188ms step_avg:93.75ms
step:387/1750 train_time:36284ms step_avg:93.76ms
step:388/1750 train_time:36380ms step_avg:93.76ms
step:389/1750 train_time:36476ms step_avg:93.77ms
step:390/1750 train_time:36572ms step_avg:93.77ms
step:391/1750 train_time:36668ms step_avg:93.78ms
step:392/1750 train_time:36763ms step_avg:93.78ms
step:393/1750 train_time:36859ms step_avg:93.79ms
step:394/1750 train_time:36954ms step_avg:93.79ms
step:395/1750 train_time:37049ms step_avg:93.79ms
step:396/1750 train_time:37144ms step_avg:93.80ms
step:397/1750 train_time:37241ms step_avg:93.81ms
step:398/1750 train_time:37338ms step_avg:93.81ms
step:399/1750 train_time:37434ms step_avg:93.82ms
step:400/1750 train_time:37531ms step_avg:93.83ms
step:401/1750 train_time:37627ms step_avg:93.83ms
step:402/1750 train_time:37722ms step_avg:93.84ms
step:403/1750 train_time:37817ms step_avg:93.84ms
step:404/1750 train_time:37914ms step_avg:93.85ms
step:405/1750 train_time:38009ms step_avg:93.85ms
step:406/1750 train_time:38104ms step_avg:93.85ms
step:407/1750 train_time:38200ms step_avg:93.86ms
step:408/1750 train_time:38297ms step_avg:93.86ms
step:409/1750 train_time:38393ms step_avg:93.87ms
step:410/1750 train_time:38489ms step_avg:93.88ms
step:411/1750 train_time:38586ms step_avg:93.88ms
step:412/1750 train_time:38682ms step_avg:93.89ms
step:413/1750 train_time:38778ms step_avg:93.89ms
step:414/1750 train_time:38874ms step_avg:93.90ms
step:415/1750 train_time:38969ms step_avg:93.90ms
step:416/1750 train_time:39064ms step_avg:93.90ms
step:417/1750 train_time:39159ms step_avg:93.91ms
step:418/1750 train_time:39255ms step_avg:93.91ms
step:419/1750 train_time:39351ms step_avg:93.92ms
step:420/1750 train_time:39448ms step_avg:93.92ms
step:421/1750 train_time:39544ms step_avg:93.93ms
step:422/1750 train_time:39640ms step_avg:93.93ms
step:423/1750 train_time:39736ms step_avg:93.94ms
step:424/1750 train_time:39832ms step_avg:93.94ms
step:425/1750 train_time:39928ms step_avg:93.95ms
step:426/1750 train_time:40023ms step_avg:93.95ms
step:427/1750 train_time:40119ms step_avg:93.95ms
step:428/1750 train_time:40215ms step_avg:93.96ms
step:429/1750 train_time:40311ms step_avg:93.97ms
step:430/1750 train_time:40408ms step_avg:93.97ms
step:431/1750 train_time:40504ms step_avg:93.98ms
step:432/1750 train_time:40600ms step_avg:93.98ms
step:433/1750 train_time:40697ms step_avg:93.99ms
step:434/1750 train_time:40793ms step_avg:93.99ms
step:435/1750 train_time:40888ms step_avg:94.00ms
step:436/1750 train_time:40984ms step_avg:94.00ms
step:437/1750 train_time:41080ms step_avg:94.00ms
step:438/1750 train_time:41176ms step_avg:94.01ms
step:439/1750 train_time:41271ms step_avg:94.01ms
step:440/1750 train_time:41367ms step_avg:94.02ms
step:441/1750 train_time:41463ms step_avg:94.02ms
step:442/1750 train_time:41560ms step_avg:94.03ms
step:443/1750 train_time:41657ms step_avg:94.03ms
step:444/1750 train_time:41753ms step_avg:94.04ms
step:445/1750 train_time:41850ms step_avg:94.04ms
step:446/1750 train_time:41945ms step_avg:94.05ms
step:447/1750 train_time:42041ms step_avg:94.05ms
step:448/1750 train_time:42137ms step_avg:94.06ms
step:449/1750 train_time:42232ms step_avg:94.06ms
step:450/1750 train_time:42329ms step_avg:94.06ms
step:451/1750 train_time:42425ms step_avg:94.07ms
step:452/1750 train_time:42521ms step_avg:94.07ms
step:453/1750 train_time:42617ms step_avg:94.08ms
step:454/1750 train_time:42713ms step_avg:94.08ms
step:455/1750 train_time:42809ms step_avg:94.09ms
step:456/1750 train_time:42905ms step_avg:94.09ms
step:457/1750 train_time:43000ms step_avg:94.09ms
step:458/1750 train_time:43097ms step_avg:94.10ms
step:459/1750 train_time:43192ms step_avg:94.10ms
step:460/1750 train_time:43288ms step_avg:94.10ms
step:461/1750 train_time:43384ms step_avg:94.11ms
step:462/1750 train_time:43480ms step_avg:94.11ms
step:463/1750 train_time:43576ms step_avg:94.12ms
step:464/1750 train_time:43672ms step_avg:94.12ms
step:465/1750 train_time:43769ms step_avg:94.13ms
step:466/1750 train_time:43864ms step_avg:94.13ms
step:467/1750 train_time:43960ms step_avg:94.13ms
step:468/1750 train_time:44057ms step_avg:94.14ms
step:469/1750 train_time:44153ms step_avg:94.14ms
step:470/1750 train_time:44249ms step_avg:94.15ms
step:471/1750 train_time:44345ms step_avg:94.15ms
step:472/1750 train_time:44440ms step_avg:94.15ms
step:473/1750 train_time:44537ms step_avg:94.16ms
step:474/1750 train_time:44633ms step_avg:94.16ms
step:475/1750 train_time:44729ms step_avg:94.17ms
step:476/1750 train_time:44825ms step_avg:94.17ms
step:477/1750 train_time:44921ms step_avg:94.17ms
step:478/1750 train_time:45017ms step_avg:94.18ms
step:479/1750 train_time:45113ms step_avg:94.18ms
step:480/1750 train_time:45209ms step_avg:94.18ms
step:481/1750 train_time:45304ms step_avg:94.19ms
step:482/1750 train_time:45399ms step_avg:94.19ms
step:483/1750 train_time:45496ms step_avg:94.19ms
step:484/1750 train_time:45591ms step_avg:94.20ms
step:485/1750 train_time:45688ms step_avg:94.20ms
step:486/1750 train_time:45783ms step_avg:94.20ms
step:487/1750 train_time:45879ms step_avg:94.21ms
step:488/1750 train_time:45975ms step_avg:94.21ms
step:489/1750 train_time:46070ms step_avg:94.21ms
step:490/1750 train_time:46166ms step_avg:94.22ms
step:491/1750 train_time:46262ms step_avg:94.22ms
step:492/1750 train_time:46358ms step_avg:94.22ms
step:493/1750 train_time:46454ms step_avg:94.23ms
step:494/1750 train_time:46549ms step_avg:94.23ms
step:495/1750 train_time:46645ms step_avg:94.23ms
step:496/1750 train_time:46741ms step_avg:94.24ms
step:497/1750 train_time:46837ms step_avg:94.24ms
step:498/1750 train_time:46934ms step_avg:94.25ms
step:499/1750 train_time:47030ms step_avg:94.25ms
step:500/1750 train_time:47127ms step_avg:94.25ms
step:500/1750 val_loss:3.7449 train_time:47212ms step_avg:94.42ms
step:501/1750 train_time:47233ms step_avg:94.28ms
step:502/1750 train_time:47326ms step_avg:94.28ms
step:503/1750 train_time:47424ms step_avg:94.28ms
step:504/1750 train_time:47521ms step_avg:94.29ms
step:505/1750 train_time:47617ms step_avg:94.29ms
step:506/1750 train_time:47712ms step_avg:94.29ms
step:507/1750 train_time:47807ms step_avg:94.29ms
step:508/1750 train_time:47902ms step_avg:94.29ms
step:509/1750 train_time:47997ms step_avg:94.30ms
step:510/1750 train_time:48092ms step_avg:94.30ms
step:511/1750 train_time:48189ms step_avg:94.30ms
step:512/1750 train_time:48287ms step_avg:94.31ms
step:513/1750 train_time:48385ms step_avg:94.32ms
step:514/1750 train_time:48481ms step_avg:94.32ms
step:515/1750 train_time:48577ms step_avg:94.32ms
step:516/1750 train_time:48673ms step_avg:94.33ms
step:517/1750 train_time:48769ms step_avg:94.33ms
step:518/1750 train_time:48864ms step_avg:94.33ms
step:519/1750 train_time:48959ms step_avg:94.33ms
step:520/1750 train_time:49054ms step_avg:94.33ms
step:521/1750 train_time:49150ms step_avg:94.34ms
step:522/1750 train_time:49247ms step_avg:94.34ms
step:523/1750 train_time:49344ms step_avg:94.35ms
step:524/1750 train_time:49441ms step_avg:94.35ms
step:525/1750 train_time:49538ms step_avg:94.36ms
step:526/1750 train_time:49635ms step_avg:94.36ms
step:527/1750 train_time:49732ms step_avg:94.37ms
step:528/1750 train_time:49827ms step_avg:94.37ms
step:529/1750 train_time:49924ms step_avg:94.37ms
step:530/1750 train_time:50019ms step_avg:94.38ms
step:531/1750 train_time:50115ms step_avg:94.38ms
step:532/1750 train_time:50212ms step_avg:94.38ms
step:533/1750 train_time:50309ms step_avg:94.39ms
step:534/1750 train_time:50406ms step_avg:94.39ms
step:535/1750 train_time:50503ms step_avg:94.40ms
step:536/1750 train_time:50600ms step_avg:94.40ms
step:537/1750 train_time:50698ms step_avg:94.41ms
step:538/1750 train_time:50794ms step_avg:94.41ms
step:539/1750 train_time:50889ms step_avg:94.41ms
step:540/1750 train_time:50986ms step_avg:94.42ms
step:541/1750 train_time:51081ms step_avg:94.42ms
step:542/1750 train_time:51177ms step_avg:94.42ms
step:543/1750 train_time:51274ms step_avg:94.43ms
step:544/1750 train_time:51371ms step_avg:94.43ms
step:545/1750 train_time:51469ms step_avg:94.44ms
step:546/1750 train_time:51565ms step_avg:94.44ms
step:547/1750 train_time:51662ms step_avg:94.45ms
step:548/1750 train_time:51758ms step_avg:94.45ms
step:549/1750 train_time:51855ms step_avg:94.45ms
step:550/1750 train_time:51951ms step_avg:94.46ms
step:551/1750 train_time:52047ms step_avg:94.46ms
step:552/1750 train_time:52143ms step_avg:94.46ms
step:553/1750 train_time:52239ms step_avg:94.47ms
step:554/1750 train_time:52336ms step_avg:94.47ms
step:555/1750 train_time:52433ms step_avg:94.47ms
step:556/1750 train_time:52529ms step_avg:94.48ms
step:557/1750 train_time:52626ms step_avg:94.48ms
step:558/1750 train_time:52722ms step_avg:94.48ms
step:559/1750 train_time:52819ms step_avg:94.49ms
step:560/1750 train_time:52916ms step_avg:94.49ms
step:561/1750 train_time:53011ms step_avg:94.49ms
step:562/1750 train_time:53107ms step_avg:94.50ms
step:563/1750 train_time:53203ms step_avg:94.50ms
step:564/1750 train_time:53300ms step_avg:94.50ms
step:565/1750 train_time:53397ms step_avg:94.51ms
step:566/1750 train_time:53494ms step_avg:94.51ms
step:567/1750 train_time:53590ms step_avg:94.52ms
step:568/1750 train_time:53687ms step_avg:94.52ms
step:569/1750 train_time:53784ms step_avg:94.52ms
step:570/1750 train_time:53880ms step_avg:94.53ms
step:571/1750 train_time:53976ms step_avg:94.53ms
step:572/1750 train_time:54073ms step_avg:94.53ms
step:573/1750 train_time:54169ms step_avg:94.54ms
step:574/1750 train_time:54266ms step_avg:94.54ms
step:575/1750 train_time:54363ms step_avg:94.54ms
step:576/1750 train_time:54458ms step_avg:94.54ms
step:577/1750 train_time:54554ms step_avg:94.55ms
step:578/1750 train_time:54650ms step_avg:94.55ms
step:579/1750 train_time:54747ms step_avg:94.55ms
step:580/1750 train_time:54844ms step_avg:94.56ms
step:581/1750 train_time:54940ms step_avg:94.56ms
step:582/1750 train_time:55037ms step_avg:94.57ms
step:583/1750 train_time:55134ms step_avg:94.57ms
step:584/1750 train_time:55230ms step_avg:94.57ms
step:585/1750 train_time:55327ms step_avg:94.58ms
step:586/1750 train_time:55423ms step_avg:94.58ms
step:587/1750 train_time:55519ms step_avg:94.58ms
step:588/1750 train_time:55616ms step_avg:94.58ms
step:589/1750 train_time:55712ms step_avg:94.59ms
step:590/1750 train_time:55808ms step_avg:94.59ms
step:591/1750 train_time:55904ms step_avg:94.59ms
step:592/1750 train_time:56000ms step_avg:94.59ms
step:593/1750 train_time:56096ms step_avg:94.60ms
step:594/1750 train_time:56193ms step_avg:94.60ms
step:595/1750 train_time:56290ms step_avg:94.61ms
step:596/1750 train_time:56387ms step_avg:94.61ms
step:597/1750 train_time:56483ms step_avg:94.61ms
step:598/1750 train_time:56579ms step_avg:94.61ms
step:599/1750 train_time:56675ms step_avg:94.62ms
step:600/1750 train_time:56771ms step_avg:94.62ms
step:601/1750 train_time:56868ms step_avg:94.62ms
step:602/1750 train_time:56964ms step_avg:94.63ms
step:603/1750 train_time:57060ms step_avg:94.63ms
step:604/1750 train_time:57157ms step_avg:94.63ms
step:605/1750 train_time:57253ms step_avg:94.63ms
step:606/1750 train_time:57349ms step_avg:94.64ms
step:607/1750 train_time:57446ms step_avg:94.64ms
step:608/1750 train_time:57543ms step_avg:94.64ms
step:609/1750 train_time:57639ms step_avg:94.64ms
step:610/1750 train_time:57736ms step_avg:94.65ms
step:611/1750 train_time:57832ms step_avg:94.65ms
step:612/1750 train_time:57928ms step_avg:94.65ms
step:613/1750 train_time:58025ms step_avg:94.66ms
step:614/1750 train_time:58121ms step_avg:94.66ms
step:615/1750 train_time:58217ms step_avg:94.66ms
step:616/1750 train_time:58313ms step_avg:94.66ms
step:617/1750 train_time:58410ms step_avg:94.67ms
step:618/1750 train_time:58507ms step_avg:94.67ms
step:619/1750 train_time:58604ms step_avg:94.67ms
step:620/1750 train_time:58700ms step_avg:94.68ms
step:621/1750 train_time:58797ms step_avg:94.68ms
step:622/1750 train_time:58893ms step_avg:94.68ms
step:623/1750 train_time:58989ms step_avg:94.69ms
step:624/1750 train_time:59085ms step_avg:94.69ms
step:625/1750 train_time:59181ms step_avg:94.69ms
step:625/1750 val_loss:3.6587 train_time:59267ms step_avg:94.83ms
step:626/1750 train_time:59288ms step_avg:94.71ms
step:627/1750 train_time:59381ms step_avg:94.71ms
step:628/1750 train_time:59479ms step_avg:94.71ms
step:629/1750 train_time:59576ms step_avg:94.72ms
step:630/1750 train_time:59672ms step_avg:94.72ms
step:631/1750 train_time:59767ms step_avg:94.72ms
step:632/1750 train_time:59863ms step_avg:94.72ms
step:633/1750 train_time:59959ms step_avg:94.72ms
step:634/1750 train_time:60055ms step_avg:94.72ms
step:635/1750 train_time:60150ms step_avg:94.73ms
step:636/1750 train_time:60248ms step_avg:94.73ms
step:637/1750 train_time:60347ms step_avg:94.74ms
step:638/1750 train_time:60444ms step_avg:94.74ms
step:639/1750 train_time:60543ms step_avg:94.75ms
step:640/1750 train_time:60639ms step_avg:94.75ms
step:641/1750 train_time:60735ms step_avg:94.75ms
step:642/1750 train_time:60831ms step_avg:94.75ms
step:643/1750 train_time:60926ms step_avg:94.75ms
step:644/1750 train_time:61022ms step_avg:94.76ms
step:645/1750 train_time:61118ms step_avg:94.76ms
step:646/1750 train_time:61214ms step_avg:94.76ms
step:647/1750 train_time:61312ms step_avg:94.76ms
step:648/1750 train_time:61409ms step_avg:94.77ms
step:649/1750 train_time:61507ms step_avg:94.77ms
step:650/1750 train_time:61604ms step_avg:94.78ms
step:651/1750 train_time:61702ms step_avg:94.78ms
step:652/1750 train_time:61800ms step_avg:94.79ms
step:653/1750 train_time:61897ms step_avg:94.79ms
step:654/1750 train_time:61995ms step_avg:94.79ms
step:655/1750 train_time:62092ms step_avg:94.80ms
step:656/1750 train_time:62189ms step_avg:94.80ms
step:657/1750 train_time:62287ms step_avg:94.80ms
step:658/1750 train_time:62386ms step_avg:94.81ms
step:659/1750 train_time:62485ms step_avg:94.82ms
step:660/1750 train_time:62584ms step_avg:94.82ms
step:661/1750 train_time:62683ms step_avg:94.83ms
step:662/1750 train_time:62781ms step_avg:94.84ms
step:663/1750 train_time:62880ms step_avg:94.84ms
step:664/1750 train_time:62977ms step_avg:94.84ms
step:665/1750 train_time:63075ms step_avg:94.85ms
step:666/1750 train_time:63172ms step_avg:94.85ms
step:667/1750 train_time:63269ms step_avg:94.86ms
step:668/1750 train_time:63366ms step_avg:94.86ms
step:669/1750 train_time:63465ms step_avg:94.87ms
step:670/1750 train_time:63563ms step_avg:94.87ms
step:671/1750 train_time:63661ms step_avg:94.88ms
step:672/1750 train_time:63759ms step_avg:94.88ms
step:673/1750 train_time:63857ms step_avg:94.88ms
step:674/1750 train_time:63954ms step_avg:94.89ms
step:675/1750 train_time:64052ms step_avg:94.89ms
step:676/1750 train_time:64149ms step_avg:94.89ms
step:677/1750 train_time:64247ms step_avg:94.90ms
step:678/1750 train_time:64345ms step_avg:94.90ms
step:679/1750 train_time:64443ms step_avg:94.91ms
step:680/1750 train_time:64541ms step_avg:94.91ms
step:681/1750 train_time:64639ms step_avg:94.92ms
step:682/1750 train_time:64737ms step_avg:94.92ms
step:683/1750 train_time:64834ms step_avg:94.93ms
step:684/1750 train_time:64933ms step_avg:94.93ms
step:685/1750 train_time:65030ms step_avg:94.93ms
step:686/1750 train_time:65127ms step_avg:94.94ms
step:687/1750 train_time:65225ms step_avg:94.94ms
step:688/1750 train_time:65323ms step_avg:94.95ms
step:689/1750 train_time:65420ms step_avg:94.95ms
step:690/1750 train_time:65518ms step_avg:94.95ms
step:691/1750 train_time:65616ms step_avg:94.96ms
step:692/1750 train_time:65715ms step_avg:94.96ms
step:693/1750 train_time:65812ms step_avg:94.97ms
step:694/1750 train_time:65909ms step_avg:94.97ms
step:695/1750 train_time:66007ms step_avg:94.97ms
step:696/1750 train_time:66105ms step_avg:94.98ms
step:697/1750 train_time:66203ms step_avg:94.98ms
step:698/1750 train_time:66301ms step_avg:94.99ms
step:699/1750 train_time:66399ms step_avg:94.99ms
step:700/1750 train_time:66497ms step_avg:95.00ms
step:701/1750 train_time:66596ms step_avg:95.00ms
step:702/1750 train_time:66693ms step_avg:95.00ms
step:703/1750 train_time:66791ms step_avg:95.01ms
step:704/1750 train_time:66888ms step_avg:95.01ms
step:705/1750 train_time:66986ms step_avg:95.02ms
step:706/1750 train_time:67083ms step_avg:95.02ms
step:707/1750 train_time:67181ms step_avg:95.02ms
step:708/1750 train_time:67279ms step_avg:95.03ms
step:709/1750 train_time:67377ms step_avg:95.03ms
step:710/1750 train_time:67475ms step_avg:95.03ms
step:711/1750 train_time:67573ms step_avg:95.04ms
step:712/1750 train_time:67670ms step_avg:95.04ms
step:713/1750 train_time:67767ms step_avg:95.04ms
step:714/1750 train_time:67865ms step_avg:95.05ms
step:715/1750 train_time:67963ms step_avg:95.05ms
step:716/1750 train_time:68061ms step_avg:95.06ms
step:717/1750 train_time:68158ms step_avg:95.06ms
step:718/1750 train_time:68256ms step_avg:95.06ms
step:719/1750 train_time:68354ms step_avg:95.07ms
step:720/1750 train_time:68452ms step_avg:95.07ms
step:721/1750 train_time:68549ms step_avg:95.08ms
step:722/1750 train_time:68647ms step_avg:95.08ms
step:723/1750 train_time:68745ms step_avg:95.08ms
step:724/1750 train_time:68843ms step_avg:95.09ms
step:725/1750 train_time:68942ms step_avg:95.09ms
step:726/1750 train_time:69040ms step_avg:95.10ms
step:727/1750 train_time:69137ms step_avg:95.10ms
step:728/1750 train_time:69235ms step_avg:95.10ms
step:729/1750 train_time:69333ms step_avg:95.11ms
step:730/1750 train_time:69430ms step_avg:95.11ms
step:731/1750 train_time:69527ms step_avg:95.11ms
step:732/1750 train_time:69625ms step_avg:95.12ms
step:733/1750 train_time:69722ms step_avg:95.12ms
step:734/1750 train_time:69820ms step_avg:95.12ms
step:735/1750 train_time:69918ms step_avg:95.13ms
step:736/1750 train_time:70016ms step_avg:95.13ms
step:737/1750 train_time:70114ms step_avg:95.13ms
step:738/1750 train_time:70213ms step_avg:95.14ms
step:739/1750 train_time:70310ms step_avg:95.14ms
step:740/1750 train_time:70408ms step_avg:95.15ms
step:741/1750 train_time:70505ms step_avg:95.15ms
step:742/1750 train_time:70603ms step_avg:95.15ms
step:743/1750 train_time:70701ms step_avg:95.16ms
step:744/1750 train_time:70798ms step_avg:95.16ms
step:745/1750 train_time:70896ms step_avg:95.16ms
step:746/1750 train_time:70993ms step_avg:95.16ms
step:747/1750 train_time:71091ms step_avg:95.17ms
step:748/1750 train_time:71189ms step_avg:95.17ms
step:749/1750 train_time:71287ms step_avg:95.18ms
step:750/1750 train_time:71385ms step_avg:95.18ms
step:750/1750 val_loss:3.5960 train_time:71472ms step_avg:95.30ms
step:751/1750 train_time:71494ms step_avg:95.20ms
step:752/1750 train_time:71589ms step_avg:95.20ms
step:753/1750 train_time:71689ms step_avg:95.20ms
step:754/1750 train_time:71786ms step_avg:95.21ms
step:755/1750 train_time:71883ms step_avg:95.21ms
step:756/1750 train_time:71980ms step_avg:95.21ms
step:757/1750 train_time:72077ms step_avg:95.21ms
step:758/1750 train_time:72174ms step_avg:95.22ms
step:759/1750 train_time:72271ms step_avg:95.22ms
step:760/1750 train_time:72368ms step_avg:95.22ms
step:761/1750 train_time:72467ms step_avg:95.23ms
step:762/1750 train_time:72566ms step_avg:95.23ms
step:763/1750 train_time:72666ms step_avg:95.24ms
step:764/1750 train_time:72764ms step_avg:95.24ms
step:765/1750 train_time:72862ms step_avg:95.24ms
step:766/1750 train_time:72959ms step_avg:95.25ms
step:767/1750 train_time:73056ms step_avg:95.25ms
step:768/1750 train_time:73153ms step_avg:95.25ms
step:769/1750 train_time:73250ms step_avg:95.25ms
step:770/1750 train_time:73346ms step_avg:95.25ms
step:771/1750 train_time:73445ms step_avg:95.26ms
step:772/1750 train_time:73545ms step_avg:95.27ms
step:773/1750 train_time:73645ms step_avg:95.27ms
step:774/1750 train_time:73744ms step_avg:95.28ms
step:775/1750 train_time:73842ms step_avg:95.28ms
step:776/1750 train_time:73940ms step_avg:95.28ms
step:777/1750 train_time:74038ms step_avg:95.29ms
step:778/1750 train_time:74135ms step_avg:95.29ms
step:779/1750 train_time:74232ms step_avg:95.29ms
step:780/1750 train_time:74329ms step_avg:95.29ms
step:781/1750 train_time:74427ms step_avg:95.30ms
step:782/1750 train_time:74526ms step_avg:95.30ms
step:783/1750 train_time:74624ms step_avg:95.31ms
step:784/1750 train_time:74723ms step_avg:95.31ms
step:785/1750 train_time:74822ms step_avg:95.31ms
step:786/1750 train_time:74920ms step_avg:95.32ms
step:787/1750 train_time:75018ms step_avg:95.32ms
step:788/1750 train_time:75115ms step_avg:95.32ms
step:789/1750 train_time:75214ms step_avg:95.33ms
step:790/1750 train_time:75311ms step_avg:95.33ms
step:791/1750 train_time:75409ms step_avg:95.33ms
step:792/1750 train_time:75507ms step_avg:95.34ms
step:793/1750 train_time:75606ms step_avg:95.34ms
step:794/1750 train_time:75706ms step_avg:95.35ms
step:795/1750 train_time:75804ms step_avg:95.35ms
step:796/1750 train_time:75903ms step_avg:95.36ms
step:797/1750 train_time:76001ms step_avg:95.36ms
step:798/1750 train_time:76100ms step_avg:95.36ms
step:799/1750 train_time:76199ms step_avg:95.37ms
step:800/1750 train_time:76298ms step_avg:95.37ms
step:801/1750 train_time:76396ms step_avg:95.38ms
step:802/1750 train_time:76494ms step_avg:95.38ms
step:803/1750 train_time:76593ms step_avg:95.38ms
step:804/1750 train_time:76691ms step_avg:95.39ms
step:805/1750 train_time:76790ms step_avg:95.39ms
step:806/1750 train_time:76889ms step_avg:95.40ms
step:807/1750 train_time:76987ms step_avg:95.40ms
step:808/1750 train_time:77085ms step_avg:95.40ms
step:809/1750 train_time:77184ms step_avg:95.41ms
step:810/1750 train_time:77283ms step_avg:95.41ms
step:811/1750 train_time:77383ms step_avg:95.42ms
step:812/1750 train_time:77482ms step_avg:95.42ms
step:813/1750 train_time:77581ms step_avg:95.43ms
step:814/1750 train_time:77679ms step_avg:95.43ms
step:815/1750 train_time:77777ms step_avg:95.43ms
step:816/1750 train_time:77875ms step_avg:95.44ms
step:817/1750 train_time:77973ms step_avg:95.44ms
step:818/1750 train_time:78071ms step_avg:95.44ms
step:819/1750 train_time:78169ms step_avg:95.44ms
step:820/1750 train_time:78266ms step_avg:95.45ms
step:821/1750 train_time:78364ms step_avg:95.45ms
step:822/1750 train_time:78462ms step_avg:95.45ms
step:823/1750 train_time:78560ms step_avg:95.46ms
step:824/1750 train_time:78659ms step_avg:95.46ms
step:825/1750 train_time:78757ms step_avg:95.46ms
step:826/1750 train_time:78855ms step_avg:95.47ms
step:827/1750 train_time:78953ms step_avg:95.47ms
step:828/1750 train_time:79051ms step_avg:95.47ms
step:829/1750 train_time:79149ms step_avg:95.47ms
step:830/1750 train_time:79247ms step_avg:95.48ms
step:831/1750 train_time:79345ms step_avg:95.48ms
step:832/1750 train_time:79444ms step_avg:95.49ms
step:833/1750 train_time:79543ms step_avg:95.49ms
step:834/1750 train_time:79642ms step_avg:95.49ms
step:835/1750 train_time:79741ms step_avg:95.50ms
step:836/1750 train_time:79840ms step_avg:95.50ms
step:837/1750 train_time:79939ms step_avg:95.51ms
step:838/1750 train_time:80038ms step_avg:95.51ms
step:839/1750 train_time:80138ms step_avg:95.52ms
step:840/1750 train_time:80236ms step_avg:95.52ms
step:841/1750 train_time:80334ms step_avg:95.52ms
step:842/1750 train_time:80434ms step_avg:95.53ms
step:843/1750 train_time:80533ms step_avg:95.53ms
step:844/1750 train_time:80631ms step_avg:95.53ms
step:845/1750 train_time:80730ms step_avg:95.54ms
step:846/1750 train_time:80827ms step_avg:95.54ms
step:847/1750 train_time:80926ms step_avg:95.54ms
step:848/1750 train_time:81024ms step_avg:95.55ms
step:849/1750 train_time:81123ms step_avg:95.55ms
step:850/1750 train_time:81221ms step_avg:95.55ms
step:851/1750 train_time:81320ms step_avg:95.56ms
step:852/1750 train_time:81419ms step_avg:95.56ms
step:853/1750 train_time:81517ms step_avg:95.56ms
step:854/1750 train_time:81614ms step_avg:95.57ms
step:855/1750 train_time:81713ms step_avg:95.57ms
step:856/1750 train_time:81811ms step_avg:95.57ms
step:857/1750 train_time:81909ms step_avg:95.58ms
step:858/1750 train_time:82007ms step_avg:95.58ms
step:859/1750 train_time:82105ms step_avg:95.58ms
step:860/1750 train_time:82203ms step_avg:95.59ms
step:861/1750 train_time:82302ms step_avg:95.59ms
step:862/1750 train_time:82400ms step_avg:95.59ms
step:863/1750 train_time:82500ms step_avg:95.60ms
step:864/1750 train_time:82598ms step_avg:95.60ms
step:865/1750 train_time:82696ms step_avg:95.60ms
step:866/1750 train_time:82794ms step_avg:95.61ms
step:867/1750 train_time:82893ms step_avg:95.61ms
step:868/1750 train_time:82991ms step_avg:95.61ms
step:869/1750 train_time:83089ms step_avg:95.61ms
step:870/1750 train_time:83187ms step_avg:95.62ms
step:871/1750 train_time:83286ms step_avg:95.62ms
step:872/1750 train_time:83383ms step_avg:95.62ms
step:873/1750 train_time:83482ms step_avg:95.63ms
step:874/1750 train_time:83581ms step_avg:95.63ms
step:875/1750 train_time:83679ms step_avg:95.63ms
step:875/1750 val_loss:3.5473 train_time:83767ms step_avg:95.73ms
step:876/1750 train_time:83788ms step_avg:95.65ms
step:877/1750 train_time:83885ms step_avg:95.65ms
step:878/1750 train_time:83985ms step_avg:95.66ms
step:879/1750 train_time:84084ms step_avg:95.66ms
step:880/1750 train_time:84182ms step_avg:95.66ms
step:881/1750 train_time:84280ms step_avg:95.66ms
step:882/1750 train_time:84377ms step_avg:95.67ms
step:883/1750 train_time:84473ms step_avg:95.67ms
step:884/1750 train_time:84570ms step_avg:95.67ms
step:885/1750 train_time:84667ms step_avg:95.67ms
step:886/1750 train_time:84767ms step_avg:95.67ms
step:887/1750 train_time:84868ms step_avg:95.68ms
step:888/1750 train_time:84967ms step_avg:95.68ms
step:889/1750 train_time:85067ms step_avg:95.69ms
step:890/1750 train_time:85165ms step_avg:95.69ms
step:891/1750 train_time:85264ms step_avg:95.69ms
step:892/1750 train_time:85361ms step_avg:95.70ms
step:893/1750 train_time:85459ms step_avg:95.70ms
step:894/1750 train_time:85556ms step_avg:95.70ms
step:895/1750 train_time:85654ms step_avg:95.70ms
step:896/1750 train_time:85753ms step_avg:95.71ms
step:897/1750 train_time:85852ms step_avg:95.71ms
step:898/1750 train_time:85950ms step_avg:95.71ms
step:899/1750 train_time:86049ms step_avg:95.72ms
step:900/1750 train_time:86147ms step_avg:95.72ms
step:901/1750 train_time:86245ms step_avg:95.72ms
step:902/1750 train_time:86343ms step_avg:95.72ms
step:903/1750 train_time:86442ms step_avg:95.73ms
step:904/1750 train_time:86540ms step_avg:95.73ms
step:905/1750 train_time:86638ms step_avg:95.73ms
step:906/1750 train_time:86736ms step_avg:95.74ms
step:907/1750 train_time:86834ms step_avg:95.74ms
step:908/1750 train_time:86933ms step_avg:95.74ms
step:909/1750 train_time:87032ms step_avg:95.74ms
step:910/1750 train_time:87131ms step_avg:95.75ms
step:911/1750 train_time:87231ms step_avg:95.75ms
step:912/1750 train_time:87330ms step_avg:95.76ms
step:913/1750 train_time:87430ms step_avg:95.76ms
step:914/1750 train_time:87529ms step_avg:95.77ms
step:915/1750 train_time:87629ms step_avg:95.77ms
step:916/1750 train_time:87728ms step_avg:95.77ms
step:917/1750 train_time:87827ms step_avg:95.78ms
step:918/1750 train_time:87928ms step_avg:95.78ms
step:919/1750 train_time:88027ms step_avg:95.79ms
step:920/1750 train_time:88127ms step_avg:95.79ms
step:921/1750 train_time:88226ms step_avg:95.79ms
step:922/1750 train_time:88326ms step_avg:95.80ms
step:923/1750 train_time:88426ms step_avg:95.80ms
step:924/1750 train_time:88525ms step_avg:95.81ms
step:925/1750 train_time:88625ms step_avg:95.81ms
step:926/1750 train_time:88725ms step_avg:95.82ms
step:927/1750 train_time:88826ms step_avg:95.82ms
step:928/1750 train_time:88926ms step_avg:95.83ms
step:929/1750 train_time:89025ms step_avg:95.83ms
step:930/1750 train_time:89126ms step_avg:95.83ms
step:931/1750 train_time:89226ms step_avg:95.84ms
step:932/1750 train_time:89325ms step_avg:95.84ms
step:933/1750 train_time:89425ms step_avg:95.85ms
step:934/1750 train_time:89525ms step_avg:95.85ms
step:935/1750 train_time:89625ms step_avg:95.86ms
step:936/1750 train_time:89725ms step_avg:95.86ms
step:937/1750 train_time:89825ms step_avg:95.86ms
step:938/1750 train_time:89926ms step_avg:95.87ms
step:939/1750 train_time:90026ms step_avg:95.87ms
step:940/1750 train_time:90126ms step_avg:95.88ms
step:941/1750 train_time:90226ms step_avg:95.88ms
step:942/1750 train_time:90326ms step_avg:95.89ms
step:943/1750 train_time:90426ms step_avg:95.89ms
step:944/1750 train_time:90526ms step_avg:95.90ms
step:945/1750 train_time:90625ms step_avg:95.90ms
step:946/1750 train_time:90725ms step_avg:95.90ms
step:947/1750 train_time:90825ms step_avg:95.91ms
step:948/1750 train_time:90925ms step_avg:95.91ms
step:949/1750 train_time:91025ms step_avg:95.92ms
step:950/1750 train_time:91124ms step_avg:95.92ms
step:951/1750 train_time:91224ms step_avg:95.92ms
step:952/1750 train_time:91324ms step_avg:95.93ms
step:953/1750 train_time:91424ms step_avg:95.93ms
step:954/1750 train_time:91523ms step_avg:95.94ms
step:955/1750 train_time:91623ms step_avg:95.94ms
step:956/1750 train_time:91723ms step_avg:95.94ms
step:957/1750 train_time:91823ms step_avg:95.95ms
step:958/1750 train_time:91923ms step_avg:95.95ms
step:959/1750 train_time:92023ms step_avg:95.96ms
step:960/1750 train_time:92124ms step_avg:95.96ms
step:961/1750 train_time:92224ms step_avg:95.97ms
step:962/1750 train_time:92324ms step_avg:95.97ms
step:963/1750 train_time:92424ms step_avg:95.97ms
step:964/1750 train_time:92524ms step_avg:95.98ms
step:965/1750 train_time:92623ms step_avg:95.98ms
step:966/1750 train_time:92722ms step_avg:95.99ms
step:967/1750 train_time:92823ms step_avg:95.99ms
step:968/1750 train_time:92924ms step_avg:96.00ms
step:969/1750 train_time:93025ms step_avg:96.00ms
step:970/1750 train_time:93125ms step_avg:96.00ms
step:971/1750 train_time:93225ms step_avg:96.01ms
step:972/1750 train_time:93325ms step_avg:96.01ms
step:973/1750 train_time:93425ms step_avg:96.02ms
step:974/1750 train_time:93524ms step_avg:96.02ms
step:975/1750 train_time:93623ms step_avg:96.02ms
step:976/1750 train_time:93723ms step_avg:96.03ms
step:977/1750 train_time:93823ms step_avg:96.03ms
step:978/1750 train_time:93923ms step_avg:96.04ms
step:979/1750 train_time:94023ms step_avg:96.04ms
step:980/1750 train_time:94123ms step_avg:96.04ms
step:981/1750 train_time:94222ms step_avg:96.05ms
step:982/1750 train_time:94322ms step_avg:96.05ms
step:983/1750 train_time:94421ms step_avg:96.05ms
step:984/1750 train_time:94521ms step_avg:96.06ms
step:985/1750 train_time:94620ms step_avg:96.06ms
step:986/1750 train_time:94719ms step_avg:96.06ms
step:987/1750 train_time:94819ms step_avg:96.07ms
step:988/1750 train_time:94919ms step_avg:96.07ms
step:989/1750 train_time:95018ms step_avg:96.08ms
step:990/1750 train_time:95118ms step_avg:96.08ms
step:991/1750 train_time:95218ms step_avg:96.08ms
step:992/1750 train_time:95318ms step_avg:96.09ms
step:993/1750 train_time:95417ms step_avg:96.09ms
step:994/1750 train_time:95516ms step_avg:96.09ms
step:995/1750 train_time:95616ms step_avg:96.10ms
step:996/1750 train_time:95716ms step_avg:96.10ms
step:997/1750 train_time:95815ms step_avg:96.10ms
step:998/1750 train_time:95915ms step_avg:96.11ms
step:999/1750 train_time:96015ms step_avg:96.11ms
step:1000/1750 train_time:96114ms step_avg:96.11ms
step:1000/1750 val_loss:3.5069 train_time:96203ms step_avg:96.20ms
step:1001/1750 train_time:96223ms step_avg:96.13ms
step:1002/1750 train_time:96324ms step_avg:96.13ms
step:1003/1750 train_time:96425ms step_avg:96.14ms
step:1004/1750 train_time:96523ms step_avg:96.14ms
step:1005/1750 train_time:96622ms step_avg:96.14ms
step:1006/1750 train_time:96720ms step_avg:96.14ms
step:1007/1750 train_time:96818ms step_avg:96.15ms
step:1008/1750 train_time:96916ms step_avg:96.15ms
step:1009/1750 train_time:97016ms step_avg:96.15ms
step:1010/1750 train_time:97114ms step_avg:96.15ms
step:1011/1750 train_time:97215ms step_avg:96.16ms
step:1012/1750 train_time:97317ms step_avg:96.16ms
step:1013/1750 train_time:97418ms step_avg:96.17ms
step:1014/1750 train_time:97519ms step_avg:96.17ms
step:1015/1750 train_time:97619ms step_avg:96.18ms
step:1016/1750 train_time:97718ms step_avg:96.18ms
step:1017/1750 train_time:97817ms step_avg:96.18ms
step:1018/1750 train_time:97915ms step_avg:96.18ms
step:1019/1750 train_time:98013ms step_avg:96.19ms
step:1020/1750 train_time:98112ms step_avg:96.19ms
step:1021/1750 train_time:98213ms step_avg:96.19ms
step:1022/1750 train_time:98313ms step_avg:96.20ms
step:1023/1750 train_time:98412ms step_avg:96.20ms
step:1024/1750 train_time:98513ms step_avg:96.20ms
step:1025/1750 train_time:98613ms step_avg:96.21ms
step:1026/1750 train_time:98713ms step_avg:96.21ms
step:1027/1750 train_time:98813ms step_avg:96.21ms
step:1028/1750 train_time:98911ms step_avg:96.22ms
step:1029/1750 train_time:99011ms step_avg:96.22ms
step:1030/1750 train_time:99110ms step_avg:96.22ms
step:1031/1750 train_time:99211ms step_avg:96.23ms
step:1032/1750 train_time:99312ms step_avg:96.23ms
step:1033/1750 train_time:99412ms step_avg:96.24ms
step:1034/1750 train_time:99511ms step_avg:96.24ms
step:1035/1750 train_time:99611ms step_avg:96.24ms
step:1036/1750 train_time:99710ms step_avg:96.25ms
step:1037/1750 train_time:99811ms step_avg:96.25ms
step:1038/1750 train_time:99910ms step_avg:96.25ms
step:1039/1750 train_time:100009ms step_avg:96.26ms
step:1040/1750 train_time:100109ms step_avg:96.26ms
step:1041/1750 train_time:100209ms step_avg:96.26ms
step:1042/1750 train_time:100308ms step_avg:96.27ms
step:1043/1750 train_time:100409ms step_avg:96.27ms
step:1044/1750 train_time:100510ms step_avg:96.27ms
step:1045/1750 train_time:100609ms step_avg:96.28ms
step:1046/1750 train_time:100709ms step_avg:96.28ms
step:1047/1750 train_time:100809ms step_avg:96.28ms
step:1048/1750 train_time:100908ms step_avg:96.29ms
step:1049/1750 train_time:101007ms step_avg:96.29ms
step:1050/1750 train_time:101107ms step_avg:96.29ms
step:1051/1750 train_time:101208ms step_avg:96.30ms
step:1052/1750 train_time:101307ms step_avg:96.30ms
step:1053/1750 train_time:101407ms step_avg:96.30ms
step:1054/1750 train_time:101506ms step_avg:96.31ms
step:1055/1750 train_time:101606ms step_avg:96.31ms
step:1056/1750 train_time:101706ms step_avg:96.31ms
step:1057/1750 train_time:101806ms step_avg:96.32ms
step:1058/1750 train_time:101905ms step_avg:96.32ms
step:1059/1750 train_time:102005ms step_avg:96.32ms
step:1060/1750 train_time:102359ms step_avg:96.57ms
step:1061/1750 train_time:102458ms step_avg:96.57ms
step:1062/1750 train_time:102556ms step_avg:96.57ms
step:1063/1750 train_time:102654ms step_avg:96.57ms
step:1064/1750 train_time:102753ms step_avg:96.57ms
step:1065/1750 train_time:102852ms step_avg:96.57ms
step:1066/1750 train_time:102950ms step_avg:96.58ms
step:1067/1750 train_time:103049ms step_avg:96.58ms
step:1068/1750 train_time:103147ms step_avg:96.58ms
step:1069/1750 train_time:103247ms step_avg:96.58ms
step:1070/1750 train_time:103351ms step_avg:96.59ms
step:1071/1750 train_time:103454ms step_avg:96.60ms
step:1072/1750 train_time:103553ms step_avg:96.60ms
step:1073/1750 train_time:103652ms step_avg:96.60ms
step:1074/1750 train_time:103751ms step_avg:96.60ms
step:1075/1750 train_time:103849ms step_avg:96.60ms
step:1076/1750 train_time:103948ms step_avg:96.61ms
step:1077/1750 train_time:104047ms step_avg:96.61ms
step:1078/1750 train_time:104146ms step_avg:96.61ms
step:1079/1750 train_time:104247ms step_avg:96.61ms
step:1080/1750 train_time:104348ms step_avg:96.62ms
step:1081/1750 train_time:104449ms step_avg:96.62ms
step:1082/1750 train_time:104550ms step_avg:96.63ms
step:1083/1750 train_time:104649ms step_avg:96.63ms
step:1084/1750 train_time:104750ms step_avg:96.63ms
step:1085/1750 train_time:104848ms step_avg:96.63ms
step:1086/1750 train_time:104947ms step_avg:96.64ms
step:1087/1750 train_time:105332ms step_avg:96.90ms
step:1088/1750 train_time:105429ms step_avg:96.90ms
step:1089/1750 train_time:105527ms step_avg:96.90ms
step:1090/1750 train_time:105626ms step_avg:96.90ms
step:1091/1750 train_time:105724ms step_avg:96.91ms
step:1092/1750 train_time:105823ms step_avg:96.91ms
step:1093/1750 train_time:105922ms step_avg:96.91ms
step:1094/1750 train_time:106020ms step_avg:96.91ms
step:1095/1750 train_time:106119ms step_avg:96.91ms
step:1096/1750 train_time:106221ms step_avg:96.92ms
step:1097/1750 train_time:106323ms step_avg:96.92ms
step:1098/1750 train_time:106423ms step_avg:96.92ms
step:1099/1750 train_time:106523ms step_avg:96.93ms
step:1100/1750 train_time:106623ms step_avg:96.93ms
step:1101/1750 train_time:106722ms step_avg:96.93ms
step:1102/1750 train_time:106821ms step_avg:96.93ms
step:1103/1750 train_time:106920ms step_avg:96.94ms
step:1104/1750 train_time:107019ms step_avg:96.94ms
step:1105/1750 train_time:107410ms step_avg:97.20ms
step:1106/1750 train_time:107507ms step_avg:97.20ms
step:1107/1750 train_time:107606ms step_avg:97.21ms
step:1108/1750 train_time:107704ms step_avg:97.21ms
step:1109/1750 train_time:107803ms step_avg:97.21ms
step:1110/1750 train_time:107901ms step_avg:97.21ms
step:1111/1750 train_time:108000ms step_avg:97.21ms
step:1112/1750 train_time:108100ms step_avg:97.21ms
step:1113/1750 train_time:108199ms step_avg:97.21ms
step:1114/1750 train_time:108300ms step_avg:97.22ms
step:1115/1750 train_time:108403ms step_avg:97.22ms
step:1116/1750 train_time:108504ms step_avg:97.23ms
step:1117/1750 train_time:108603ms step_avg:97.23ms
step:1118/1750 train_time:108701ms step_avg:97.23ms
step:1119/1750 train_time:108800ms step_avg:97.23ms
step:1120/1750 train_time:108899ms step_avg:97.23ms
step:1121/1750 train_time:108999ms step_avg:97.23ms
step:1122/1750 train_time:109098ms step_avg:97.24ms
step:1123/1750 train_time:109198ms step_avg:97.24ms
step:1124/1750 train_time:109299ms step_avg:97.24ms
step:1125/1750 train_time:109400ms step_avg:97.24ms
step:1125/1750 val_loss:3.4563 train_time:109489ms step_avg:97.32ms
step:1126/1750 train_time:109509ms step_avg:97.25ms
step:1127/1750 train_time:109610ms step_avg:97.26ms
step:1128/1750 train_time:109714ms step_avg:97.26ms
step:1129/1750 train_time:109814ms step_avg:97.27ms
step:1130/1750 train_time:109913ms step_avg:97.27ms
step:1131/1750 train_time:110011ms step_avg:97.27ms
step:1132/1750 train_time:110110ms step_avg:97.27ms
step:1133/1750 train_time:110208ms step_avg:97.27ms
step:1134/1750 train_time:110306ms step_avg:97.27ms
step:1135/1750 train_time:110405ms step_avg:97.27ms
step:1136/1750 train_time:110505ms step_avg:97.28ms
step:1137/1750 train_time:110608ms step_avg:97.28ms
step:1138/1750 train_time:110709ms step_avg:97.28ms
step:1139/1750 train_time:110809ms step_avg:97.29ms
step:1140/1750 train_time:110908ms step_avg:97.29ms
step:1141/1750 train_time:111008ms step_avg:97.29ms
step:1142/1750 train_time:111107ms step_avg:97.29ms
step:1143/1750 train_time:111206ms step_avg:97.29ms
step:1144/1750 train_time:111305ms step_avg:97.29ms
step:1145/1750 train_time:111403ms step_avg:97.30ms
step:1146/1750 train_time:111504ms step_avg:97.30ms
step:1147/1750 train_time:111604ms step_avg:97.30ms
step:1148/1750 train_time:111705ms step_avg:97.30ms
step:1149/1750 train_time:111806ms step_avg:97.31ms
step:1150/1750 train_time:111906ms step_avg:97.31ms
step:1151/1750 train_time:112005ms step_avg:97.31ms
step:1152/1750 train_time:112104ms step_avg:97.31ms
step:1153/1750 train_time:112203ms step_avg:97.31ms
step:1154/1750 train_time:112302ms step_avg:97.32ms
step:1155/1750 train_time:112400ms step_avg:97.32ms
step:1156/1750 train_time:112500ms step_avg:97.32ms
step:1157/1750 train_time:112600ms step_avg:97.32ms
step:1158/1750 train_time:112700ms step_avg:97.32ms
step:1159/1750 train_time:112800ms step_avg:97.33ms
step:1160/1750 train_time:113155ms step_avg:97.55ms
step:1161/1750 train_time:113253ms step_avg:97.55ms
step:1162/1750 train_time:113351ms step_avg:97.55ms
step:1163/1750 train_time:113450ms step_avg:97.55ms
step:1164/1750 train_time:113549ms step_avg:97.55ms
step:1165/1750 train_time:113646ms step_avg:97.55ms
step:1166/1750 train_time:113744ms step_avg:97.55ms
step:1167/1750 train_time:114101ms step_avg:97.77ms
step:1168/1750 train_time:114200ms step_avg:97.77ms
step:1169/1750 train_time:114713ms step_avg:98.13ms
step:1170/1750 train_time:114811ms step_avg:98.13ms
step:1171/1750 train_time:114910ms step_avg:98.13ms
step:1172/1750 train_time:115010ms step_avg:98.13ms
step:1173/1750 train_time:115109ms step_avg:98.13ms
step:1174/1750 train_time:115209ms step_avg:98.13ms
step:1175/1750 train_time:115308ms step_avg:98.13ms
step:1176/1750 train_time:115407ms step_avg:98.14ms
step:1177/1750 train_time:115507ms step_avg:98.14ms
step:1178/1750 train_time:115610ms step_avg:98.14ms
step:1179/1750 train_time:115716ms step_avg:98.15ms
step:1180/1750 train_time:115817ms step_avg:98.15ms
step:1181/1750 train_time:115918ms step_avg:98.15ms
step:1182/1750 train_time:116019ms step_avg:98.15ms
step:1183/1750 train_time:116119ms step_avg:98.16ms
step:1184/1750 train_time:116220ms step_avg:98.16ms
step:1185/1750 train_time:116320ms step_avg:98.16ms
step:1186/1750 train_time:116420ms step_avg:98.16ms
step:1187/1750 train_time:116521ms step_avg:98.16ms
step:1188/1750 train_time:116622ms step_avg:98.17ms
step:1189/1750 train_time:116722ms step_avg:98.17ms
step:1190/1750 train_time:116823ms step_avg:98.17ms
step:1191/1750 train_time:116923ms step_avg:98.17ms
step:1192/1750 train_time:117024ms step_avg:98.17ms
step:1193/1750 train_time:117124ms step_avg:98.18ms
step:1194/1750 train_time:117225ms step_avg:98.18ms
step:1195/1750 train_time:117324ms step_avg:98.18ms
step:1196/1750 train_time:117424ms step_avg:98.18ms
step:1197/1750 train_time:117524ms step_avg:98.18ms
step:1198/1750 train_time:117624ms step_avg:98.18ms
step:1199/1750 train_time:117726ms step_avg:98.19ms
step:1200/1750 train_time:117828ms step_avg:98.19ms
step:1201/1750 train_time:117930ms step_avg:98.19ms
step:1202/1750 train_time:118031ms step_avg:98.20ms
step:1203/1750 train_time:118131ms step_avg:98.20ms
step:1204/1750 train_time:118231ms step_avg:98.20ms
step:1205/1750 train_time:118331ms step_avg:98.20ms
step:1206/1750 train_time:118431ms step_avg:98.20ms
step:1207/1750 train_time:118532ms step_avg:98.20ms
step:1208/1750 train_time:118633ms step_avg:98.21ms
step:1209/1750 train_time:118736ms step_avg:98.21ms
step:1210/1750 train_time:118839ms step_avg:98.21ms
step:1211/1750 train_time:119378ms step_avg:98.58ms
step:1212/1750 train_time:119441ms step_avg:98.55ms
step:1213/1750 train_time:119538ms step_avg:98.55ms
step:1214/1750 train_time:119637ms step_avg:98.55ms
step:1215/1750 train_time:119737ms step_avg:98.55ms
step:1216/1750 train_time:119839ms step_avg:98.55ms
step:1217/1750 train_time:119939ms step_avg:98.55ms
step:1218/1750 train_time:120038ms step_avg:98.55ms
step:1219/1750 train_time:120139ms step_avg:98.56ms
step:1220/1750 train_time:120240ms step_avg:98.56ms
step:1221/1750 train_time:120343ms step_avg:98.56ms
step:1222/1750 train_time:120445ms step_avg:98.56ms
step:1223/1750 train_time:120547ms step_avg:98.57ms
step:1224/1750 train_time:120647ms step_avg:98.57ms
step:1225/1750 train_time:120748ms step_avg:98.57ms
step:1226/1750 train_time:120848ms step_avg:98.57ms
step:1227/1750 train_time:120948ms step_avg:98.57ms
step:1228/1750 train_time:121049ms step_avg:98.57ms
step:1229/1750 train_time:121150ms step_avg:98.58ms
step:1230/1750 train_time:121250ms step_avg:98.58ms
step:1231/1750 train_time:121352ms step_avg:98.58ms
step:1232/1750 train_time:121454ms step_avg:98.58ms
step:1233/1750 train_time:121556ms step_avg:98.59ms
step:1234/1750 train_time:121659ms step_avg:98.59ms
step:1235/1750 train_time:121760ms step_avg:98.59ms
step:1236/1750 train_time:121860ms step_avg:98.59ms
step:1237/1750 train_time:121961ms step_avg:98.59ms
step:1238/1750 train_time:122060ms step_avg:98.59ms
step:1239/1750 train_time:122161ms step_avg:98.60ms
step:1240/1750 train_time:122261ms step_avg:98.60ms
step:1241/1750 train_time:122364ms step_avg:98.60ms
step:1242/1750 train_time:122464ms step_avg:98.60ms
step:1243/1750 train_time:122565ms step_avg:98.60ms
step:1244/1750 train_time:122666ms step_avg:98.61ms
step:1245/1750 train_time:122766ms step_avg:98.61ms
step:1246/1750 train_time:122867ms step_avg:98.61ms
step:1247/1750 train_time:122967ms step_avg:98.61ms
step:1248/1750 train_time:123068ms step_avg:98.61ms
step:1249/1750 train_time:123170ms step_avg:98.61ms
step:1250/1750 train_time:123270ms step_avg:98.62ms
step:1250/1750 val_loss:3.4115 train_time:123360ms step_avg:98.69ms
step:1251/1750 train_time:123381ms step_avg:98.63ms
step:1252/1750 train_time:123480ms step_avg:98.63ms
step:1253/1750 train_time:123581ms step_avg:98.63ms
step:1254/1750 train_time:123683ms step_avg:98.63ms
step:1255/1750 train_time:123784ms step_avg:98.63ms
step:1256/1750 train_time:123883ms step_avg:98.63ms
step:1257/1750 train_time:123983ms step_avg:98.63ms
step:1258/1750 train_time:124083ms step_avg:98.64ms
step:1259/1750 train_time:124183ms step_avg:98.64ms
step:1260/1750 train_time:124285ms step_avg:98.64ms
step:1261/1750 train_time:124387ms step_avg:98.64ms
step:1262/1750 train_time:124488ms step_avg:98.64ms
step:1263/1750 train_time:124588ms step_avg:98.64ms
step:1264/1750 train_time:124688ms step_avg:98.65ms
step:1265/1750 train_time:124787ms step_avg:98.65ms
step:1266/1750 train_time:124887ms step_avg:98.65ms
step:1267/1750 train_time:124987ms step_avg:98.65ms
step:1268/1750 train_time:125086ms step_avg:98.65ms
step:1269/1750 train_time:125186ms step_avg:98.65ms
step:1270/1750 train_time:125286ms step_avg:98.65ms
step:1271/1750 train_time:125388ms step_avg:98.65ms
step:1272/1750 train_time:125488ms step_avg:98.65ms
step:1273/1750 train_time:125588ms step_avg:98.66ms
step:1274/1750 train_time:125688ms step_avg:98.66ms
step:1275/1750 train_time:125788ms step_avg:98.66ms
step:1276/1750 train_time:125889ms step_avg:98.66ms
step:1277/1750 train_time:125989ms step_avg:98.66ms
step:1278/1750 train_time:126090ms step_avg:98.66ms
step:1279/1750 train_time:126190ms step_avg:98.66ms
step:1280/1750 train_time:126292ms step_avg:98.67ms
step:1281/1750 train_time:126394ms step_avg:98.67ms
step:1282/1750 train_time:126495ms step_avg:98.67ms
step:1283/1750 train_time:126596ms step_avg:98.67ms
step:1284/1750 train_time:126696ms step_avg:98.67ms
step:1285/1750 train_time:126797ms step_avg:98.67ms
step:1286/1750 train_time:126897ms step_avg:98.68ms
step:1287/1750 train_time:126999ms step_avg:98.68ms
step:1288/1750 train_time:127100ms step_avg:98.68ms
step:1289/1750 train_time:127203ms step_avg:98.68ms
step:1290/1750 train_time:127304ms step_avg:98.69ms
step:1291/1750 train_time:127405ms step_avg:98.69ms
step:1292/1750 train_time:127506ms step_avg:98.69ms
step:1293/1750 train_time:127606ms step_avg:98.69ms
step:1294/1750 train_time:127707ms step_avg:98.69ms
step:1295/1750 train_time:127807ms step_avg:98.69ms
step:1296/1750 train_time:127908ms step_avg:98.69ms
step:1297/1750 train_time:128008ms step_avg:98.70ms
step:1298/1750 train_time:128108ms step_avg:98.70ms
step:1299/1750 train_time:128210ms step_avg:98.70ms
step:1300/1750 train_time:128311ms step_avg:98.70ms
step:1301/1750 train_time:128413ms step_avg:98.70ms
step:1302/1750 train_time:128514ms step_avg:98.70ms
step:1303/1750 train_time:128615ms step_avg:98.71ms
step:1304/1750 train_time:128716ms step_avg:98.71ms
step:1305/1750 train_time:128818ms step_avg:98.71ms
step:1306/1750 train_time:128918ms step_avg:98.71ms
step:1307/1750 train_time:129018ms step_avg:98.71ms
step:1308/1750 train_time:129120ms step_avg:98.72ms
step:1309/1750 train_time:129222ms step_avg:98.72ms
step:1310/1750 train_time:129323ms step_avg:98.72ms
step:1311/1750 train_time:129426ms step_avg:98.72ms
step:1312/1750 train_time:129526ms step_avg:98.72ms
step:1313/1750 train_time:129627ms step_avg:98.73ms
step:1314/1750 train_time:129727ms step_avg:98.73ms
step:1315/1750 train_time:129828ms step_avg:98.73ms
step:1316/1750 train_time:129928ms step_avg:98.73ms
step:1317/1750 train_time:130028ms step_avg:98.73ms
step:1318/1750 train_time:130130ms step_avg:98.73ms
step:1319/1750 train_time:130230ms step_avg:98.73ms
step:1320/1750 train_time:130334ms step_avg:98.74ms
step:1321/1750 train_time:130436ms step_avg:98.74ms
step:1322/1750 train_time:130537ms step_avg:98.74ms
step:1323/1750 train_time:130637ms step_avg:98.74ms
step:1324/1750 train_time:130739ms step_avg:98.75ms
step:1325/1750 train_time:130840ms step_avg:98.75ms
step:1326/1750 train_time:130943ms step_avg:98.75ms
step:1327/1750 train_time:131045ms step_avg:98.75ms
step:1328/1750 train_time:131146ms step_avg:98.75ms
step:1329/1750 train_time:131247ms step_avg:98.76ms
step:1330/1750 train_time:131347ms step_avg:98.76ms
step:1331/1750 train_time:131447ms step_avg:98.76ms
step:1332/1750 train_time:131547ms step_avg:98.76ms
step:1333/1750 train_time:131648ms step_avg:98.76ms
step:1334/1750 train_time:131749ms step_avg:98.76ms
step:1335/1750 train_time:131851ms step_avg:98.76ms
step:1336/1750 train_time:131953ms step_avg:98.77ms
step:1337/1750 train_time:132054ms step_avg:98.77ms
step:1338/1750 train_time:132155ms step_avg:98.77ms
step:1339/1750 train_time:132255ms step_avg:98.77ms
step:1340/1750 train_time:132356ms step_avg:98.77ms
step:1341/1750 train_time:132457ms step_avg:98.77ms
step:1342/1750 train_time:132557ms step_avg:98.78ms
step:1343/1750 train_time:132656ms step_avg:98.78ms
step:1344/1750 train_time:132757ms step_avg:98.78ms
step:1345/1750 train_time:132859ms step_avg:98.78ms
step:1346/1750 train_time:132961ms step_avg:98.78ms
step:1347/1750 train_time:133063ms step_avg:98.78ms
step:1348/1750 train_time:133164ms step_avg:98.79ms
step:1349/1750 train_time:133265ms step_avg:98.79ms
step:1350/1750 train_time:133367ms step_avg:98.79ms
step:1351/1750 train_time:133467ms step_avg:98.79ms
step:1352/1750 train_time:133566ms step_avg:98.79ms
step:1353/1750 train_time:133667ms step_avg:98.79ms
step:1354/1750 train_time:133767ms step_avg:98.79ms
step:1355/1750 train_time:133868ms step_avg:98.80ms
step:1356/1750 train_time:133969ms step_avg:98.80ms
step:1357/1750 train_time:134070ms step_avg:98.80ms
step:1358/1750 train_time:134171ms step_avg:98.80ms
step:1359/1750 train_time:134272ms step_avg:98.80ms
step:1360/1750 train_time:134373ms step_avg:98.80ms
step:1361/1750 train_time:134473ms step_avg:98.80ms
step:1362/1750 train_time:134574ms step_avg:98.81ms
step:1363/1750 train_time:134676ms step_avg:98.81ms
step:1364/1750 train_time:134777ms step_avg:98.81ms
step:1365/1750 train_time:134878ms step_avg:98.81ms
step:1366/1750 train_time:134979ms step_avg:98.81ms
step:1367/1750 train_time:135080ms step_avg:98.81ms
step:1368/1750 train_time:135183ms step_avg:98.82ms
step:1369/1750 train_time:135285ms step_avg:98.82ms
step:1370/1750 train_time:135385ms step_avg:98.82ms
step:1371/1750 train_time:135485ms step_avg:98.82ms
step:1372/1750 train_time:135586ms step_avg:98.82ms
step:1373/1750 train_time:135686ms step_avg:98.82ms
step:1374/1750 train_time:135787ms step_avg:98.83ms
step:1375/1750 train_time:135887ms step_avg:98.83ms
step:1375/1750 val_loss:3.3719 train_time:135977ms step_avg:98.89ms
step:1376/1750 train_time:135997ms step_avg:98.83ms
step:1377/1750 train_time:136095ms step_avg:98.83ms
step:1378/1750 train_time:136196ms step_avg:98.84ms
step:1379/1750 train_time:136297ms step_avg:98.84ms
step:1380/1750 train_time:136399ms step_avg:98.84ms
step:1381/1750 train_time:136500ms step_avg:98.84ms
step:1382/1750 train_time:136599ms step_avg:98.84ms
step:1383/1750 train_time:136698ms step_avg:98.84ms
step:1384/1750 train_time:136798ms step_avg:98.84ms
step:1385/1750 train_time:136899ms step_avg:98.84ms
step:1386/1750 train_time:137003ms step_avg:98.85ms
step:1387/1750 train_time:137105ms step_avg:98.85ms
step:1388/1750 train_time:137206ms step_avg:98.85ms
step:1389/1750 train_time:137307ms step_avg:98.85ms
step:1390/1750 train_time:137407ms step_avg:98.85ms
step:1391/1750 train_time:137507ms step_avg:98.85ms
step:1392/1750 train_time:137606ms step_avg:98.86ms
step:1393/1750 train_time:137707ms step_avg:98.86ms
step:1394/1750 train_time:137807ms step_avg:98.86ms
step:1395/1750 train_time:137908ms step_avg:98.86ms
step:1396/1750 train_time:138009ms step_avg:98.86ms
step:1397/1750 train_time:138111ms step_avg:98.86ms
step:1398/1750 train_time:138212ms step_avg:98.86ms
step:1399/1750 train_time:138313ms step_avg:98.87ms
step:1400/1750 train_time:138415ms step_avg:98.87ms
step:1401/1750 train_time:138516ms step_avg:98.87ms
step:1402/1750 train_time:138616ms step_avg:98.87ms
step:1403/1750 train_time:138717ms step_avg:98.87ms
step:1404/1750 train_time:138818ms step_avg:98.87ms
step:1405/1750 train_time:138919ms step_avg:98.87ms
step:1406/1750 train_time:139020ms step_avg:98.88ms
step:1407/1750 train_time:139121ms step_avg:98.88ms
step:1408/1750 train_time:139223ms step_avg:98.88ms
step:1409/1750 train_time:139325ms step_avg:98.88ms
step:1410/1750 train_time:139427ms step_avg:98.88ms
step:1411/1750 train_time:139527ms step_avg:98.89ms
step:1412/1750 train_time:139628ms step_avg:98.89ms
step:1413/1750 train_time:139728ms step_avg:98.89ms
step:1414/1750 train_time:139828ms step_avg:98.89ms
step:1415/1750 train_time:139930ms step_avg:98.89ms
step:1416/1750 train_time:140029ms step_avg:98.89ms
step:1417/1750 train_time:140131ms step_avg:98.89ms
step:1418/1750 train_time:140232ms step_avg:98.89ms
step:1419/1750 train_time:140333ms step_avg:98.90ms
step:1420/1750 train_time:140435ms step_avg:98.90ms
step:1421/1750 train_time:140535ms step_avg:98.90ms
step:1422/1750 train_time:140635ms step_avg:98.90ms
step:1423/1750 train_time:140736ms step_avg:98.90ms
step:1424/1750 train_time:140838ms step_avg:98.90ms
step:1425/1750 train_time:140938ms step_avg:98.90ms
step:1426/1750 train_time:141039ms step_avg:98.91ms
step:1427/1750 train_time:141142ms step_avg:98.91ms
step:1428/1750 train_time:141244ms step_avg:98.91ms
step:1429/1750 train_time:141346ms step_avg:98.91ms
step:1430/1750 train_time:141449ms step_avg:98.92ms
step:1431/1750 train_time:141550ms step_avg:98.92ms
step:1432/1750 train_time:141652ms step_avg:98.92ms
step:1433/1750 train_time:141754ms step_avg:98.92ms
step:1434/1750 train_time:141855ms step_avg:98.92ms
step:1435/1750 train_time:141959ms step_avg:98.93ms
step:1436/1750 train_time:142062ms step_avg:98.93ms
step:1437/1750 train_time:142165ms step_avg:98.93ms
step:1438/1750 train_time:142267ms step_avg:98.93ms
step:1439/1750 train_time:142371ms step_avg:98.94ms
step:1440/1750 train_time:142473ms step_avg:98.94ms
step:1441/1750 train_time:142575ms step_avg:98.94ms
step:1442/1750 train_time:142675ms step_avg:98.94ms
step:1443/1750 train_time:142777ms step_avg:98.94ms
step:1444/1750 train_time:142879ms step_avg:98.95ms
step:1445/1750 train_time:142980ms step_avg:98.95ms
step:1446/1750 train_time:143081ms step_avg:98.95ms
step:1447/1750 train_time:143183ms step_avg:98.95ms
step:1448/1750 train_time:143288ms step_avg:98.96ms
step:1449/1750 train_time:143389ms step_avg:98.96ms
step:1450/1750 train_time:143491ms step_avg:98.96ms
step:1451/1750 train_time:143591ms step_avg:98.96ms
step:1452/1750 train_time:143692ms step_avg:98.96ms
step:1453/1750 train_time:143794ms step_avg:98.96ms
step:1454/1750 train_time:143897ms step_avg:98.97ms
step:1455/1750 train_time:143998ms step_avg:98.97ms
step:1456/1750 train_time:144100ms step_avg:98.97ms
step:1457/1750 train_time:144202ms step_avg:98.97ms
step:1458/1750 train_time:144305ms step_avg:98.97ms
step:1459/1750 train_time:144408ms step_avg:98.98ms
step:1460/1750 train_time:144509ms step_avg:98.98ms
step:1461/1750 train_time:144612ms step_avg:98.98ms
step:1462/1750 train_time:144714ms step_avg:98.98ms
step:1463/1750 train_time:144815ms step_avg:98.98ms
step:1464/1750 train_time:144916ms step_avg:98.99ms
step:1465/1750 train_time:145018ms step_avg:98.99ms
step:1466/1750 train_time:145119ms step_avg:98.99ms
step:1467/1750 train_time:145221ms step_avg:98.99ms
step:1468/1750 train_time:145323ms step_avg:98.99ms
step:1469/1750 train_time:145426ms step_avg:99.00ms
step:1470/1750 train_time:145529ms step_avg:99.00ms
step:1471/1750 train_time:145631ms step_avg:99.00ms
step:1472/1750 train_time:145733ms step_avg:99.00ms
step:1473/1750 train_time:145834ms step_avg:99.00ms
step:1474/1750 train_time:145935ms step_avg:99.01ms
step:1475/1750 train_time:146036ms step_avg:99.01ms
step:1476/1750 train_time:146139ms step_avg:99.01ms
step:1477/1750 train_time:146241ms step_avg:99.01ms
step:1478/1750 train_time:146343ms step_avg:99.01ms
step:1479/1750 train_time:146445ms step_avg:99.02ms
step:1480/1750 train_time:146548ms step_avg:99.02ms
step:1481/1750 train_time:146650ms step_avg:99.02ms
step:1482/1750 train_time:146752ms step_avg:99.02ms
step:1483/1750 train_time:146854ms step_avg:99.02ms
step:1484/1750 train_time:146956ms step_avg:99.03ms
step:1485/1750 train_time:147058ms step_avg:99.03ms
step:1486/1750 train_time:147161ms step_avg:99.03ms
step:1487/1750 train_time:147262ms step_avg:99.03ms
step:1488/1750 train_time:147365ms step_avg:99.04ms
step:1489/1750 train_time:147467ms step_avg:99.04ms
step:1490/1750 train_time:147569ms step_avg:99.04ms
step:1491/1750 train_time:147671ms step_avg:99.04ms
step:1492/1750 train_time:147772ms step_avg:99.04ms
step:1493/1750 train_time:147872ms step_avg:99.04ms
step:1494/1750 train_time:147974ms step_avg:99.05ms
step:1495/1750 train_time:148076ms step_avg:99.05ms
step:1496/1750 train_time:148178ms step_avg:99.05ms
step:1497/1750 train_time:148279ms step_avg:99.05ms
step:1498/1750 train_time:148381ms step_avg:99.05ms
step:1499/1750 train_time:148483ms step_avg:99.05ms
step:1500/1750 train_time:148585ms step_avg:99.06ms
step:1500/1750 val_loss:3.3361 train_time:148677ms step_avg:99.12ms
step:1501/1750 train_time:148697ms step_avg:99.07ms
step:1502/1750 train_time:148802ms step_avg:99.07ms
step:1503/1750 train_time:148904ms step_avg:99.07ms
step:1504/1750 train_time:149005ms step_avg:99.07ms
step:1505/1750 train_time:149106ms step_avg:99.07ms
step:1506/1750 train_time:149206ms step_avg:99.07ms
step:1507/1750 train_time:149307ms step_avg:99.08ms
step:1508/1750 train_time:149408ms step_avg:99.08ms
step:1509/1750 train_time:149509ms step_avg:99.08ms
step:1510/1750 train_time:149610ms step_avg:99.08ms
step:1511/1750 train_time:149716ms step_avg:99.08ms
step:1512/1750 train_time:149818ms step_avg:99.09ms
step:1513/1750 train_time:149920ms step_avg:99.09ms
step:1514/1750 train_time:150022ms step_avg:99.09ms
step:1515/1750 train_time:150126ms step_avg:99.09ms
step:1516/1750 train_time:150227ms step_avg:99.09ms
step:1517/1750 train_time:150327ms step_avg:99.10ms
step:1518/1750 train_time:150428ms step_avg:99.10ms
step:1519/1750 train_time:150530ms step_avg:99.10ms
step:1520/1750 train_time:150632ms step_avg:99.10ms
step:1521/1750 train_time:150733ms step_avg:99.10ms
step:1522/1750 train_time:150835ms step_avg:99.10ms
step:1523/1750 train_time:150937ms step_avg:99.11ms
step:1524/1750 train_time:151039ms step_avg:99.11ms
step:1525/1750 train_time:151142ms step_avg:99.11ms
step:1526/1750 train_time:151244ms step_avg:99.11ms
step:1527/1750 train_time:151346ms step_avg:99.11ms
step:1528/1750 train_time:151451ms step_avg:99.12ms
step:1529/1750 train_time:151552ms step_avg:99.12ms
step:1530/1750 train_time:151656ms step_avg:99.12ms
step:1531/1750 train_time:151756ms step_avg:99.12ms
step:1532/1750 train_time:151858ms step_avg:99.12ms
step:1533/1750 train_time:151960ms step_avg:99.13ms
step:1534/1750 train_time:152061ms step_avg:99.13ms
step:1535/1750 train_time:152162ms step_avg:99.13ms
step:1536/1750 train_time:152264ms step_avg:99.13ms
step:1537/1750 train_time:152366ms step_avg:99.13ms
step:1538/1750 train_time:152466ms step_avg:99.13ms
step:1539/1750 train_time:152568ms step_avg:99.13ms
step:1540/1750 train_time:152670ms step_avg:99.14ms
step:1541/1750 train_time:152772ms step_avg:99.14ms
step:1542/1750 train_time:152875ms step_avg:99.14ms
step:1543/1750 train_time:152976ms step_avg:99.14ms
step:1544/1750 train_time:153079ms step_avg:99.14ms
step:1545/1750 train_time:153181ms step_avg:99.15ms
step:1546/1750 train_time:153282ms step_avg:99.15ms
step:1547/1750 train_time:153385ms step_avg:99.15ms
step:1548/1750 train_time:153488ms step_avg:99.15ms
step:1549/1750 train_time:153591ms step_avg:99.15ms
step:1550/1750 train_time:153692ms step_avg:99.16ms
step:1551/1750 train_time:153795ms step_avg:99.16ms
step:1552/1750 train_time:153896ms step_avg:99.16ms
step:1553/1750 train_time:153997ms step_avg:99.16ms
step:1554/1750 train_time:154100ms step_avg:99.16ms
step:1555/1750 train_time:154201ms step_avg:99.16ms
step:1556/1750 train_time:154302ms step_avg:99.17ms
step:1557/1750 train_time:154405ms step_avg:99.17ms
step:1558/1750 train_time:154508ms step_avg:99.17ms
step:1559/1750 train_time:154611ms step_avg:99.17ms
step:1560/1750 train_time:154713ms step_avg:99.17ms
step:1561/1750 train_time:154814ms step_avg:99.18ms
step:1562/1750 train_time:154916ms step_avg:99.18ms
step:1563/1750 train_time:155019ms step_avg:99.18ms
step:1564/1750 train_time:155120ms step_avg:99.18ms
step:1565/1750 train_time:155220ms step_avg:99.18ms
step:1566/1750 train_time:155322ms step_avg:99.18ms
step:1567/1750 train_time:155425ms step_avg:99.19ms
step:1568/1750 train_time:155527ms step_avg:99.19ms
step:1569/1750 train_time:155629ms step_avg:99.19ms
step:1570/1750 train_time:155731ms step_avg:99.19ms
step:1571/1750 train_time:155833ms step_avg:99.19ms
step:1572/1750 train_time:155934ms step_avg:99.19ms
step:1573/1750 train_time:156037ms step_avg:99.20ms
step:1574/1750 train_time:156139ms step_avg:99.20ms
step:1575/1750 train_time:156241ms step_avg:99.20ms
step:1576/1750 train_time:156343ms step_avg:99.20ms
step:1577/1750 train_time:156446ms step_avg:99.20ms
step:1578/1750 train_time:156548ms step_avg:99.21ms
step:1579/1750 train_time:156650ms step_avg:99.21ms
step:1580/1750 train_time:156753ms step_avg:99.21ms
step:1581/1750 train_time:156853ms step_avg:99.21ms
step:1582/1750 train_time:156954ms step_avg:99.21ms
step:1583/1750 train_time:157058ms step_avg:99.22ms
step:1584/1750 train_time:157161ms step_avg:99.22ms
step:1585/1750 train_time:157263ms step_avg:99.22ms
step:1586/1750 train_time:157366ms step_avg:99.22ms
step:1587/1750 train_time:157467ms step_avg:99.22ms
step:1588/1750 train_time:157569ms step_avg:99.22ms
step:1589/1750 train_time:157670ms step_avg:99.23ms
step:1590/1750 train_time:157771ms step_avg:99.23ms
step:1591/1750 train_time:157872ms step_avg:99.23ms
step:1592/1750 train_time:157974ms step_avg:99.23ms
step:1593/1750 train_time:158076ms step_avg:99.23ms
step:1594/1750 train_time:158182ms step_avg:99.24ms
step:1595/1750 train_time:158283ms step_avg:99.24ms
step:1596/1750 train_time:158385ms step_avg:99.24ms
step:1597/1750 train_time:158487ms step_avg:99.24ms
step:1598/1750 train_time:158588ms step_avg:99.24ms
step:1599/1750 train_time:158689ms step_avg:99.24ms
step:1600/1750 train_time:158792ms step_avg:99.24ms
step:1601/1750 train_time:158893ms step_avg:99.25ms
step:1602/1750 train_time:158994ms step_avg:99.25ms
step:1603/1750 train_time:159098ms step_avg:99.25ms
step:1604/1750 train_time:159199ms step_avg:99.25ms
step:1605/1750 train_time:159303ms step_avg:99.25ms
step:1606/1750 train_time:159405ms step_avg:99.26ms
step:1607/1750 train_time:159506ms step_avg:99.26ms
step:1608/1750 train_time:159608ms step_avg:99.26ms
step:1609/1750 train_time:159710ms step_avg:99.26ms
step:1610/1750 train_time:159812ms step_avg:99.26ms
step:1611/1750 train_time:159913ms step_avg:99.26ms
step:1612/1750 train_time:160016ms step_avg:99.27ms
step:1613/1750 train_time:160118ms step_avg:99.27ms
step:1614/1750 train_time:160219ms step_avg:99.27ms
step:1615/1750 train_time:160319ms step_avg:99.27ms
step:1616/1750 train_time:160421ms step_avg:99.27ms
step:1617/1750 train_time:160524ms step_avg:99.27ms
step:1618/1750 train_time:160626ms step_avg:99.27ms
step:1619/1750 train_time:160728ms step_avg:99.28ms
step:1620/1750 train_time:160832ms step_avg:99.28ms
step:1621/1750 train_time:160932ms step_avg:99.28ms
step:1622/1750 train_time:161034ms step_avg:99.28ms
step:1623/1750 train_time:161137ms step_avg:99.28ms
step:1624/1750 train_time:161241ms step_avg:99.29ms
step:1625/1750 train_time:161343ms step_avg:99.29ms
step:1625/1750 val_loss:3.3057 train_time:161435ms step_avg:99.34ms
step:1626/1750 train_time:161456ms step_avg:99.30ms
step:1627/1750 train_time:161556ms step_avg:99.30ms
step:1628/1750 train_time:161660ms step_avg:99.30ms
step:1629/1750 train_time:161761ms step_avg:99.30ms
step:1630/1750 train_time:161862ms step_avg:99.30ms
step:1631/1750 train_time:161963ms step_avg:99.30ms
step:1632/1750 train_time:162064ms step_avg:99.30ms
step:1633/1750 train_time:162164ms step_avg:99.30ms
step:1634/1750 train_time:162267ms step_avg:99.31ms
step:1635/1750 train_time:162367ms step_avg:99.31ms
step:1636/1750 train_time:162471ms step_avg:99.31ms
step:1637/1750 train_time:162575ms step_avg:99.31ms
step:1638/1750 train_time:162678ms step_avg:99.31ms
step:1639/1750 train_time:162780ms step_avg:99.32ms
step:1640/1750 train_time:162882ms step_avg:99.32ms
step:1641/1750 train_time:162983ms step_avg:99.32ms
step:1642/1750 train_time:163085ms step_avg:99.32ms
step:1643/1750 train_time:163185ms step_avg:99.32ms
step:1644/1750 train_time:163286ms step_avg:99.32ms
step:1645/1750 train_time:163387ms step_avg:99.32ms
step:1646/1750 train_time:163492ms step_avg:99.33ms
step:1647/1750 train_time:163596ms step_avg:99.33ms
step:1648/1750 train_time:163699ms step_avg:99.33ms
step:1649/1750 train_time:163801ms step_avg:99.33ms
step:1650/1750 train_time:163903ms step_avg:99.33ms
step:1651/1750 train_time:164004ms step_avg:99.34ms
step:1652/1750 train_time:164106ms step_avg:99.34ms
step:1653/1750 train_time:164208ms step_avg:99.34ms
step:1654/1750 train_time:164308ms step_avg:99.34ms
step:1655/1750 train_time:164410ms step_avg:99.34ms
step:1656/1750 train_time:164513ms step_avg:99.34ms
step:1657/1750 train_time:164615ms step_avg:99.34ms
step:1658/1750 train_time:164716ms step_avg:99.35ms
step:1659/1750 train_time:164820ms step_avg:99.35ms
step:1660/1750 train_time:164922ms step_avg:99.35ms
step:1661/1750 train_time:165024ms step_avg:99.35ms
step:1662/1750 train_time:165127ms step_avg:99.35ms
step:1663/1750 train_time:165229ms step_avg:99.36ms
step:1664/1750 train_time:165331ms step_avg:99.36ms
step:1665/1750 train_time:165435ms step_avg:99.36ms
step:1666/1750 train_time:165537ms step_avg:99.36ms
step:1667/1750 train_time:165639ms step_avg:99.36ms
step:1668/1750 train_time:165742ms step_avg:99.37ms
step:1669/1750 train_time:165844ms step_avg:99.37ms
step:1670/1750 train_time:165944ms step_avg:99.37ms
step:1671/1750 train_time:166045ms step_avg:99.37ms
step:1672/1750 train_time:166147ms step_avg:99.37ms
step:1673/1750 train_time:166249ms step_avg:99.37ms
step:1674/1750 train_time:166350ms step_avg:99.37ms
step:1675/1750 train_time:166452ms step_avg:99.37ms
step:1676/1750 train_time:166555ms step_avg:99.38ms
step:1677/1750 train_time:166657ms step_avg:99.38ms
step:1678/1750 train_time:166760ms step_avg:99.38ms
step:1679/1750 train_time:166862ms step_avg:99.38ms
step:1680/1750 train_time:166963ms step_avg:99.38ms
step:1681/1750 train_time:167065ms step_avg:99.38ms
step:1682/1750 train_time:167170ms step_avg:99.39ms
step:1683/1750 train_time:167272ms step_avg:99.39ms
step:1684/1750 train_time:167374ms step_avg:99.39ms
step:1685/1750 train_time:167477ms step_avg:99.39ms
step:1686/1750 train_time:167578ms step_avg:99.39ms
step:1687/1750 train_time:167681ms step_avg:99.40ms
step:1688/1750 train_time:167784ms step_avg:99.40ms
step:1689/1750 train_time:167887ms step_avg:99.40ms
step:1690/1750 train_time:167989ms step_avg:99.40ms
step:1691/1750 train_time:168091ms step_avg:99.40ms
step:1692/1750 train_time:168193ms step_avg:99.41ms
step:1693/1750 train_time:168297ms step_avg:99.41ms
step:1694/1750 train_time:168401ms step_avg:99.41ms
step:1695/1750 train_time:168504ms step_avg:99.41ms
step:1696/1750 train_time:168606ms step_avg:99.41ms
step:1697/1750 train_time:168711ms step_avg:99.42ms
step:1698/1750 train_time:168813ms step_avg:99.42ms
step:1699/1750 train_time:168916ms step_avg:99.42ms
step:1700/1750 train_time:169020ms step_avg:99.42ms
step:1701/1750 train_time:169122ms step_avg:99.42ms
step:1702/1750 train_time:169226ms step_avg:99.43ms
step:1703/1750 train_time:169328ms step_avg:99.43ms
step:1704/1750 train_time:169430ms step_avg:99.43ms
step:1705/1750 train_time:169531ms step_avg:99.43ms
step:1706/1750 train_time:169634ms step_avg:99.43ms
step:1707/1750 train_time:169737ms step_avg:99.44ms
step:1708/1750 train_time:169841ms step_avg:99.44ms
step:1709/1750 train_time:169943ms step_avg:99.44ms
step:1710/1750 train_time:170046ms step_avg:99.44ms
step:1711/1750 train_time:170151ms step_avg:99.45ms
step:1712/1750 train_time:170253ms step_avg:99.45ms
step:1713/1750 train_time:170356ms step_avg:99.45ms
step:1714/1750 train_time:170460ms step_avg:99.45ms
step:1715/1750 train_time:170564ms step_avg:99.45ms
step:1716/1750 train_time:170666ms step_avg:99.46ms
step:1717/1750 train_time:170769ms step_avg:99.46ms
step:1718/1750 train_time:170871ms step_avg:99.46ms
step:1719/1750 train_time:170976ms step_avg:99.46ms
step:1720/1750 train_time:171079ms step_avg:99.46ms
step:1721/1750 train_time:171181ms step_avg:99.47ms
step:1722/1750 train_time:171284ms step_avg:99.47ms
step:1723/1750 train_time:171386ms step_avg:99.47ms
step:1724/1750 train_time:171489ms step_avg:99.47ms
step:1725/1750 train_time:171593ms step_avg:99.47ms
step:1726/1750 train_time:171694ms step_avg:99.48ms
step:1727/1750 train_time:171798ms step_avg:99.48ms
step:1728/1750 train_time:171902ms step_avg:99.48ms
step:1729/1750 train_time:172005ms step_avg:99.48ms
step:1730/1750 train_time:172106ms step_avg:99.48ms
step:1731/1750 train_time:172210ms step_avg:99.49ms
step:1732/1750 train_time:172313ms step_avg:99.49ms
step:1733/1750 train_time:172415ms step_avg:99.49ms
step:1734/1750 train_time:172520ms step_avg:99.49ms
step:1735/1750 train_time:172622ms step_avg:99.49ms
step:1736/1750 train_time:172723ms step_avg:99.49ms
step:1737/1750 train_time:172826ms step_avg:99.50ms
step:1738/1750 train_time:172928ms step_avg:99.50ms
step:1739/1750 train_time:173030ms step_avg:99.50ms
step:1740/1750 train_time:173132ms step_avg:99.50ms
step:1741/1750 train_time:173238ms step_avg:99.51ms
step:1742/1750 train_time:173342ms step_avg:99.51ms
step:1743/1750 train_time:173445ms step_avg:99.51ms
step:1744/1750 train_time:173549ms step_avg:99.51ms
step:1745/1750 train_time:173650ms step_avg:99.51ms
step:1746/1750 train_time:173753ms step_avg:99.51ms
step:1747/1750 train_time:173857ms step_avg:99.52ms
step:1748/1750 train_time:173960ms step_avg:99.52ms
step:1749/1750 train_time:174062ms step_avg:99.52ms
step:1750/1750 train_time:174164ms step_avg:99.52ms
step:1750/1750 val_loss:3.2827 train_time:174256ms step_avg:99.57ms
peak memory allocated: 33278 MiB reserved: 49014 MiB
