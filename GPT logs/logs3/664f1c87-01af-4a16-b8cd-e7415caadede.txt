import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)
                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_reduce_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 27.5
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos : pos + max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()
    starts = []
    for i in range(1, len(boundary_positions)):
        end = boundary_positions[i].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == dist.get_world_size():
                return starts, end - pos
            start = end
    assert False # increase max_batch_span if necessary

def distributed_data_generator(filename_pattern: str, batch_size: int, align_to_bos: bool):
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    max_batch_span = 2 * batch_size if align_to_bos else batch_size # provide buffer to handle samples up to length local_batch_size
    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            batch_span = batch_size
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1750 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.11 | packaged by conda-forge | (main, Jun  4 2025, 14:45:31) [GCC 13.3.0]
Running PyTorch 2.9.0.dev20250824+cu126 compiled for CUDA 12.6
Sun Aug 24 16:13:59 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:04:00.0 Off |                    0 |
| N/A   32C    P0            117W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:23:00.0 Off |                    0 |
| N/A   29C    P0            113W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:43:00.0 Off |                    0 |
| N/A   31C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   29C    P0            115W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:84:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:A3:00.0 Off |                    0 |
| N/A   30C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:C3:00.0 Off |                    0 |
| N/A   31C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E4:00.0 Off |                    0 |
| N/A   28C    P0            112W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.07ms
step:1/1750 train_time:142ms step_avg:141.85ms
step:2/1750 train_time:169ms step_avg:84.29ms
step:3/1750 train_time:238ms step_avg:79.35ms
step:4/1750 train_time:329ms step_avg:82.34ms
step:5/1750 train_time:421ms step_avg:84.28ms
step:6/1750 train_time:514ms step_avg:85.62ms
step:7/1750 train_time:607ms step_avg:86.68ms
step:8/1750 train_time:699ms step_avg:87.34ms
step:9/1750 train_time:791ms step_avg:87.89ms
step:10/1750 train_time:883ms step_avg:88.35ms
step:11/1750 train_time:976ms step_avg:88.70ms
step:12/1750 train_time:1073ms step_avg:89.39ms
step:13/1750 train_time:1169ms step_avg:89.93ms
step:14/1750 train_time:1263ms step_avg:90.19ms
step:15/1750 train_time:1356ms step_avg:90.39ms
step:16/1750 train_time:1448ms step_avg:90.53ms
step:17/1750 train_time:1541ms step_avg:90.68ms
step:18/1750 train_time:1634ms step_avg:90.78ms
step:19/1750 train_time:1726ms step_avg:90.87ms
step:20/1750 train_time:1818ms step_avg:90.92ms
step:21/1750 train_time:1911ms step_avg:91.02ms
step:22/1750 train_time:2004ms step_avg:91.09ms
step:23/1750 train_time:2098ms step_avg:91.23ms
step:24/1750 train_time:2193ms step_avg:91.36ms
step:25/1750 train_time:2287ms step_avg:91.48ms
step:26/1750 train_time:2380ms step_avg:91.55ms
step:27/1750 train_time:2473ms step_avg:91.60ms
step:28/1750 train_time:2566ms step_avg:91.63ms
step:29/1750 train_time:2660ms step_avg:91.71ms
step:30/1750 train_time:2752ms step_avg:91.75ms
step:31/1750 train_time:2845ms step_avg:91.79ms
step:32/1750 train_time:2939ms step_avg:91.84ms
step:33/1750 train_time:3032ms step_avg:91.88ms
step:34/1750 train_time:3126ms step_avg:91.95ms
step:35/1750 train_time:3221ms step_avg:92.02ms
step:36/1750 train_time:3314ms step_avg:92.06ms
step:37/1750 train_time:3407ms step_avg:92.09ms
step:38/1750 train_time:3500ms step_avg:92.12ms
step:39/1750 train_time:3594ms step_avg:92.15ms
step:40/1750 train_time:3687ms step_avg:92.17ms
step:41/1750 train_time:3780ms step_avg:92.20ms
step:42/1750 train_time:3873ms step_avg:92.22ms
step:43/1750 train_time:3965ms step_avg:92.22ms
step:44/1750 train_time:4059ms step_avg:92.25ms
step:45/1750 train_time:4152ms step_avg:92.27ms
step:46/1750 train_time:4246ms step_avg:92.30ms
step:47/1750 train_time:4340ms step_avg:92.34ms
step:48/1750 train_time:4433ms step_avg:92.36ms
step:49/1750 train_time:4527ms step_avg:92.38ms
step:50/1750 train_time:4620ms step_avg:92.39ms
step:51/1750 train_time:4712ms step_avg:92.40ms
step:52/1750 train_time:4805ms step_avg:92.40ms
step:53/1750 train_time:4897ms step_avg:92.40ms
step:54/1750 train_time:4991ms step_avg:92.43ms
step:55/1750 train_time:5084ms step_avg:92.44ms
step:56/1750 train_time:5178ms step_avg:92.46ms
step:57/1750 train_time:5272ms step_avg:92.50ms
step:58/1750 train_time:5366ms step_avg:92.51ms
step:59/1750 train_time:5459ms step_avg:92.53ms
step:60/1750 train_time:5553ms step_avg:92.55ms
step:61/1750 train_time:5646ms step_avg:92.55ms
step:62/1750 train_time:5738ms step_avg:92.55ms
step:63/1750 train_time:5831ms step_avg:92.56ms
step:64/1750 train_time:5924ms step_avg:92.56ms
step:65/1750 train_time:6017ms step_avg:92.56ms
step:66/1750 train_time:6111ms step_avg:92.59ms
step:67/1750 train_time:6203ms step_avg:92.59ms
step:68/1750 train_time:6296ms step_avg:92.59ms
step:69/1750 train_time:6390ms step_avg:92.61ms
step:70/1750 train_time:6484ms step_avg:92.62ms
step:71/1750 train_time:6577ms step_avg:92.64ms
step:72/1750 train_time:6670ms step_avg:92.64ms
step:73/1750 train_time:6764ms step_avg:92.65ms
step:74/1750 train_time:6857ms step_avg:92.66ms
step:75/1750 train_time:6950ms step_avg:92.67ms
step:76/1750 train_time:7042ms step_avg:92.66ms
step:77/1750 train_time:7135ms step_avg:92.67ms
step:78/1750 train_time:7229ms step_avg:92.68ms
step:79/1750 train_time:7323ms step_avg:92.69ms
step:80/1750 train_time:7416ms step_avg:92.70ms
step:81/1750 train_time:7509ms step_avg:92.71ms
step:82/1750 train_time:7603ms step_avg:92.72ms
step:83/1750 train_time:7696ms step_avg:92.73ms
step:84/1750 train_time:7789ms step_avg:92.73ms
step:85/1750 train_time:7883ms step_avg:92.74ms
step:86/1750 train_time:7975ms step_avg:92.74ms
step:87/1750 train_time:8068ms step_avg:92.74ms
step:88/1750 train_time:8162ms step_avg:92.74ms
step:89/1750 train_time:8255ms step_avg:92.75ms
step:90/1750 train_time:8348ms step_avg:92.75ms
step:91/1750 train_time:8441ms step_avg:92.76ms
step:92/1750 train_time:8534ms step_avg:92.76ms
step:93/1750 train_time:8627ms step_avg:92.76ms
step:94/1750 train_time:8721ms step_avg:92.78ms
step:95/1750 train_time:8814ms step_avg:92.77ms
step:96/1750 train_time:8907ms step_avg:92.78ms
step:97/1750 train_time:9000ms step_avg:92.79ms
step:98/1750 train_time:9093ms step_avg:92.79ms
step:99/1750 train_time:9186ms step_avg:92.79ms
step:100/1750 train_time:9280ms step_avg:92.80ms
step:101/1750 train_time:9373ms step_avg:92.80ms
step:102/1750 train_time:9466ms step_avg:92.81ms
step:103/1750 train_time:9560ms step_avg:92.82ms
step:104/1750 train_time:9654ms step_avg:92.82ms
step:105/1750 train_time:9747ms step_avg:92.83ms
step:106/1750 train_time:9840ms step_avg:92.83ms
step:107/1750 train_time:9933ms step_avg:92.83ms
step:108/1750 train_time:10026ms step_avg:92.84ms
step:109/1750 train_time:10120ms step_avg:92.84ms
step:110/1750 train_time:10213ms step_avg:92.84ms
step:111/1750 train_time:10306ms step_avg:92.84ms
step:112/1750 train_time:10399ms step_avg:92.85ms
step:113/1750 train_time:10492ms step_avg:92.85ms
step:114/1750 train_time:10585ms step_avg:92.85ms
step:115/1750 train_time:10678ms step_avg:92.86ms
step:116/1750 train_time:10772ms step_avg:92.86ms
step:117/1750 train_time:10865ms step_avg:92.86ms
step:118/1750 train_time:10960ms step_avg:92.88ms
step:119/1750 train_time:11053ms step_avg:92.88ms
step:120/1750 train_time:11145ms step_avg:92.88ms
step:121/1750 train_time:11239ms step_avg:92.88ms
step:122/1750 train_time:11332ms step_avg:92.88ms
step:123/1750 train_time:11425ms step_avg:92.88ms
step:124/1750 train_time:11518ms step_avg:92.89ms
step:125/1750 train_time:11612ms step_avg:92.89ms
step:125/1750 val_loss:4.6483 train_time:11700ms step_avg:93.60ms
step:126/1750 train_time:11725ms step_avg:93.06ms
step:127/1750 train_time:11806ms step_avg:92.96ms
step:128/1750 train_time:11905ms step_avg:93.01ms
step:129/1750 train_time:11999ms step_avg:93.02ms
step:130/1750 train_time:12092ms step_avg:93.02ms
step:131/1750 train_time:12185ms step_avg:93.01ms
step:132/1750 train_time:12277ms step_avg:93.01ms
step:133/1750 train_time:12369ms step_avg:93.00ms
step:134/1750 train_time:12462ms step_avg:93.00ms
step:135/1750 train_time:12554ms step_avg:92.99ms
step:136/1750 train_time:12647ms step_avg:92.99ms
step:137/1750 train_time:12742ms step_avg:93.01ms
step:138/1750 train_time:12839ms step_avg:93.03ms
step:139/1750 train_time:12933ms step_avg:93.04ms
step:140/1750 train_time:13028ms step_avg:93.06ms
step:141/1750 train_time:13122ms step_avg:93.06ms
step:142/1750 train_time:13215ms step_avg:93.06ms
step:143/1750 train_time:13308ms step_avg:93.06ms
step:144/1750 train_time:13401ms step_avg:93.06ms
step:145/1750 train_time:13494ms step_avg:93.06ms
step:146/1750 train_time:13587ms step_avg:93.06ms
step:147/1750 train_time:13680ms step_avg:93.06ms
step:148/1750 train_time:13775ms step_avg:93.07ms
step:149/1750 train_time:13868ms step_avg:93.08ms
step:150/1750 train_time:13963ms step_avg:93.09ms
step:151/1750 train_time:14057ms step_avg:93.09ms
step:152/1750 train_time:14150ms step_avg:93.10ms
step:153/1750 train_time:14244ms step_avg:93.10ms
step:154/1750 train_time:14337ms step_avg:93.10ms
step:155/1750 train_time:14430ms step_avg:93.10ms
step:156/1750 train_time:14522ms step_avg:93.09ms
step:157/1750 train_time:14615ms step_avg:93.09ms
step:158/1750 train_time:14708ms step_avg:93.09ms
step:159/1750 train_time:14802ms step_avg:93.09ms
step:160/1750 train_time:14896ms step_avg:93.10ms
step:161/1750 train_time:14990ms step_avg:93.11ms
step:162/1750 train_time:15085ms step_avg:93.11ms
step:163/1750 train_time:15178ms step_avg:93.12ms
step:164/1750 train_time:15271ms step_avg:93.12ms
step:165/1750 train_time:15365ms step_avg:93.12ms
step:166/1750 train_time:15458ms step_avg:93.12ms
step:167/1750 train_time:15551ms step_avg:93.12ms
step:168/1750 train_time:15644ms step_avg:93.12ms
step:169/1750 train_time:15738ms step_avg:93.12ms
step:170/1750 train_time:15831ms step_avg:93.12ms
step:171/1750 train_time:15925ms step_avg:93.13ms
step:172/1750 train_time:16019ms step_avg:93.13ms
step:173/1750 train_time:16113ms step_avg:93.14ms
step:174/1750 train_time:16207ms step_avg:93.14ms
step:175/1750 train_time:16300ms step_avg:93.15ms
step:176/1750 train_time:16394ms step_avg:93.15ms
step:177/1750 train_time:16487ms step_avg:93.15ms
step:178/1750 train_time:16580ms step_avg:93.14ms
step:179/1750 train_time:16673ms step_avg:93.15ms
step:180/1750 train_time:16766ms step_avg:93.15ms
step:181/1750 train_time:16860ms step_avg:93.15ms
step:182/1750 train_time:16954ms step_avg:93.15ms
step:183/1750 train_time:17048ms step_avg:93.16ms
step:184/1750 train_time:17141ms step_avg:93.16ms
step:185/1750 train_time:17234ms step_avg:93.16ms
step:186/1750 train_time:17328ms step_avg:93.16ms
step:187/1750 train_time:17421ms step_avg:93.16ms
step:188/1750 train_time:17514ms step_avg:93.16ms
step:189/1750 train_time:17607ms step_avg:93.16ms
step:190/1750 train_time:17701ms step_avg:93.16ms
step:191/1750 train_time:17794ms step_avg:93.16ms
step:192/1750 train_time:17888ms step_avg:93.17ms
step:193/1750 train_time:17982ms step_avg:93.17ms
step:194/1750 train_time:18075ms step_avg:93.17ms
step:195/1750 train_time:18169ms step_avg:93.17ms
step:196/1750 train_time:18263ms step_avg:93.18ms
step:197/1750 train_time:18357ms step_avg:93.18ms
step:198/1750 train_time:18450ms step_avg:93.18ms
step:199/1750 train_time:18543ms step_avg:93.18ms
step:200/1750 train_time:18636ms step_avg:93.18ms
step:201/1750 train_time:18729ms step_avg:93.18ms
step:202/1750 train_time:18823ms step_avg:93.18ms
step:203/1750 train_time:18917ms step_avg:93.19ms
step:204/1750 train_time:19010ms step_avg:93.19ms
step:205/1750 train_time:19104ms step_avg:93.19ms
step:206/1750 train_time:19198ms step_avg:93.19ms
step:207/1750 train_time:19292ms step_avg:93.20ms
step:208/1750 train_time:19385ms step_avg:93.20ms
step:209/1750 train_time:19479ms step_avg:93.20ms
step:210/1750 train_time:19573ms step_avg:93.20ms
step:211/1750 train_time:19666ms step_avg:93.21ms
step:212/1750 train_time:19760ms step_avg:93.21ms
step:213/1750 train_time:19853ms step_avg:93.21ms
step:214/1750 train_time:19947ms step_avg:93.21ms
step:215/1750 train_time:20042ms step_avg:93.22ms
step:216/1750 train_time:20135ms step_avg:93.22ms
step:217/1750 train_time:20228ms step_avg:93.22ms
step:218/1750 train_time:20322ms step_avg:93.22ms
step:219/1750 train_time:20416ms step_avg:93.22ms
step:220/1750 train_time:20509ms step_avg:93.22ms
step:221/1750 train_time:20603ms step_avg:93.23ms
step:222/1750 train_time:20696ms step_avg:93.23ms
step:223/1750 train_time:20790ms step_avg:93.23ms
step:224/1750 train_time:20884ms step_avg:93.23ms
step:225/1750 train_time:20977ms step_avg:93.23ms
step:226/1750 train_time:21070ms step_avg:93.23ms
step:227/1750 train_time:21164ms step_avg:93.23ms
step:228/1750 train_time:21258ms step_avg:93.23ms
step:229/1750 train_time:21352ms step_avg:93.24ms
step:230/1750 train_time:21446ms step_avg:93.24ms
step:231/1750 train_time:21539ms step_avg:93.24ms
step:232/1750 train_time:21632ms step_avg:93.24ms
step:233/1750 train_time:21726ms step_avg:93.24ms
step:234/1750 train_time:21819ms step_avg:93.24ms
step:235/1750 train_time:21913ms step_avg:93.25ms
step:236/1750 train_time:22007ms step_avg:93.25ms
step:237/1750 train_time:22100ms step_avg:93.25ms
step:238/1750 train_time:22194ms step_avg:93.25ms
step:239/1750 train_time:22288ms step_avg:93.25ms
step:240/1750 train_time:22381ms step_avg:93.25ms
step:241/1750 train_time:22475ms step_avg:93.26ms
step:242/1750 train_time:22568ms step_avg:93.26ms
step:243/1750 train_time:22662ms step_avg:93.26ms
step:244/1750 train_time:22756ms step_avg:93.26ms
step:245/1750 train_time:22850ms step_avg:93.26ms
step:246/1750 train_time:22943ms step_avg:93.26ms
step:247/1750 train_time:23036ms step_avg:93.26ms
step:248/1750 train_time:23130ms step_avg:93.26ms
step:249/1750 train_time:23224ms step_avg:93.27ms
step:250/1750 train_time:23318ms step_avg:93.27ms
step:250/1750 val_loss:4.0928 train_time:23407ms step_avg:93.63ms
step:251/1750 train_time:23430ms step_avg:93.35ms
step:252/1750 train_time:23514ms step_avg:93.31ms
step:253/1750 train_time:23613ms step_avg:93.33ms
step:254/1750 train_time:23710ms step_avg:93.34ms
step:255/1750 train_time:23802ms step_avg:93.34ms
step:256/1750 train_time:23895ms step_avg:93.34ms
step:257/1750 train_time:23987ms step_avg:93.33ms
step:258/1750 train_time:24079ms step_avg:93.33ms
step:259/1750 train_time:24172ms step_avg:93.33ms
step:260/1750 train_time:24264ms step_avg:93.32ms
step:261/1750 train_time:24359ms step_avg:93.33ms
step:262/1750 train_time:24456ms step_avg:93.34ms
step:263/1750 train_time:24551ms step_avg:93.35ms
step:264/1750 train_time:24647ms step_avg:93.36ms
step:265/1750 train_time:24742ms step_avg:93.37ms
step:266/1750 train_time:24835ms step_avg:93.36ms
step:267/1750 train_time:24929ms step_avg:93.37ms
step:268/1750 train_time:25022ms step_avg:93.37ms
step:269/1750 train_time:25115ms step_avg:93.37ms
step:270/1750 train_time:25209ms step_avg:93.37ms
step:271/1750 train_time:25303ms step_avg:93.37ms
step:272/1750 train_time:25398ms step_avg:93.38ms
step:273/1750 train_time:25494ms step_avg:93.38ms
step:274/1750 train_time:25588ms step_avg:93.39ms
step:275/1750 train_time:25683ms step_avg:93.39ms
step:276/1750 train_time:25777ms step_avg:93.40ms
step:277/1750 train_time:25871ms step_avg:93.40ms
step:278/1750 train_time:25965ms step_avg:93.40ms
step:279/1750 train_time:26058ms step_avg:93.40ms
step:280/1750 train_time:26152ms step_avg:93.40ms
step:281/1750 train_time:26246ms step_avg:93.40ms
step:282/1750 train_time:26341ms step_avg:93.41ms
step:283/1750 train_time:26435ms step_avg:93.41ms
step:284/1750 train_time:26529ms step_avg:93.41ms
step:285/1750 train_time:26623ms step_avg:93.42ms
step:286/1750 train_time:26718ms step_avg:93.42ms
step:287/1750 train_time:26812ms step_avg:93.42ms
step:288/1750 train_time:26905ms step_avg:93.42ms
step:289/1750 train_time:26999ms step_avg:93.42ms
step:290/1750 train_time:27093ms step_avg:93.42ms
step:291/1750 train_time:27186ms step_avg:93.42ms
step:292/1750 train_time:27280ms step_avg:93.42ms
step:293/1750 train_time:27374ms step_avg:93.43ms
step:294/1750 train_time:27468ms step_avg:93.43ms
step:295/1750 train_time:27563ms step_avg:93.43ms
step:296/1750 train_time:27657ms step_avg:93.43ms
step:297/1750 train_time:27751ms step_avg:93.44ms
step:298/1750 train_time:27845ms step_avg:93.44ms
step:299/1750 train_time:27939ms step_avg:93.44ms
step:300/1750 train_time:28033ms step_avg:93.44ms
step:301/1750 train_time:28127ms step_avg:93.44ms
step:302/1750 train_time:28221ms step_avg:93.45ms
step:303/1750 train_time:28314ms step_avg:93.45ms
step:304/1750 train_time:28408ms step_avg:93.45ms
step:305/1750 train_time:28502ms step_avg:93.45ms
step:306/1750 train_time:28597ms step_avg:93.45ms
step:307/1750 train_time:28691ms step_avg:93.46ms
step:308/1750 train_time:28786ms step_avg:93.46ms
step:309/1750 train_time:28880ms step_avg:93.46ms
step:310/1750 train_time:28974ms step_avg:93.47ms
step:311/1750 train_time:29068ms step_avg:93.46ms
step:312/1750 train_time:29162ms step_avg:93.47ms
step:313/1750 train_time:29256ms step_avg:93.47ms
step:314/1750 train_time:29349ms step_avg:93.47ms
step:315/1750 train_time:29443ms step_avg:93.47ms
step:316/1750 train_time:29538ms step_avg:93.47ms
step:317/1750 train_time:29632ms step_avg:93.48ms
step:318/1750 train_time:29726ms step_avg:93.48ms
step:319/1750 train_time:29820ms step_avg:93.48ms
step:320/1750 train_time:29914ms step_avg:93.48ms
step:321/1750 train_time:30008ms step_avg:93.48ms
step:322/1750 train_time:30102ms step_avg:93.49ms
step:323/1750 train_time:30196ms step_avg:93.49ms
step:324/1750 train_time:30290ms step_avg:93.49ms
step:325/1750 train_time:30384ms step_avg:93.49ms
step:326/1750 train_time:30478ms step_avg:93.49ms
step:327/1750 train_time:30572ms step_avg:93.49ms
step:328/1750 train_time:30666ms step_avg:93.49ms
step:329/1750 train_time:30761ms step_avg:93.50ms
step:330/1750 train_time:30855ms step_avg:93.50ms
step:331/1750 train_time:30949ms step_avg:93.50ms
step:332/1750 train_time:31043ms step_avg:93.50ms
step:333/1750 train_time:31137ms step_avg:93.50ms
step:334/1750 train_time:31231ms step_avg:93.51ms
step:335/1750 train_time:31326ms step_avg:93.51ms
step:336/1750 train_time:31420ms step_avg:93.51ms
step:337/1750 train_time:31515ms step_avg:93.52ms
step:338/1750 train_time:31609ms step_avg:93.52ms
step:339/1750 train_time:31703ms step_avg:93.52ms
step:340/1750 train_time:31797ms step_avg:93.52ms
step:341/1750 train_time:31892ms step_avg:93.53ms
step:342/1750 train_time:31987ms step_avg:93.53ms
step:343/1750 train_time:32081ms step_avg:93.53ms
step:344/1750 train_time:32174ms step_avg:93.53ms
step:345/1750 train_time:32269ms step_avg:93.53ms
step:346/1750 train_time:32363ms step_avg:93.53ms
step:347/1750 train_time:32457ms step_avg:93.54ms
step:348/1750 train_time:32551ms step_avg:93.54ms
step:349/1750 train_time:32646ms step_avg:93.54ms
step:350/1750 train_time:32741ms step_avg:93.55ms
step:351/1750 train_time:32835ms step_avg:93.55ms
step:352/1750 train_time:32930ms step_avg:93.55ms
step:353/1750 train_time:33024ms step_avg:93.55ms
step:354/1750 train_time:33119ms step_avg:93.56ms
step:355/1750 train_time:33212ms step_avg:93.56ms
step:356/1750 train_time:33306ms step_avg:93.55ms
step:357/1750 train_time:33400ms step_avg:93.56ms
step:358/1750 train_time:33494ms step_avg:93.56ms
step:359/1750 train_time:33588ms step_avg:93.56ms
step:360/1750 train_time:33682ms step_avg:93.56ms
step:361/1750 train_time:33777ms step_avg:93.56ms
step:362/1750 train_time:33871ms step_avg:93.57ms
step:363/1750 train_time:33964ms step_avg:93.57ms
step:364/1750 train_time:34059ms step_avg:93.57ms
step:365/1750 train_time:34153ms step_avg:93.57ms
step:366/1750 train_time:34246ms step_avg:93.57ms
step:367/1750 train_time:34340ms step_avg:93.57ms
step:368/1750 train_time:34434ms step_avg:93.57ms
step:369/1750 train_time:34528ms step_avg:93.57ms
step:370/1750 train_time:34622ms step_avg:93.57ms
step:371/1750 train_time:34718ms step_avg:93.58ms
step:372/1750 train_time:34812ms step_avg:93.58ms
step:373/1750 train_time:34906ms step_avg:93.58ms
step:374/1750 train_time:35000ms step_avg:93.58ms
step:375/1750 train_time:35094ms step_avg:93.58ms
step:375/1750 val_loss:3.8857 train_time:35183ms step_avg:93.82ms
step:376/1750 train_time:35206ms step_avg:93.63ms
step:377/1750 train_time:35290ms step_avg:93.61ms
step:378/1750 train_time:35389ms step_avg:93.62ms
step:379/1750 train_time:35484ms step_avg:93.62ms
step:380/1750 train_time:35577ms step_avg:93.62ms
step:381/1750 train_time:35670ms step_avg:93.62ms
step:382/1750 train_time:35763ms step_avg:93.62ms
step:383/1750 train_time:35856ms step_avg:93.62ms
step:384/1750 train_time:35949ms step_avg:93.62ms
step:385/1750 train_time:36042ms step_avg:93.62ms
step:386/1750 train_time:36136ms step_avg:93.62ms
step:387/1750 train_time:36233ms step_avg:93.63ms
step:388/1750 train_time:36328ms step_avg:93.63ms
step:389/1750 train_time:36424ms step_avg:93.63ms
step:390/1750 train_time:36518ms step_avg:93.64ms
step:391/1750 train_time:36614ms step_avg:93.64ms
step:392/1750 train_time:36709ms step_avg:93.65ms
step:393/1750 train_time:36804ms step_avg:93.65ms
step:394/1750 train_time:36899ms step_avg:93.65ms
step:395/1750 train_time:36994ms step_avg:93.65ms
step:396/1750 train_time:37090ms step_avg:93.66ms
step:397/1750 train_time:37187ms step_avg:93.67ms
step:398/1750 train_time:37284ms step_avg:93.68ms
step:399/1750 train_time:37381ms step_avg:93.69ms
step:400/1750 train_time:37477ms step_avg:93.69ms
step:401/1750 train_time:37573ms step_avg:93.70ms
step:402/1750 train_time:37669ms step_avg:93.70ms
step:403/1750 train_time:37765ms step_avg:93.71ms
step:404/1750 train_time:37861ms step_avg:93.72ms
step:405/1750 train_time:37957ms step_avg:93.72ms
step:406/1750 train_time:38052ms step_avg:93.72ms
step:407/1750 train_time:38148ms step_avg:93.73ms
step:408/1750 train_time:38244ms step_avg:93.74ms
step:409/1750 train_time:38340ms step_avg:93.74ms
step:410/1750 train_time:38437ms step_avg:93.75ms
step:411/1750 train_time:38533ms step_avg:93.75ms
step:412/1750 train_time:38628ms step_avg:93.76ms
step:413/1750 train_time:38724ms step_avg:93.76ms
step:414/1750 train_time:38820ms step_avg:93.77ms
step:415/1750 train_time:38915ms step_avg:93.77ms
step:416/1750 train_time:39010ms step_avg:93.77ms
step:417/1750 train_time:39106ms step_avg:93.78ms
step:418/1750 train_time:39202ms step_avg:93.79ms
step:419/1750 train_time:39298ms step_avg:93.79ms
step:420/1750 train_time:39395ms step_avg:93.80ms
step:421/1750 train_time:39490ms step_avg:93.80ms
step:422/1750 train_time:39586ms step_avg:93.81ms
step:423/1750 train_time:39682ms step_avg:93.81ms
step:424/1750 train_time:39777ms step_avg:93.81ms
step:425/1750 train_time:39873ms step_avg:93.82ms
step:426/1750 train_time:39969ms step_avg:93.82ms
step:427/1750 train_time:40065ms step_avg:93.83ms
step:428/1750 train_time:40161ms step_avg:93.83ms
step:429/1750 train_time:40257ms step_avg:93.84ms
step:430/1750 train_time:40352ms step_avg:93.84ms
step:431/1750 train_time:40448ms step_avg:93.85ms
step:432/1750 train_time:40544ms step_avg:93.85ms
step:433/1750 train_time:40640ms step_avg:93.86ms
step:434/1750 train_time:40735ms step_avg:93.86ms
step:435/1750 train_time:40831ms step_avg:93.86ms
step:436/1750 train_time:40927ms step_avg:93.87ms
step:437/1750 train_time:41023ms step_avg:93.87ms
step:438/1750 train_time:41118ms step_avg:93.88ms
step:439/1750 train_time:41214ms step_avg:93.88ms
step:440/1750 train_time:41310ms step_avg:93.89ms
step:441/1750 train_time:41406ms step_avg:93.89ms
step:442/1750 train_time:41502ms step_avg:93.90ms
step:443/1750 train_time:41598ms step_avg:93.90ms
step:444/1750 train_time:41694ms step_avg:93.91ms
step:445/1750 train_time:41790ms step_avg:93.91ms
step:446/1750 train_time:41886ms step_avg:93.91ms
step:447/1750 train_time:41981ms step_avg:93.92ms
step:448/1750 train_time:42078ms step_avg:93.92ms
step:449/1750 train_time:42173ms step_avg:93.93ms
step:450/1750 train_time:42269ms step_avg:93.93ms
step:451/1750 train_time:42365ms step_avg:93.94ms
step:452/1750 train_time:42461ms step_avg:93.94ms
step:453/1750 train_time:42557ms step_avg:93.95ms
step:454/1750 train_time:42653ms step_avg:93.95ms
step:455/1750 train_time:42748ms step_avg:93.95ms
step:456/1750 train_time:42844ms step_avg:93.96ms
step:457/1750 train_time:42939ms step_avg:93.96ms
step:458/1750 train_time:43035ms step_avg:93.96ms
step:459/1750 train_time:43131ms step_avg:93.97ms
step:460/1750 train_time:43227ms step_avg:93.97ms
step:461/1750 train_time:43323ms step_avg:93.98ms
step:462/1750 train_time:43419ms step_avg:93.98ms
step:463/1750 train_time:43515ms step_avg:93.99ms
step:464/1750 train_time:43611ms step_avg:93.99ms
step:465/1750 train_time:43707ms step_avg:93.99ms
step:466/1750 train_time:43803ms step_avg:94.00ms
step:467/1750 train_time:43898ms step_avg:94.00ms
step:468/1750 train_time:43995ms step_avg:94.01ms
step:469/1750 train_time:44090ms step_avg:94.01ms
step:470/1750 train_time:44186ms step_avg:94.01ms
step:471/1750 train_time:44282ms step_avg:94.02ms
step:472/1750 train_time:44378ms step_avg:94.02ms
step:473/1750 train_time:44474ms step_avg:94.03ms
step:474/1750 train_time:44569ms step_avg:94.03ms
step:475/1750 train_time:44665ms step_avg:94.03ms
step:476/1750 train_time:44761ms step_avg:94.04ms
step:477/1750 train_time:44857ms step_avg:94.04ms
step:478/1750 train_time:44953ms step_avg:94.04ms
step:479/1750 train_time:45048ms step_avg:94.05ms
step:480/1750 train_time:45144ms step_avg:94.05ms
step:481/1750 train_time:45240ms step_avg:94.05ms
step:482/1750 train_time:45336ms step_avg:94.06ms
step:483/1750 train_time:45431ms step_avg:94.06ms
step:484/1750 train_time:45527ms step_avg:94.06ms
step:485/1750 train_time:45623ms step_avg:94.07ms
step:486/1750 train_time:45718ms step_avg:94.07ms
step:487/1750 train_time:45814ms step_avg:94.07ms
step:488/1750 train_time:45910ms step_avg:94.08ms
step:489/1750 train_time:46007ms step_avg:94.08ms
step:490/1750 train_time:46102ms step_avg:94.09ms
step:491/1750 train_time:46197ms step_avg:94.09ms
step:492/1750 train_time:46293ms step_avg:94.09ms
step:493/1750 train_time:46389ms step_avg:94.10ms
step:494/1750 train_time:46485ms step_avg:94.10ms
step:495/1750 train_time:46580ms step_avg:94.10ms
step:496/1750 train_time:46676ms step_avg:94.10ms
step:497/1750 train_time:46771ms step_avg:94.11ms
step:498/1750 train_time:46867ms step_avg:94.11ms
step:499/1750 train_time:46963ms step_avg:94.11ms
step:500/1750 train_time:47059ms step_avg:94.12ms
step:500/1750 val_loss:3.7417 train_time:47150ms step_avg:94.30ms
step:501/1750 train_time:47171ms step_avg:94.15ms
step:502/1750 train_time:47258ms step_avg:94.14ms
step:503/1750 train_time:47356ms step_avg:94.15ms
step:504/1750 train_time:47452ms step_avg:94.15ms
step:505/1750 train_time:47547ms step_avg:94.15ms
step:506/1750 train_time:47642ms step_avg:94.15ms
step:507/1750 train_time:47738ms step_avg:94.16ms
step:508/1750 train_time:47833ms step_avg:94.16ms
step:509/1750 train_time:47928ms step_avg:94.16ms
step:510/1750 train_time:48022ms step_avg:94.16ms
step:511/1750 train_time:48119ms step_avg:94.17ms
step:512/1750 train_time:48217ms step_avg:94.17ms
step:513/1750 train_time:48313ms step_avg:94.18ms
step:514/1750 train_time:48409ms step_avg:94.18ms
step:515/1750 train_time:48505ms step_avg:94.18ms
step:516/1750 train_time:48600ms step_avg:94.19ms
step:517/1750 train_time:48696ms step_avg:94.19ms
step:518/1750 train_time:48792ms step_avg:94.19ms
step:519/1750 train_time:48887ms step_avg:94.19ms
step:520/1750 train_time:48983ms step_avg:94.20ms
step:521/1750 train_time:49079ms step_avg:94.20ms
step:522/1750 train_time:49176ms step_avg:94.21ms
step:523/1750 train_time:49273ms step_avg:94.21ms
step:524/1750 train_time:49371ms step_avg:94.22ms
step:525/1750 train_time:49467ms step_avg:94.22ms
step:526/1750 train_time:49563ms step_avg:94.23ms
step:527/1750 train_time:49659ms step_avg:94.23ms
step:528/1750 train_time:49755ms step_avg:94.23ms
step:529/1750 train_time:49851ms step_avg:94.24ms
step:530/1750 train_time:49947ms step_avg:94.24ms
step:531/1750 train_time:50043ms step_avg:94.24ms
step:532/1750 train_time:50139ms step_avg:94.25ms
step:533/1750 train_time:50236ms step_avg:94.25ms
step:534/1750 train_time:50333ms step_avg:94.26ms
step:535/1750 train_time:50429ms step_avg:94.26ms
step:536/1750 train_time:50526ms step_avg:94.26ms
step:537/1750 train_time:50623ms step_avg:94.27ms
step:538/1750 train_time:50719ms step_avg:94.27ms
step:539/1750 train_time:50815ms step_avg:94.28ms
step:540/1750 train_time:50910ms step_avg:94.28ms
step:541/1750 train_time:51006ms step_avg:94.28ms
step:542/1750 train_time:51103ms step_avg:94.29ms
step:543/1750 train_time:51199ms step_avg:94.29ms
step:544/1750 train_time:51296ms step_avg:94.29ms
step:545/1750 train_time:51392ms step_avg:94.30ms
step:546/1750 train_time:51489ms step_avg:94.30ms
step:547/1750 train_time:51585ms step_avg:94.31ms
step:548/1750 train_time:51681ms step_avg:94.31ms
step:549/1750 train_time:51777ms step_avg:94.31ms
step:550/1750 train_time:51873ms step_avg:94.32ms
step:551/1750 train_time:51969ms step_avg:94.32ms
step:552/1750 train_time:52066ms step_avg:94.32ms
step:553/1750 train_time:52162ms step_avg:94.32ms
step:554/1750 train_time:52257ms step_avg:94.33ms
step:555/1750 train_time:52354ms step_avg:94.33ms
step:556/1750 train_time:52450ms step_avg:94.33ms
step:557/1750 train_time:52547ms step_avg:94.34ms
step:558/1750 train_time:52643ms step_avg:94.34ms
step:559/1750 train_time:52740ms step_avg:94.35ms
step:560/1750 train_time:52837ms step_avg:94.35ms
step:561/1750 train_time:52934ms step_avg:94.36ms
step:562/1750 train_time:53031ms step_avg:94.36ms
step:563/1750 train_time:53128ms step_avg:94.37ms
step:564/1750 train_time:53223ms step_avg:94.37ms
step:565/1750 train_time:53319ms step_avg:94.37ms
step:566/1750 train_time:53414ms step_avg:94.37ms
step:567/1750 train_time:53510ms step_avg:94.37ms
step:568/1750 train_time:53607ms step_avg:94.38ms
step:569/1750 train_time:53704ms step_avg:94.38ms
step:570/1750 train_time:53800ms step_avg:94.39ms
step:571/1750 train_time:53896ms step_avg:94.39ms
step:572/1750 train_time:53992ms step_avg:94.39ms
step:573/1750 train_time:54088ms step_avg:94.39ms
step:574/1750 train_time:54183ms step_avg:94.40ms
step:575/1750 train_time:54279ms step_avg:94.40ms
step:576/1750 train_time:54376ms step_avg:94.40ms
step:577/1750 train_time:54472ms step_avg:94.41ms
step:578/1750 train_time:54568ms step_avg:94.41ms
step:579/1750 train_time:54664ms step_avg:94.41ms
step:580/1750 train_time:54760ms step_avg:94.41ms
step:581/1750 train_time:54857ms step_avg:94.42ms
step:582/1750 train_time:54953ms step_avg:94.42ms
step:583/1750 train_time:55049ms step_avg:94.42ms
step:584/1750 train_time:55145ms step_avg:94.43ms
step:585/1750 train_time:55241ms step_avg:94.43ms
step:586/1750 train_time:55338ms step_avg:94.43ms
step:587/1750 train_time:55435ms step_avg:94.44ms
step:588/1750 train_time:55531ms step_avg:94.44ms
step:589/1750 train_time:55627ms step_avg:94.44ms
step:590/1750 train_time:55723ms step_avg:94.45ms
step:591/1750 train_time:55819ms step_avg:94.45ms
step:592/1750 train_time:55916ms step_avg:94.45ms
step:593/1750 train_time:56013ms step_avg:94.46ms
step:594/1750 train_time:56109ms step_avg:94.46ms
step:595/1750 train_time:56206ms step_avg:94.46ms
step:596/1750 train_time:56302ms step_avg:94.47ms
step:597/1750 train_time:56399ms step_avg:94.47ms
step:598/1750 train_time:56495ms step_avg:94.47ms
step:599/1750 train_time:56591ms step_avg:94.48ms
step:600/1750 train_time:56687ms step_avg:94.48ms
step:601/1750 train_time:56783ms step_avg:94.48ms
step:602/1750 train_time:56880ms step_avg:94.48ms
step:603/1750 train_time:56976ms step_avg:94.49ms
step:604/1750 train_time:57072ms step_avg:94.49ms
step:605/1750 train_time:57168ms step_avg:94.49ms
step:606/1750 train_time:57263ms step_avg:94.49ms
step:607/1750 train_time:57360ms step_avg:94.50ms
step:608/1750 train_time:57456ms step_avg:94.50ms
step:609/1750 train_time:57553ms step_avg:94.50ms
step:610/1750 train_time:57649ms step_avg:94.51ms
step:611/1750 train_time:57745ms step_avg:94.51ms
step:612/1750 train_time:57841ms step_avg:94.51ms
step:613/1750 train_time:57937ms step_avg:94.51ms
step:614/1750 train_time:58033ms step_avg:94.52ms
step:615/1750 train_time:58130ms step_avg:94.52ms
step:616/1750 train_time:58227ms step_avg:94.52ms
step:617/1750 train_time:58323ms step_avg:94.53ms
step:618/1750 train_time:58419ms step_avg:94.53ms
step:619/1750 train_time:58515ms step_avg:94.53ms
step:620/1750 train_time:58611ms step_avg:94.53ms
step:621/1750 train_time:58707ms step_avg:94.54ms
step:622/1750 train_time:58804ms step_avg:94.54ms
step:623/1750 train_time:58899ms step_avg:94.54ms
step:624/1750 train_time:58995ms step_avg:94.54ms
step:625/1750 train_time:59091ms step_avg:94.55ms
step:625/1750 val_loss:3.6577 train_time:59182ms step_avg:94.69ms
step:626/1750 train_time:59204ms step_avg:94.58ms
step:627/1750 train_time:59290ms step_avg:94.56ms
step:628/1750 train_time:59389ms step_avg:94.57ms
step:629/1750 train_time:59486ms step_avg:94.57ms
step:630/1750 train_time:59581ms step_avg:94.57ms
step:631/1750 train_time:59676ms step_avg:94.57ms
step:632/1750 train_time:59772ms step_avg:94.58ms
step:633/1750 train_time:59867ms step_avg:94.58ms
step:634/1750 train_time:59962ms step_avg:94.58ms
step:635/1750 train_time:60058ms step_avg:94.58ms
step:636/1750 train_time:60157ms step_avg:94.59ms
step:637/1750 train_time:60257ms step_avg:94.59ms
step:638/1750 train_time:60354ms step_avg:94.60ms
step:639/1750 train_time:60453ms step_avg:94.61ms
step:640/1750 train_time:60550ms step_avg:94.61ms
step:641/1750 train_time:60645ms step_avg:94.61ms
step:642/1750 train_time:60741ms step_avg:94.61ms
step:643/1750 train_time:60836ms step_avg:94.61ms
step:644/1750 train_time:60932ms step_avg:94.61ms
step:645/1750 train_time:61027ms step_avg:94.62ms
step:646/1750 train_time:61124ms step_avg:94.62ms
step:647/1750 train_time:61222ms step_avg:94.62ms
step:648/1750 train_time:61320ms step_avg:94.63ms
step:649/1750 train_time:61418ms step_avg:94.63ms
step:650/1750 train_time:61514ms step_avg:94.64ms
step:651/1750 train_time:61612ms step_avg:94.64ms
step:652/1750 train_time:61710ms step_avg:94.65ms
step:653/1750 train_time:61807ms step_avg:94.65ms
step:654/1750 train_time:61904ms step_avg:94.65ms
step:655/1750 train_time:62001ms step_avg:94.66ms
step:656/1750 train_time:62098ms step_avg:94.66ms
step:657/1750 train_time:62196ms step_avg:94.67ms
step:658/1750 train_time:62294ms step_avg:94.67ms
step:659/1750 train_time:62392ms step_avg:94.68ms
step:660/1750 train_time:62490ms step_avg:94.68ms
step:661/1750 train_time:62588ms step_avg:94.69ms
step:662/1750 train_time:62686ms step_avg:94.69ms
step:663/1750 train_time:62784ms step_avg:94.70ms
step:664/1750 train_time:62881ms step_avg:94.70ms
step:665/1750 train_time:62978ms step_avg:94.70ms
step:666/1750 train_time:63075ms step_avg:94.71ms
step:667/1750 train_time:63172ms step_avg:94.71ms
step:668/1750 train_time:63270ms step_avg:94.72ms
step:669/1750 train_time:63368ms step_avg:94.72ms
step:670/1750 train_time:63467ms step_avg:94.73ms
step:671/1750 train_time:63566ms step_avg:94.73ms
step:672/1750 train_time:63664ms step_avg:94.74ms
step:673/1750 train_time:63762ms step_avg:94.74ms
step:674/1750 train_time:63860ms step_avg:94.75ms
step:675/1750 train_time:63957ms step_avg:94.75ms
step:676/1750 train_time:64055ms step_avg:94.76ms
step:677/1750 train_time:64153ms step_avg:94.76ms
step:678/1750 train_time:64252ms step_avg:94.77ms
step:679/1750 train_time:64350ms step_avg:94.77ms
step:680/1750 train_time:64449ms step_avg:94.78ms
step:681/1750 train_time:64547ms step_avg:94.78ms
step:682/1750 train_time:64644ms step_avg:94.79ms
step:683/1750 train_time:64741ms step_avg:94.79ms
step:684/1750 train_time:64839ms step_avg:94.79ms
step:685/1750 train_time:64935ms step_avg:94.80ms
step:686/1750 train_time:65033ms step_avg:94.80ms
step:687/1750 train_time:65131ms step_avg:94.80ms
step:688/1750 train_time:65228ms step_avg:94.81ms
step:689/1750 train_time:65327ms step_avg:94.81ms
step:690/1750 train_time:65424ms step_avg:94.82ms
step:691/1750 train_time:65522ms step_avg:94.82ms
step:692/1750 train_time:65620ms step_avg:94.83ms
step:693/1750 train_time:65718ms step_avg:94.83ms
step:694/1750 train_time:65816ms step_avg:94.84ms
step:695/1750 train_time:65913ms step_avg:94.84ms
step:696/1750 train_time:66010ms step_avg:94.84ms
step:697/1750 train_time:66107ms step_avg:94.85ms
step:698/1750 train_time:66204ms step_avg:94.85ms
step:699/1750 train_time:66303ms step_avg:94.85ms
step:700/1750 train_time:66400ms step_avg:94.86ms
step:701/1750 train_time:66496ms step_avg:94.86ms
step:702/1750 train_time:66594ms step_avg:94.86ms
step:703/1750 train_time:66694ms step_avg:94.87ms
step:704/1750 train_time:66792ms step_avg:94.88ms
step:705/1750 train_time:66890ms step_avg:94.88ms
step:706/1750 train_time:66989ms step_avg:94.88ms
step:707/1750 train_time:67086ms step_avg:94.89ms
step:708/1750 train_time:67183ms step_avg:94.89ms
step:709/1750 train_time:67281ms step_avg:94.90ms
step:710/1750 train_time:67377ms step_avg:94.90ms
step:711/1750 train_time:67475ms step_avg:94.90ms
step:712/1750 train_time:67573ms step_avg:94.91ms
step:713/1750 train_time:67671ms step_avg:94.91ms
step:714/1750 train_time:67769ms step_avg:94.91ms
step:715/1750 train_time:67866ms step_avg:94.92ms
step:716/1750 train_time:67963ms step_avg:94.92ms
step:717/1750 train_time:68060ms step_avg:94.92ms
step:718/1750 train_time:68158ms step_avg:94.93ms
step:719/1750 train_time:68255ms step_avg:94.93ms
step:720/1750 train_time:68353ms step_avg:94.93ms
step:721/1750 train_time:68450ms step_avg:94.94ms
step:722/1750 train_time:68549ms step_avg:94.94ms
step:723/1750 train_time:68646ms step_avg:94.95ms
step:724/1750 train_time:68744ms step_avg:94.95ms
step:725/1750 train_time:68842ms step_avg:94.95ms
step:726/1750 train_time:68939ms step_avg:94.96ms
step:727/1750 train_time:69037ms step_avg:94.96ms
step:728/1750 train_time:69135ms step_avg:94.97ms
step:729/1750 train_time:69232ms step_avg:94.97ms
step:730/1750 train_time:69329ms step_avg:94.97ms
step:731/1750 train_time:69426ms step_avg:94.97ms
step:732/1750 train_time:69524ms step_avg:94.98ms
step:733/1750 train_time:69622ms step_avg:94.98ms
step:734/1750 train_time:69720ms step_avg:94.99ms
step:735/1750 train_time:69819ms step_avg:94.99ms
step:736/1750 train_time:69916ms step_avg:94.99ms
step:737/1750 train_time:70014ms step_avg:95.00ms
step:738/1750 train_time:70111ms step_avg:95.00ms
step:739/1750 train_time:70209ms step_avg:95.01ms
step:740/1750 train_time:70306ms step_avg:95.01ms
step:741/1750 train_time:70404ms step_avg:95.01ms
step:742/1750 train_time:70501ms step_avg:95.02ms
step:743/1750 train_time:70599ms step_avg:95.02ms
step:744/1750 train_time:70697ms step_avg:95.02ms
step:745/1750 train_time:70794ms step_avg:95.03ms
step:746/1750 train_time:70893ms step_avg:95.03ms
step:747/1750 train_time:70992ms step_avg:95.04ms
step:748/1750 train_time:71089ms step_avg:95.04ms
step:749/1750 train_time:71187ms step_avg:95.04ms
step:750/1750 train_time:71284ms step_avg:95.05ms
step:750/1750 val_loss:3.5961 train_time:71377ms step_avg:95.17ms
step:751/1750 train_time:71399ms step_avg:95.07ms
step:752/1750 train_time:71487ms step_avg:95.06ms
step:753/1750 train_time:71586ms step_avg:95.07ms
step:754/1750 train_time:71684ms step_avg:95.07ms
step:755/1750 train_time:71782ms step_avg:95.08ms
step:756/1750 train_time:71879ms step_avg:95.08ms
step:757/1750 train_time:71976ms step_avg:95.08ms
step:758/1750 train_time:72073ms step_avg:95.08ms
step:759/1750 train_time:72170ms step_avg:95.09ms
step:760/1750 train_time:72268ms step_avg:95.09ms
step:761/1750 train_time:72366ms step_avg:95.09ms
step:762/1750 train_time:72465ms step_avg:95.10ms
step:763/1750 train_time:72565ms step_avg:95.10ms
step:764/1750 train_time:72664ms step_avg:95.11ms
step:765/1750 train_time:72761ms step_avg:95.11ms
step:766/1750 train_time:72859ms step_avg:95.12ms
step:767/1750 train_time:72956ms step_avg:95.12ms
step:768/1750 train_time:73053ms step_avg:95.12ms
step:769/1750 train_time:73150ms step_avg:95.12ms
step:770/1750 train_time:73247ms step_avg:95.13ms
step:771/1750 train_time:73345ms step_avg:95.13ms
step:772/1750 train_time:73443ms step_avg:95.13ms
step:773/1750 train_time:73541ms step_avg:95.14ms
step:774/1750 train_time:73640ms step_avg:95.14ms
step:775/1750 train_time:73739ms step_avg:95.15ms
step:776/1750 train_time:73836ms step_avg:95.15ms
step:777/1750 train_time:73933ms step_avg:95.15ms
step:778/1750 train_time:74030ms step_avg:95.15ms
step:779/1750 train_time:74128ms step_avg:95.16ms
step:780/1750 train_time:74225ms step_avg:95.16ms
step:781/1750 train_time:74323ms step_avg:95.16ms
step:782/1750 train_time:74422ms step_avg:95.17ms
step:783/1750 train_time:74521ms step_avg:95.17ms
step:784/1750 train_time:74619ms step_avg:95.18ms
step:785/1750 train_time:74718ms step_avg:95.18ms
step:786/1750 train_time:74817ms step_avg:95.19ms
step:787/1750 train_time:74914ms step_avg:95.19ms
step:788/1750 train_time:75012ms step_avg:95.19ms
step:789/1750 train_time:75110ms step_avg:95.20ms
step:790/1750 train_time:75208ms step_avg:95.20ms
step:791/1750 train_time:75305ms step_avg:95.20ms
step:792/1750 train_time:75402ms step_avg:95.20ms
step:793/1750 train_time:75500ms step_avg:95.21ms
step:794/1750 train_time:75599ms step_avg:95.21ms
step:795/1750 train_time:75697ms step_avg:95.22ms
step:796/1750 train_time:75795ms step_avg:95.22ms
step:797/1750 train_time:75893ms step_avg:95.22ms
step:798/1750 train_time:75991ms step_avg:95.23ms
step:799/1750 train_time:76089ms step_avg:95.23ms
step:800/1750 train_time:76186ms step_avg:95.23ms
step:801/1750 train_time:76284ms step_avg:95.24ms
step:802/1750 train_time:76382ms step_avg:95.24ms
step:803/1750 train_time:76480ms step_avg:95.24ms
step:804/1750 train_time:76578ms step_avg:95.25ms
step:805/1750 train_time:76676ms step_avg:95.25ms
step:806/1750 train_time:76774ms step_avg:95.25ms
step:807/1750 train_time:76872ms step_avg:95.26ms
step:808/1750 train_time:76969ms step_avg:95.26ms
step:809/1750 train_time:77067ms step_avg:95.26ms
step:810/1750 train_time:77165ms step_avg:95.27ms
step:811/1750 train_time:77263ms step_avg:95.27ms
step:812/1750 train_time:77361ms step_avg:95.27ms
step:813/1750 train_time:77460ms step_avg:95.28ms
step:814/1750 train_time:77559ms step_avg:95.28ms
step:815/1750 train_time:77657ms step_avg:95.28ms
step:816/1750 train_time:77755ms step_avg:95.29ms
step:817/1750 train_time:77853ms step_avg:95.29ms
step:818/1750 train_time:77951ms step_avg:95.29ms
step:819/1750 train_time:78048ms step_avg:95.30ms
step:820/1750 train_time:78146ms step_avg:95.30ms
step:821/1750 train_time:78243ms step_avg:95.30ms
step:822/1750 train_time:78341ms step_avg:95.31ms
step:823/1750 train_time:78439ms step_avg:95.31ms
step:824/1750 train_time:78537ms step_avg:95.31ms
step:825/1750 train_time:78635ms step_avg:95.32ms
step:826/1750 train_time:78734ms step_avg:95.32ms
step:827/1750 train_time:78832ms step_avg:95.32ms
step:828/1750 train_time:78930ms step_avg:95.33ms
step:829/1750 train_time:79028ms step_avg:95.33ms
step:830/1750 train_time:79126ms step_avg:95.33ms
step:831/1750 train_time:79224ms step_avg:95.34ms
step:832/1750 train_time:79321ms step_avg:95.34ms
step:833/1750 train_time:79419ms step_avg:95.34ms
step:834/1750 train_time:79516ms step_avg:95.34ms
step:835/1750 train_time:79614ms step_avg:95.35ms
step:836/1750 train_time:79712ms step_avg:95.35ms
step:837/1750 train_time:79810ms step_avg:95.35ms
step:838/1750 train_time:79907ms step_avg:95.35ms
step:839/1750 train_time:80005ms step_avg:95.36ms
step:840/1750 train_time:80103ms step_avg:95.36ms
step:841/1750 train_time:80201ms step_avg:95.36ms
step:842/1750 train_time:80299ms step_avg:95.37ms
step:843/1750 train_time:80397ms step_avg:95.37ms
step:844/1750 train_time:80495ms step_avg:95.37ms
step:845/1750 train_time:80593ms step_avg:95.38ms
step:846/1750 train_time:80691ms step_avg:95.38ms
step:847/1750 train_time:80789ms step_avg:95.38ms
step:848/1750 train_time:80887ms step_avg:95.39ms
step:849/1750 train_time:80984ms step_avg:95.39ms
step:850/1750 train_time:81082ms step_avg:95.39ms
step:851/1750 train_time:81180ms step_avg:95.39ms
step:852/1750 train_time:81279ms step_avg:95.40ms
step:853/1750 train_time:81377ms step_avg:95.40ms
step:854/1750 train_time:81475ms step_avg:95.40ms
step:855/1750 train_time:81573ms step_avg:95.41ms
step:856/1750 train_time:81670ms step_avg:95.41ms
step:857/1750 train_time:81768ms step_avg:95.41ms
step:858/1750 train_time:81866ms step_avg:95.42ms
step:859/1750 train_time:81965ms step_avg:95.42ms
step:860/1750 train_time:82063ms step_avg:95.42ms
step:861/1750 train_time:82161ms step_avg:95.43ms
step:862/1750 train_time:82260ms step_avg:95.43ms
step:863/1750 train_time:82359ms step_avg:95.43ms
step:864/1750 train_time:82456ms step_avg:95.44ms
step:865/1750 train_time:82555ms step_avg:95.44ms
step:866/1750 train_time:82652ms step_avg:95.44ms
step:867/1750 train_time:82751ms step_avg:95.44ms
step:868/1750 train_time:82848ms step_avg:95.45ms
step:869/1750 train_time:82946ms step_avg:95.45ms
step:870/1750 train_time:83044ms step_avg:95.45ms
step:871/1750 train_time:83142ms step_avg:95.46ms
step:872/1750 train_time:83239ms step_avg:95.46ms
step:873/1750 train_time:83338ms step_avg:95.46ms
step:874/1750 train_time:83436ms step_avg:95.46ms
step:875/1750 train_time:83534ms step_avg:95.47ms
step:875/1750 val_loss:3.5432 train_time:83626ms step_avg:95.57ms
step:876/1750 train_time:83647ms step_avg:95.49ms
step:877/1750 train_time:83736ms step_avg:95.48ms
step:878/1750 train_time:83836ms step_avg:95.49ms
step:879/1750 train_time:83935ms step_avg:95.49ms
step:880/1750 train_time:84033ms step_avg:95.49ms
step:881/1750 train_time:84131ms step_avg:95.50ms
step:882/1750 train_time:84229ms step_avg:95.50ms
step:883/1750 train_time:84326ms step_avg:95.50ms
step:884/1750 train_time:84423ms step_avg:95.50ms
step:885/1750 train_time:84520ms step_avg:95.50ms
step:886/1750 train_time:84619ms step_avg:95.51ms
step:887/1750 train_time:84718ms step_avg:95.51ms
step:888/1750 train_time:84817ms step_avg:95.52ms
step:889/1750 train_time:84916ms step_avg:95.52ms
step:890/1750 train_time:85015ms step_avg:95.52ms
step:891/1750 train_time:85113ms step_avg:95.52ms
step:892/1750 train_time:85210ms step_avg:95.53ms
step:893/1750 train_time:85306ms step_avg:95.53ms
step:894/1750 train_time:85403ms step_avg:95.53ms
step:895/1750 train_time:85501ms step_avg:95.53ms
step:896/1750 train_time:85599ms step_avg:95.53ms
step:897/1750 train_time:85697ms step_avg:95.54ms
step:898/1750 train_time:85795ms step_avg:95.54ms
step:899/1750 train_time:85893ms step_avg:95.54ms
step:900/1750 train_time:85991ms step_avg:95.55ms
step:901/1750 train_time:86088ms step_avg:95.55ms
step:902/1750 train_time:86186ms step_avg:95.55ms
step:903/1750 train_time:86285ms step_avg:95.55ms
step:904/1750 train_time:86382ms step_avg:95.56ms
step:905/1750 train_time:86479ms step_avg:95.56ms
step:906/1750 train_time:86577ms step_avg:95.56ms
step:907/1750 train_time:86675ms step_avg:95.56ms
step:908/1750 train_time:86774ms step_avg:95.57ms
step:909/1750 train_time:86872ms step_avg:95.57ms
step:910/1750 train_time:86971ms step_avg:95.57ms
step:911/1750 train_time:87070ms step_avg:95.58ms
step:912/1750 train_time:87169ms step_avg:95.58ms
step:913/1750 train_time:87269ms step_avg:95.58ms
step:914/1750 train_time:87367ms step_avg:95.59ms
step:915/1750 train_time:87466ms step_avg:95.59ms
step:916/1750 train_time:87566ms step_avg:95.60ms
step:917/1750 train_time:87665ms step_avg:95.60ms
step:918/1750 train_time:87767ms step_avg:95.61ms
step:919/1750 train_time:87868ms step_avg:95.61ms
step:920/1750 train_time:87970ms step_avg:95.62ms
step:921/1750 train_time:88069ms step_avg:95.62ms
step:922/1750 train_time:88167ms step_avg:95.63ms
step:923/1750 train_time:88267ms step_avg:95.63ms
step:924/1750 train_time:88367ms step_avg:95.63ms
step:925/1750 train_time:88465ms step_avg:95.64ms
step:926/1750 train_time:88564ms step_avg:95.64ms
step:927/1750 train_time:88663ms step_avg:95.65ms
step:928/1750 train_time:88763ms step_avg:95.65ms
step:929/1750 train_time:88863ms step_avg:95.65ms
step:930/1750 train_time:88963ms step_avg:95.66ms
step:931/1750 train_time:89061ms step_avg:95.66ms
step:932/1750 train_time:89160ms step_avg:95.67ms
step:933/1750 train_time:89261ms step_avg:95.67ms
step:934/1750 train_time:89360ms step_avg:95.67ms
step:935/1750 train_time:89460ms step_avg:95.68ms
step:936/1750 train_time:89559ms step_avg:95.68ms
step:937/1750 train_time:89658ms step_avg:95.69ms
step:938/1750 train_time:89758ms step_avg:95.69ms
step:939/1750 train_time:89857ms step_avg:95.69ms
step:940/1750 train_time:89957ms step_avg:95.70ms
step:941/1750 train_time:90057ms step_avg:95.70ms
step:942/1750 train_time:90157ms step_avg:95.71ms
step:943/1750 train_time:90256ms step_avg:95.71ms
step:944/1750 train_time:90357ms step_avg:95.72ms
step:945/1750 train_time:90457ms step_avg:95.72ms
step:946/1750 train_time:90556ms step_avg:95.73ms
step:947/1750 train_time:90656ms step_avg:95.73ms
step:948/1750 train_time:90756ms step_avg:95.73ms
step:949/1750 train_time:90855ms step_avg:95.74ms
step:950/1750 train_time:90955ms step_avg:95.74ms
step:951/1750 train_time:91054ms step_avg:95.75ms
step:952/1750 train_time:91153ms step_avg:95.75ms
step:953/1750 train_time:91253ms step_avg:95.75ms
step:954/1750 train_time:91352ms step_avg:95.76ms
step:955/1750 train_time:91452ms step_avg:95.76ms
step:956/1750 train_time:91550ms step_avg:95.76ms
step:957/1750 train_time:91649ms step_avg:95.77ms
step:958/1750 train_time:91747ms step_avg:95.77ms
step:959/1750 train_time:91846ms step_avg:95.77ms
step:960/1750 train_time:91947ms step_avg:95.78ms
step:961/1750 train_time:92047ms step_avg:95.78ms
step:962/1750 train_time:92147ms step_avg:95.79ms
step:963/1750 train_time:92248ms step_avg:95.79ms
step:964/1750 train_time:92348ms step_avg:95.80ms
step:965/1750 train_time:92448ms step_avg:95.80ms
step:966/1750 train_time:92547ms step_avg:95.80ms
step:967/1750 train_time:92646ms step_avg:95.81ms
step:968/1750 train_time:92745ms step_avg:95.81ms
step:969/1750 train_time:92845ms step_avg:95.82ms
step:970/1750 train_time:92944ms step_avg:95.82ms
step:971/1750 train_time:93045ms step_avg:95.82ms
step:972/1750 train_time:93144ms step_avg:95.83ms
step:973/1750 train_time:93244ms step_avg:95.83ms
step:974/1750 train_time:93343ms step_avg:95.83ms
step:975/1750 train_time:93442ms step_avg:95.84ms
step:976/1750 train_time:93542ms step_avg:95.84ms
step:977/1750 train_time:93641ms step_avg:95.85ms
step:978/1750 train_time:93740ms step_avg:95.85ms
step:979/1750 train_time:93840ms step_avg:95.85ms
step:980/1750 train_time:93939ms step_avg:95.86ms
step:981/1750 train_time:94039ms step_avg:95.86ms
step:982/1750 train_time:94138ms step_avg:95.86ms
step:983/1750 train_time:94238ms step_avg:95.87ms
step:984/1750 train_time:94338ms step_avg:95.87ms
step:985/1750 train_time:94438ms step_avg:95.88ms
step:986/1750 train_time:94537ms step_avg:95.88ms
step:987/1750 train_time:94638ms step_avg:95.88ms
step:988/1750 train_time:94738ms step_avg:95.89ms
step:989/1750 train_time:94838ms step_avg:95.89ms
step:990/1750 train_time:94938ms step_avg:95.90ms
step:991/1750 train_time:95038ms step_avg:95.90ms
step:992/1750 train_time:95138ms step_avg:95.90ms
step:993/1750 train_time:95237ms step_avg:95.91ms
step:994/1750 train_time:95337ms step_avg:95.91ms
step:995/1750 train_time:95438ms step_avg:95.92ms
step:996/1750 train_time:95538ms step_avg:95.92ms
step:997/1750 train_time:95638ms step_avg:95.93ms
step:998/1750 train_time:95738ms step_avg:95.93ms
step:999/1750 train_time:95837ms step_avg:95.93ms
step:1000/1750 train_time:95937ms step_avg:95.94ms
step:1000/1750 val_loss:3.5040 train_time:96032ms step_avg:96.03ms
step:1001/1750 train_time:96053ms step_avg:95.96ms
step:1002/1750 train_time:96144ms step_avg:95.95ms
step:1003/1750 train_time:96245ms step_avg:95.96ms
step:1004/1750 train_time:96344ms step_avg:95.96ms
step:1005/1750 train_time:96442ms step_avg:95.96ms
step:1006/1750 train_time:96541ms step_avg:95.97ms
step:1007/1750 train_time:96640ms step_avg:95.97ms
step:1008/1750 train_time:96738ms step_avg:95.97ms
step:1009/1750 train_time:96837ms step_avg:95.97ms
step:1010/1750 train_time:96936ms step_avg:95.98ms
step:1011/1750 train_time:97037ms step_avg:95.98ms
step:1012/1750 train_time:97138ms step_avg:95.99ms
step:1013/1750 train_time:97239ms step_avg:95.99ms
step:1014/1750 train_time:97339ms step_avg:96.00ms
step:1015/1750 train_time:97438ms step_avg:96.00ms
step:1016/1750 train_time:97538ms step_avg:96.00ms
step:1017/1750 train_time:97637ms step_avg:96.01ms
step:1018/1750 train_time:97735ms step_avg:96.01ms
step:1019/1750 train_time:97833ms step_avg:96.01ms
step:1020/1750 train_time:97933ms step_avg:96.01ms
step:1021/1750 train_time:98033ms step_avg:96.02ms
step:1022/1750 train_time:98133ms step_avg:96.02ms
step:1023/1750 train_time:98234ms step_avg:96.03ms
step:1024/1750 train_time:98336ms step_avg:96.03ms
step:1025/1750 train_time:98435ms step_avg:96.03ms
step:1026/1750 train_time:98535ms step_avg:96.04ms
step:1027/1750 train_time:98634ms step_avg:96.04ms
step:1028/1750 train_time:98733ms step_avg:96.04ms
step:1029/1750 train_time:98833ms step_avg:96.05ms
step:1030/1750 train_time:98932ms step_avg:96.05ms
step:1031/1750 train_time:99031ms step_avg:96.05ms
step:1032/1750 train_time:99132ms step_avg:96.06ms
step:1033/1750 train_time:99232ms step_avg:96.06ms
step:1034/1750 train_time:99332ms step_avg:96.07ms
step:1035/1750 train_time:99432ms step_avg:96.07ms
step:1036/1750 train_time:99533ms step_avg:96.07ms
step:1037/1750 train_time:99633ms step_avg:96.08ms
step:1038/1750 train_time:99732ms step_avg:96.08ms
step:1039/1750 train_time:99831ms step_avg:96.08ms
step:1040/1750 train_time:99931ms step_avg:96.09ms
step:1041/1750 train_time:100030ms step_avg:96.09ms
step:1042/1750 train_time:100130ms step_avg:96.09ms
step:1043/1750 train_time:100231ms step_avg:96.10ms
step:1044/1750 train_time:100331ms step_avg:96.10ms
step:1045/1750 train_time:100432ms step_avg:96.11ms
step:1046/1750 train_time:100531ms step_avg:96.11ms
step:1047/1750 train_time:100631ms step_avg:96.11ms
step:1048/1750 train_time:100731ms step_avg:96.12ms
step:1049/1750 train_time:100830ms step_avg:96.12ms
step:1050/1750 train_time:100929ms step_avg:96.12ms
step:1051/1750 train_time:101029ms step_avg:96.13ms
step:1052/1750 train_time:101129ms step_avg:96.13ms
step:1053/1750 train_time:101228ms step_avg:96.13ms
step:1054/1750 train_time:101328ms step_avg:96.14ms
step:1055/1750 train_time:101427ms step_avg:96.14ms
step:1056/1750 train_time:101526ms step_avg:96.14ms
step:1057/1750 train_time:101626ms step_avg:96.15ms
step:1058/1750 train_time:101725ms step_avg:96.15ms
step:1059/1750 train_time:101824ms step_avg:96.15ms
step:1060/1750 train_time:101923ms step_avg:96.15ms
step:1061/1750 train_time:102023ms step_avg:96.16ms
step:1062/1750 train_time:102123ms step_avg:96.16ms
step:1063/1750 train_time:102223ms step_avg:96.16ms
step:1064/1750 train_time:102323ms step_avg:96.17ms
step:1065/1750 train_time:102422ms step_avg:96.17ms
step:1066/1750 train_time:102522ms step_avg:96.17ms
step:1067/1750 train_time:102623ms step_avg:96.18ms
step:1068/1750 train_time:102722ms step_avg:96.18ms
step:1069/1750 train_time:102822ms step_avg:96.19ms
step:1070/1750 train_time:102923ms step_avg:96.19ms
step:1071/1750 train_time:103023ms step_avg:96.19ms
step:1072/1750 train_time:103123ms step_avg:96.20ms
step:1073/1750 train_time:103222ms step_avg:96.20ms
step:1074/1750 train_time:103322ms step_avg:96.20ms
step:1075/1750 train_time:103422ms step_avg:96.21ms
step:1076/1750 train_time:103521ms step_avg:96.21ms
step:1077/1750 train_time:103622ms step_avg:96.21ms
step:1078/1750 train_time:103722ms step_avg:96.22ms
step:1079/1750 train_time:103822ms step_avg:96.22ms
step:1080/1750 train_time:103921ms step_avg:96.22ms
step:1081/1750 train_time:104021ms step_avg:96.23ms
step:1082/1750 train_time:104121ms step_avg:96.23ms
step:1083/1750 train_time:104221ms step_avg:96.23ms
step:1084/1750 train_time:104321ms step_avg:96.24ms
step:1085/1750 train_time:104420ms step_avg:96.24ms
step:1086/1750 train_time:104520ms step_avg:96.24ms
step:1087/1750 train_time:104619ms step_avg:96.25ms
step:1088/1750 train_time:104719ms step_avg:96.25ms
step:1089/1750 train_time:104818ms step_avg:96.25ms
step:1090/1750 train_time:104918ms step_avg:96.26ms
step:1091/1750 train_time:105018ms step_avg:96.26ms
step:1092/1750 train_time:105118ms step_avg:96.26ms
step:1093/1750 train_time:105218ms step_avg:96.27ms
step:1094/1750 train_time:105318ms step_avg:96.27ms
step:1095/1750 train_time:105418ms step_avg:96.27ms
step:1096/1750 train_time:105518ms step_avg:96.28ms
step:1097/1750 train_time:105618ms step_avg:96.28ms
step:1098/1750 train_time:105718ms step_avg:96.28ms
step:1099/1750 train_time:105818ms step_avg:96.29ms
step:1100/1750 train_time:105917ms step_avg:96.29ms
step:1101/1750 train_time:106017ms step_avg:96.29ms
step:1102/1750 train_time:106117ms step_avg:96.29ms
step:1103/1750 train_time:106215ms step_avg:96.30ms
step:1104/1750 train_time:106315ms step_avg:96.30ms
step:1105/1750 train_time:106415ms step_avg:96.30ms
step:1106/1750 train_time:106515ms step_avg:96.31ms
step:1107/1750 train_time:106614ms step_avg:96.31ms
step:1108/1750 train_time:106715ms step_avg:96.31ms
step:1109/1750 train_time:106815ms step_avg:96.32ms
step:1110/1750 train_time:106915ms step_avg:96.32ms
step:1111/1750 train_time:107015ms step_avg:96.32ms
step:1112/1750 train_time:107115ms step_avg:96.33ms
step:1113/1750 train_time:107216ms step_avg:96.33ms
step:1114/1750 train_time:107315ms step_avg:96.33ms
step:1115/1750 train_time:107415ms step_avg:96.34ms
step:1116/1750 train_time:107515ms step_avg:96.34ms
step:1117/1750 train_time:107615ms step_avg:96.34ms
step:1118/1750 train_time:107714ms step_avg:96.35ms
step:1119/1750 train_time:107814ms step_avg:96.35ms
step:1120/1750 train_time:107914ms step_avg:96.35ms
step:1121/1750 train_time:108014ms step_avg:96.36ms
step:1122/1750 train_time:108114ms step_avg:96.36ms
step:1123/1750 train_time:108213ms step_avg:96.36ms
step:1124/1750 train_time:108313ms step_avg:96.36ms
step:1125/1750 train_time:108413ms step_avg:96.37ms
step:1125/1750 val_loss:3.4525 train_time:108508ms step_avg:96.45ms
step:1126/1750 train_time:108529ms step_avg:96.38ms
step:1127/1750 train_time:108619ms step_avg:96.38ms
step:1128/1750 train_time:108718ms step_avg:96.38ms
step:1129/1750 train_time:108817ms step_avg:96.38ms
step:1130/1750 train_time:108915ms step_avg:96.39ms
step:1131/1750 train_time:109013ms step_avg:96.39ms
step:1132/1750 train_time:109112ms step_avg:96.39ms
step:1133/1750 train_time:109211ms step_avg:96.39ms
step:1134/1750 train_time:109310ms step_avg:96.39ms
step:1135/1750 train_time:109409ms step_avg:96.40ms
step:1136/1750 train_time:109510ms step_avg:96.40ms
step:1137/1750 train_time:109612ms step_avg:96.40ms
step:1138/1750 train_time:109713ms step_avg:96.41ms
step:1139/1750 train_time:109812ms step_avg:96.41ms
step:1140/1750 train_time:109912ms step_avg:96.41ms
step:1141/1750 train_time:110011ms step_avg:96.42ms
step:1142/1750 train_time:110110ms step_avg:96.42ms
step:1143/1750 train_time:110209ms step_avg:96.42ms
step:1144/1750 train_time:110307ms step_avg:96.42ms
step:1145/1750 train_time:110407ms step_avg:96.43ms
step:1146/1750 train_time:110506ms step_avg:96.43ms
step:1147/1750 train_time:110607ms step_avg:96.43ms
step:1148/1750 train_time:110707ms step_avg:96.43ms
step:1149/1750 train_time:110807ms step_avg:96.44ms
step:1150/1750 train_time:110907ms step_avg:96.44ms
step:1151/1750 train_time:111007ms step_avg:96.44ms
step:1152/1750 train_time:111106ms step_avg:96.45ms
step:1153/1750 train_time:111206ms step_avg:96.45ms
step:1154/1750 train_time:111305ms step_avg:96.45ms
step:1155/1750 train_time:111404ms step_avg:96.45ms
step:1156/1750 train_time:111504ms step_avg:96.46ms
step:1157/1750 train_time:111604ms step_avg:96.46ms
step:1158/1750 train_time:111703ms step_avg:96.46ms
step:1159/1750 train_time:111803ms step_avg:96.47ms
step:1160/1750 train_time:111903ms step_avg:96.47ms
step:1161/1750 train_time:112003ms step_avg:96.47ms
step:1162/1750 train_time:112102ms step_avg:96.47ms
step:1163/1750 train_time:112202ms step_avg:96.48ms
step:1164/1750 train_time:112302ms step_avg:96.48ms
step:1165/1750 train_time:112402ms step_avg:96.48ms
step:1166/1750 train_time:112502ms step_avg:96.49ms
step:1167/1750 train_time:112601ms step_avg:96.49ms
step:1168/1750 train_time:112701ms step_avg:96.49ms
step:1169/1750 train_time:112802ms step_avg:96.49ms
step:1170/1750 train_time:112903ms step_avg:96.50ms
step:1171/1750 train_time:113004ms step_avg:96.50ms
step:1172/1750 train_time:113106ms step_avg:96.51ms
step:1173/1750 train_time:113207ms step_avg:96.51ms
step:1174/1750 train_time:113307ms step_avg:96.51ms
step:1175/1750 train_time:113408ms step_avg:96.52ms
step:1176/1750 train_time:113509ms step_avg:96.52ms
step:1177/1750 train_time:113611ms step_avg:96.53ms
step:1178/1750 train_time:113712ms step_avg:96.53ms
step:1179/1750 train_time:113817ms step_avg:96.54ms
step:1180/1750 train_time:113916ms step_avg:96.54ms
step:1181/1750 train_time:114016ms step_avg:96.54ms
step:1182/1750 train_time:114117ms step_avg:96.55ms
step:1183/1750 train_time:114217ms step_avg:96.55ms
step:1184/1750 train_time:114318ms step_avg:96.55ms
step:1185/1750 train_time:114421ms step_avg:96.56ms
step:1186/1750 train_time:114522ms step_avg:96.56ms
step:1187/1750 train_time:114622ms step_avg:96.56ms
step:1188/1750 train_time:114725ms step_avg:96.57ms
step:1189/1750 train_time:114826ms step_avg:96.57ms
step:1190/1750 train_time:114926ms step_avg:96.58ms
step:1191/1750 train_time:115027ms step_avg:96.58ms
step:1192/1750 train_time:115128ms step_avg:96.58ms
step:1193/1750 train_time:115229ms step_avg:96.59ms
step:1194/1750 train_time:115329ms step_avg:96.59ms
step:1195/1750 train_time:115430ms step_avg:96.59ms
step:1196/1750 train_time:115532ms step_avg:96.60ms
step:1197/1750 train_time:115634ms step_avg:96.60ms
step:1198/1750 train_time:115734ms step_avg:96.61ms
step:1199/1750 train_time:115835ms step_avg:96.61ms
step:1200/1750 train_time:115934ms step_avg:96.61ms
step:1201/1750 train_time:116035ms step_avg:96.62ms
step:1202/1750 train_time:116135ms step_avg:96.62ms
step:1203/1750 train_time:116235ms step_avg:96.62ms
step:1204/1750 train_time:116336ms step_avg:96.62ms
step:1205/1750 train_time:116436ms step_avg:96.63ms
step:1206/1750 train_time:116536ms step_avg:96.63ms
step:1207/1750 train_time:116637ms step_avg:96.63ms
step:1208/1750 train_time:116737ms step_avg:96.64ms
step:1209/1750 train_time:116837ms step_avg:96.64ms
step:1210/1750 train_time:116937ms step_avg:96.64ms
step:1211/1750 train_time:117039ms step_avg:96.65ms
step:1212/1750 train_time:117138ms step_avg:96.65ms
step:1213/1750 train_time:117239ms step_avg:96.65ms
step:1214/1750 train_time:117339ms step_avg:96.65ms
step:1215/1750 train_time:117441ms step_avg:96.66ms
step:1216/1750 train_time:117542ms step_avg:96.66ms
step:1217/1750 train_time:117642ms step_avg:96.67ms
step:1218/1750 train_time:117744ms step_avg:96.67ms
step:1219/1750 train_time:117846ms step_avg:96.67ms
step:1220/1750 train_time:117947ms step_avg:96.68ms
step:1221/1750 train_time:118047ms step_avg:96.68ms
step:1222/1750 train_time:118147ms step_avg:96.68ms
step:1223/1750 train_time:118248ms step_avg:96.69ms
step:1224/1750 train_time:118348ms step_avg:96.69ms
step:1225/1750 train_time:118451ms step_avg:96.69ms
step:1226/1750 train_time:118551ms step_avg:96.70ms
step:1227/1750 train_time:118653ms step_avg:96.70ms
step:1228/1750 train_time:118753ms step_avg:96.70ms
step:1229/1750 train_time:118854ms step_avg:96.71ms
step:1230/1750 train_time:118954ms step_avg:96.71ms
step:1231/1750 train_time:119054ms step_avg:96.71ms
step:1232/1750 train_time:119155ms step_avg:96.72ms
step:1233/1750 train_time:119254ms step_avg:96.72ms
step:1234/1750 train_time:119357ms step_avg:96.72ms
step:1235/1750 train_time:119456ms step_avg:96.73ms
step:1236/1750 train_time:119558ms step_avg:96.73ms
step:1237/1750 train_time:119658ms step_avg:96.73ms
step:1238/1750 train_time:119758ms step_avg:96.74ms
step:1239/1750 train_time:119858ms step_avg:96.74ms
step:1240/1750 train_time:119958ms step_avg:96.74ms
step:1241/1750 train_time:120060ms step_avg:96.74ms
step:1242/1750 train_time:120161ms step_avg:96.75ms
step:1243/1750 train_time:120262ms step_avg:96.75ms
step:1244/1750 train_time:120363ms step_avg:96.75ms
step:1245/1750 train_time:120464ms step_avg:96.76ms
step:1246/1750 train_time:120566ms step_avg:96.76ms
step:1247/1750 train_time:120666ms step_avg:96.76ms
step:1248/1750 train_time:120767ms step_avg:96.77ms
step:1249/1750 train_time:120867ms step_avg:96.77ms
step:1250/1750 train_time:120968ms step_avg:96.77ms
step:1250/1750 val_loss:3.4078 train_time:121063ms step_avg:96.85ms
step:1251/1750 train_time:121085ms step_avg:96.79ms
step:1252/1750 train_time:121180ms step_avg:96.79ms
step:1253/1750 train_time:121280ms step_avg:96.79ms
step:1254/1750 train_time:121381ms step_avg:96.79ms
step:1255/1750 train_time:121480ms step_avg:96.80ms
step:1256/1750 train_time:121580ms step_avg:96.80ms
step:1257/1750 train_time:121679ms step_avg:96.80ms
step:1258/1750 train_time:121778ms step_avg:96.80ms
step:1259/1750 train_time:121877ms step_avg:96.80ms
step:1260/1750 train_time:121978ms step_avg:96.81ms
step:1261/1750 train_time:122081ms step_avg:96.81ms
step:1262/1750 train_time:122184ms step_avg:96.82ms
step:1263/1750 train_time:122285ms step_avg:96.82ms
step:1264/1750 train_time:122384ms step_avg:96.82ms
step:1265/1750 train_time:122484ms step_avg:96.82ms
step:1266/1750 train_time:122583ms step_avg:96.83ms
step:1267/1750 train_time:122683ms step_avg:96.83ms
step:1268/1750 train_time:122783ms step_avg:96.83ms
step:1269/1750 train_time:122883ms step_avg:96.83ms
step:1270/1750 train_time:122985ms step_avg:96.84ms
step:1271/1750 train_time:123087ms step_avg:96.84ms
step:1272/1750 train_time:123188ms step_avg:96.85ms
step:1273/1750 train_time:123288ms step_avg:96.85ms
step:1274/1750 train_time:123388ms step_avg:96.85ms
step:1275/1750 train_time:123488ms step_avg:96.85ms
step:1276/1750 train_time:123590ms step_avg:96.86ms
step:1277/1750 train_time:123691ms step_avg:96.86ms
step:1278/1750 train_time:123791ms step_avg:96.86ms
step:1279/1750 train_time:123891ms step_avg:96.87ms
step:1280/1750 train_time:123991ms step_avg:96.87ms
step:1281/1750 train_time:124093ms step_avg:96.87ms
step:1282/1750 train_time:124194ms step_avg:96.88ms
step:1283/1750 train_time:124295ms step_avg:96.88ms
step:1284/1750 train_time:124396ms step_avg:96.88ms
step:1285/1750 train_time:124497ms step_avg:96.89ms
step:1286/1750 train_time:124598ms step_avg:96.89ms
step:1287/1750 train_time:124699ms step_avg:96.89ms
step:1288/1750 train_time:124800ms step_avg:96.89ms
step:1289/1750 train_time:124900ms step_avg:96.90ms
step:1290/1750 train_time:125001ms step_avg:96.90ms
step:1291/1750 train_time:125102ms step_avg:96.90ms
step:1292/1750 train_time:125203ms step_avg:96.91ms
step:1293/1750 train_time:125303ms step_avg:96.91ms
step:1294/1750 train_time:125405ms step_avg:96.91ms
step:1295/1750 train_time:125506ms step_avg:96.92ms
step:1296/1750 train_time:125606ms step_avg:96.92ms
step:1297/1750 train_time:125707ms step_avg:96.92ms
step:1298/1750 train_time:125809ms step_avg:96.93ms
step:1299/1750 train_time:125911ms step_avg:96.93ms
step:1300/1750 train_time:126011ms step_avg:96.93ms
step:1301/1750 train_time:126112ms step_avg:96.93ms
step:1302/1750 train_time:126214ms step_avg:96.94ms
step:1303/1750 train_time:126315ms step_avg:96.94ms
step:1304/1750 train_time:126416ms step_avg:96.94ms
step:1305/1750 train_time:126517ms step_avg:96.95ms
step:1306/1750 train_time:126620ms step_avg:96.95ms
step:1307/1750 train_time:126720ms step_avg:96.96ms
step:1308/1750 train_time:126820ms step_avg:96.96ms
step:1309/1750 train_time:126920ms step_avg:96.96ms
step:1310/1750 train_time:127021ms step_avg:96.96ms
step:1311/1750 train_time:127122ms step_avg:96.97ms
step:1312/1750 train_time:127224ms step_avg:96.97ms
step:1313/1750 train_time:127325ms step_avg:96.97ms
step:1314/1750 train_time:127426ms step_avg:96.98ms
step:1315/1750 train_time:127526ms step_avg:96.98ms
step:1316/1750 train_time:127628ms step_avg:96.98ms
step:1317/1750 train_time:127729ms step_avg:96.99ms
step:1318/1750 train_time:127830ms step_avg:96.99ms
step:1319/1750 train_time:127932ms step_avg:96.99ms
step:1320/1750 train_time:128033ms step_avg:96.99ms
step:1321/1750 train_time:128136ms step_avg:97.00ms
step:1322/1750 train_time:128238ms step_avg:97.00ms
step:1323/1750 train_time:128339ms step_avg:97.01ms
step:1324/1750 train_time:128439ms step_avg:97.01ms
step:1325/1750 train_time:128539ms step_avg:97.01ms
step:1326/1750 train_time:128641ms step_avg:97.01ms
step:1327/1750 train_time:128741ms step_avg:97.02ms
step:1328/1750 train_time:128841ms step_avg:97.02ms
step:1329/1750 train_time:128942ms step_avg:97.02ms
step:1330/1750 train_time:129043ms step_avg:97.03ms
step:1331/1750 train_time:129146ms step_avg:97.03ms
step:1332/1750 train_time:129248ms step_avg:97.03ms
step:1333/1750 train_time:129350ms step_avg:97.04ms
step:1334/1750 train_time:129450ms step_avg:97.04ms
step:1335/1750 train_time:129551ms step_avg:97.04ms
step:1336/1750 train_time:129653ms step_avg:97.05ms
step:1337/1750 train_time:129753ms step_avg:97.05ms
step:1338/1750 train_time:129855ms step_avg:97.05ms
step:1339/1750 train_time:129957ms step_avg:97.05ms
step:1340/1750 train_time:130057ms step_avg:97.06ms
step:1341/1750 train_time:130158ms step_avg:97.06ms
step:1342/1750 train_time:130259ms step_avg:97.06ms
step:1343/1750 train_time:130359ms step_avg:97.07ms
step:1344/1750 train_time:130459ms step_avg:97.07ms
step:1345/1750 train_time:130560ms step_avg:97.07ms
step:1346/1750 train_time:130661ms step_avg:97.07ms
step:1347/1750 train_time:130762ms step_avg:97.08ms
step:1348/1750 train_time:130864ms step_avg:97.08ms
step:1349/1750 train_time:130966ms step_avg:97.08ms
step:1350/1750 train_time:131067ms step_avg:97.09ms
step:1351/1750 train_time:131169ms step_avg:97.09ms
step:1352/1750 train_time:131269ms step_avg:97.09ms
step:1353/1750 train_time:131369ms step_avg:97.09ms
step:1354/1750 train_time:131470ms step_avg:97.10ms
step:1355/1750 train_time:131571ms step_avg:97.10ms
step:1356/1750 train_time:131671ms step_avg:97.10ms
step:1357/1750 train_time:131772ms step_avg:97.11ms
step:1358/1750 train_time:131874ms step_avg:97.11ms
step:1359/1750 train_time:131974ms step_avg:97.11ms
step:1360/1750 train_time:132075ms step_avg:97.11ms
step:1361/1750 train_time:132176ms step_avg:97.12ms
step:1362/1750 train_time:132276ms step_avg:97.12ms
step:1363/1750 train_time:132379ms step_avg:97.12ms
step:1364/1750 train_time:132480ms step_avg:97.13ms
step:1365/1750 train_time:132580ms step_avg:97.13ms
step:1366/1750 train_time:132681ms step_avg:97.13ms
step:1367/1750 train_time:132781ms step_avg:97.13ms
step:1368/1750 train_time:132882ms step_avg:97.14ms
step:1369/1750 train_time:132984ms step_avg:97.14ms
step:1370/1750 train_time:133084ms step_avg:97.14ms
step:1371/1750 train_time:133186ms step_avg:97.14ms
step:1372/1750 train_time:133287ms step_avg:97.15ms
step:1373/1750 train_time:133388ms step_avg:97.15ms
step:1374/1750 train_time:133488ms step_avg:97.15ms
step:1375/1750 train_time:133591ms step_avg:97.16ms
step:1375/1750 val_loss:3.3674 train_time:133686ms step_avg:97.23ms
step:1376/1750 train_time:133707ms step_avg:97.17ms
step:1377/1750 train_time:133802ms step_avg:97.17ms
step:1378/1750 train_time:133903ms step_avg:97.17ms
step:1379/1750 train_time:134003ms step_avg:97.17ms
step:1380/1750 train_time:134105ms step_avg:97.18ms
step:1381/1750 train_time:134204ms step_avg:97.18ms
step:1382/1750 train_time:134303ms step_avg:97.18ms
step:1383/1750 train_time:134402ms step_avg:97.18ms
step:1384/1750 train_time:134501ms step_avg:97.18ms
step:1385/1750 train_time:134601ms step_avg:97.19ms
step:1386/1750 train_time:134704ms step_avg:97.19ms
step:1387/1750 train_time:134807ms step_avg:97.19ms
step:1388/1750 train_time:134909ms step_avg:97.20ms
step:1389/1750 train_time:135010ms step_avg:97.20ms
step:1390/1750 train_time:135110ms step_avg:97.20ms
step:1391/1750 train_time:135212ms step_avg:97.20ms
step:1392/1750 train_time:135313ms step_avg:97.21ms
step:1393/1750 train_time:135414ms step_avg:97.21ms
step:1394/1750 train_time:135513ms step_avg:97.21ms
step:1395/1750 train_time:135614ms step_avg:97.21ms
step:1396/1750 train_time:135716ms step_avg:97.22ms
step:1397/1750 train_time:135818ms step_avg:97.22ms
step:1398/1750 train_time:135920ms step_avg:97.22ms
step:1399/1750 train_time:136022ms step_avg:97.23ms
step:1400/1750 train_time:136122ms step_avg:97.23ms
step:1401/1750 train_time:136222ms step_avg:97.23ms
step:1402/1750 train_time:136322ms step_avg:97.23ms
step:1403/1750 train_time:136423ms step_avg:97.24ms
step:1404/1750 train_time:136523ms step_avg:97.24ms
step:1405/1750 train_time:136624ms step_avg:97.24ms
step:1406/1750 train_time:136726ms step_avg:97.24ms
step:1407/1750 train_time:136827ms step_avg:97.25ms
step:1408/1750 train_time:136930ms step_avg:97.25ms
step:1409/1750 train_time:137032ms step_avg:97.25ms
step:1410/1750 train_time:137133ms step_avg:97.26ms
step:1411/1750 train_time:137233ms step_avg:97.26ms
step:1412/1750 train_time:137335ms step_avg:97.26ms
step:1413/1750 train_time:137436ms step_avg:97.27ms
step:1414/1750 train_time:137536ms step_avg:97.27ms
step:1415/1750 train_time:137638ms step_avg:97.27ms
step:1416/1750 train_time:137739ms step_avg:97.27ms
step:1417/1750 train_time:137841ms step_avg:97.28ms
step:1418/1750 train_time:137941ms step_avg:97.28ms
step:1419/1750 train_time:138042ms step_avg:97.28ms
step:1420/1750 train_time:138141ms step_avg:97.28ms
step:1421/1750 train_time:138242ms step_avg:97.28ms
step:1422/1750 train_time:138341ms step_avg:97.29ms
step:1423/1750 train_time:138441ms step_avg:97.29ms
step:1424/1750 train_time:138542ms step_avg:97.29ms
step:1425/1750 train_time:138642ms step_avg:97.29ms
step:1426/1750 train_time:138744ms step_avg:97.30ms
step:1427/1750 train_time:138845ms step_avg:97.30ms
step:1428/1750 train_time:138947ms step_avg:97.30ms
step:1429/1750 train_time:139048ms step_avg:97.30ms
step:1430/1750 train_time:139151ms step_avg:97.31ms
step:1431/1750 train_time:139254ms step_avg:97.31ms
step:1432/1750 train_time:139354ms step_avg:97.31ms
step:1433/1750 train_time:139456ms step_avg:97.32ms
step:1434/1750 train_time:139557ms step_avg:97.32ms
step:1435/1750 train_time:139658ms step_avg:97.32ms
step:1436/1750 train_time:139759ms step_avg:97.33ms
step:1437/1750 train_time:139861ms step_avg:97.33ms
step:1438/1750 train_time:139962ms step_avg:97.33ms
step:1439/1750 train_time:140065ms step_avg:97.34ms
step:1440/1750 train_time:140168ms step_avg:97.34ms
step:1441/1750 train_time:140270ms step_avg:97.34ms
step:1442/1750 train_time:140371ms step_avg:97.34ms
step:1443/1750 train_time:140472ms step_avg:97.35ms
step:1444/1750 train_time:140574ms step_avg:97.35ms
step:1445/1750 train_time:140676ms step_avg:97.35ms
step:1446/1750 train_time:140778ms step_avg:97.36ms
step:1447/1750 train_time:140880ms step_avg:97.36ms
step:1448/1750 train_time:140985ms step_avg:97.37ms
step:1449/1750 train_time:141087ms step_avg:97.37ms
step:1450/1750 train_time:141189ms step_avg:97.37ms
step:1451/1750 train_time:141290ms step_avg:97.37ms
step:1452/1750 train_time:141391ms step_avg:97.38ms
step:1453/1750 train_time:141493ms step_avg:97.38ms
step:1454/1750 train_time:141596ms step_avg:97.38ms
step:1455/1750 train_time:141698ms step_avg:97.39ms
step:1456/1750 train_time:141800ms step_avg:97.39ms
step:1457/1750 train_time:141902ms step_avg:97.39ms
step:1458/1750 train_time:142005ms step_avg:97.40ms
step:1459/1750 train_time:142107ms step_avg:97.40ms
step:1460/1750 train_time:142207ms step_avg:97.40ms
step:1461/1750 train_time:142310ms step_avg:97.41ms
step:1462/1750 train_time:142412ms step_avg:97.41ms
step:1463/1750 train_time:142514ms step_avg:97.41ms
step:1464/1750 train_time:142616ms step_avg:97.42ms
step:1465/1750 train_time:142718ms step_avg:97.42ms
step:1466/1750 train_time:142821ms step_avg:97.42ms
step:1467/1750 train_time:142923ms step_avg:97.43ms
step:1468/1750 train_time:143024ms step_avg:97.43ms
step:1469/1750 train_time:143126ms step_avg:97.43ms
step:1470/1750 train_time:143227ms step_avg:97.43ms
step:1471/1750 train_time:143328ms step_avg:97.44ms
step:1472/1750 train_time:143429ms step_avg:97.44ms
step:1473/1750 train_time:143531ms step_avg:97.44ms
step:1474/1750 train_time:143633ms step_avg:97.44ms
step:1475/1750 train_time:143735ms step_avg:97.45ms
step:1476/1750 train_time:143837ms step_avg:97.45ms
step:1477/1750 train_time:143940ms step_avg:97.45ms
step:1478/1750 train_time:144042ms step_avg:97.46ms
step:1479/1750 train_time:144143ms step_avg:97.46ms
step:1480/1750 train_time:144245ms step_avg:97.46ms
step:1481/1750 train_time:144346ms step_avg:97.47ms
step:1482/1750 train_time:144448ms step_avg:97.47ms
step:1483/1750 train_time:144549ms step_avg:97.47ms
step:1484/1750 train_time:144653ms step_avg:97.47ms
step:1485/1750 train_time:144757ms step_avg:97.48ms
step:1486/1750 train_time:144858ms step_avg:97.48ms
step:1487/1750 train_time:144960ms step_avg:97.48ms
step:1488/1750 train_time:145062ms step_avg:97.49ms
step:1489/1750 train_time:145164ms step_avg:97.49ms
step:1490/1750 train_time:145267ms step_avg:97.49ms
step:1491/1750 train_time:145369ms step_avg:97.50ms
step:1492/1750 train_time:145470ms step_avg:97.50ms
step:1493/1750 train_time:145571ms step_avg:97.50ms
step:1494/1750 train_time:145673ms step_avg:97.51ms
step:1495/1750 train_time:145774ms step_avg:97.51ms
step:1496/1750 train_time:145875ms step_avg:97.51ms
step:1497/1750 train_time:145977ms step_avg:97.51ms
step:1498/1750 train_time:146079ms step_avg:97.52ms
step:1499/1750 train_time:146181ms step_avg:97.52ms
step:1500/1750 train_time:146283ms step_avg:97.52ms
step:1500/1750 val_loss:3.3320 train_time:146379ms step_avg:97.59ms
step:1501/1750 train_time:146400ms step_avg:97.53ms
step:1502/1750 train_time:146493ms step_avg:97.53ms
step:1503/1750 train_time:146595ms step_avg:97.54ms
step:1504/1750 train_time:146695ms step_avg:97.54ms
step:1505/1750 train_time:146796ms step_avg:97.54ms
step:1506/1750 train_time:146896ms step_avg:97.54ms
step:1507/1750 train_time:146997ms step_avg:97.54ms
step:1508/1750 train_time:147097ms step_avg:97.54ms
step:1509/1750 train_time:147198ms step_avg:97.55ms
step:1510/1750 train_time:147299ms step_avg:97.55ms
step:1511/1750 train_time:147404ms step_avg:97.55ms
step:1512/1750 train_time:147508ms step_avg:97.56ms
step:1513/1750 train_time:147609ms step_avg:97.56ms
step:1514/1750 train_time:147711ms step_avg:97.56ms
step:1515/1750 train_time:147814ms step_avg:97.57ms
step:1516/1750 train_time:147915ms step_avg:97.57ms
step:1517/1750 train_time:148016ms step_avg:97.57ms
step:1518/1750 train_time:148116ms step_avg:97.57ms
step:1519/1750 train_time:148218ms step_avg:97.58ms
step:1520/1750 train_time:148319ms step_avg:97.58ms
step:1521/1750 train_time:148422ms step_avg:97.58ms
step:1522/1750 train_time:148524ms step_avg:97.58ms
step:1523/1750 train_time:148625ms step_avg:97.59ms
step:1524/1750 train_time:148728ms step_avg:97.59ms
step:1525/1750 train_time:148832ms step_avg:97.59ms
step:1526/1750 train_time:148934ms step_avg:97.60ms
step:1527/1750 train_time:149036ms step_avg:97.60ms
step:1528/1750 train_time:149141ms step_avg:97.61ms
step:1529/1750 train_time:149243ms step_avg:97.61ms
step:1530/1750 train_time:149345ms step_avg:97.61ms
step:1531/1750 train_time:149446ms step_avg:97.61ms
step:1532/1750 train_time:149548ms step_avg:97.62ms
step:1533/1750 train_time:149649ms step_avg:97.62ms
step:1534/1750 train_time:149751ms step_avg:97.62ms
step:1535/1750 train_time:149853ms step_avg:97.62ms
step:1536/1750 train_time:149954ms step_avg:97.63ms
step:1537/1750 train_time:150056ms step_avg:97.63ms
step:1538/1750 train_time:150157ms step_avg:97.63ms
step:1539/1750 train_time:150258ms step_avg:97.63ms
step:1540/1750 train_time:150361ms step_avg:97.64ms
step:1541/1750 train_time:150463ms step_avg:97.64ms
step:1542/1750 train_time:150566ms step_avg:97.64ms
step:1543/1750 train_time:150667ms step_avg:97.65ms
step:1544/1750 train_time:150769ms step_avg:97.65ms
step:1545/1750 train_time:150872ms step_avg:97.65ms
step:1546/1750 train_time:150973ms step_avg:97.65ms
step:1547/1750 train_time:151075ms step_avg:97.66ms
step:1548/1750 train_time:151177ms step_avg:97.66ms
step:1549/1750 train_time:151279ms step_avg:97.66ms
step:1550/1750 train_time:151379ms step_avg:97.66ms
step:1551/1750 train_time:151481ms step_avg:97.67ms
step:1552/1750 train_time:151583ms step_avg:97.67ms
step:1553/1750 train_time:151684ms step_avg:97.67ms
step:1554/1750 train_time:151786ms step_avg:97.67ms
step:1555/1750 train_time:151889ms step_avg:97.68ms
step:1556/1750 train_time:151990ms step_avg:97.68ms
step:1557/1750 train_time:152092ms step_avg:97.68ms
step:1558/1750 train_time:152195ms step_avg:97.69ms
step:1559/1750 train_time:152298ms step_avg:97.69ms
step:1560/1750 train_time:152399ms step_avg:97.69ms
step:1561/1750 train_time:152500ms step_avg:97.69ms
step:1562/1750 train_time:152602ms step_avg:97.70ms
step:1563/1750 train_time:152707ms step_avg:97.70ms
step:1564/1750 train_time:152808ms step_avg:97.70ms
step:1565/1750 train_time:152909ms step_avg:97.71ms
step:1566/1750 train_time:153011ms step_avg:97.71ms
step:1567/1750 train_time:153113ms step_avg:97.71ms
step:1568/1750 train_time:153215ms step_avg:97.71ms
step:1569/1750 train_time:153317ms step_avg:97.72ms
step:1570/1750 train_time:153419ms step_avg:97.72ms
step:1571/1750 train_time:153520ms step_avg:97.72ms
step:1572/1750 train_time:153620ms step_avg:97.72ms
step:1573/1750 train_time:153723ms step_avg:97.73ms
step:1574/1750 train_time:153825ms step_avg:97.73ms
step:1575/1750 train_time:153927ms step_avg:97.73ms
step:1576/1750 train_time:154030ms step_avg:97.73ms
step:1577/1750 train_time:154133ms step_avg:97.74ms
step:1578/1750 train_time:154234ms step_avg:97.74ms
step:1579/1750 train_time:154335ms step_avg:97.74ms
step:1580/1750 train_time:154439ms step_avg:97.75ms
step:1581/1750 train_time:154540ms step_avg:97.75ms
step:1582/1750 train_time:154641ms step_avg:97.75ms
step:1583/1750 train_time:154744ms step_avg:97.75ms
step:1584/1750 train_time:154847ms step_avg:97.76ms
step:1585/1750 train_time:154950ms step_avg:97.76ms
step:1586/1750 train_time:155053ms step_avg:97.76ms
step:1587/1750 train_time:155154ms step_avg:97.77ms
step:1588/1750 train_time:155256ms step_avg:97.77ms
step:1589/1750 train_time:155357ms step_avg:97.77ms
step:1590/1750 train_time:155458ms step_avg:97.77ms
step:1591/1750 train_time:155560ms step_avg:97.78ms
step:1592/1750 train_time:155663ms step_avg:97.78ms
step:1593/1750 train_time:155764ms step_avg:97.78ms
step:1594/1750 train_time:155869ms step_avg:97.78ms
step:1595/1750 train_time:155971ms step_avg:97.79ms
step:1596/1750 train_time:156073ms step_avg:97.79ms
step:1597/1750 train_time:156175ms step_avg:97.79ms
step:1598/1750 train_time:156277ms step_avg:97.80ms
step:1599/1750 train_time:156378ms step_avg:97.80ms
step:1600/1750 train_time:156480ms step_avg:97.80ms
step:1601/1750 train_time:156581ms step_avg:97.80ms
step:1602/1750 train_time:156683ms step_avg:97.80ms
step:1603/1750 train_time:156785ms step_avg:97.81ms
step:1604/1750 train_time:156887ms step_avg:97.81ms
step:1605/1750 train_time:156990ms step_avg:97.81ms
step:1606/1750 train_time:157092ms step_avg:97.82ms
step:1607/1750 train_time:157193ms step_avg:97.82ms
step:1608/1750 train_time:157295ms step_avg:97.82ms
step:1609/1750 train_time:157397ms step_avg:97.82ms
step:1610/1750 train_time:157499ms step_avg:97.83ms
step:1611/1750 train_time:157600ms step_avg:97.83ms
step:1612/1750 train_time:157701ms step_avg:97.83ms
step:1613/1750 train_time:157803ms step_avg:97.83ms
step:1614/1750 train_time:157905ms step_avg:97.83ms
step:1615/1750 train_time:158006ms step_avg:97.84ms
step:1616/1750 train_time:158108ms step_avg:97.84ms
step:1617/1750 train_time:158211ms step_avg:97.84ms
step:1618/1750 train_time:158312ms step_avg:97.84ms
step:1619/1750 train_time:158414ms step_avg:97.85ms
step:1620/1750 train_time:158518ms step_avg:97.85ms
step:1621/1750 train_time:158619ms step_avg:97.85ms
step:1622/1750 train_time:158721ms step_avg:97.86ms
step:1623/1750 train_time:158822ms step_avg:97.86ms
step:1624/1750 train_time:158925ms step_avg:97.86ms
step:1625/1750 train_time:159029ms step_avg:97.86ms
step:1625/1750 val_loss:3.3018 train_time:159124ms step_avg:97.92ms
step:1626/1750 train_time:159145ms step_avg:97.88ms
step:1627/1750 train_time:159240ms step_avg:97.87ms
step:1628/1750 train_time:159343ms step_avg:97.88ms
step:1629/1750 train_time:159446ms step_avg:97.88ms
step:1630/1750 train_time:159546ms step_avg:97.88ms
step:1631/1750 train_time:159647ms step_avg:97.88ms
step:1632/1750 train_time:159748ms step_avg:97.88ms
step:1633/1750 train_time:159848ms step_avg:97.89ms
step:1634/1750 train_time:159952ms step_avg:97.89ms
step:1635/1750 train_time:160054ms step_avg:97.89ms
step:1636/1750 train_time:160158ms step_avg:97.90ms
step:1637/1750 train_time:160261ms step_avg:97.90ms
step:1638/1750 train_time:160364ms step_avg:97.90ms
step:1639/1750 train_time:160465ms step_avg:97.90ms
step:1640/1750 train_time:160566ms step_avg:97.91ms
step:1641/1750 train_time:160667ms step_avg:97.91ms
step:1642/1750 train_time:160767ms step_avg:97.91ms
step:1643/1750 train_time:160867ms step_avg:97.91ms
step:1644/1750 train_time:160969ms step_avg:97.91ms
step:1645/1750 train_time:161072ms step_avg:97.92ms
step:1646/1750 train_time:161175ms step_avg:97.92ms
step:1647/1750 train_time:161281ms step_avg:97.92ms
step:1648/1750 train_time:161384ms step_avg:97.93ms
step:1649/1750 train_time:161485ms step_avg:97.93ms
step:1650/1750 train_time:161585ms step_avg:97.93ms
step:1651/1750 train_time:161687ms step_avg:97.93ms
step:1652/1750 train_time:161788ms step_avg:97.93ms
step:1653/1750 train_time:161890ms step_avg:97.94ms
step:1654/1750 train_time:161991ms step_avg:97.94ms
step:1655/1750 train_time:162094ms step_avg:97.94ms
step:1656/1750 train_time:162197ms step_avg:97.94ms
step:1657/1750 train_time:162298ms step_avg:97.95ms
step:1658/1750 train_time:162400ms step_avg:97.95ms
step:1659/1750 train_time:162505ms step_avg:97.95ms
step:1660/1750 train_time:162605ms step_avg:97.96ms
step:1661/1750 train_time:162707ms step_avg:97.96ms
step:1662/1750 train_time:162810ms step_avg:97.96ms
step:1663/1750 train_time:162912ms step_avg:97.96ms
step:1664/1750 train_time:163013ms step_avg:97.96ms
step:1665/1750 train_time:163117ms step_avg:97.97ms
step:1666/1750 train_time:163220ms step_avg:97.97ms
step:1667/1750 train_time:163322ms step_avg:97.97ms
step:1668/1750 train_time:163426ms step_avg:97.98ms
step:1669/1750 train_time:163527ms step_avg:97.98ms
step:1670/1750 train_time:163628ms step_avg:97.98ms
step:1671/1750 train_time:163730ms step_avg:97.98ms
step:1672/1750 train_time:163831ms step_avg:97.98ms
step:1673/1750 train_time:163932ms step_avg:97.99ms
step:1674/1750 train_time:164034ms step_avg:97.99ms
step:1675/1750 train_time:164137ms step_avg:97.99ms
step:1676/1750 train_time:164239ms step_avg:97.99ms
step:1677/1750 train_time:164341ms step_avg:98.00ms
step:1678/1750 train_time:164445ms step_avg:98.00ms
step:1679/1750 train_time:164546ms step_avg:98.00ms
step:1680/1750 train_time:164646ms step_avg:98.00ms
step:1681/1750 train_time:164748ms step_avg:98.01ms
step:1682/1750 train_time:164853ms step_avg:98.01ms
step:1683/1750 train_time:164954ms step_avg:98.01ms
step:1684/1750 train_time:165056ms step_avg:98.01ms
step:1685/1750 train_time:165160ms step_avg:98.02ms
step:1686/1750 train_time:165262ms step_avg:98.02ms
step:1687/1750 train_time:165363ms step_avg:98.02ms
step:1688/1750 train_time:165466ms step_avg:98.03ms
step:1689/1750 train_time:165567ms step_avg:98.03ms
step:1690/1750 train_time:165669ms step_avg:98.03ms
step:1691/1750 train_time:165771ms step_avg:98.03ms
step:1692/1750 train_time:165873ms step_avg:98.03ms
step:1693/1750 train_time:165976ms step_avg:98.04ms
step:1694/1750 train_time:166080ms step_avg:98.04ms
step:1695/1750 train_time:166184ms step_avg:98.04ms
step:1696/1750 train_time:166287ms step_avg:98.05ms
step:1697/1750 train_time:166391ms step_avg:98.05ms
step:1698/1750 train_time:166493ms step_avg:98.05ms
step:1699/1750 train_time:166596ms step_avg:98.06ms
step:1700/1750 train_time:166699ms step_avg:98.06ms
step:1701/1750 train_time:166802ms step_avg:98.06ms
step:1702/1750 train_time:166907ms step_avg:98.07ms
step:1703/1750 train_time:167009ms step_avg:98.07ms
step:1704/1750 train_time:167112ms step_avg:98.07ms
step:1705/1750 train_time:167214ms step_avg:98.07ms
step:1706/1750 train_time:167316ms step_avg:98.08ms
step:1707/1750 train_time:167419ms step_avg:98.08ms
step:1708/1750 train_time:167523ms step_avg:98.08ms
step:1709/1750 train_time:167625ms step_avg:98.08ms
step:1710/1750 train_time:167727ms step_avg:98.09ms
step:1711/1750 train_time:167830ms step_avg:98.09ms
step:1712/1750 train_time:167932ms step_avg:98.09ms
step:1713/1750 train_time:168037ms step_avg:98.09ms
step:1714/1750 train_time:168138ms step_avg:98.10ms
step:1715/1750 train_time:168243ms step_avg:98.10ms
step:1716/1750 train_time:168346ms step_avg:98.10ms
step:1717/1750 train_time:168448ms step_avg:98.11ms
step:1718/1750 train_time:168549ms step_avg:98.11ms
step:1719/1750 train_time:168655ms step_avg:98.11ms
step:1720/1750 train_time:168758ms step_avg:98.11ms
step:1721/1750 train_time:168860ms step_avg:98.12ms
step:1722/1750 train_time:168963ms step_avg:98.12ms
step:1723/1750 train_time:169064ms step_avg:98.12ms
step:1724/1750 train_time:169167ms step_avg:98.12ms
step:1725/1750 train_time:169271ms step_avg:98.13ms
step:1726/1750 train_time:169373ms step_avg:98.13ms
step:1727/1750 train_time:169476ms step_avg:98.13ms
step:1728/1750 train_time:169579ms step_avg:98.14ms
step:1729/1750 train_time:169682ms step_avg:98.14ms
step:1730/1750 train_time:169785ms step_avg:98.14ms
step:1731/1750 train_time:169888ms step_avg:98.14ms
step:1732/1750 train_time:169990ms step_avg:98.15ms
step:1733/1750 train_time:170093ms step_avg:98.15ms
step:1734/1750 train_time:170197ms step_avg:98.15ms
step:1735/1750 train_time:170300ms step_avg:98.16ms
step:1736/1750 train_time:170402ms step_avg:98.16ms
step:1737/1750 train_time:170505ms step_avg:98.16ms
step:1738/1750 train_time:170607ms step_avg:98.16ms
step:1739/1750 train_time:170710ms step_avg:98.17ms
step:1740/1750 train_time:170811ms step_avg:98.17ms
step:1741/1750 train_time:170918ms step_avg:98.17ms
step:1742/1750 train_time:171021ms step_avg:98.18ms
step:1743/1750 train_time:171124ms step_avg:98.18ms
step:1744/1750 train_time:171227ms step_avg:98.18ms
step:1745/1750 train_time:171328ms step_avg:98.18ms
step:1746/1750 train_time:171430ms step_avg:98.18ms
step:1747/1750 train_time:171533ms step_avg:98.19ms
step:1748/1750 train_time:171636ms step_avg:98.19ms
step:1749/1750 train_time:171738ms step_avg:98.19ms
step:1750/1750 train_time:171841ms step_avg:98.19ms
step:1750/1750 val_loss:3.2786 train_time:171938ms step_avg:98.25ms
peak memory allocated: 33278 MiB reserved: 49034 MiB
