import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X
'''
import torch.utils.dlpack as thd
from cupyx.scipy.sparse.linalg import svds as cupyx_svds

def several_sv_svds_approximation(W_torch, k, num_iter=50):
    """SVD approximation using the top k singular values and corresponding vectors."""
    # Store original device and dtype
    original_device = W_torch.device
    original_dtype = W_torch.dtype
    
    W = cp.from_dlpack(thd.to_dlpack(W_torch)).astype(cp.float32)
    U, S, Vt = cupyx_svds(W, k=min([k, W.shape[0] - 1, W.shape[1] - 1]), maxiter=num_iter, which='LM')

    # Convert back to torch tensors and ensure they're on the correct device
    approx_torch_U = thd.from_dlpack(U.toDlpack()).to(device=original_device, dtype=original_dtype)
    approx_torch_S = thd.from_dlpack(S.toDlpack()).to(device=original_device, dtype=original_dtype)
    approx_torch_Vt = thd.from_dlpack(Vt.toDlpack()).to(device=original_device, dtype=original_dtype)
    
    return approx_torch_U, approx_torch_S, approx_torch_Vt

class NormNeon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    # v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    u, s, vt = several_sv_svds_approximation(grad.bfloat16(), 5)
                    p.add_(other=u@vt, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()
'''
class FastNormMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, sgd_coeff=0):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, sgd_coeff=sgd_coeff)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        self.sgd_coeff = sgd_coeff

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            sgd_coeff = group["sgd_coeff"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    # assume: grad is a Tensor, momentum_buffer exists, momentum, sgd_coeff, eps, eff_lr are defined
                    with torch.no_grad():
                        # inplace blend with momentum (keeps original semantic: grad becomes the lerped tensor)
                        grad.lerp_(momentum_buffer, momentum)

                        # compute zeropower part from the (post-lerp) grad, in bfloat16 to match the original code
                        if sgd_coeff != 1:
                            # create update_part in bfloat16 (may allocate once per call; unavoidable if zeropower expects bfloat16)
                            update_part = zeropower_via_newtonschulz5(grad.to(torch.bfloat16), 5)

                            # upcast update_part to grad dtype to do mixed-dtype blending without extra temporaries later
                            if update_part.dtype != grad.dtype:
                                update_part = update_part.to(grad.dtype)

                        # compute inverse norm scalar efficiently (no large temp tensors)
                        # - view(-1) is cheap (no copy)
                        # - dot(view, view) is a single reduction
                        norm_sq = grad.view(-1).dot(grad.view(-1))
                        inv_norm = (norm_sq + eps).rsqrt()  # stable rsqrt on the scalar

                        # normalize grad in-place (so we avoid allocating g_normalized)
                        grad.mul_(inv_norm)   # now grad == g_normalized (in-place)

                        # build final update (reuse update_part buffer if available)
                        if sgd_coeff != 1:
                            # update_part := (1 - sgd_coeff) * update_part + sgd_coeff * grad
                            # do it in-place on update_part to avoid creating another temp
                            update_part.mul_(1 - sgd_coeff)    # in-place scale
                            update_part.lerp_(grad, sgd_coeff) # in-place linear interpolation
                            update = update_part
                        else:
                            # if sgd_coeff == 1 we can use grad directly (already normalized in-place)
                            update = grad

                        # apply update (same as original)
                        p.add_(other=update, alpha=-eff_lr)
                    '''
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    eps = 1e-12
                    g_normalized = grad / (grad.norm() + eps)
                    if sgd_coeff != 1:
                        update_part = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                        update = (1 - sgd_coeff) * update_part + sgd_coeff * g_normalized
                    else:
                        update = sgd_coeff * g_normalized
                    p.add_(other=update, alpha=-eff_lr)
                    '''
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class NormMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, sgd_coeff=0):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, sgd_coeff=sgd_coeff)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        self.sgd_coeff = sgd_coeff

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            sgd_coeff = group["sgd_coeff"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    eps = 1e-12
                    g_normalized = grad / (grad.norm() + eps)
                    if sgd_coeff != 1:
                        update_part = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                        update = (1 - sgd_coeff) * update_part + sgd_coeff * g_normalized
                    else:
                        update = sgd_coeff * g_normalized
                    p.add_(other=update, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()
class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)
                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_reduce_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 27.5
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos : pos + max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()
    starts = []
    for i in range(1, len(boundary_positions)):
        end = boundary_positions[i].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == dist.get_world_size():
                return starts, end - pos
            start = end
    assert False # increase max_batch_span if necessary

def distributed_data_generator(filename_pattern: str, batch_size: int, align_to_bos: bool):
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    max_batch_span = 2 * batch_size if align_to_bos else batch_size # provide buffer to handle samples up to length local_batch_size
    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            batch_span = batch_size
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1750 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
from datetime import datetime
if master_process:
    # run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_id = str(int(time.time() * 1000))
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0)
optimizer2 = NormMuon(hidden_matrix_params, lr=0.07, momentum=0.95, weight_decay=0.0, sgd_coeff=0.5)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.11 | packaged by conda-forge | (main, Jun  4 2025, 14:45:31) [GCC 13.3.0]
Running PyTorch 2.9.0.dev20250824+cu126 compiled for CUDA 12.6
Sun Aug 24 17:44:34 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:04:00.0 Off |                    0 |
| N/A   39C    P0            121W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:23:00.0 Off |                    0 |
| N/A   33C    P0            115W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:43:00.0 Off |                    0 |
| N/A   37C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   32C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:84:00.0 Off |                    0 |
| N/A   38C    P0            121W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:A3:00.0 Off |                    0 |
| N/A   33C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:C3:00.0 Off |                    0 |
| N/A   39C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E4:00.0 Off |                    0 |
| N/A   31C    P0            113W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.05ms
step:1/1750 train_time:139ms step_avg:138.93ms
step:2/1750 train_time:163ms step_avg:81.44ms
step:3/1750 train_time:241ms step_avg:80.35ms
step:4/1750 train_time:332ms step_avg:83.11ms
step:5/1750 train_time:426ms step_avg:85.18ms
step:6/1750 train_time:518ms step_avg:86.37ms
step:7/1750 train_time:611ms step_avg:87.30ms
step:8/1750 train_time:703ms step_avg:87.91ms
step:9/1750 train_time:796ms step_avg:88.40ms
step:10/1750 train_time:888ms step_avg:88.78ms
step:11/1750 train_time:980ms step_avg:89.11ms
step:12/1750 train_time:1075ms step_avg:89.55ms
step:13/1750 train_time:1171ms step_avg:90.10ms
step:14/1750 train_time:1267ms step_avg:90.51ms
step:15/1750 train_time:1361ms step_avg:90.71ms
step:16/1750 train_time:1453ms step_avg:90.84ms
step:17/1750 train_time:1547ms step_avg:90.98ms
step:18/1750 train_time:1639ms step_avg:91.06ms
step:19/1750 train_time:1732ms step_avg:91.16ms
step:20/1750 train_time:1825ms step_avg:91.23ms
step:21/1750 train_time:1918ms step_avg:91.32ms
step:22/1750 train_time:2011ms step_avg:91.43ms
step:23/1750 train_time:2105ms step_avg:91.54ms
step:24/1750 train_time:2200ms step_avg:91.65ms
step:25/1750 train_time:2294ms step_avg:91.76ms
step:26/1750 train_time:2387ms step_avg:91.82ms
step:27/1750 train_time:2480ms step_avg:91.87ms
step:28/1750 train_time:2574ms step_avg:91.92ms
step:29/1750 train_time:2667ms step_avg:91.95ms
step:30/1750 train_time:2759ms step_avg:91.97ms
step:31/1750 train_time:2854ms step_avg:92.05ms
step:32/1750 train_time:2947ms step_avg:92.08ms
step:33/1750 train_time:3040ms step_avg:92.11ms
step:34/1750 train_time:3134ms step_avg:92.16ms
step:35/1750 train_time:3228ms step_avg:92.23ms
step:36/1750 train_time:3323ms step_avg:92.30ms
step:37/1750 train_time:3417ms step_avg:92.34ms
step:38/1750 train_time:3510ms step_avg:92.37ms
step:39/1750 train_time:3602ms step_avg:92.37ms
step:40/1750 train_time:3696ms step_avg:92.39ms
step:41/1750 train_time:3788ms step_avg:92.39ms
step:42/1750 train_time:3881ms step_avg:92.40ms
step:43/1750 train_time:3974ms step_avg:92.42ms
step:44/1750 train_time:4067ms step_avg:92.43ms
step:45/1750 train_time:4161ms step_avg:92.46ms
step:46/1750 train_time:4255ms step_avg:92.49ms
step:47/1750 train_time:4348ms step_avg:92.52ms
step:48/1750 train_time:4442ms step_avg:92.54ms
step:49/1750 train_time:4536ms step_avg:92.56ms
step:50/1750 train_time:4629ms step_avg:92.58ms
step:51/1750 train_time:4722ms step_avg:92.58ms
step:52/1750 train_time:4815ms step_avg:92.60ms
step:53/1750 train_time:4908ms step_avg:92.60ms
step:54/1750 train_time:5001ms step_avg:92.62ms
step:55/1750 train_time:5095ms step_avg:92.63ms
step:56/1750 train_time:5188ms step_avg:92.65ms
step:57/1750 train_time:5282ms step_avg:92.66ms
step:58/1750 train_time:5376ms step_avg:92.70ms
step:59/1750 train_time:5469ms step_avg:92.69ms
step:60/1750 train_time:5563ms step_avg:92.71ms
step:61/1750 train_time:5655ms step_avg:92.71ms
step:62/1750 train_time:5749ms step_avg:92.72ms
step:63/1750 train_time:5841ms step_avg:92.72ms
step:64/1750 train_time:5934ms step_avg:92.72ms
step:65/1750 train_time:6028ms step_avg:92.74ms
step:66/1750 train_time:6121ms step_avg:92.74ms
step:67/1750 train_time:6215ms step_avg:92.76ms
step:68/1750 train_time:6308ms step_avg:92.77ms
step:69/1750 train_time:6402ms step_avg:92.78ms
step:70/1750 train_time:6496ms step_avg:92.79ms
step:71/1750 train_time:6589ms step_avg:92.80ms
step:72/1750 train_time:6682ms step_avg:92.80ms
step:73/1750 train_time:6776ms step_avg:92.82ms
step:74/1750 train_time:6869ms step_avg:92.82ms
step:75/1750 train_time:6961ms step_avg:92.82ms
step:76/1750 train_time:7055ms step_avg:92.83ms
step:77/1750 train_time:7148ms step_avg:92.84ms
step:78/1750 train_time:7242ms step_avg:92.85ms
step:79/1750 train_time:7336ms step_avg:92.86ms
step:80/1750 train_time:7430ms step_avg:92.87ms
step:81/1750 train_time:7523ms step_avg:92.88ms
step:82/1750 train_time:7617ms step_avg:92.89ms
step:83/1750 train_time:7710ms step_avg:92.89ms
step:84/1750 train_time:7804ms step_avg:92.90ms
step:85/1750 train_time:7897ms step_avg:92.90ms
step:86/1750 train_time:7989ms step_avg:92.90ms
step:87/1750 train_time:8082ms step_avg:92.90ms
step:88/1750 train_time:8177ms step_avg:92.92ms
step:89/1750 train_time:8270ms step_avg:92.92ms
step:90/1750 train_time:8363ms step_avg:92.92ms
step:91/1750 train_time:8457ms step_avg:92.93ms
step:92/1750 train_time:8550ms step_avg:92.94ms
step:93/1750 train_time:8644ms step_avg:92.95ms
step:94/1750 train_time:8737ms step_avg:92.95ms
step:95/1750 train_time:8830ms step_avg:92.95ms
step:96/1750 train_time:8923ms step_avg:92.95ms
step:97/1750 train_time:9017ms step_avg:92.96ms
step:98/1750 train_time:9109ms step_avg:92.95ms
step:99/1750 train_time:9203ms step_avg:92.96ms
step:100/1750 train_time:9296ms step_avg:92.96ms
step:101/1750 train_time:9389ms step_avg:92.96ms
step:102/1750 train_time:9483ms step_avg:92.97ms
step:103/1750 train_time:9577ms step_avg:92.98ms
step:104/1750 train_time:9670ms step_avg:92.98ms
step:105/1750 train_time:9763ms step_avg:92.98ms
step:106/1750 train_time:9856ms step_avg:92.98ms
step:107/1750 train_time:9950ms step_avg:92.99ms
step:108/1750 train_time:10044ms step_avg:93.00ms
step:109/1750 train_time:10137ms step_avg:93.00ms
step:110/1750 train_time:10231ms step_avg:93.01ms
step:111/1750 train_time:10324ms step_avg:93.01ms
step:112/1750 train_time:10417ms step_avg:93.01ms
step:113/1750 train_time:10511ms step_avg:93.01ms
step:114/1750 train_time:10604ms step_avg:93.02ms
step:115/1750 train_time:10698ms step_avg:93.02ms
step:116/1750 train_time:10792ms step_avg:93.03ms
step:117/1750 train_time:10885ms step_avg:93.03ms
step:118/1750 train_time:10978ms step_avg:93.03ms
step:119/1750 train_time:11071ms step_avg:93.03ms
step:120/1750 train_time:11164ms step_avg:93.03ms
step:121/1750 train_time:11257ms step_avg:93.04ms
step:122/1750 train_time:11352ms step_avg:93.05ms
step:123/1750 train_time:11445ms step_avg:93.05ms
step:124/1750 train_time:11538ms step_avg:93.05ms
step:125/1750 train_time:11631ms step_avg:93.05ms
step:125/1750 val_loss:4.6590 train_time:11714ms step_avg:93.71ms
step:126/1750 train_time:11735ms step_avg:93.14ms
step:127/1750 train_time:11825ms step_avg:93.11ms
step:128/1750 train_time:11927ms step_avg:93.18ms
step:129/1750 train_time:12022ms step_avg:93.19ms
step:130/1750 train_time:12115ms step_avg:93.20ms
step:131/1750 train_time:12208ms step_avg:93.19ms
step:132/1750 train_time:12301ms step_avg:93.19ms
step:133/1750 train_time:12394ms step_avg:93.19ms
step:134/1750 train_time:12487ms step_avg:93.19ms
step:135/1750 train_time:12580ms step_avg:93.18ms
step:136/1750 train_time:12673ms step_avg:93.19ms
step:137/1750 train_time:12768ms step_avg:93.20ms
step:138/1750 train_time:12864ms step_avg:93.22ms
step:139/1750 train_time:12960ms step_avg:93.24ms
step:140/1750 train_time:13055ms step_avg:93.25ms
step:141/1750 train_time:13149ms step_avg:93.25ms
step:142/1750 train_time:13242ms step_avg:93.25ms
step:143/1750 train_time:13335ms step_avg:93.25ms
step:144/1750 train_time:13428ms step_avg:93.25ms
step:145/1750 train_time:13521ms step_avg:93.25ms
step:146/1750 train_time:13614ms step_avg:93.25ms
step:147/1750 train_time:13708ms step_avg:93.25ms
step:148/1750 train_time:13802ms step_avg:93.26ms
step:149/1750 train_time:13897ms step_avg:93.27ms
step:150/1750 train_time:13991ms step_avg:93.28ms
step:151/1750 train_time:14086ms step_avg:93.28ms
step:152/1750 train_time:14180ms step_avg:93.29ms
step:153/1750 train_time:14274ms step_avg:93.29ms
step:154/1750 train_time:14368ms step_avg:93.30ms
step:155/1750 train_time:14461ms step_avg:93.29ms
step:156/1750 train_time:14553ms step_avg:93.29ms
step:157/1750 train_time:14647ms step_avg:93.29ms
step:158/1750 train_time:14740ms step_avg:93.29ms
step:159/1750 train_time:14834ms step_avg:93.30ms
step:160/1750 train_time:14929ms step_avg:93.31ms
step:161/1750 train_time:15023ms step_avg:93.31ms
step:162/1750 train_time:15118ms step_avg:93.32ms
step:163/1750 train_time:15212ms step_avg:93.33ms
step:164/1750 train_time:15307ms step_avg:93.34ms
step:165/1750 train_time:15401ms step_avg:93.34ms
step:166/1750 train_time:15494ms step_avg:93.34ms
step:167/1750 train_time:15587ms step_avg:93.34ms
step:168/1750 train_time:15680ms step_avg:93.33ms
step:169/1750 train_time:15774ms step_avg:93.34ms
step:170/1750 train_time:15868ms step_avg:93.34ms
step:171/1750 train_time:15962ms step_avg:93.35ms
step:172/1750 train_time:16057ms step_avg:93.35ms
step:173/1750 train_time:16151ms step_avg:93.36ms
step:174/1750 train_time:16246ms step_avg:93.37ms
step:175/1750 train_time:16339ms step_avg:93.37ms
step:176/1750 train_time:16433ms step_avg:93.37ms
step:177/1750 train_time:16527ms step_avg:93.37ms
step:178/1750 train_time:16620ms step_avg:93.37ms
step:179/1750 train_time:16713ms step_avg:93.37ms
step:180/1750 train_time:16808ms step_avg:93.38ms
step:181/1750 train_time:16902ms step_avg:93.38ms
step:182/1750 train_time:16995ms step_avg:93.38ms
step:183/1750 train_time:17090ms step_avg:93.39ms
step:184/1750 train_time:17183ms step_avg:93.39ms
step:185/1750 train_time:17277ms step_avg:93.39ms
step:186/1750 train_time:17371ms step_avg:93.39ms
step:187/1750 train_time:17465ms step_avg:93.39ms
step:188/1750 train_time:17558ms step_avg:93.39ms
step:189/1750 train_time:17652ms step_avg:93.39ms
step:190/1750 train_time:17745ms step_avg:93.40ms
step:191/1750 train_time:17839ms step_avg:93.40ms
step:192/1750 train_time:17932ms step_avg:93.40ms
step:193/1750 train_time:18027ms step_avg:93.40ms
step:194/1750 train_time:18121ms step_avg:93.41ms
step:195/1750 train_time:18215ms step_avg:93.41ms
step:196/1750 train_time:18309ms step_avg:93.41ms
step:197/1750 train_time:18403ms step_avg:93.42ms
step:198/1750 train_time:18496ms step_avg:93.41ms
step:199/1750 train_time:18590ms step_avg:93.42ms
step:200/1750 train_time:18684ms step_avg:93.42ms
step:201/1750 train_time:18777ms step_avg:93.42ms
step:202/1750 train_time:18871ms step_avg:93.42ms
step:203/1750 train_time:18964ms step_avg:93.42ms
step:204/1750 train_time:19057ms step_avg:93.42ms
step:205/1750 train_time:19151ms step_avg:93.42ms
step:206/1750 train_time:19246ms step_avg:93.43ms
step:207/1750 train_time:19339ms step_avg:93.43ms
step:208/1750 train_time:19433ms step_avg:93.43ms
step:209/1750 train_time:19526ms step_avg:93.43ms
step:210/1750 train_time:19620ms step_avg:93.43ms
step:211/1750 train_time:19714ms step_avg:93.43ms
step:212/1750 train_time:19808ms step_avg:93.43ms
step:213/1750 train_time:19901ms step_avg:93.43ms
step:214/1750 train_time:19995ms step_avg:93.43ms
step:215/1750 train_time:20088ms step_avg:93.43ms
step:216/1750 train_time:20182ms step_avg:93.44ms
step:217/1750 train_time:20276ms step_avg:93.44ms
step:218/1750 train_time:20370ms step_avg:93.44ms
step:219/1750 train_time:20464ms step_avg:93.44ms
step:220/1750 train_time:20557ms step_avg:93.44ms
step:221/1750 train_time:20651ms step_avg:93.45ms
step:222/1750 train_time:20745ms step_avg:93.44ms
step:223/1750 train_time:20838ms step_avg:93.44ms
step:224/1750 train_time:20932ms step_avg:93.45ms
step:225/1750 train_time:21026ms step_avg:93.45ms
step:226/1750 train_time:21119ms step_avg:93.45ms
step:227/1750 train_time:21214ms step_avg:93.45ms
step:228/1750 train_time:21308ms step_avg:93.46ms
step:229/1750 train_time:21401ms step_avg:93.46ms
step:230/1750 train_time:21495ms step_avg:93.46ms
step:231/1750 train_time:21589ms step_avg:93.46ms
step:232/1750 train_time:21682ms step_avg:93.46ms
step:233/1750 train_time:21775ms step_avg:93.46ms
step:234/1750 train_time:21870ms step_avg:93.46ms
step:235/1750 train_time:21963ms step_avg:93.46ms
step:236/1750 train_time:22057ms step_avg:93.46ms
step:237/1750 train_time:22151ms step_avg:93.46ms
step:238/1750 train_time:22245ms step_avg:93.47ms
step:239/1750 train_time:22338ms step_avg:93.47ms
step:240/1750 train_time:22432ms step_avg:93.47ms
step:241/1750 train_time:22526ms step_avg:93.47ms
step:242/1750 train_time:22620ms step_avg:93.47ms
step:243/1750 train_time:22714ms step_avg:93.47ms
step:244/1750 train_time:22808ms step_avg:93.47ms
step:245/1750 train_time:22901ms step_avg:93.48ms
step:246/1750 train_time:22995ms step_avg:93.48ms
step:247/1750 train_time:23089ms step_avg:93.48ms
step:248/1750 train_time:23182ms step_avg:93.48ms
step:249/1750 train_time:23276ms step_avg:93.48ms
step:250/1750 train_time:23370ms step_avg:93.48ms
step:250/1750 val_loss:4.1050 train_time:23453ms step_avg:93.81ms
step:251/1750 train_time:23474ms step_avg:93.52ms
step:252/1750 train_time:23569ms step_avg:93.53ms
step:253/1750 train_time:23665ms step_avg:93.54ms
step:254/1750 train_time:23758ms step_avg:93.54ms
step:255/1750 train_time:23852ms step_avg:93.54ms
step:256/1750 train_time:23945ms step_avg:93.53ms
step:257/1750 train_time:24037ms step_avg:93.53ms
step:258/1750 train_time:24129ms step_avg:93.52ms
step:259/1750 train_time:24222ms step_avg:93.52ms
step:260/1750 train_time:24315ms step_avg:93.52ms
step:261/1750 train_time:24410ms step_avg:93.52ms
step:262/1750 train_time:24506ms step_avg:93.54ms
step:263/1750 train_time:24603ms step_avg:93.55ms
step:264/1750 train_time:24698ms step_avg:93.55ms
step:265/1750 train_time:24793ms step_avg:93.56ms
step:266/1750 train_time:24887ms step_avg:93.56ms
step:267/1750 train_time:24980ms step_avg:93.56ms
step:268/1750 train_time:25074ms step_avg:93.56ms
step:269/1750 train_time:25167ms step_avg:93.56ms
step:270/1750 train_time:25260ms step_avg:93.56ms
step:271/1750 train_time:25354ms step_avg:93.56ms
step:272/1750 train_time:25449ms step_avg:93.56ms
step:273/1750 train_time:25544ms step_avg:93.57ms
step:274/1750 train_time:25640ms step_avg:93.58ms
step:275/1750 train_time:25735ms step_avg:93.58ms
step:276/1750 train_time:25829ms step_avg:93.58ms
step:277/1750 train_time:25924ms step_avg:93.59ms
step:278/1750 train_time:26018ms step_avg:93.59ms
step:279/1750 train_time:26111ms step_avg:93.59ms
step:280/1750 train_time:26205ms step_avg:93.59ms
step:281/1750 train_time:26298ms step_avg:93.59ms
step:282/1750 train_time:26391ms step_avg:93.59ms
step:283/1750 train_time:26486ms step_avg:93.59ms
step:284/1750 train_time:26580ms step_avg:93.59ms
step:285/1750 train_time:26676ms step_avg:93.60ms
step:286/1750 train_time:26770ms step_avg:93.60ms
step:287/1750 train_time:26865ms step_avg:93.61ms
step:288/1750 train_time:26959ms step_avg:93.61ms
step:289/1750 train_time:27052ms step_avg:93.61ms
step:290/1750 train_time:27146ms step_avg:93.61ms
step:291/1750 train_time:27240ms step_avg:93.61ms
step:292/1750 train_time:27333ms step_avg:93.61ms
step:293/1750 train_time:27428ms step_avg:93.61ms
step:294/1750 train_time:27522ms step_avg:93.61ms
step:295/1750 train_time:27617ms step_avg:93.62ms
step:296/1750 train_time:27711ms step_avg:93.62ms
step:297/1750 train_time:27806ms step_avg:93.62ms
step:298/1750 train_time:27900ms step_avg:93.62ms
step:299/1750 train_time:27993ms step_avg:93.62ms
step:300/1750 train_time:28088ms step_avg:93.63ms
step:301/1750 train_time:28181ms step_avg:93.63ms
step:302/1750 train_time:28275ms step_avg:93.63ms
step:303/1750 train_time:28369ms step_avg:93.63ms
step:304/1750 train_time:28463ms step_avg:93.63ms
step:305/1750 train_time:28557ms step_avg:93.63ms
step:306/1750 train_time:28652ms step_avg:93.63ms
step:307/1750 train_time:28747ms step_avg:93.64ms
step:308/1750 train_time:28841ms step_avg:93.64ms
step:309/1750 train_time:28935ms step_avg:93.64ms
step:310/1750 train_time:29029ms step_avg:93.64ms
step:311/1750 train_time:29123ms step_avg:93.64ms
step:312/1750 train_time:29217ms step_avg:93.64ms
step:313/1750 train_time:29311ms step_avg:93.64ms
step:314/1750 train_time:29405ms step_avg:93.65ms
step:315/1750 train_time:29499ms step_avg:93.65ms
step:316/1750 train_time:29592ms step_avg:93.65ms
step:317/1750 train_time:29686ms step_avg:93.65ms
step:318/1750 train_time:29780ms step_avg:93.65ms
step:319/1750 train_time:29874ms step_avg:93.65ms
step:320/1750 train_time:29968ms step_avg:93.65ms
step:321/1750 train_time:30063ms step_avg:93.65ms
step:322/1750 train_time:30157ms step_avg:93.65ms
step:323/1750 train_time:30252ms step_avg:93.66ms
step:324/1750 train_time:30347ms step_avg:93.66ms
step:325/1750 train_time:30441ms step_avg:93.66ms
step:326/1750 train_time:30535ms step_avg:93.67ms
step:327/1750 train_time:30629ms step_avg:93.67ms
step:328/1750 train_time:30724ms step_avg:93.67ms
step:329/1750 train_time:30817ms step_avg:93.67ms
step:330/1750 train_time:30911ms step_avg:93.67ms
step:331/1750 train_time:31005ms step_avg:93.67ms
step:332/1750 train_time:31099ms step_avg:93.67ms
step:333/1750 train_time:31193ms step_avg:93.67ms
step:334/1750 train_time:31288ms step_avg:93.68ms
step:335/1750 train_time:31382ms step_avg:93.68ms
step:336/1750 train_time:31475ms step_avg:93.67ms
step:337/1750 train_time:31569ms step_avg:93.68ms
step:338/1750 train_time:31663ms step_avg:93.68ms
step:339/1750 train_time:31757ms step_avg:93.68ms
step:340/1750 train_time:31851ms step_avg:93.68ms
step:341/1750 train_time:31945ms step_avg:93.68ms
step:342/1750 train_time:32039ms step_avg:93.68ms
step:343/1750 train_time:32132ms step_avg:93.68ms
step:344/1750 train_time:32227ms step_avg:93.68ms
step:345/1750 train_time:32321ms step_avg:93.69ms
step:346/1750 train_time:32415ms step_avg:93.69ms
step:347/1750 train_time:32509ms step_avg:93.69ms
step:348/1750 train_time:32603ms step_avg:93.69ms
step:349/1750 train_time:32697ms step_avg:93.69ms
step:350/1750 train_time:32791ms step_avg:93.69ms
step:351/1750 train_time:32886ms step_avg:93.69ms
step:352/1750 train_time:32979ms step_avg:93.69ms
step:353/1750 train_time:33074ms step_avg:93.69ms
step:354/1750 train_time:33168ms step_avg:93.69ms
step:355/1750 train_time:33262ms step_avg:93.70ms
step:356/1750 train_time:33355ms step_avg:93.69ms
step:357/1750 train_time:33449ms step_avg:93.70ms
step:358/1750 train_time:33544ms step_avg:93.70ms
step:359/1750 train_time:33638ms step_avg:93.70ms
step:360/1750 train_time:33732ms step_avg:93.70ms
step:361/1750 train_time:33827ms step_avg:93.70ms
step:362/1750 train_time:33921ms step_avg:93.70ms
step:363/1750 train_time:34015ms step_avg:93.71ms
step:364/1750 train_time:34109ms step_avg:93.71ms
step:365/1750 train_time:34203ms step_avg:93.71ms
step:366/1750 train_time:34297ms step_avg:93.71ms
step:367/1750 train_time:34391ms step_avg:93.71ms
step:368/1750 train_time:34485ms step_avg:93.71ms
step:369/1750 train_time:34579ms step_avg:93.71ms
step:370/1750 train_time:34673ms step_avg:93.71ms
step:371/1750 train_time:34767ms step_avg:93.71ms
step:372/1750 train_time:34861ms step_avg:93.71ms
step:373/1750 train_time:34955ms step_avg:93.71ms
step:374/1750 train_time:35049ms step_avg:93.71ms
step:375/1750 train_time:35143ms step_avg:93.72ms
step:375/1750 val_loss:3.8971 train_time:35227ms step_avg:93.94ms
step:376/1750 train_time:35249ms step_avg:93.75ms
step:377/1750 train_time:35341ms step_avg:93.74ms
step:378/1750 train_time:35437ms step_avg:93.75ms
step:379/1750 train_time:35532ms step_avg:93.75ms
step:380/1750 train_time:35625ms step_avg:93.75ms
step:381/1750 train_time:35718ms step_avg:93.75ms
step:382/1750 train_time:35811ms step_avg:93.75ms
step:383/1750 train_time:35905ms step_avg:93.75ms
step:384/1750 train_time:35998ms step_avg:93.75ms
step:385/1750 train_time:36092ms step_avg:93.75ms
step:386/1750 train_time:36186ms step_avg:93.75ms
step:387/1750 train_time:36282ms step_avg:93.75ms
step:388/1750 train_time:36378ms step_avg:93.76ms
step:389/1750 train_time:36473ms step_avg:93.76ms
step:390/1750 train_time:36567ms step_avg:93.76ms
step:391/1750 train_time:36662ms step_avg:93.77ms
step:392/1750 train_time:36758ms step_avg:93.77ms
step:393/1750 train_time:36853ms step_avg:93.77ms
step:394/1750 train_time:36949ms step_avg:93.78ms
step:395/1750 train_time:37045ms step_avg:93.78ms
step:396/1750 train_time:37140ms step_avg:93.79ms
step:397/1750 train_time:37237ms step_avg:93.79ms
step:398/1750 train_time:37333ms step_avg:93.80ms
step:399/1750 train_time:37429ms step_avg:93.81ms
step:400/1750 train_time:37525ms step_avg:93.81ms
step:401/1750 train_time:37621ms step_avg:93.82ms
step:402/1750 train_time:37718ms step_avg:93.83ms
step:403/1750 train_time:37814ms step_avg:93.83ms
step:404/1750 train_time:37909ms step_avg:93.83ms
step:405/1750 train_time:38005ms step_avg:93.84ms
step:406/1750 train_time:38100ms step_avg:93.84ms
step:407/1750 train_time:38196ms step_avg:93.85ms
step:408/1750 train_time:38292ms step_avg:93.85ms
step:409/1750 train_time:38389ms step_avg:93.86ms
step:410/1750 train_time:38485ms step_avg:93.87ms
step:411/1750 train_time:38581ms step_avg:93.87ms
step:412/1750 train_time:38677ms step_avg:93.88ms
step:413/1750 train_time:38773ms step_avg:93.88ms
step:414/1750 train_time:38869ms step_avg:93.89ms
step:415/1750 train_time:38964ms step_avg:93.89ms
step:416/1750 train_time:39060ms step_avg:93.89ms
step:417/1750 train_time:39156ms step_avg:93.90ms
step:418/1750 train_time:39251ms step_avg:93.90ms
step:419/1750 train_time:39347ms step_avg:93.91ms
step:420/1750 train_time:39443ms step_avg:93.91ms
step:421/1750 train_time:39539ms step_avg:93.92ms
step:422/1750 train_time:39635ms step_avg:93.92ms
step:423/1750 train_time:39731ms step_avg:93.93ms
step:424/1750 train_time:39827ms step_avg:93.93ms
step:425/1750 train_time:39923ms step_avg:93.94ms
step:426/1750 train_time:40018ms step_avg:93.94ms
step:427/1750 train_time:40115ms step_avg:93.95ms
step:428/1750 train_time:40210ms step_avg:93.95ms
step:429/1750 train_time:40306ms step_avg:93.95ms
step:430/1750 train_time:40402ms step_avg:93.96ms
step:431/1750 train_time:40498ms step_avg:93.96ms
step:432/1750 train_time:40596ms step_avg:93.97ms
step:433/1750 train_time:40691ms step_avg:93.98ms
step:434/1750 train_time:40787ms step_avg:93.98ms
step:435/1750 train_time:40883ms step_avg:93.98ms
step:436/1750 train_time:40980ms step_avg:93.99ms
step:437/1750 train_time:41075ms step_avg:93.99ms
step:438/1750 train_time:41171ms step_avg:94.00ms
step:439/1750 train_time:41267ms step_avg:94.00ms
step:440/1750 train_time:41363ms step_avg:94.01ms
step:441/1750 train_time:41459ms step_avg:94.01ms
step:442/1750 train_time:41555ms step_avg:94.02ms
step:443/1750 train_time:41651ms step_avg:94.02ms
step:444/1750 train_time:41747ms step_avg:94.02ms
step:445/1750 train_time:41843ms step_avg:94.03ms
step:446/1750 train_time:41939ms step_avg:94.03ms
step:447/1750 train_time:42034ms step_avg:94.04ms
step:448/1750 train_time:42130ms step_avg:94.04ms
step:449/1750 train_time:42225ms step_avg:94.04ms
step:450/1750 train_time:42321ms step_avg:94.05ms
step:451/1750 train_time:42417ms step_avg:94.05ms
step:452/1750 train_time:42513ms step_avg:94.06ms
step:453/1750 train_time:42610ms step_avg:94.06ms
step:454/1750 train_time:42706ms step_avg:94.07ms
step:455/1750 train_time:42802ms step_avg:94.07ms
step:456/1750 train_time:42898ms step_avg:94.08ms
step:457/1750 train_time:42994ms step_avg:94.08ms
step:458/1750 train_time:43090ms step_avg:94.08ms
step:459/1750 train_time:43186ms step_avg:94.09ms
step:460/1750 train_time:43282ms step_avg:94.09ms
step:461/1750 train_time:43378ms step_avg:94.09ms
step:462/1750 train_time:43474ms step_avg:94.10ms
step:463/1750 train_time:43570ms step_avg:94.10ms
step:464/1750 train_time:43665ms step_avg:94.11ms
step:465/1750 train_time:43761ms step_avg:94.11ms
step:466/1750 train_time:43858ms step_avg:94.12ms
step:467/1750 train_time:43954ms step_avg:94.12ms
step:468/1750 train_time:44050ms step_avg:94.12ms
step:469/1750 train_time:44146ms step_avg:94.13ms
step:470/1750 train_time:44242ms step_avg:94.13ms
step:471/1750 train_time:44338ms step_avg:94.14ms
step:472/1750 train_time:44434ms step_avg:94.14ms
step:473/1750 train_time:44530ms step_avg:94.14ms
step:474/1750 train_time:44626ms step_avg:94.15ms
step:475/1750 train_time:44722ms step_avg:94.15ms
step:476/1750 train_time:44818ms step_avg:94.16ms
step:477/1750 train_time:44916ms step_avg:94.16ms
step:478/1750 train_time:45013ms step_avg:94.17ms
step:479/1750 train_time:45109ms step_avg:94.17ms
step:480/1750 train_time:45204ms step_avg:94.18ms
step:481/1750 train_time:45300ms step_avg:94.18ms
step:482/1750 train_time:45396ms step_avg:94.18ms
step:483/1750 train_time:45492ms step_avg:94.19ms
step:484/1750 train_time:45588ms step_avg:94.19ms
step:485/1750 train_time:45683ms step_avg:94.19ms
step:486/1750 train_time:45779ms step_avg:94.20ms
step:487/1750 train_time:45876ms step_avg:94.20ms
step:488/1750 train_time:45972ms step_avg:94.20ms
step:489/1750 train_time:46067ms step_avg:94.21ms
step:490/1750 train_time:46163ms step_avg:94.21ms
step:491/1750 train_time:46259ms step_avg:94.21ms
step:492/1750 train_time:46355ms step_avg:94.22ms
step:493/1750 train_time:46451ms step_avg:94.22ms
step:494/1750 train_time:46546ms step_avg:94.22ms
step:495/1750 train_time:46643ms step_avg:94.23ms
step:496/1750 train_time:46740ms step_avg:94.23ms
step:497/1750 train_time:46836ms step_avg:94.24ms
step:498/1750 train_time:46932ms step_avg:94.24ms
step:499/1750 train_time:47028ms step_avg:94.25ms
step:500/1750 train_time:47124ms step_avg:94.25ms
step:500/1750 val_loss:3.7476 train_time:47208ms step_avg:94.42ms
step:501/1750 train_time:47230ms step_avg:94.27ms
step:502/1750 train_time:47324ms step_avg:94.27ms
step:503/1750 train_time:47422ms step_avg:94.28ms
step:504/1750 train_time:47518ms step_avg:94.28ms
step:505/1750 train_time:47614ms step_avg:94.28ms
step:506/1750 train_time:47709ms step_avg:94.29ms
step:507/1750 train_time:47804ms step_avg:94.29ms
step:508/1750 train_time:47898ms step_avg:94.29ms
step:509/1750 train_time:47994ms step_avg:94.29ms
step:510/1750 train_time:48089ms step_avg:94.29ms
step:511/1750 train_time:48186ms step_avg:94.30ms
step:512/1750 train_time:48284ms step_avg:94.30ms
step:513/1750 train_time:48381ms step_avg:94.31ms
step:514/1750 train_time:48478ms step_avg:94.32ms
step:515/1750 train_time:48575ms step_avg:94.32ms
step:516/1750 train_time:48671ms step_avg:94.32ms
step:517/1750 train_time:48766ms step_avg:94.33ms
step:518/1750 train_time:48861ms step_avg:94.33ms
step:519/1750 train_time:48957ms step_avg:94.33ms
step:520/1750 train_time:49052ms step_avg:94.33ms
step:521/1750 train_time:49149ms step_avg:94.34ms
step:522/1750 train_time:49245ms step_avg:94.34ms
step:523/1750 train_time:49342ms step_avg:94.35ms
step:524/1750 train_time:49439ms step_avg:94.35ms
step:525/1750 train_time:49536ms step_avg:94.35ms
step:526/1750 train_time:49633ms step_avg:94.36ms
step:527/1750 train_time:49729ms step_avg:94.36ms
step:528/1750 train_time:49824ms step_avg:94.36ms
step:529/1750 train_time:49920ms step_avg:94.37ms
step:530/1750 train_time:50015ms step_avg:94.37ms
step:531/1750 train_time:50110ms step_avg:94.37ms
step:532/1750 train_time:50207ms step_avg:94.37ms
step:533/1750 train_time:50304ms step_avg:94.38ms
step:534/1750 train_time:50401ms step_avg:94.38ms
step:535/1750 train_time:50498ms step_avg:94.39ms
step:536/1750 train_time:50595ms step_avg:94.39ms
step:537/1750 train_time:50692ms step_avg:94.40ms
step:538/1750 train_time:50788ms step_avg:94.40ms
step:539/1750 train_time:50884ms step_avg:94.40ms
step:540/1750 train_time:50980ms step_avg:94.41ms
step:541/1750 train_time:51077ms step_avg:94.41ms
step:542/1750 train_time:51175ms step_avg:94.42ms
step:543/1750 train_time:51271ms step_avg:94.42ms
step:544/1750 train_time:51368ms step_avg:94.43ms
step:545/1750 train_time:51465ms step_avg:94.43ms
step:546/1750 train_time:51562ms step_avg:94.44ms
step:547/1750 train_time:51658ms step_avg:94.44ms
step:548/1750 train_time:51755ms step_avg:94.44ms
step:549/1750 train_time:51852ms step_avg:94.45ms
step:550/1750 train_time:51947ms step_avg:94.45ms
step:551/1750 train_time:52044ms step_avg:94.45ms
step:552/1750 train_time:52139ms step_avg:94.46ms
step:553/1750 train_time:52236ms step_avg:94.46ms
step:554/1750 train_time:52333ms step_avg:94.46ms
step:555/1750 train_time:52429ms step_avg:94.47ms
step:556/1750 train_time:52526ms step_avg:94.47ms
step:557/1750 train_time:52622ms step_avg:94.47ms
step:558/1750 train_time:52719ms step_avg:94.48ms
step:559/1750 train_time:52814ms step_avg:94.48ms
step:560/1750 train_time:52910ms step_avg:94.48ms
step:561/1750 train_time:53006ms step_avg:94.48ms
step:562/1750 train_time:53102ms step_avg:94.49ms
step:563/1750 train_time:53198ms step_avg:94.49ms
step:564/1750 train_time:53296ms step_avg:94.50ms
step:565/1750 train_time:53392ms step_avg:94.50ms
step:566/1750 train_time:53488ms step_avg:94.50ms
step:567/1750 train_time:53584ms step_avg:94.50ms
step:568/1750 train_time:53680ms step_avg:94.51ms
step:569/1750 train_time:53777ms step_avg:94.51ms
step:570/1750 train_time:53873ms step_avg:94.51ms
step:571/1750 train_time:53968ms step_avg:94.52ms
step:572/1750 train_time:54066ms step_avg:94.52ms
step:573/1750 train_time:54161ms step_avg:94.52ms
step:574/1750 train_time:54258ms step_avg:94.53ms
step:575/1750 train_time:54354ms step_avg:94.53ms
step:576/1750 train_time:54451ms step_avg:94.53ms
step:577/1750 train_time:54547ms step_avg:94.54ms
step:578/1750 train_time:54643ms step_avg:94.54ms
step:579/1750 train_time:54739ms step_avg:94.54ms
step:580/1750 train_time:54835ms step_avg:94.54ms
step:581/1750 train_time:54932ms step_avg:94.55ms
step:582/1750 train_time:55028ms step_avg:94.55ms
step:583/1750 train_time:55124ms step_avg:94.55ms
step:584/1750 train_time:55220ms step_avg:94.55ms
step:585/1750 train_time:55317ms step_avg:94.56ms
step:586/1750 train_time:55414ms step_avg:94.56ms
step:587/1750 train_time:55510ms step_avg:94.56ms
step:588/1750 train_time:55606ms step_avg:94.57ms
step:589/1750 train_time:55702ms step_avg:94.57ms
step:590/1750 train_time:55799ms step_avg:94.57ms
step:591/1750 train_time:55896ms step_avg:94.58ms
step:592/1750 train_time:55993ms step_avg:94.58ms
step:593/1750 train_time:56088ms step_avg:94.58ms
step:594/1750 train_time:56185ms step_avg:94.59ms
step:595/1750 train_time:56281ms step_avg:94.59ms
step:596/1750 train_time:56378ms step_avg:94.59ms
step:597/1750 train_time:56474ms step_avg:94.60ms
step:598/1750 train_time:56569ms step_avg:94.60ms
step:599/1750 train_time:56666ms step_avg:94.60ms
step:600/1750 train_time:56762ms step_avg:94.60ms
step:601/1750 train_time:56859ms step_avg:94.61ms
step:602/1750 train_time:56955ms step_avg:94.61ms
step:603/1750 train_time:57051ms step_avg:94.61ms
step:604/1750 train_time:57147ms step_avg:94.61ms
step:605/1750 train_time:57244ms step_avg:94.62ms
step:606/1750 train_time:57340ms step_avg:94.62ms
step:607/1750 train_time:57436ms step_avg:94.62ms
step:608/1750 train_time:57533ms step_avg:94.63ms
step:609/1750 train_time:57629ms step_avg:94.63ms
step:610/1750 train_time:57725ms step_avg:94.63ms
step:611/1750 train_time:57822ms step_avg:94.63ms
step:612/1750 train_time:57918ms step_avg:94.64ms
step:613/1750 train_time:58014ms step_avg:94.64ms
step:614/1750 train_time:58110ms step_avg:94.64ms
step:615/1750 train_time:58206ms step_avg:94.64ms
step:616/1750 train_time:58302ms step_avg:94.65ms
step:617/1750 train_time:58398ms step_avg:94.65ms
step:618/1750 train_time:58495ms step_avg:94.65ms
step:619/1750 train_time:58591ms step_avg:94.65ms
step:620/1750 train_time:58687ms step_avg:94.66ms
step:621/1750 train_time:58783ms step_avg:94.66ms
step:622/1750 train_time:58880ms step_avg:94.66ms
step:623/1750 train_time:58977ms step_avg:94.67ms
step:624/1750 train_time:59074ms step_avg:94.67ms
step:625/1750 train_time:59170ms step_avg:94.67ms
step:625/1750 val_loss:3.6608 train_time:59255ms step_avg:94.81ms
step:626/1750 train_time:59277ms step_avg:94.69ms
step:627/1750 train_time:59371ms step_avg:94.69ms
step:628/1750 train_time:59471ms step_avg:94.70ms
step:629/1750 train_time:59568ms step_avg:94.70ms
step:630/1750 train_time:59664ms step_avg:94.70ms
step:631/1750 train_time:59759ms step_avg:94.71ms
step:632/1750 train_time:59855ms step_avg:94.71ms
step:633/1750 train_time:59950ms step_avg:94.71ms
step:634/1750 train_time:60045ms step_avg:94.71ms
step:635/1750 train_time:60141ms step_avg:94.71ms
step:636/1750 train_time:60238ms step_avg:94.71ms
step:637/1750 train_time:60339ms step_avg:94.72ms
step:638/1750 train_time:60438ms step_avg:94.73ms
step:639/1750 train_time:60536ms step_avg:94.74ms
step:640/1750 train_time:60632ms step_avg:94.74ms
step:641/1750 train_time:60728ms step_avg:94.74ms
step:642/1750 train_time:60824ms step_avg:94.74ms
step:643/1750 train_time:60920ms step_avg:94.74ms
step:644/1750 train_time:61016ms step_avg:94.74ms
step:645/1750 train_time:61111ms step_avg:94.75ms
step:646/1750 train_time:61208ms step_avg:94.75ms
step:647/1750 train_time:61306ms step_avg:94.75ms
step:648/1750 train_time:61403ms step_avg:94.76ms
step:649/1750 train_time:61501ms step_avg:94.76ms
step:650/1750 train_time:61598ms step_avg:94.77ms
step:651/1750 train_time:61694ms step_avg:94.77ms
step:652/1750 train_time:61791ms step_avg:94.77ms
step:653/1750 train_time:61889ms step_avg:94.78ms
step:654/1750 train_time:61987ms step_avg:94.78ms
step:655/1750 train_time:62084ms step_avg:94.79ms
step:656/1750 train_time:62182ms step_avg:94.79ms
step:657/1750 train_time:62280ms step_avg:94.80ms
step:658/1750 train_time:62378ms step_avg:94.80ms
step:659/1750 train_time:62478ms step_avg:94.81ms
step:660/1750 train_time:62575ms step_avg:94.81ms
step:661/1750 train_time:62674ms step_avg:94.82ms
step:662/1750 train_time:62771ms step_avg:94.82ms
step:663/1750 train_time:62868ms step_avg:94.82ms
step:664/1750 train_time:62965ms step_avg:94.83ms
step:665/1750 train_time:63062ms step_avg:94.83ms
step:666/1750 train_time:63159ms step_avg:94.83ms
step:667/1750 train_time:63257ms step_avg:94.84ms
step:668/1750 train_time:63355ms step_avg:94.84ms
step:669/1750 train_time:63453ms step_avg:94.85ms
step:670/1750 train_time:63552ms step_avg:94.85ms
step:671/1750 train_time:63650ms step_avg:94.86ms
step:672/1750 train_time:63748ms step_avg:94.86ms
step:673/1750 train_time:63845ms step_avg:94.87ms
step:674/1750 train_time:63942ms step_avg:94.87ms
step:675/1750 train_time:64040ms step_avg:94.87ms
step:676/1750 train_time:64137ms step_avg:94.88ms
step:677/1750 train_time:64236ms step_avg:94.88ms
step:678/1750 train_time:64334ms step_avg:94.89ms
step:679/1750 train_time:64431ms step_avg:94.89ms
step:680/1750 train_time:64530ms step_avg:94.90ms
step:681/1750 train_time:64629ms step_avg:94.90ms
step:682/1750 train_time:64727ms step_avg:94.91ms
step:683/1750 train_time:64825ms step_avg:94.91ms
step:684/1750 train_time:64922ms step_avg:94.91ms
step:685/1750 train_time:65019ms step_avg:94.92ms
step:686/1750 train_time:65118ms step_avg:94.92ms
step:687/1750 train_time:65215ms step_avg:94.93ms
step:688/1750 train_time:65313ms step_avg:94.93ms
step:689/1750 train_time:65412ms step_avg:94.94ms
step:690/1750 train_time:65509ms step_avg:94.94ms
step:691/1750 train_time:65607ms step_avg:94.95ms
step:692/1750 train_time:65706ms step_avg:94.95ms
step:693/1750 train_time:65804ms step_avg:94.96ms
step:694/1750 train_time:65901ms step_avg:94.96ms
step:695/1750 train_time:65999ms step_avg:94.96ms
step:696/1750 train_time:66098ms step_avg:94.97ms
step:697/1750 train_time:66196ms step_avg:94.97ms
step:698/1750 train_time:66293ms step_avg:94.98ms
step:699/1750 train_time:66390ms step_avg:94.98ms
step:700/1750 train_time:66489ms step_avg:94.98ms
step:701/1750 train_time:66587ms step_avg:94.99ms
step:702/1750 train_time:66686ms step_avg:94.99ms
step:703/1750 train_time:66784ms step_avg:95.00ms
step:704/1750 train_time:66882ms step_avg:95.00ms
step:705/1750 train_time:66979ms step_avg:95.01ms
step:706/1750 train_time:67077ms step_avg:95.01ms
step:707/1750 train_time:67175ms step_avg:95.01ms
step:708/1750 train_time:67273ms step_avg:95.02ms
step:709/1750 train_time:67371ms step_avg:95.02ms
step:710/1750 train_time:67468ms step_avg:95.03ms
step:711/1750 train_time:67567ms step_avg:95.03ms
step:712/1750 train_time:67665ms step_avg:95.03ms
step:713/1750 train_time:67763ms step_avg:95.04ms
step:714/1750 train_time:67860ms step_avg:95.04ms
step:715/1750 train_time:67958ms step_avg:95.05ms
step:716/1750 train_time:68055ms step_avg:95.05ms
step:717/1750 train_time:68153ms step_avg:95.05ms
step:718/1750 train_time:68250ms step_avg:95.06ms
step:719/1750 train_time:68348ms step_avg:95.06ms
step:720/1750 train_time:68447ms step_avg:95.06ms
step:721/1750 train_time:68544ms step_avg:95.07ms
step:722/1750 train_time:68642ms step_avg:95.07ms
step:723/1750 train_time:68740ms step_avg:95.08ms
step:724/1750 train_time:68838ms step_avg:95.08ms
step:725/1750 train_time:68935ms step_avg:95.08ms
step:726/1750 train_time:69033ms step_avg:95.09ms
step:727/1750 train_time:69131ms step_avg:95.09ms
step:728/1750 train_time:69229ms step_avg:95.09ms
step:729/1750 train_time:69328ms step_avg:95.10ms
step:730/1750 train_time:69425ms step_avg:95.10ms
step:731/1750 train_time:69523ms step_avg:95.11ms
step:732/1750 train_time:69621ms step_avg:95.11ms
step:733/1750 train_time:69718ms step_avg:95.11ms
step:734/1750 train_time:69816ms step_avg:95.12ms
step:735/1750 train_time:69913ms step_avg:95.12ms
step:736/1750 train_time:70012ms step_avg:95.12ms
step:737/1750 train_time:70109ms step_avg:95.13ms
step:738/1750 train_time:70207ms step_avg:95.13ms
step:739/1750 train_time:70305ms step_avg:95.14ms
step:740/1750 train_time:70402ms step_avg:95.14ms
step:741/1750 train_time:70500ms step_avg:95.14ms
step:742/1750 train_time:70598ms step_avg:95.15ms
step:743/1750 train_time:70696ms step_avg:95.15ms
step:744/1750 train_time:70794ms step_avg:95.15ms
step:745/1750 train_time:70892ms step_avg:95.16ms
step:746/1750 train_time:70990ms step_avg:95.16ms
step:747/1750 train_time:71087ms step_avg:95.16ms
step:748/1750 train_time:71186ms step_avg:95.17ms
step:749/1750 train_time:71284ms step_avg:95.17ms
step:750/1750 train_time:71381ms step_avg:95.17ms
step:750/1750 val_loss:3.5945 train_time:71467ms step_avg:95.29ms
step:751/1750 train_time:71489ms step_avg:95.19ms
step:752/1750 train_time:71587ms step_avg:95.20ms
step:753/1750 train_time:71686ms step_avg:95.20ms
step:754/1750 train_time:71784ms step_avg:95.20ms
step:755/1750 train_time:71881ms step_avg:95.21ms
step:756/1750 train_time:71979ms step_avg:95.21ms
step:757/1750 train_time:72076ms step_avg:95.21ms
step:758/1750 train_time:72174ms step_avg:95.22ms
step:759/1750 train_time:72271ms step_avg:95.22ms
step:760/1750 train_time:72368ms step_avg:95.22ms
step:761/1750 train_time:72466ms step_avg:95.22ms
step:762/1750 train_time:72566ms step_avg:95.23ms
step:763/1750 train_time:72666ms step_avg:95.24ms
step:764/1750 train_time:72765ms step_avg:95.24ms
step:765/1750 train_time:72862ms step_avg:95.24ms
step:766/1750 train_time:72960ms step_avg:95.25ms
step:767/1750 train_time:73056ms step_avg:95.25ms
step:768/1750 train_time:73155ms step_avg:95.25ms
step:769/1750 train_time:73251ms step_avg:95.26ms
step:770/1750 train_time:73348ms step_avg:95.26ms
step:771/1750 train_time:73446ms step_avg:95.26ms
step:772/1750 train_time:73545ms step_avg:95.27ms
step:773/1750 train_time:73645ms step_avg:95.27ms
step:774/1750 train_time:73743ms step_avg:95.28ms
step:775/1750 train_time:73841ms step_avg:95.28ms
step:776/1750 train_time:73938ms step_avg:95.28ms
step:777/1750 train_time:74035ms step_avg:95.28ms
step:778/1750 train_time:74133ms step_avg:95.29ms
step:779/1750 train_time:74230ms step_avg:95.29ms
step:780/1750 train_time:74328ms step_avg:95.29ms
step:781/1750 train_time:74426ms step_avg:95.30ms
step:782/1750 train_time:74524ms step_avg:95.30ms
step:783/1750 train_time:74624ms step_avg:95.30ms
step:784/1750 train_time:74723ms step_avg:95.31ms
step:785/1750 train_time:74821ms step_avg:95.31ms
step:786/1750 train_time:74920ms step_avg:95.32ms
step:787/1750 train_time:75017ms step_avg:95.32ms
step:788/1750 train_time:75115ms step_avg:95.32ms
step:789/1750 train_time:75213ms step_avg:95.33ms
step:790/1750 train_time:75310ms step_avg:95.33ms
step:791/1750 train_time:75407ms step_avg:95.33ms
step:792/1750 train_time:75506ms step_avg:95.34ms
step:793/1750 train_time:75603ms step_avg:95.34ms
step:794/1750 train_time:75701ms step_avg:95.34ms
step:795/1750 train_time:75800ms step_avg:95.35ms
step:796/1750 train_time:75898ms step_avg:95.35ms
step:797/1750 train_time:75997ms step_avg:95.35ms
step:798/1750 train_time:76095ms step_avg:95.36ms
step:799/1750 train_time:76194ms step_avg:95.36ms
step:800/1750 train_time:76293ms step_avg:95.37ms
step:801/1750 train_time:76391ms step_avg:95.37ms
step:802/1750 train_time:76489ms step_avg:95.37ms
step:803/1750 train_time:76589ms step_avg:95.38ms
step:804/1750 train_time:76687ms step_avg:95.38ms
step:805/1750 train_time:76785ms step_avg:95.38ms
step:806/1750 train_time:76883ms step_avg:95.39ms
step:807/1750 train_time:76981ms step_avg:95.39ms
step:808/1750 train_time:77080ms step_avg:95.40ms
step:809/1750 train_time:77179ms step_avg:95.40ms
step:810/1750 train_time:77277ms step_avg:95.40ms
step:811/1750 train_time:77375ms step_avg:95.41ms
step:812/1750 train_time:77473ms step_avg:95.41ms
step:813/1750 train_time:77572ms step_avg:95.41ms
step:814/1750 train_time:77670ms step_avg:95.42ms
step:815/1750 train_time:77768ms step_avg:95.42ms
step:816/1750 train_time:77866ms step_avg:95.42ms
step:817/1750 train_time:77965ms step_avg:95.43ms
step:818/1750 train_time:78063ms step_avg:95.43ms
step:819/1750 train_time:78160ms step_avg:95.43ms
step:820/1750 train_time:78259ms step_avg:95.44ms
step:821/1750 train_time:78358ms step_avg:95.44ms
step:822/1750 train_time:78456ms step_avg:95.45ms
step:823/1750 train_time:78555ms step_avg:95.45ms
step:824/1750 train_time:78653ms step_avg:95.45ms
step:825/1750 train_time:78751ms step_avg:95.46ms
step:826/1750 train_time:78848ms step_avg:95.46ms
step:827/1750 train_time:78946ms step_avg:95.46ms
step:828/1750 train_time:79045ms step_avg:95.46ms
step:829/1750 train_time:79142ms step_avg:95.47ms
step:830/1750 train_time:79241ms step_avg:95.47ms
step:831/1750 train_time:79340ms step_avg:95.47ms
step:832/1750 train_time:79438ms step_avg:95.48ms
step:833/1750 train_time:79537ms step_avg:95.48ms
step:834/1750 train_time:79634ms step_avg:95.48ms
step:835/1750 train_time:79733ms step_avg:95.49ms
step:836/1750 train_time:79831ms step_avg:95.49ms
step:837/1750 train_time:79929ms step_avg:95.49ms
step:838/1750 train_time:80028ms step_avg:95.50ms
step:839/1750 train_time:80126ms step_avg:95.50ms
step:840/1750 train_time:80224ms step_avg:95.50ms
step:841/1750 train_time:80321ms step_avg:95.51ms
step:842/1750 train_time:80419ms step_avg:95.51ms
step:843/1750 train_time:80518ms step_avg:95.51ms
step:844/1750 train_time:80616ms step_avg:95.52ms
step:845/1750 train_time:80714ms step_avg:95.52ms
step:846/1750 train_time:80811ms step_avg:95.52ms
step:847/1750 train_time:80910ms step_avg:95.53ms
step:848/1750 train_time:81009ms step_avg:95.53ms
step:849/1750 train_time:81107ms step_avg:95.53ms
step:850/1750 train_time:81205ms step_avg:95.53ms
step:851/1750 train_time:81302ms step_avg:95.54ms
step:852/1750 train_time:81400ms step_avg:95.54ms
step:853/1750 train_time:81498ms step_avg:95.54ms
step:854/1750 train_time:81596ms step_avg:95.55ms
step:855/1750 train_time:81695ms step_avg:95.55ms
step:856/1750 train_time:81793ms step_avg:95.55ms
step:857/1750 train_time:81891ms step_avg:95.56ms
step:858/1750 train_time:81990ms step_avg:95.56ms
step:859/1750 train_time:82087ms step_avg:95.56ms
step:860/1750 train_time:82185ms step_avg:95.56ms
step:861/1750 train_time:82283ms step_avg:95.57ms
step:862/1750 train_time:82380ms step_avg:95.57ms
step:863/1750 train_time:82479ms step_avg:95.57ms
step:864/1750 train_time:82576ms step_avg:95.57ms
step:865/1750 train_time:82674ms step_avg:95.58ms
step:866/1750 train_time:82772ms step_avg:95.58ms
step:867/1750 train_time:82871ms step_avg:95.58ms
step:868/1750 train_time:82968ms step_avg:95.59ms
step:869/1750 train_time:83066ms step_avg:95.59ms
step:870/1750 train_time:83164ms step_avg:95.59ms
step:871/1750 train_time:83263ms step_avg:95.59ms
step:872/1750 train_time:83360ms step_avg:95.60ms
step:873/1750 train_time:83458ms step_avg:95.60ms
step:874/1750 train_time:83556ms step_avg:95.60ms
step:875/1750 train_time:83654ms step_avg:95.60ms
step:875/1750 val_loss:3.5467 train_time:83740ms step_avg:95.70ms
step:876/1750 train_time:83762ms step_avg:95.62ms
step:877/1750 train_time:83858ms step_avg:95.62ms
step:878/1750 train_time:83958ms step_avg:95.62ms
step:879/1750 train_time:84057ms step_avg:95.63ms
step:880/1750 train_time:84155ms step_avg:95.63ms
step:881/1750 train_time:84252ms step_avg:95.63ms
step:882/1750 train_time:84350ms step_avg:95.63ms
step:883/1750 train_time:84447ms step_avg:95.64ms
step:884/1750 train_time:84543ms step_avg:95.64ms
step:885/1750 train_time:84640ms step_avg:95.64ms
step:886/1750 train_time:84739ms step_avg:95.64ms
step:887/1750 train_time:84839ms step_avg:95.65ms
step:888/1750 train_time:84938ms step_avg:95.65ms
step:889/1750 train_time:85039ms step_avg:95.66ms
step:890/1750 train_time:85137ms step_avg:95.66ms
step:891/1750 train_time:85235ms step_avg:95.66ms
step:892/1750 train_time:85333ms step_avg:95.67ms
step:893/1750 train_time:85431ms step_avg:95.67ms
step:894/1750 train_time:85528ms step_avg:95.67ms
step:895/1750 train_time:85625ms step_avg:95.67ms
step:896/1750 train_time:85723ms step_avg:95.67ms
step:897/1750 train_time:85821ms step_avg:95.68ms
step:898/1750 train_time:85920ms step_avg:95.68ms
step:899/1750 train_time:86018ms step_avg:95.68ms
step:900/1750 train_time:86117ms step_avg:95.69ms
step:901/1750 train_time:86215ms step_avg:95.69ms
step:902/1750 train_time:86313ms step_avg:95.69ms
step:903/1750 train_time:86411ms step_avg:95.69ms
step:904/1750 train_time:86508ms step_avg:95.69ms
step:905/1750 train_time:86605ms step_avg:95.70ms
step:906/1750 train_time:86703ms step_avg:95.70ms
step:907/1750 train_time:86801ms step_avg:95.70ms
step:908/1750 train_time:86900ms step_avg:95.70ms
step:909/1750 train_time:86999ms step_avg:95.71ms
step:910/1750 train_time:87098ms step_avg:95.71ms
step:911/1750 train_time:87199ms step_avg:95.72ms
step:912/1750 train_time:87299ms step_avg:95.72ms
step:913/1750 train_time:87398ms step_avg:95.73ms
step:914/1750 train_time:87498ms step_avg:95.73ms
step:915/1750 train_time:87598ms step_avg:95.74ms
step:916/1750 train_time:87699ms step_avg:95.74ms
step:917/1750 train_time:87799ms step_avg:95.75ms
step:918/1750 train_time:87899ms step_avg:95.75ms
step:919/1750 train_time:87998ms step_avg:95.75ms
step:920/1750 train_time:88100ms step_avg:95.76ms
step:921/1750 train_time:88199ms step_avg:95.76ms
step:922/1750 train_time:88299ms step_avg:95.77ms
step:923/1750 train_time:88399ms step_avg:95.77ms
step:924/1750 train_time:88499ms step_avg:95.78ms
step:925/1750 train_time:88599ms step_avg:95.78ms
step:926/1750 train_time:88700ms step_avg:95.79ms
step:927/1750 train_time:88799ms step_avg:95.79ms
step:928/1750 train_time:88899ms step_avg:95.80ms
step:929/1750 train_time:88998ms step_avg:95.80ms
step:930/1750 train_time:89098ms step_avg:95.80ms
step:931/1750 train_time:89198ms step_avg:95.81ms
step:932/1750 train_time:89298ms step_avg:95.81ms
step:933/1750 train_time:89398ms step_avg:95.82ms
step:934/1750 train_time:89498ms step_avg:95.82ms
step:935/1750 train_time:89599ms step_avg:95.83ms
step:936/1750 train_time:89698ms step_avg:95.83ms
step:937/1750 train_time:89799ms step_avg:95.84ms
step:938/1750 train_time:89899ms step_avg:95.84ms
step:939/1750 train_time:89999ms step_avg:95.85ms
step:940/1750 train_time:90099ms step_avg:95.85ms
step:941/1750 train_time:90200ms step_avg:95.86ms
step:942/1750 train_time:90299ms step_avg:95.86ms
step:943/1750 train_time:90399ms step_avg:95.86ms
step:944/1750 train_time:90499ms step_avg:95.87ms
step:945/1750 train_time:90599ms step_avg:95.87ms
step:946/1750 train_time:90698ms step_avg:95.88ms
step:947/1750 train_time:90798ms step_avg:95.88ms
step:948/1750 train_time:90898ms step_avg:95.88ms
step:949/1750 train_time:90997ms step_avg:95.89ms
step:950/1750 train_time:91097ms step_avg:95.89ms
step:951/1750 train_time:91197ms step_avg:95.90ms
step:952/1750 train_time:91297ms step_avg:95.90ms
step:953/1750 train_time:91397ms step_avg:95.90ms
step:954/1750 train_time:91497ms step_avg:95.91ms
step:955/1750 train_time:91598ms step_avg:95.91ms
step:956/1750 train_time:91698ms step_avg:95.92ms
step:957/1750 train_time:91799ms step_avg:95.92ms
step:958/1750 train_time:91899ms step_avg:95.93ms
step:959/1750 train_time:91999ms step_avg:95.93ms
step:960/1750 train_time:92099ms step_avg:95.94ms
step:961/1750 train_time:92199ms step_avg:95.94ms
step:962/1750 train_time:92298ms step_avg:95.94ms
step:963/1750 train_time:92398ms step_avg:95.95ms
step:964/1750 train_time:92498ms step_avg:95.95ms
step:965/1750 train_time:92599ms step_avg:95.96ms
step:966/1750 train_time:92698ms step_avg:95.96ms
step:967/1750 train_time:92798ms step_avg:95.96ms
step:968/1750 train_time:92898ms step_avg:95.97ms
step:969/1750 train_time:92998ms step_avg:95.97ms
step:970/1750 train_time:93098ms step_avg:95.98ms
step:971/1750 train_time:93198ms step_avg:95.98ms
step:972/1750 train_time:93298ms step_avg:95.99ms
step:973/1750 train_time:93397ms step_avg:95.99ms
step:974/1750 train_time:93496ms step_avg:95.99ms
step:975/1750 train_time:93596ms step_avg:96.00ms
step:976/1750 train_time:93696ms step_avg:96.00ms
step:977/1750 train_time:93796ms step_avg:96.00ms
step:978/1750 train_time:93895ms step_avg:96.01ms
step:979/1750 train_time:93996ms step_avg:96.01ms
step:980/1750 train_time:94096ms step_avg:96.02ms
step:981/1750 train_time:94196ms step_avg:96.02ms
step:982/1750 train_time:94294ms step_avg:96.02ms
step:983/1750 train_time:94394ms step_avg:96.03ms
step:984/1750 train_time:94494ms step_avg:96.03ms
step:985/1750 train_time:94593ms step_avg:96.03ms
step:986/1750 train_time:94691ms step_avg:96.04ms
step:987/1750 train_time:94791ms step_avg:96.04ms
step:988/1750 train_time:94891ms step_avg:96.04ms
step:989/1750 train_time:94990ms step_avg:96.05ms
step:990/1750 train_time:95090ms step_avg:96.05ms
step:991/1750 train_time:95190ms step_avg:96.05ms
step:992/1750 train_time:95290ms step_avg:96.06ms
step:993/1750 train_time:95389ms step_avg:96.06ms
step:994/1750 train_time:95487ms step_avg:96.06ms
step:995/1750 train_time:95587ms step_avg:96.07ms
step:996/1750 train_time:95688ms step_avg:96.07ms
step:997/1750 train_time:95787ms step_avg:96.08ms
step:998/1750 train_time:95885ms step_avg:96.08ms
step:999/1750 train_time:95984ms step_avg:96.08ms
step:1000/1750 train_time:96084ms step_avg:96.08ms
step:1000/1750 val_loss:3.5067 train_time:96172ms step_avg:96.17ms
step:1001/1750 train_time:96194ms step_avg:96.10ms
step:1002/1750 train_time:96294ms step_avg:96.10ms
step:1003/1750 train_time:96396ms step_avg:96.11ms
step:1004/1750 train_time:96495ms step_avg:96.11ms
step:1005/1750 train_time:96593ms step_avg:96.11ms
step:1006/1750 train_time:96692ms step_avg:96.12ms
step:1007/1750 train_time:96790ms step_avg:96.12ms
step:1008/1750 train_time:96889ms step_avg:96.12ms
step:1009/1750 train_time:96988ms step_avg:96.12ms
step:1010/1750 train_time:97086ms step_avg:96.13ms
step:1011/1750 train_time:97189ms step_avg:96.13ms
step:1012/1750 train_time:97292ms step_avg:96.14ms
step:1013/1750 train_time:97392ms step_avg:96.14ms
step:1014/1750 train_time:97491ms step_avg:96.15ms
step:1015/1750 train_time:97591ms step_avg:96.15ms
step:1016/1750 train_time:97689ms step_avg:96.15ms
step:1017/1750 train_time:97788ms step_avg:96.15ms
step:1018/1750 train_time:97886ms step_avg:96.15ms
step:1019/1750 train_time:97984ms step_avg:96.16ms
step:1020/1750 train_time:98083ms step_avg:96.16ms
step:1021/1750 train_time:98183ms step_avg:96.16ms
step:1022/1750 train_time:98283ms step_avg:96.17ms
step:1023/1750 train_time:98384ms step_avg:96.17ms
step:1024/1750 train_time:98484ms step_avg:96.18ms
step:1025/1750 train_time:98584ms step_avg:96.18ms
step:1026/1750 train_time:98683ms step_avg:96.18ms
step:1027/1750 train_time:98782ms step_avg:96.19ms
step:1028/1750 train_time:98882ms step_avg:96.19ms
step:1029/1750 train_time:98983ms step_avg:96.19ms
step:1030/1750 train_time:99082ms step_avg:96.20ms
step:1031/1750 train_time:99181ms step_avg:96.20ms
step:1032/1750 train_time:99282ms step_avg:96.20ms
step:1033/1750 train_time:99381ms step_avg:96.21ms
step:1034/1750 train_time:99481ms step_avg:96.21ms
step:1035/1750 train_time:99580ms step_avg:96.21ms
step:1036/1750 train_time:99679ms step_avg:96.22ms
step:1037/1750 train_time:99779ms step_avg:96.22ms
step:1038/1750 train_time:99878ms step_avg:96.22ms
step:1039/1750 train_time:99978ms step_avg:96.23ms
step:1040/1750 train_time:100079ms step_avg:96.23ms
step:1041/1750 train_time:100179ms step_avg:96.23ms
step:1042/1750 train_time:100279ms step_avg:96.24ms
step:1043/1750 train_time:100380ms step_avg:96.24ms
step:1044/1750 train_time:100480ms step_avg:96.25ms
step:1045/1750 train_time:100580ms step_avg:96.25ms
step:1046/1750 train_time:100681ms step_avg:96.25ms
step:1047/1750 train_time:100780ms step_avg:96.26ms
step:1048/1750 train_time:100880ms step_avg:96.26ms
step:1049/1750 train_time:100979ms step_avg:96.26ms
step:1050/1750 train_time:101080ms step_avg:96.27ms
step:1051/1750 train_time:101180ms step_avg:96.27ms
step:1052/1750 train_time:101280ms step_avg:96.27ms
step:1053/1750 train_time:101380ms step_avg:96.28ms
step:1054/1750 train_time:101480ms step_avg:96.28ms
step:1055/1750 train_time:101580ms step_avg:96.28ms
step:1056/1750 train_time:101680ms step_avg:96.29ms
step:1057/1750 train_time:101780ms step_avg:96.29ms
step:1058/1750 train_time:101880ms step_avg:96.29ms
step:1059/1750 train_time:101980ms step_avg:96.30ms
step:1060/1750 train_time:102080ms step_avg:96.30ms
step:1061/1750 train_time:102181ms step_avg:96.31ms
step:1062/1750 train_time:102281ms step_avg:96.31ms
step:1063/1750 train_time:102381ms step_avg:96.31ms
step:1064/1750 train_time:102481ms step_avg:96.32ms
step:1065/1750 train_time:102581ms step_avg:96.32ms
step:1066/1750 train_time:102681ms step_avg:96.32ms
step:1067/1750 train_time:102781ms step_avg:96.33ms
step:1068/1750 train_time:102880ms step_avg:96.33ms
step:1069/1750 train_time:103237ms step_avg:96.57ms
step:1070/1750 train_time:103335ms step_avg:96.57ms
step:1071/1750 train_time:103432ms step_avg:96.58ms
step:1072/1750 train_time:103532ms step_avg:96.58ms
step:1073/1750 train_time:103629ms step_avg:96.58ms
step:1074/1750 train_time:103728ms step_avg:96.58ms
step:1075/1750 train_time:103826ms step_avg:96.58ms
step:1076/1750 train_time:103925ms step_avg:96.58ms
step:1077/1750 train_time:104023ms step_avg:96.59ms
step:1078/1750 train_time:104126ms step_avg:96.59ms
step:1079/1750 train_time:104232ms step_avg:96.60ms
step:1080/1750 train_time:104331ms step_avg:96.60ms
step:1081/1750 train_time:104430ms step_avg:96.61ms
step:1082/1750 train_time:104529ms step_avg:96.61ms
step:1083/1750 train_time:104627ms step_avg:96.61ms
step:1084/1750 train_time:104726ms step_avg:96.61ms
step:1085/1750 train_time:104824ms step_avg:96.61ms
step:1086/1750 train_time:104923ms step_avg:96.61ms
step:1087/1750 train_time:105022ms step_avg:96.62ms
step:1088/1750 train_time:105123ms step_avg:96.62ms
step:1089/1750 train_time:105224ms step_avg:96.62ms
step:1090/1750 train_time:105325ms step_avg:96.63ms
step:1091/1750 train_time:105426ms step_avg:96.63ms
step:1092/1750 train_time:105526ms step_avg:96.64ms
step:1093/1750 train_time:105625ms step_avg:96.64ms
step:1094/1750 train_time:105724ms step_avg:96.64ms
step:1095/1750 train_time:105823ms step_avg:96.64ms
step:1096/1750 train_time:105922ms step_avg:96.64ms
step:1097/1750 train_time:106021ms step_avg:96.65ms
step:1098/1750 train_time:106122ms step_avg:96.65ms
step:1099/1750 train_time:106223ms step_avg:96.65ms
step:1100/1750 train_time:106323ms step_avg:96.66ms
step:1101/1750 train_time:106423ms step_avg:96.66ms
step:1102/1750 train_time:106522ms step_avg:96.66ms
step:1103/1750 train_time:106622ms step_avg:96.67ms
step:1104/1750 train_time:107122ms step_avg:97.03ms
step:1105/1750 train_time:107184ms step_avg:97.00ms
step:1106/1750 train_time:107282ms step_avg:97.00ms
step:1107/1750 train_time:107380ms step_avg:97.00ms
step:1108/1750 train_time:107479ms step_avg:97.00ms
step:1109/1750 train_time:107578ms step_avg:97.00ms
step:1110/1750 train_time:107677ms step_avg:97.01ms
step:1111/1750 train_time:107776ms step_avg:97.01ms
step:1112/1750 train_time:107875ms step_avg:97.01ms
step:1113/1750 train_time:108275ms step_avg:97.28ms
step:1114/1750 train_time:108373ms step_avg:97.28ms
step:1115/1750 train_time:108471ms step_avg:97.28ms
step:1116/1750 train_time:108569ms step_avg:97.28ms
step:1117/1750 train_time:108667ms step_avg:97.29ms
step:1118/1750 train_time:108765ms step_avg:97.29ms
step:1119/1750 train_time:108863ms step_avg:97.29ms
step:1120/1750 train_time:108961ms step_avg:97.29ms
step:1121/1750 train_time:109060ms step_avg:97.29ms
step:1122/1750 train_time:109164ms step_avg:97.29ms
step:1123/1750 train_time:109267ms step_avg:97.30ms
step:1124/1750 train_time:109366ms step_avg:97.30ms
step:1125/1750 train_time:109466ms step_avg:97.30ms
step:1125/1750 val_loss:3.4554 train_time:109554ms step_avg:97.38ms
step:1126/1750 train_time:109576ms step_avg:97.31ms
step:1127/1750 train_time:109673ms step_avg:97.31ms
step:1128/1750 train_time:109774ms step_avg:97.32ms
step:1129/1750 train_time:109875ms step_avg:97.32ms
step:1130/1750 train_time:109974ms step_avg:97.32ms
step:1131/1750 train_time:110073ms step_avg:97.32ms
step:1132/1750 train_time:110172ms step_avg:97.33ms
step:1133/1750 train_time:110271ms step_avg:97.33ms
step:1134/1750 train_time:110370ms step_avg:97.33ms
step:1135/1750 train_time:110469ms step_avg:97.33ms
step:1136/1750 train_time:110571ms step_avg:97.33ms
step:1137/1750 train_time:110672ms step_avg:97.34ms
step:1138/1750 train_time:110774ms step_avg:97.34ms
step:1139/1750 train_time:110874ms step_avg:97.34ms
step:1140/1750 train_time:111278ms step_avg:97.61ms
step:1141/1750 train_time:111376ms step_avg:97.61ms
step:1142/1750 train_time:111474ms step_avg:97.61ms
step:1143/1750 train_time:111573ms step_avg:97.61ms
step:1144/1750 train_time:111672ms step_avg:97.62ms
step:1145/1750 train_time:111771ms step_avg:97.62ms
step:1146/1750 train_time:111869ms step_avg:97.62ms
step:1147/1750 train_time:111967ms step_avg:97.62ms
step:1148/1750 train_time:112066ms step_avg:97.62ms
step:1149/1750 train_time:112169ms step_avg:97.62ms
step:1150/1750 train_time:112275ms step_avg:97.63ms
step:1151/1750 train_time:112375ms step_avg:97.63ms
step:1152/1750 train_time:112474ms step_avg:97.63ms
step:1153/1750 train_time:112573ms step_avg:97.63ms
step:1154/1750 train_time:112671ms step_avg:97.64ms
step:1155/1750 train_time:112770ms step_avg:97.64ms
step:1156/1750 train_time:112869ms step_avg:97.64ms
step:1157/1750 train_time:112967ms step_avg:97.64ms
step:1158/1750 train_time:113328ms step_avg:97.87ms
step:1159/1750 train_time:113426ms step_avg:97.87ms
step:1160/1750 train_time:113524ms step_avg:97.87ms
step:1161/1750 train_time:113623ms step_avg:97.87ms
step:1162/1750 train_time:113721ms step_avg:97.87ms
step:1163/1750 train_time:113820ms step_avg:97.87ms
step:1164/1750 train_time:113919ms step_avg:97.87ms
step:1165/1750 train_time:114018ms step_avg:97.87ms
step:1166/1750 train_time:114116ms step_avg:97.87ms
step:1167/1750 train_time:114218ms step_avg:97.87ms
step:1168/1750 train_time:114321ms step_avg:97.88ms
step:1169/1750 train_time:114422ms step_avg:97.88ms
step:1170/1750 train_time:114522ms step_avg:97.88ms
step:1171/1750 train_time:114622ms step_avg:97.88ms
step:1172/1750 train_time:114722ms step_avg:97.89ms
step:1173/1750 train_time:114822ms step_avg:97.89ms
step:1174/1750 train_time:114921ms step_avg:97.89ms
step:1175/1750 train_time:115021ms step_avg:97.89ms
step:1176/1750 train_time:115378ms step_avg:98.11ms
step:1177/1750 train_time:115476ms step_avg:98.11ms
step:1178/1750 train_time:115575ms step_avg:98.11ms
step:1179/1750 train_time:115677ms step_avg:98.11ms
step:1180/1750 train_time:115776ms step_avg:98.12ms
step:1181/1750 train_time:115876ms step_avg:98.12ms
step:1182/1750 train_time:115977ms step_avg:98.12ms
step:1183/1750 train_time:116076ms step_avg:98.12ms
step:1184/1750 train_time:116177ms step_avg:98.12ms
step:1185/1750 train_time:116562ms step_avg:98.36ms
step:1186/1750 train_time:116662ms step_avg:98.37ms
step:1187/1750 train_time:116760ms step_avg:98.37ms
step:1188/1750 train_time:116859ms step_avg:98.37ms
step:1189/1750 train_time:116958ms step_avg:98.37ms
step:1190/1750 train_time:117057ms step_avg:98.37ms
step:1191/1750 train_time:117156ms step_avg:98.37ms
step:1192/1750 train_time:117256ms step_avg:98.37ms
step:1193/1750 train_time:117355ms step_avg:98.37ms
step:1194/1750 train_time:117460ms step_avg:98.37ms
step:1195/1750 train_time:117568ms step_avg:98.38ms
step:1196/1750 train_time:117669ms step_avg:98.39ms
step:1197/1750 train_time:117770ms step_avg:98.39ms
step:1198/1750 train_time:117870ms step_avg:98.39ms
step:1199/1750 train_time:117971ms step_avg:98.39ms
step:1200/1750 train_time:118071ms step_avg:98.39ms
step:1201/1750 train_time:118171ms step_avg:98.39ms
step:1202/1750 train_time:118271ms step_avg:98.40ms
step:1203/1750 train_time:118373ms step_avg:98.40ms
step:1204/1750 train_time:118476ms step_avg:98.40ms
step:1205/1750 train_time:118579ms step_avg:98.41ms
step:1206/1750 train_time:118680ms step_avg:98.41ms
step:1207/1750 train_time:118781ms step_avg:98.41ms
step:1208/1750 train_time:118882ms step_avg:98.41ms
step:1209/1750 train_time:118983ms step_avg:98.41ms
step:1210/1750 train_time:119082ms step_avg:98.42ms
step:1211/1750 train_time:119490ms step_avg:98.67ms
step:1212/1750 train_time:119588ms step_avg:98.67ms
step:1213/1750 train_time:119688ms step_avg:98.67ms
step:1214/1750 train_time:119787ms step_avg:98.67ms
step:1215/1750 train_time:119886ms step_avg:98.67ms
step:1216/1750 train_time:119987ms step_avg:98.67ms
step:1217/1750 train_time:120086ms step_avg:98.67ms
step:1218/1750 train_time:120186ms step_avg:98.67ms
step:1219/1750 train_time:120285ms step_avg:98.68ms
step:1220/1750 train_time:120388ms step_avg:98.68ms
step:1221/1750 train_time:120495ms step_avg:98.69ms
step:1222/1750 train_time:120596ms step_avg:98.69ms
step:1223/1750 train_time:120696ms step_avg:98.69ms
step:1224/1750 train_time:120797ms step_avg:98.69ms
step:1225/1750 train_time:120898ms step_avg:98.69ms
step:1226/1750 train_time:120999ms step_avg:98.69ms
step:1227/1750 train_time:121100ms step_avg:98.70ms
step:1228/1750 train_time:121200ms step_avg:98.70ms
step:1229/1750 train_time:121300ms step_avg:98.70ms
step:1230/1750 train_time:121401ms step_avg:98.70ms
step:1231/1750 train_time:121503ms step_avg:98.70ms
step:1232/1750 train_time:121603ms step_avg:98.70ms
step:1233/1750 train_time:121703ms step_avg:98.70ms
step:1234/1750 train_time:121804ms step_avg:98.71ms
step:1235/1750 train_time:121905ms step_avg:98.71ms
step:1236/1750 train_time:122006ms step_avg:98.71ms
step:1237/1750 train_time:122106ms step_avg:98.71ms
step:1238/1750 train_time:122206ms step_avg:98.71ms
step:1239/1750 train_time:122307ms step_avg:98.71ms
step:1240/1750 train_time:122408ms step_avg:98.72ms
step:1241/1750 train_time:122509ms step_avg:98.72ms
step:1242/1750 train_time:122612ms step_avg:98.72ms
step:1243/1750 train_time:122712ms step_avg:98.72ms
step:1244/1750 train_time:122813ms step_avg:98.72ms
step:1245/1750 train_time:122914ms step_avg:98.73ms
step:1246/1750 train_time:123015ms step_avg:98.73ms
step:1247/1750 train_time:123116ms step_avg:98.73ms
step:1248/1750 train_time:123217ms step_avg:98.73ms
step:1249/1750 train_time:123316ms step_avg:98.73ms
step:1250/1750 train_time:123417ms step_avg:98.73ms
step:1250/1750 val_loss:3.4106 train_time:123508ms step_avg:98.81ms
step:1251/1750 train_time:123529ms step_avg:98.74ms
step:1252/1750 train_time:123628ms step_avg:98.74ms
step:1253/1750 train_time:123730ms step_avg:98.75ms
step:1254/1750 train_time:123831ms step_avg:98.75ms
step:1255/1750 train_time:123931ms step_avg:98.75ms
step:1256/1750 train_time:124030ms step_avg:98.75ms
step:1257/1750 train_time:124129ms step_avg:98.75ms
step:1258/1750 train_time:124229ms step_avg:98.75ms
step:1259/1750 train_time:124329ms step_avg:98.75ms
step:1260/1750 train_time:124428ms step_avg:98.75ms
step:1261/1750 train_time:124530ms step_avg:98.75ms
step:1262/1750 train_time:124633ms step_avg:98.76ms
step:1263/1750 train_time:124735ms step_avg:98.76ms
step:1264/1750 train_time:124836ms step_avg:98.76ms
step:1265/1750 train_time:124938ms step_avg:98.77ms
step:1266/1750 train_time:125038ms step_avg:98.77ms
step:1267/1750 train_time:125138ms step_avg:98.77ms
step:1268/1750 train_time:125238ms step_avg:98.77ms
step:1269/1750 train_time:125338ms step_avg:98.77ms
step:1270/1750 train_time:125439ms step_avg:98.77ms
step:1271/1750 train_time:125540ms step_avg:98.77ms
step:1272/1750 train_time:125641ms step_avg:98.77ms
step:1273/1750 train_time:125743ms step_avg:98.78ms
step:1274/1750 train_time:125844ms step_avg:98.78ms
step:1275/1750 train_time:125945ms step_avg:98.78ms
step:1276/1750 train_time:126046ms step_avg:98.78ms
step:1277/1750 train_time:126146ms step_avg:98.78ms
step:1278/1750 train_time:126247ms step_avg:98.79ms
step:1279/1750 train_time:126348ms step_avg:98.79ms
step:1280/1750 train_time:126448ms step_avg:98.79ms
step:1281/1750 train_time:126549ms step_avg:98.79ms
step:1282/1750 train_time:126650ms step_avg:98.79ms
step:1283/1750 train_time:126751ms step_avg:98.79ms
step:1284/1750 train_time:126853ms step_avg:98.80ms
step:1285/1750 train_time:126954ms step_avg:98.80ms
step:1286/1750 train_time:127055ms step_avg:98.80ms
step:1287/1750 train_time:127157ms step_avg:98.80ms
step:1288/1750 train_time:127257ms step_avg:98.80ms
step:1289/1750 train_time:127358ms step_avg:98.80ms
step:1290/1750 train_time:127458ms step_avg:98.80ms
step:1291/1750 train_time:127559ms step_avg:98.81ms
step:1292/1750 train_time:127660ms step_avg:98.81ms
step:1293/1750 train_time:127761ms step_avg:98.81ms
step:1294/1750 train_time:127864ms step_avg:98.81ms
step:1295/1750 train_time:127965ms step_avg:98.81ms
step:1296/1750 train_time:128066ms step_avg:98.82ms
step:1297/1750 train_time:128167ms step_avg:98.82ms
step:1298/1750 train_time:128268ms step_avg:98.82ms
step:1299/1750 train_time:128368ms step_avg:98.82ms
step:1300/1750 train_time:128469ms step_avg:98.82ms
step:1301/1750 train_time:128570ms step_avg:98.82ms
step:1302/1750 train_time:128670ms step_avg:98.82ms
step:1303/1750 train_time:128771ms step_avg:98.83ms
step:1304/1750 train_time:128873ms step_avg:98.83ms
step:1305/1750 train_time:128975ms step_avg:98.83ms
step:1306/1750 train_time:129076ms step_avg:98.83ms
step:1307/1750 train_time:129177ms step_avg:98.84ms
step:1308/1750 train_time:129279ms step_avg:98.84ms
step:1309/1750 train_time:129379ms step_avg:98.84ms
step:1310/1750 train_time:129481ms step_avg:98.84ms
step:1311/1750 train_time:129582ms step_avg:98.84ms
step:1312/1750 train_time:129684ms step_avg:98.84ms
step:1313/1750 train_time:129786ms step_avg:98.85ms
step:1314/1750 train_time:129886ms step_avg:98.85ms
step:1315/1750 train_time:129987ms step_avg:98.85ms
step:1316/1750 train_time:130087ms step_avg:98.85ms
step:1317/1750 train_time:130187ms step_avg:98.85ms
step:1318/1750 train_time:130288ms step_avg:98.85ms
step:1319/1750 train_time:130388ms step_avg:98.85ms
step:1320/1750 train_time:130490ms step_avg:98.86ms
step:1321/1750 train_time:130592ms step_avg:98.86ms
step:1322/1750 train_time:130694ms step_avg:98.86ms
step:1323/1750 train_time:130795ms step_avg:98.86ms
step:1324/1750 train_time:130896ms step_avg:98.86ms
step:1325/1750 train_time:130998ms step_avg:98.87ms
step:1326/1750 train_time:131100ms step_avg:98.87ms
step:1327/1750 train_time:131201ms step_avg:98.87ms
step:1328/1750 train_time:131301ms step_avg:98.87ms
step:1329/1750 train_time:131402ms step_avg:98.87ms
step:1330/1750 train_time:131503ms step_avg:98.87ms
step:1331/1750 train_time:131605ms step_avg:98.88ms
step:1332/1750 train_time:131707ms step_avg:98.88ms
step:1333/1750 train_time:131808ms step_avg:98.88ms
step:1334/1750 train_time:131908ms step_avg:98.88ms
step:1335/1750 train_time:132010ms step_avg:98.88ms
step:1336/1750 train_time:132110ms step_avg:98.89ms
step:1337/1750 train_time:132211ms step_avg:98.89ms
step:1338/1750 train_time:132311ms step_avg:98.89ms
step:1339/1750 train_time:132413ms step_avg:98.89ms
step:1340/1750 train_time:132514ms step_avg:98.89ms
step:1341/1750 train_time:132616ms step_avg:98.89ms
step:1342/1750 train_time:132718ms step_avg:98.90ms
step:1343/1750 train_time:132819ms step_avg:98.90ms
step:1344/1750 train_time:132920ms step_avg:98.90ms
step:1345/1750 train_time:133021ms step_avg:98.90ms
step:1346/1750 train_time:133123ms step_avg:98.90ms
step:1347/1750 train_time:133223ms step_avg:98.90ms
step:1348/1750 train_time:133326ms step_avg:98.91ms
step:1349/1750 train_time:133426ms step_avg:98.91ms
step:1350/1750 train_time:133527ms step_avg:98.91ms
step:1351/1750 train_time:133628ms step_avg:98.91ms
step:1352/1750 train_time:133729ms step_avg:98.91ms
step:1353/1750 train_time:133830ms step_avg:98.91ms
step:1354/1750 train_time:133929ms step_avg:98.91ms
step:1355/1750 train_time:134030ms step_avg:98.92ms
step:1356/1750 train_time:134131ms step_avg:98.92ms
step:1357/1750 train_time:134232ms step_avg:98.92ms
step:1358/1750 train_time:134333ms step_avg:98.92ms
step:1359/1750 train_time:134435ms step_avg:98.92ms
step:1360/1750 train_time:134536ms step_avg:98.92ms
step:1361/1750 train_time:134637ms step_avg:98.93ms
step:1362/1750 train_time:134738ms step_avg:98.93ms
step:1363/1750 train_time:134839ms step_avg:98.93ms
step:1364/1750 train_time:134941ms step_avg:98.93ms
step:1365/1750 train_time:135042ms step_avg:98.93ms
step:1366/1750 train_time:135143ms step_avg:98.93ms
step:1367/1750 train_time:135244ms step_avg:98.94ms
step:1368/1750 train_time:135347ms step_avg:98.94ms
step:1369/1750 train_time:135446ms step_avg:98.94ms
step:1370/1750 train_time:135547ms step_avg:98.94ms
step:1371/1750 train_time:135648ms step_avg:98.94ms
step:1372/1750 train_time:135749ms step_avg:98.94ms
step:1373/1750 train_time:135849ms step_avg:98.94ms
step:1374/1750 train_time:135949ms step_avg:98.94ms
step:1375/1750 train_time:136050ms step_avg:98.95ms
step:1375/1750 val_loss:3.3698 train_time:136139ms step_avg:99.01ms
step:1376/1750 train_time:136161ms step_avg:98.95ms
step:1377/1750 train_time:136263ms step_avg:98.96ms
step:1378/1750 train_time:136365ms step_avg:98.96ms
step:1379/1750 train_time:136466ms step_avg:98.96ms
step:1380/1750 train_time:136568ms step_avg:98.96ms
step:1381/1750 train_time:136667ms step_avg:98.96ms
step:1382/1750 train_time:136767ms step_avg:98.96ms
step:1383/1750 train_time:136867ms step_avg:98.96ms
step:1384/1750 train_time:136967ms step_avg:98.96ms
step:1385/1750 train_time:137069ms step_avg:98.97ms
step:1386/1750 train_time:137172ms step_avg:98.97ms
step:1387/1750 train_time:137275ms step_avg:98.97ms
step:1388/1750 train_time:137376ms step_avg:98.97ms
step:1389/1750 train_time:137476ms step_avg:98.98ms
step:1390/1750 train_time:137577ms step_avg:98.98ms
step:1391/1750 train_time:137677ms step_avg:98.98ms
step:1392/1750 train_time:137779ms step_avg:98.98ms
step:1393/1750 train_time:137880ms step_avg:98.98ms
step:1394/1750 train_time:137981ms step_avg:98.98ms
step:1395/1750 train_time:138083ms step_avg:98.98ms
step:1396/1750 train_time:138184ms step_avg:98.99ms
step:1397/1750 train_time:138287ms step_avg:98.99ms
step:1398/1750 train_time:138389ms step_avg:98.99ms
step:1399/1750 train_time:138490ms step_avg:98.99ms
step:1400/1750 train_time:138592ms step_avg:98.99ms
step:1401/1750 train_time:138693ms step_avg:99.00ms
step:1402/1750 train_time:138794ms step_avg:99.00ms
step:1403/1750 train_time:138895ms step_avg:99.00ms
step:1404/1750 train_time:138996ms step_avg:99.00ms
step:1405/1750 train_time:139097ms step_avg:99.00ms
step:1406/1750 train_time:139198ms step_avg:99.00ms
step:1407/1750 train_time:139300ms step_avg:99.01ms
step:1408/1750 train_time:139402ms step_avg:99.01ms
step:1409/1750 train_time:139504ms step_avg:99.01ms
step:1410/1750 train_time:139605ms step_avg:99.01ms
step:1411/1750 train_time:139707ms step_avg:99.01ms
step:1412/1750 train_time:139809ms step_avg:99.01ms
step:1413/1750 train_time:139909ms step_avg:99.02ms
step:1414/1750 train_time:140010ms step_avg:99.02ms
step:1415/1750 train_time:140111ms step_avg:99.02ms
step:1416/1750 train_time:140211ms step_avg:99.02ms
step:1417/1750 train_time:140313ms step_avg:99.02ms
step:1418/1750 train_time:140413ms step_avg:99.02ms
step:1419/1750 train_time:140514ms step_avg:99.02ms
step:1420/1750 train_time:140614ms step_avg:99.02ms
step:1421/1750 train_time:140715ms step_avg:99.03ms
step:1422/1750 train_time:140815ms step_avg:99.03ms
step:1423/1750 train_time:140916ms step_avg:99.03ms
step:1424/1750 train_time:141019ms step_avg:99.03ms
step:1425/1750 train_time:141120ms step_avg:99.03ms
step:1426/1750 train_time:141222ms step_avg:99.03ms
step:1427/1750 train_time:141323ms step_avg:99.04ms
step:1428/1750 train_time:141425ms step_avg:99.04ms
step:1429/1750 train_time:141527ms step_avg:99.04ms
step:1430/1750 train_time:141629ms step_avg:99.04ms
step:1431/1750 train_time:141731ms step_avg:99.04ms
step:1432/1750 train_time:141832ms step_avg:99.04ms
step:1433/1750 train_time:141935ms step_avg:99.05ms
step:1434/1750 train_time:142036ms step_avg:99.05ms
step:1435/1750 train_time:142138ms step_avg:99.05ms
step:1436/1750 train_time:142240ms step_avg:99.05ms
step:1437/1750 train_time:142343ms step_avg:99.06ms
step:1438/1750 train_time:142444ms step_avg:99.06ms
step:1439/1750 train_time:142548ms step_avg:99.06ms
step:1440/1750 train_time:142650ms step_avg:99.06ms
step:1441/1750 train_time:142752ms step_avg:99.06ms
step:1442/1750 train_time:142853ms step_avg:99.07ms
step:1443/1750 train_time:142955ms step_avg:99.07ms
step:1444/1750 train_time:143056ms step_avg:99.07ms
step:1445/1750 train_time:143158ms step_avg:99.07ms
step:1446/1750 train_time:143259ms step_avg:99.07ms
step:1447/1750 train_time:143360ms step_avg:99.07ms
step:1448/1750 train_time:143465ms step_avg:99.08ms
step:1449/1750 train_time:143567ms step_avg:99.08ms
step:1450/1750 train_time:143669ms step_avg:99.08ms
step:1451/1750 train_time:143771ms step_avg:99.08ms
step:1452/1750 train_time:143872ms step_avg:99.09ms
step:1453/1750 train_time:143975ms step_avg:99.09ms
step:1454/1750 train_time:144078ms step_avg:99.09ms
step:1455/1750 train_time:144178ms step_avg:99.09ms
step:1456/1750 train_time:144279ms step_avg:99.09ms
step:1457/1750 train_time:144381ms step_avg:99.10ms
step:1458/1750 train_time:144485ms step_avg:99.10ms
step:1459/1750 train_time:144588ms step_avg:99.10ms
step:1460/1750 train_time:144689ms step_avg:99.10ms
step:1461/1750 train_time:144792ms step_avg:99.10ms
step:1462/1750 train_time:144894ms step_avg:99.11ms
step:1463/1750 train_time:144995ms step_avg:99.11ms
step:1464/1750 train_time:145096ms step_avg:99.11ms
step:1465/1750 train_time:145198ms step_avg:99.11ms
step:1466/1750 train_time:145298ms step_avg:99.11ms
step:1467/1750 train_time:145400ms step_avg:99.11ms
step:1468/1750 train_time:145503ms step_avg:99.12ms
step:1469/1750 train_time:145605ms step_avg:99.12ms
step:1470/1750 train_time:145707ms step_avg:99.12ms
step:1471/1750 train_time:145811ms step_avg:99.12ms
step:1472/1750 train_time:145913ms step_avg:99.13ms
step:1473/1750 train_time:146015ms step_avg:99.13ms
step:1474/1750 train_time:146115ms step_avg:99.13ms
step:1475/1750 train_time:146216ms step_avg:99.13ms
step:1476/1750 train_time:146317ms step_avg:99.13ms
step:1477/1750 train_time:146420ms step_avg:99.13ms
step:1478/1750 train_time:146522ms step_avg:99.14ms
step:1479/1750 train_time:146624ms step_avg:99.14ms
step:1480/1750 train_time:146727ms step_avg:99.14ms
step:1481/1750 train_time:146828ms step_avg:99.14ms
step:1482/1750 train_time:146930ms step_avg:99.14ms
step:1483/1750 train_time:147032ms step_avg:99.14ms
step:1484/1750 train_time:147135ms step_avg:99.15ms
step:1485/1750 train_time:147238ms step_avg:99.15ms
step:1486/1750 train_time:147339ms step_avg:99.15ms
step:1487/1750 train_time:147440ms step_avg:99.15ms
step:1488/1750 train_time:147543ms step_avg:99.16ms
step:1489/1750 train_time:147645ms step_avg:99.16ms
step:1490/1750 train_time:147749ms step_avg:99.16ms
step:1491/1750 train_time:147850ms step_avg:99.16ms
step:1492/1750 train_time:147951ms step_avg:99.16ms
step:1493/1750 train_time:148053ms step_avg:99.16ms
step:1494/1750 train_time:148155ms step_avg:99.17ms
step:1495/1750 train_time:148255ms step_avg:99.17ms
step:1496/1750 train_time:148357ms step_avg:99.17ms
step:1497/1750 train_time:148458ms step_avg:99.17ms
step:1498/1750 train_time:148560ms step_avg:99.17ms
step:1499/1750 train_time:148662ms step_avg:99.17ms
step:1500/1750 train_time:148764ms step_avg:99.18ms
step:1500/1750 val_loss:3.3350 train_time:148855ms step_avg:99.24ms
step:1501/1750 train_time:148876ms step_avg:99.18ms
step:1502/1750 train_time:148980ms step_avg:99.19ms
step:1503/1750 train_time:149080ms step_avg:99.19ms
step:1504/1750 train_time:149180ms step_avg:99.19ms
step:1505/1750 train_time:149280ms step_avg:99.19ms
step:1506/1750 train_time:149380ms step_avg:99.19ms
step:1507/1750 train_time:149481ms step_avg:99.19ms
step:1508/1750 train_time:149581ms step_avg:99.19ms
step:1509/1750 train_time:149682ms step_avg:99.19ms
step:1510/1750 train_time:149784ms step_avg:99.19ms
step:1511/1750 train_time:149891ms step_avg:99.20ms
step:1512/1750 train_time:149994ms step_avg:99.20ms
step:1513/1750 train_time:150096ms step_avg:99.20ms
step:1514/1750 train_time:150197ms step_avg:99.21ms
step:1515/1750 train_time:150302ms step_avg:99.21ms
step:1516/1750 train_time:150402ms step_avg:99.21ms
step:1517/1750 train_time:150502ms step_avg:99.21ms
step:1518/1750 train_time:150602ms step_avg:99.21ms
step:1519/1750 train_time:150705ms step_avg:99.21ms
step:1520/1750 train_time:150806ms step_avg:99.21ms
step:1521/1750 train_time:150909ms step_avg:99.22ms
step:1522/1750 train_time:151013ms step_avg:99.22ms
step:1523/1750 train_time:151116ms step_avg:99.22ms
step:1524/1750 train_time:151220ms step_avg:99.23ms
step:1525/1750 train_time:151322ms step_avg:99.23ms
step:1526/1750 train_time:151423ms step_avg:99.23ms
step:1527/1750 train_time:151524ms step_avg:99.23ms
step:1528/1750 train_time:151628ms step_avg:99.23ms
step:1529/1750 train_time:151730ms step_avg:99.23ms
step:1530/1750 train_time:151834ms step_avg:99.24ms
step:1531/1750 train_time:151935ms step_avg:99.24ms
step:1532/1750 train_time:152038ms step_avg:99.24ms
step:1533/1750 train_time:152140ms step_avg:99.24ms
step:1534/1750 train_time:152241ms step_avg:99.24ms
step:1535/1750 train_time:152343ms step_avg:99.25ms
step:1536/1750 train_time:152444ms step_avg:99.25ms
step:1537/1750 train_time:152545ms step_avg:99.25ms
step:1538/1750 train_time:152646ms step_avg:99.25ms
step:1539/1750 train_time:152749ms step_avg:99.25ms
step:1540/1750 train_time:152852ms step_avg:99.25ms
step:1541/1750 train_time:152956ms step_avg:99.26ms
step:1542/1750 train_time:153059ms step_avg:99.26ms
step:1543/1750 train_time:153161ms step_avg:99.26ms
step:1544/1750 train_time:153263ms step_avg:99.26ms
step:1545/1750 train_time:153364ms step_avg:99.26ms
step:1546/1750 train_time:153465ms step_avg:99.27ms
step:1547/1750 train_time:153567ms step_avg:99.27ms
step:1548/1750 train_time:153669ms step_avg:99.27ms
step:1549/1750 train_time:153771ms step_avg:99.27ms
step:1550/1750 train_time:153873ms step_avg:99.27ms
step:1551/1750 train_time:153976ms step_avg:99.28ms
step:1552/1750 train_time:154079ms step_avg:99.28ms
step:1553/1750 train_time:154182ms step_avg:99.28ms
step:1554/1750 train_time:154283ms step_avg:99.28ms
step:1555/1750 train_time:154384ms step_avg:99.28ms
step:1556/1750 train_time:154486ms step_avg:99.28ms
step:1557/1750 train_time:154588ms step_avg:99.29ms
step:1558/1750 train_time:154691ms step_avg:99.29ms
step:1559/1750 train_time:154795ms step_avg:99.29ms
step:1560/1750 train_time:154896ms step_avg:99.29ms
step:1561/1750 train_time:154998ms step_avg:99.29ms
step:1562/1750 train_time:155101ms step_avg:99.30ms
step:1563/1750 train_time:155205ms step_avg:99.30ms
step:1564/1750 train_time:155307ms step_avg:99.30ms
step:1565/1750 train_time:155408ms step_avg:99.30ms
step:1566/1750 train_time:155509ms step_avg:99.30ms
step:1567/1750 train_time:155611ms step_avg:99.30ms
step:1568/1750 train_time:155712ms step_avg:99.31ms
step:1569/1750 train_time:155814ms step_avg:99.31ms
step:1570/1750 train_time:155917ms step_avg:99.31ms
step:1571/1750 train_time:156020ms step_avg:99.31ms
step:1572/1750 train_time:156121ms step_avg:99.31ms
step:1573/1750 train_time:156222ms step_avg:99.31ms
step:1574/1750 train_time:156324ms step_avg:99.32ms
step:1575/1750 train_time:156426ms step_avg:99.32ms
step:1576/1750 train_time:156528ms step_avg:99.32ms
step:1577/1750 train_time:156631ms step_avg:99.32ms
step:1578/1750 train_time:156733ms step_avg:99.32ms
step:1579/1750 train_time:156836ms step_avg:99.33ms
step:1580/1750 train_time:156938ms step_avg:99.33ms
step:1581/1750 train_time:157040ms step_avg:99.33ms
step:1582/1750 train_time:157141ms step_avg:99.33ms
step:1583/1750 train_time:157244ms step_avg:99.33ms
step:1584/1750 train_time:157347ms step_avg:99.34ms
step:1585/1750 train_time:157450ms step_avg:99.34ms
step:1586/1750 train_time:157552ms step_avg:99.34ms
step:1587/1750 train_time:157654ms step_avg:99.34ms
step:1588/1750 train_time:157756ms step_avg:99.34ms
step:1589/1750 train_time:157857ms step_avg:99.34ms
step:1590/1750 train_time:157959ms step_avg:99.35ms
step:1591/1750 train_time:158061ms step_avg:99.35ms
step:1592/1750 train_time:158163ms step_avg:99.35ms
step:1593/1750 train_time:158264ms step_avg:99.35ms
step:1594/1750 train_time:158369ms step_avg:99.35ms
step:1595/1750 train_time:158471ms step_avg:99.35ms
step:1596/1750 train_time:158573ms step_avg:99.36ms
step:1597/1750 train_time:158674ms step_avg:99.36ms
step:1598/1750 train_time:158777ms step_avg:99.36ms
step:1599/1750 train_time:158878ms step_avg:99.36ms
step:1600/1750 train_time:158980ms step_avg:99.36ms
step:1601/1750 train_time:159082ms step_avg:99.36ms
step:1602/1750 train_time:159184ms step_avg:99.37ms
step:1603/1750 train_time:159285ms step_avg:99.37ms
step:1604/1750 train_time:159386ms step_avg:99.37ms
step:1605/1750 train_time:159489ms step_avg:99.37ms
step:1606/1750 train_time:159593ms step_avg:99.37ms
step:1607/1750 train_time:159694ms step_avg:99.37ms
step:1608/1750 train_time:159795ms step_avg:99.37ms
step:1609/1750 train_time:159896ms step_avg:99.38ms
step:1610/1750 train_time:159998ms step_avg:99.38ms
step:1611/1750 train_time:160101ms step_avg:99.38ms
step:1612/1750 train_time:160202ms step_avg:99.38ms
step:1613/1750 train_time:160304ms step_avg:99.38ms
step:1614/1750 train_time:160405ms step_avg:99.38ms
step:1615/1750 train_time:160507ms step_avg:99.39ms
step:1616/1750 train_time:160609ms step_avg:99.39ms
step:1617/1750 train_time:160712ms step_avg:99.39ms
step:1618/1750 train_time:160814ms step_avg:99.39ms
step:1619/1750 train_time:160915ms step_avg:99.39ms
step:1620/1750 train_time:161018ms step_avg:99.39ms
step:1621/1750 train_time:161120ms step_avg:99.40ms
step:1622/1750 train_time:161222ms step_avg:99.40ms
step:1623/1750 train_time:161324ms step_avg:99.40ms
step:1624/1750 train_time:161427ms step_avg:99.40ms
step:1625/1750 train_time:161531ms step_avg:99.40ms
step:1625/1750 val_loss:3.3044 train_time:161622ms step_avg:99.46ms
step:1626/1750 train_time:161643ms step_avg:99.41ms
step:1627/1750 train_time:161744ms step_avg:99.41ms
step:1628/1750 train_time:161846ms step_avg:99.41ms
step:1629/1750 train_time:161947ms step_avg:99.42ms
step:1630/1750 train_time:162048ms step_avg:99.42ms
step:1631/1750 train_time:162150ms step_avg:99.42ms
step:1632/1750 train_time:162251ms step_avg:99.42ms
step:1633/1750 train_time:162352ms step_avg:99.42ms
step:1634/1750 train_time:162454ms step_avg:99.42ms
step:1635/1750 train_time:162555ms step_avg:99.42ms
step:1636/1750 train_time:162658ms step_avg:99.42ms
step:1637/1750 train_time:162762ms step_avg:99.43ms
step:1638/1750 train_time:162864ms step_avg:99.43ms
step:1639/1750 train_time:162967ms step_avg:99.43ms
step:1640/1750 train_time:163069ms step_avg:99.43ms
step:1641/1750 train_time:163169ms step_avg:99.43ms
step:1642/1750 train_time:163271ms step_avg:99.43ms
step:1643/1750 train_time:163371ms step_avg:99.43ms
step:1644/1750 train_time:163472ms step_avg:99.44ms
step:1645/1750 train_time:163573ms step_avg:99.44ms
step:1646/1750 train_time:163677ms step_avg:99.44ms
step:1647/1750 train_time:163783ms step_avg:99.44ms
step:1648/1750 train_time:163886ms step_avg:99.45ms
step:1649/1750 train_time:163987ms step_avg:99.45ms
step:1650/1750 train_time:164089ms step_avg:99.45ms
step:1651/1750 train_time:164191ms step_avg:99.45ms
step:1652/1750 train_time:164292ms step_avg:99.45ms
step:1653/1750 train_time:164393ms step_avg:99.45ms
step:1654/1750 train_time:164493ms step_avg:99.45ms
step:1655/1750 train_time:164596ms step_avg:99.45ms
step:1656/1750 train_time:164699ms step_avg:99.46ms
step:1657/1750 train_time:164802ms step_avg:99.46ms
step:1658/1750 train_time:164905ms step_avg:99.46ms
step:1659/1750 train_time:165008ms step_avg:99.46ms
step:1660/1750 train_time:165110ms step_avg:99.46ms
step:1661/1750 train_time:165213ms step_avg:99.47ms
step:1662/1750 train_time:165316ms step_avg:99.47ms
step:1663/1750 train_time:165418ms step_avg:99.47ms
step:1664/1750 train_time:165520ms step_avg:99.47ms
step:1665/1750 train_time:165625ms step_avg:99.47ms
step:1666/1750 train_time:165727ms step_avg:99.48ms
step:1667/1750 train_time:165829ms step_avg:99.48ms
step:1668/1750 train_time:165933ms step_avg:99.48ms
step:1669/1750 train_time:166036ms step_avg:99.48ms
step:1670/1750 train_time:166138ms step_avg:99.48ms
step:1671/1750 train_time:166240ms step_avg:99.49ms
step:1672/1750 train_time:166342ms step_avg:99.49ms
step:1673/1750 train_time:166444ms step_avg:99.49ms
step:1674/1750 train_time:166545ms step_avg:99.49ms
step:1675/1750 train_time:166648ms step_avg:99.49ms
step:1676/1750 train_time:166751ms step_avg:99.49ms
step:1677/1750 train_time:166853ms step_avg:99.49ms
step:1678/1750 train_time:166955ms step_avg:99.50ms
step:1679/1750 train_time:167057ms step_avg:99.50ms
step:1680/1750 train_time:167159ms step_avg:99.50ms
step:1681/1750 train_time:167261ms step_avg:99.50ms
step:1682/1750 train_time:167365ms step_avg:99.50ms
step:1683/1750 train_time:167466ms step_avg:99.50ms
step:1684/1750 train_time:167568ms step_avg:99.51ms
step:1685/1750 train_time:167670ms step_avg:99.51ms
step:1686/1750 train_time:167771ms step_avg:99.51ms
step:1687/1750 train_time:167873ms step_avg:99.51ms
step:1688/1750 train_time:167975ms step_avg:99.51ms
step:1689/1750 train_time:168077ms step_avg:99.51ms
step:1690/1750 train_time:168180ms step_avg:99.51ms
step:1691/1750 train_time:168283ms step_avg:99.52ms
step:1692/1750 train_time:168385ms step_avg:99.52ms
step:1693/1750 train_time:168488ms step_avg:99.52ms
step:1694/1750 train_time:168593ms step_avg:99.52ms
step:1695/1750 train_time:168696ms step_avg:99.53ms
step:1696/1750 train_time:168797ms step_avg:99.53ms
step:1697/1750 train_time:168902ms step_avg:99.53ms
step:1698/1750 train_time:169004ms step_avg:99.53ms
step:1699/1750 train_time:169107ms step_avg:99.53ms
step:1700/1750 train_time:169211ms step_avg:99.54ms
step:1701/1750 train_time:169314ms step_avg:99.54ms
step:1702/1750 train_time:169421ms step_avg:99.54ms
step:1703/1750 train_time:169523ms step_avg:99.54ms
step:1704/1750 train_time:169626ms step_avg:99.55ms
step:1705/1750 train_time:169729ms step_avg:99.55ms
step:1706/1750 train_time:169831ms step_avg:99.55ms
step:1707/1750 train_time:169935ms step_avg:99.55ms
step:1708/1750 train_time:170038ms step_avg:99.55ms
step:1709/1750 train_time:170141ms step_avg:99.56ms
step:1710/1750 train_time:170243ms step_avg:99.56ms
step:1711/1750 train_time:170348ms step_avg:99.56ms
step:1712/1750 train_time:170450ms step_avg:99.56ms
step:1713/1750 train_time:170554ms step_avg:99.56ms
step:1714/1750 train_time:170658ms step_avg:99.57ms
step:1715/1750 train_time:170762ms step_avg:99.57ms
step:1716/1750 train_time:170864ms step_avg:99.57ms
step:1717/1750 train_time:170967ms step_avg:99.57ms
step:1718/1750 train_time:171069ms step_avg:99.57ms
step:1719/1750 train_time:171173ms step_avg:99.58ms
step:1720/1750 train_time:171276ms step_avg:99.58ms
step:1721/1750 train_time:171378ms step_avg:99.58ms
step:1722/1750 train_time:171482ms step_avg:99.58ms
step:1723/1750 train_time:171584ms step_avg:99.58ms
step:1724/1750 train_time:171690ms step_avg:99.59ms
step:1725/1750 train_time:171792ms step_avg:99.59ms
step:1726/1750 train_time:171893ms step_avg:99.59ms
step:1727/1750 train_time:171996ms step_avg:99.59ms
step:1728/1750 train_time:172099ms step_avg:99.59ms
step:1729/1750 train_time:172203ms step_avg:99.60ms
step:1730/1750 train_time:172304ms step_avg:99.60ms
step:1731/1750 train_time:172408ms step_avg:99.60ms
step:1732/1750 train_time:172511ms step_avg:99.60ms
step:1733/1750 train_time:172613ms step_avg:99.60ms
step:1734/1750 train_time:172718ms step_avg:99.61ms
step:1735/1750 train_time:172820ms step_avg:99.61ms
step:1736/1750 train_time:172923ms step_avg:99.61ms
step:1737/1750 train_time:173026ms step_avg:99.61ms
step:1738/1750 train_time:173128ms step_avg:99.61ms
step:1739/1750 train_time:173229ms step_avg:99.61ms
step:1740/1750 train_time:173331ms step_avg:99.62ms
step:1741/1750 train_time:173436ms step_avg:99.62ms
step:1742/1750 train_time:173540ms step_avg:99.62ms
step:1743/1750 train_time:173644ms step_avg:99.62ms
step:1744/1750 train_time:173749ms step_avg:99.63ms
step:1745/1750 train_time:173850ms step_avg:99.63ms
step:1746/1750 train_time:173952ms step_avg:99.63ms
step:1747/1750 train_time:174056ms step_avg:99.63ms
step:1748/1750 train_time:174160ms step_avg:99.63ms
step:1749/1750 train_time:174262ms step_avg:99.64ms
step:1750/1750 train_time:174365ms step_avg:99.64ms
step:1750/1750 val_loss:3.2809 train_time:174455ms step_avg:99.69ms
peak memory allocated: 33278 MiB reserved: 49274 MiB
