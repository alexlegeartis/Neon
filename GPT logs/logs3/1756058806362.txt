import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X
'''
import torch.utils.dlpack as thd
from cupyx.scipy.sparse.linalg import svds as cupyx_svds

def several_sv_svds_approximation(W_torch, k, num_iter=50):
    """SVD approximation using the top k singular values and corresponding vectors."""
    # Store original device and dtype
    original_device = W_torch.device
    original_dtype = W_torch.dtype
    
    W = cp.from_dlpack(thd.to_dlpack(W_torch)).astype(cp.float32)
    U, S, Vt = cupyx_svds(W, k=min([k, W.shape[0] - 1, W.shape[1] - 1]), maxiter=num_iter, which='LM')

    # Convert back to torch tensors and ensure they're on the correct device
    approx_torch_U = thd.from_dlpack(U.toDlpack()).to(device=original_device, dtype=original_dtype)
    approx_torch_S = thd.from_dlpack(S.toDlpack()).to(device=original_device, dtype=original_dtype)
    approx_torch_Vt = thd.from_dlpack(Vt.toDlpack()).to(device=original_device, dtype=original_dtype)
    
    return approx_torch_U, approx_torch_S, approx_torch_Vt

class NormNeon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    # v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    u, s, vt = several_sv_svds_approximation(grad.bfloat16(), 5)
                    p.add_(other=u@vt, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()
'''
class FastNormMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, sgd_coeff=0):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, sgd_coeff=sgd_coeff)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        self.sgd_coeff = sgd_coeff

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            sgd_coeff = group["sgd_coeff"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    # assume: grad is a Tensor, momentum_buffer exists, momentum, sgd_coeff, eps, eff_lr are defined
                    with torch.no_grad():
                        # inplace blend with momentum (keeps original semantic: grad becomes the lerped tensor)
                        grad.lerp_(momentum_buffer, momentum)

                        # compute zeropower part from the (post-lerp) grad, in bfloat16 to match the original code
                        if sgd_coeff != 1:
                            # create update_part in bfloat16 (may allocate once per call; unavoidable if zeropower expects bfloat16)
                            update_part = zeropower_via_newtonschulz5(grad.to(torch.bfloat16), 5)

                            # upcast update_part to grad dtype to do mixed-dtype blending without extra temporaries later
                            if update_part.dtype != grad.dtype:
                                update_part = update_part.to(grad.dtype)

                        # compute inverse norm scalar efficiently (no large temp tensors)
                        # - view(-1) is cheap (no copy)
                        # - dot(view, view) is a single reduction
                        norm_sq = grad.view(-1).dot(grad.view(-1))
                        inv_norm = (norm_sq + eps).rsqrt()  # stable rsqrt on the scalar

                        # normalize grad in-place (so we avoid allocating g_normalized)
                        grad.mul_(inv_norm)   # now grad == g_normalized (in-place)

                        # build final update (reuse update_part buffer if available)
                        if sgd_coeff != 1:
                            # update_part := (1 - sgd_coeff) * update_part + sgd_coeff * grad
                            # do it in-place on update_part to avoid creating another temp
                            update_part.mul_(1 - sgd_coeff)    # in-place scale
                            update_part.lerp_(grad, sgd_coeff) # in-place linear interpolation
                            update = update_part
                        else:
                            # if sgd_coeff == 1 we can use grad directly (already normalized in-place)
                            update = grad

                        # apply update (same as original)
                        p.add_(other=update, alpha=-eff_lr)
                    '''
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    eps = 1e-12
                    g_normalized = grad / (grad.norm() + eps)
                    if sgd_coeff != 1:
                        update_part = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                        update = (1 - sgd_coeff) * update_part + sgd_coeff * g_normalized
                    else:
                        update = sgd_coeff * g_normalized
                    p.add_(other=update, alpha=-eff_lr)
                    '''
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class NormMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, sgd_coeff=0):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, sgd_coeff=sgd_coeff)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        self.sgd_coeff = sgd_coeff

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            sgd_coeff = group["sgd_coeff"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    eps = 1e-12
                    g_normalized = grad / (grad.norm() + eps)
                    if sgd_coeff != 1:
                        update_part = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                        update = (1 - sgd_coeff) * update_part + sgd_coeff * g_normalized
                    else:
                        update = sgd_coeff * g_normalized
                    p.add_(other=update, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()
class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)
                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_reduce_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 27.5
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos : pos + max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()
    starts = []
    for i in range(1, len(boundary_positions)):
        end = boundary_positions[i].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == dist.get_world_size():
                return starts, end - pos
            start = end
    assert False # increase max_batch_span if necessary

def distributed_data_generator(filename_pattern: str, batch_size: int, align_to_bos: bool):
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    max_batch_span = 2 * batch_size if align_to_bos else batch_size # provide buffer to handle samples up to length local_batch_size
    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            batch_span = batch_size
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1750 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
from datetime import datetime
if master_process:
    # run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_id = str(int(time.time() * 1000))
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0)
optimizer2 = NormMuon(hidden_matrix_params, lr=0.07, momentum=0.95, weight_decay=0.0001, sgd_coeff=0.5)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.11 | packaged by conda-forge | (main, Jun  4 2025, 14:45:31) [GCC 13.3.0]
Running PyTorch 2.9.0.dev20250824+cu126 compiled for CUDA 12.6
Sun Aug 24 18:06:46 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:04:00.0 Off |                    0 |
| N/A   41C    P0            123W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:23:00.0 Off |                    0 |
| N/A   34C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:43:00.0 Off |                    0 |
| N/A   39C    P0            125W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   34C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:84:00.0 Off |                    0 |
| N/A   40C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:A3:00.0 Off |                    0 |
| N/A   34C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:C3:00.0 Off |                    0 |
| N/A   41C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E4:00.0 Off |                    0 |
| N/A   33C    P0            113W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.06ms
step:1/1750 train_time:140ms step_avg:139.78ms
step:2/1750 train_time:161ms step_avg:80.66ms
step:3/1750 train_time:241ms step_avg:80.36ms
step:4/1750 train_time:333ms step_avg:83.14ms
step:5/1750 train_time:425ms step_avg:84.99ms
step:6/1750 train_time:517ms step_avg:86.23ms
step:7/1750 train_time:609ms step_avg:87.06ms
step:8/1750 train_time:702ms step_avg:87.71ms
step:9/1750 train_time:794ms step_avg:88.20ms
step:10/1750 train_time:886ms step_avg:88.62ms
step:11/1750 train_time:979ms step_avg:89.02ms
step:12/1750 train_time:1075ms step_avg:89.56ms
step:13/1750 train_time:1171ms step_avg:90.11ms
step:14/1750 train_time:1267ms step_avg:90.47ms
step:15/1750 train_time:1361ms step_avg:90.71ms
step:16/1750 train_time:1453ms step_avg:90.82ms
step:17/1750 train_time:1546ms step_avg:90.93ms
step:18/1750 train_time:1639ms step_avg:91.04ms
step:19/1750 train_time:1731ms step_avg:91.13ms
step:20/1750 train_time:1824ms step_avg:91.21ms
step:21/1750 train_time:1917ms step_avg:91.30ms
step:22/1750 train_time:2010ms step_avg:91.35ms
step:23/1750 train_time:2105ms step_avg:91.50ms
step:24/1750 train_time:2200ms step_avg:91.66ms
step:25/1750 train_time:2294ms step_avg:91.75ms
step:26/1750 train_time:2388ms step_avg:91.85ms
step:27/1750 train_time:2481ms step_avg:91.89ms
step:28/1750 train_time:2574ms step_avg:91.92ms
step:29/1750 train_time:2667ms step_avg:91.97ms
step:30/1750 train_time:2760ms step_avg:92.00ms
step:31/1750 train_time:2853ms step_avg:92.04ms
step:32/1750 train_time:2946ms step_avg:92.05ms
step:33/1750 train_time:3040ms step_avg:92.12ms
step:34/1750 train_time:3134ms step_avg:92.19ms
step:35/1750 train_time:3229ms step_avg:92.25ms
step:36/1750 train_time:3323ms step_avg:92.32ms
step:37/1750 train_time:3417ms step_avg:92.36ms
step:38/1750 train_time:3510ms step_avg:92.37ms
step:39/1750 train_time:3603ms step_avg:92.39ms
step:40/1750 train_time:3696ms step_avg:92.40ms
step:41/1750 train_time:3788ms step_avg:92.40ms
step:42/1750 train_time:3882ms step_avg:92.42ms
step:43/1750 train_time:3975ms step_avg:92.43ms
step:44/1750 train_time:4068ms step_avg:92.45ms
step:45/1750 train_time:4161ms step_avg:92.48ms
step:46/1750 train_time:4256ms step_avg:92.51ms
step:47/1750 train_time:4349ms step_avg:92.53ms
step:48/1750 train_time:4442ms step_avg:92.55ms
step:49/1750 train_time:4536ms step_avg:92.57ms
step:50/1750 train_time:4629ms step_avg:92.57ms
step:51/1750 train_time:4722ms step_avg:92.59ms
step:52/1750 train_time:4816ms step_avg:92.61ms
step:53/1750 train_time:4908ms step_avg:92.60ms
step:54/1750 train_time:5001ms step_avg:92.61ms
step:55/1750 train_time:5095ms step_avg:92.63ms
step:56/1750 train_time:5188ms step_avg:92.65ms
step:57/1750 train_time:5283ms step_avg:92.68ms
step:58/1750 train_time:5376ms step_avg:92.69ms
step:59/1750 train_time:5469ms step_avg:92.70ms
step:60/1750 train_time:5562ms step_avg:92.70ms
step:61/1750 train_time:5655ms step_avg:92.70ms
step:62/1750 train_time:5748ms step_avg:92.71ms
step:63/1750 train_time:5841ms step_avg:92.71ms
step:64/1750 train_time:5935ms step_avg:92.73ms
step:65/1750 train_time:6028ms step_avg:92.73ms
step:66/1750 train_time:6121ms step_avg:92.74ms
step:67/1750 train_time:6215ms step_avg:92.76ms
step:68/1750 train_time:6308ms step_avg:92.77ms
step:69/1750 train_time:6402ms step_avg:92.78ms
step:70/1750 train_time:6495ms step_avg:92.78ms
step:71/1750 train_time:6588ms step_avg:92.79ms
step:72/1750 train_time:6682ms step_avg:92.80ms
step:73/1750 train_time:6776ms step_avg:92.82ms
step:74/1750 train_time:6868ms step_avg:92.81ms
step:75/1750 train_time:6961ms step_avg:92.81ms
step:76/1750 train_time:7054ms step_avg:92.81ms
step:77/1750 train_time:7147ms step_avg:92.82ms
step:78/1750 train_time:7241ms step_avg:92.83ms
step:79/1750 train_time:7335ms step_avg:92.85ms
step:80/1750 train_time:7428ms step_avg:92.85ms
step:81/1750 train_time:7521ms step_avg:92.86ms
step:82/1750 train_time:7615ms step_avg:92.87ms
step:83/1750 train_time:7707ms step_avg:92.86ms
step:84/1750 train_time:7801ms step_avg:92.87ms
step:85/1750 train_time:7894ms step_avg:92.87ms
step:86/1750 train_time:7987ms step_avg:92.87ms
step:87/1750 train_time:8081ms step_avg:92.88ms
step:88/1750 train_time:8173ms step_avg:92.88ms
step:89/1750 train_time:8267ms step_avg:92.89ms
step:90/1750 train_time:8361ms step_avg:92.90ms
step:91/1750 train_time:8455ms step_avg:92.91ms
step:92/1750 train_time:8548ms step_avg:92.91ms
step:93/1750 train_time:8641ms step_avg:92.91ms
step:94/1750 train_time:8734ms step_avg:92.92ms
step:95/1750 train_time:8827ms step_avg:92.91ms
step:96/1750 train_time:8920ms step_avg:92.92ms
step:97/1750 train_time:9014ms step_avg:92.93ms
step:98/1750 train_time:9107ms step_avg:92.93ms
step:99/1750 train_time:9200ms step_avg:92.93ms
step:100/1750 train_time:9294ms step_avg:92.94ms
step:101/1750 train_time:9387ms step_avg:92.95ms
step:102/1750 train_time:9481ms step_avg:92.95ms
step:103/1750 train_time:9575ms step_avg:92.96ms
step:104/1750 train_time:9668ms step_avg:92.96ms
step:105/1750 train_time:9761ms step_avg:92.97ms
step:106/1750 train_time:9855ms step_avg:92.97ms
step:107/1750 train_time:9948ms step_avg:92.97ms
step:108/1750 train_time:10041ms step_avg:92.98ms
step:109/1750 train_time:10134ms step_avg:92.97ms
step:110/1750 train_time:10227ms step_avg:92.97ms
step:111/1750 train_time:10320ms step_avg:92.98ms
step:112/1750 train_time:10414ms step_avg:92.98ms
step:113/1750 train_time:10507ms step_avg:92.99ms
step:114/1750 train_time:10601ms step_avg:92.99ms
step:115/1750 train_time:10694ms step_avg:92.99ms
step:116/1750 train_time:10787ms step_avg:92.99ms
step:117/1750 train_time:10880ms step_avg:92.99ms
step:118/1750 train_time:10973ms step_avg:92.99ms
step:119/1750 train_time:11066ms step_avg:93.00ms
step:120/1750 train_time:11160ms step_avg:93.00ms
step:121/1750 train_time:11253ms step_avg:93.00ms
step:122/1750 train_time:11347ms step_avg:93.00ms
step:123/1750 train_time:11440ms step_avg:93.01ms
step:124/1750 train_time:11533ms step_avg:93.01ms
step:125/1750 train_time:11627ms step_avg:93.01ms
step:125/1750 val_loss:4.6525 train_time:11710ms step_avg:93.68ms
step:126/1750 train_time:11732ms step_avg:93.11ms
step:127/1750 train_time:11821ms step_avg:93.08ms
step:128/1750 train_time:11926ms step_avg:93.17ms
step:129/1750 train_time:12020ms step_avg:93.18ms
step:130/1750 train_time:12114ms step_avg:93.18ms
step:131/1750 train_time:12206ms step_avg:93.18ms
step:132/1750 train_time:12299ms step_avg:93.17ms
step:133/1750 train_time:12391ms step_avg:93.16ms
step:134/1750 train_time:12483ms step_avg:93.16ms
step:135/1750 train_time:12576ms step_avg:93.16ms
step:136/1750 train_time:12668ms step_avg:93.15ms
step:137/1750 train_time:12762ms step_avg:93.16ms
step:138/1750 train_time:12859ms step_avg:93.18ms
step:139/1750 train_time:12954ms step_avg:93.20ms
step:140/1750 train_time:13049ms step_avg:93.20ms
step:141/1750 train_time:13142ms step_avg:93.21ms
step:142/1750 train_time:13236ms step_avg:93.21ms
step:143/1750 train_time:13329ms step_avg:93.21ms
step:144/1750 train_time:13422ms step_avg:93.21ms
step:145/1750 train_time:13516ms step_avg:93.21ms
step:146/1750 train_time:13608ms step_avg:93.21ms
step:147/1750 train_time:13701ms step_avg:93.21ms
step:148/1750 train_time:13795ms step_avg:93.21ms
step:149/1750 train_time:13890ms step_avg:93.22ms
step:150/1750 train_time:13986ms step_avg:93.24ms
step:151/1750 train_time:14081ms step_avg:93.25ms
step:152/1750 train_time:14174ms step_avg:93.25ms
step:153/1750 train_time:14268ms step_avg:93.25ms
step:154/1750 train_time:14361ms step_avg:93.25ms
step:155/1750 train_time:14455ms step_avg:93.25ms
step:156/1750 train_time:14548ms step_avg:93.25ms
step:157/1750 train_time:14641ms step_avg:93.25ms
step:158/1750 train_time:14735ms step_avg:93.26ms
step:159/1750 train_time:14828ms step_avg:93.26ms
step:160/1750 train_time:14923ms step_avg:93.27ms
step:161/1750 train_time:15018ms step_avg:93.28ms
step:162/1750 train_time:15113ms step_avg:93.29ms
step:163/1750 train_time:15206ms step_avg:93.29ms
step:164/1750 train_time:15300ms step_avg:93.29ms
step:165/1750 train_time:15393ms step_avg:93.29ms
step:166/1750 train_time:15486ms step_avg:93.29ms
step:167/1750 train_time:15579ms step_avg:93.29ms
step:168/1750 train_time:15672ms step_avg:93.28ms
step:169/1750 train_time:15765ms step_avg:93.29ms
step:170/1750 train_time:15859ms step_avg:93.29ms
step:171/1750 train_time:15953ms step_avg:93.29ms
step:172/1750 train_time:16048ms step_avg:93.30ms
step:173/1750 train_time:16141ms step_avg:93.30ms
step:174/1750 train_time:16235ms step_avg:93.31ms
step:175/1750 train_time:16328ms step_avg:93.30ms
step:176/1750 train_time:16422ms step_avg:93.30ms
step:177/1750 train_time:16516ms step_avg:93.31ms
step:178/1750 train_time:16609ms step_avg:93.31ms
step:179/1750 train_time:16703ms step_avg:93.31ms
step:180/1750 train_time:16796ms step_avg:93.31ms
step:181/1750 train_time:16889ms step_avg:93.31ms
step:182/1750 train_time:16983ms step_avg:93.31ms
step:183/1750 train_time:17077ms step_avg:93.31ms
step:184/1750 train_time:17170ms step_avg:93.31ms
step:185/1750 train_time:17264ms step_avg:93.32ms
step:186/1750 train_time:17358ms step_avg:93.32ms
step:187/1750 train_time:17452ms step_avg:93.33ms
step:188/1750 train_time:17545ms step_avg:93.32ms
step:189/1750 train_time:17638ms step_avg:93.32ms
step:190/1750 train_time:17732ms step_avg:93.33ms
step:191/1750 train_time:17825ms step_avg:93.33ms
step:192/1750 train_time:17919ms step_avg:93.33ms
step:193/1750 train_time:18013ms step_avg:93.33ms
step:194/1750 train_time:18106ms step_avg:93.33ms
step:195/1750 train_time:18200ms step_avg:93.33ms
step:196/1750 train_time:18294ms step_avg:93.34ms
step:197/1750 train_time:18387ms step_avg:93.33ms
step:198/1750 train_time:18741ms step_avg:94.65ms
step:199/1750 train_time:18833ms step_avg:94.64ms
step:200/1750 train_time:18925ms step_avg:94.63ms
step:201/1750 train_time:19018ms step_avg:94.62ms
step:202/1750 train_time:19111ms step_avg:94.61ms
step:203/1750 train_time:19203ms step_avg:94.60ms
step:204/1750 train_time:19296ms step_avg:94.59ms
step:205/1750 train_time:19389ms step_avg:94.58ms
step:206/1750 train_time:19482ms step_avg:94.57ms
step:207/1750 train_time:19576ms step_avg:94.57ms
step:208/1750 train_time:19675ms step_avg:94.59ms
step:209/1750 train_time:19770ms step_avg:94.59ms
step:210/1750 train_time:19866ms step_avg:94.60ms
step:211/1750 train_time:19959ms step_avg:94.59ms
step:212/1750 train_time:20052ms step_avg:94.58ms
step:213/1750 train_time:20145ms step_avg:94.58ms
step:214/1750 train_time:20238ms step_avg:94.57ms
step:215/1750 train_time:20330ms step_avg:94.56ms
step:216/1750 train_time:20423ms step_avg:94.55ms
step:217/1750 train_time:20517ms step_avg:94.55ms
step:218/1750 train_time:20612ms step_avg:94.55ms
step:219/1750 train_time:20706ms step_avg:94.55ms
step:220/1750 train_time:20800ms step_avg:94.55ms
step:221/1750 train_time:20894ms step_avg:94.54ms
step:222/1750 train_time:20988ms step_avg:94.54ms
step:223/1750 train_time:21081ms step_avg:94.54ms
step:224/1750 train_time:21175ms step_avg:94.53ms
step:225/1750 train_time:21268ms step_avg:94.52ms
step:226/1750 train_time:21360ms step_avg:94.51ms
step:227/1750 train_time:21454ms step_avg:94.51ms
step:228/1750 train_time:21547ms step_avg:94.50ms
step:229/1750 train_time:21640ms step_avg:94.50ms
step:230/1750 train_time:21735ms step_avg:94.50ms
step:231/1750 train_time:21829ms step_avg:94.50ms
step:232/1750 train_time:21923ms step_avg:94.50ms
step:233/1750 train_time:22017ms step_avg:94.49ms
step:234/1750 train_time:22111ms step_avg:94.49ms
step:235/1750 train_time:22204ms step_avg:94.48ms
step:236/1750 train_time:22297ms step_avg:94.48ms
step:237/1750 train_time:22389ms step_avg:94.47ms
step:238/1750 train_time:22483ms step_avg:94.47ms
step:239/1750 train_time:22577ms step_avg:94.46ms
step:240/1750 train_time:22671ms step_avg:94.46ms
step:241/1750 train_time:22765ms step_avg:94.46ms
step:242/1750 train_time:22858ms step_avg:94.46ms
step:243/1750 train_time:22953ms step_avg:94.46ms
step:244/1750 train_time:23046ms step_avg:94.45ms
step:245/1750 train_time:23140ms step_avg:94.45ms
step:246/1750 train_time:23233ms step_avg:94.44ms
step:247/1750 train_time:23327ms step_avg:94.44ms
step:248/1750 train_time:23420ms step_avg:94.44ms
step:249/1750 train_time:23514ms step_avg:94.43ms
step:250/1750 train_time:23608ms step_avg:94.43ms
step:250/1750 val_loss:4.1022 train_time:23691ms step_avg:94.76ms
step:251/1750 train_time:23712ms step_avg:94.47ms
step:252/1750 train_time:23803ms step_avg:94.45ms
step:253/1750 train_time:23901ms step_avg:94.47ms
step:254/1750 train_time:23995ms step_avg:94.47ms
step:255/1750 train_time:24087ms step_avg:94.46ms
step:256/1750 train_time:24180ms step_avg:94.45ms
step:257/1750 train_time:24274ms step_avg:94.45ms
step:258/1750 train_time:24368ms step_avg:94.45ms
step:259/1750 train_time:24460ms step_avg:94.44ms
step:260/1750 train_time:24554ms step_avg:94.44ms
step:261/1750 train_time:24649ms step_avg:94.44ms
step:262/1750 train_time:24744ms step_avg:94.44ms
step:263/1750 train_time:24841ms step_avg:94.45ms
step:264/1750 train_time:24936ms step_avg:94.45ms
step:265/1750 train_time:25030ms step_avg:94.45ms
step:266/1750 train_time:25124ms step_avg:94.45ms
step:267/1750 train_time:25217ms step_avg:94.45ms
step:268/1750 train_time:25312ms step_avg:94.45ms
step:269/1750 train_time:25405ms step_avg:94.44ms
step:270/1750 train_time:25498ms step_avg:94.44ms
step:271/1750 train_time:25593ms step_avg:94.44ms
step:272/1750 train_time:25688ms step_avg:94.44ms
step:273/1750 train_time:25782ms step_avg:94.44ms
step:274/1750 train_time:25876ms step_avg:94.44ms
step:275/1750 train_time:25971ms step_avg:94.44ms
step:276/1750 train_time:26064ms step_avg:94.44ms
step:277/1750 train_time:26159ms step_avg:94.44ms
step:278/1750 train_time:26253ms step_avg:94.43ms
step:279/1750 train_time:26346ms step_avg:94.43ms
step:280/1750 train_time:26440ms step_avg:94.43ms
step:281/1750 train_time:26534ms step_avg:94.43ms
step:282/1750 train_time:26627ms step_avg:94.42ms
step:283/1750 train_time:26721ms step_avg:94.42ms
step:284/1750 train_time:26816ms step_avg:94.42ms
step:285/1750 train_time:26911ms step_avg:94.42ms
step:286/1750 train_time:27004ms step_avg:94.42ms
step:287/1750 train_time:27099ms step_avg:94.42ms
step:288/1750 train_time:27193ms step_avg:94.42ms
step:289/1750 train_time:27287ms step_avg:94.42ms
step:290/1750 train_time:27380ms step_avg:94.41ms
step:291/1750 train_time:27474ms step_avg:94.41ms
step:292/1750 train_time:27568ms step_avg:94.41ms
step:293/1750 train_time:27662ms step_avg:94.41ms
step:294/1750 train_time:27756ms step_avg:94.41ms
step:295/1750 train_time:27851ms step_avg:94.41ms
step:296/1750 train_time:27945ms step_avg:94.41ms
step:297/1750 train_time:28039ms step_avg:94.41ms
step:298/1750 train_time:28134ms step_avg:94.41ms
step:299/1750 train_time:28228ms step_avg:94.41ms
step:300/1750 train_time:28321ms step_avg:94.40ms
step:301/1750 train_time:28415ms step_avg:94.40ms
step:302/1750 train_time:28509ms step_avg:94.40ms
step:303/1750 train_time:28603ms step_avg:94.40ms
step:304/1750 train_time:28696ms step_avg:94.40ms
step:305/1750 train_time:28791ms step_avg:94.40ms
step:306/1750 train_time:28886ms step_avg:94.40ms
step:307/1750 train_time:28980ms step_avg:94.40ms
step:308/1750 train_time:29074ms step_avg:94.40ms
step:309/1750 train_time:29168ms step_avg:94.40ms
step:310/1750 train_time:29262ms step_avg:94.39ms
step:311/1750 train_time:29356ms step_avg:94.39ms
step:312/1750 train_time:29450ms step_avg:94.39ms
step:313/1750 train_time:29544ms step_avg:94.39ms
step:314/1750 train_time:29638ms step_avg:94.39ms
step:315/1750 train_time:29733ms step_avg:94.39ms
step:316/1750 train_time:29827ms step_avg:94.39ms
step:317/1750 train_time:29921ms step_avg:94.39ms
step:318/1750 train_time:30015ms step_avg:94.39ms
step:319/1750 train_time:30110ms step_avg:94.39ms
step:320/1750 train_time:30204ms step_avg:94.39ms
step:321/1750 train_time:30298ms step_avg:94.39ms
step:322/1750 train_time:30392ms step_avg:94.39ms
step:323/1750 train_time:30487ms step_avg:94.39ms
step:324/1750 train_time:30580ms step_avg:94.38ms
step:325/1750 train_time:30674ms step_avg:94.38ms
step:326/1750 train_time:30768ms step_avg:94.38ms
step:327/1750 train_time:30862ms step_avg:94.38ms
step:328/1750 train_time:30956ms step_avg:94.38ms
step:329/1750 train_time:31051ms step_avg:94.38ms
step:330/1750 train_time:31144ms step_avg:94.38ms
step:331/1750 train_time:31238ms step_avg:94.38ms
step:332/1750 train_time:31333ms step_avg:94.38ms
step:333/1750 train_time:31427ms step_avg:94.38ms
step:334/1750 train_time:31521ms step_avg:94.37ms
step:335/1750 train_time:31615ms step_avg:94.37ms
step:336/1750 train_time:31709ms step_avg:94.37ms
step:337/1750 train_time:31802ms step_avg:94.37ms
step:338/1750 train_time:31895ms step_avg:94.37ms
step:339/1750 train_time:31990ms step_avg:94.37ms
step:340/1750 train_time:32084ms step_avg:94.36ms
step:341/1750 train_time:32178ms step_avg:94.36ms
step:342/1750 train_time:32273ms step_avg:94.37ms
step:343/1750 train_time:32366ms step_avg:94.36ms
step:344/1750 train_time:32460ms step_avg:94.36ms
step:345/1750 train_time:32555ms step_avg:94.36ms
step:346/1750 train_time:32649ms step_avg:94.36ms
step:347/1750 train_time:32743ms step_avg:94.36ms
step:348/1750 train_time:32837ms step_avg:94.36ms
step:349/1750 train_time:32931ms step_avg:94.36ms
step:350/1750 train_time:33025ms step_avg:94.36ms
step:351/1750 train_time:33120ms step_avg:94.36ms
step:352/1750 train_time:33215ms step_avg:94.36ms
step:353/1750 train_time:33309ms step_avg:94.36ms
step:354/1750 train_time:33402ms step_avg:94.36ms
step:355/1750 train_time:33496ms step_avg:94.36ms
step:356/1750 train_time:33591ms step_avg:94.36ms
step:357/1750 train_time:33686ms step_avg:94.36ms
step:358/1750 train_time:33780ms step_avg:94.36ms
step:359/1750 train_time:33874ms step_avg:94.36ms
step:360/1750 train_time:33968ms step_avg:94.36ms
step:361/1750 train_time:34062ms step_avg:94.36ms
step:362/1750 train_time:34157ms step_avg:94.36ms
step:363/1750 train_time:34251ms step_avg:94.36ms
step:364/1750 train_time:34346ms step_avg:94.36ms
step:365/1750 train_time:34440ms step_avg:94.36ms
step:366/1750 train_time:34534ms step_avg:94.36ms
step:367/1750 train_time:34629ms step_avg:94.36ms
step:368/1750 train_time:34723ms step_avg:94.36ms
step:369/1750 train_time:34817ms step_avg:94.36ms
step:370/1750 train_time:34912ms step_avg:94.36ms
step:371/1750 train_time:35006ms step_avg:94.36ms
step:372/1750 train_time:35101ms step_avg:94.36ms
step:373/1750 train_time:35195ms step_avg:94.36ms
step:374/1750 train_time:35289ms step_avg:94.36ms
step:375/1750 train_time:35383ms step_avg:94.36ms
step:375/1750 val_loss:3.8907 train_time:35467ms step_avg:94.58ms
step:376/1750 train_time:35491ms step_avg:94.39ms
step:377/1750 train_time:35578ms step_avg:94.37ms
step:378/1750 train_time:35677ms step_avg:94.38ms
step:379/1750 train_time:35771ms step_avg:94.38ms
step:380/1750 train_time:35864ms step_avg:94.38ms
step:381/1750 train_time:35958ms step_avg:94.38ms
step:382/1750 train_time:36051ms step_avg:94.37ms
step:383/1750 train_time:36144ms step_avg:94.37ms
step:384/1750 train_time:36237ms step_avg:94.37ms
step:385/1750 train_time:36331ms step_avg:94.37ms
step:386/1750 train_time:36425ms step_avg:94.36ms
step:387/1750 train_time:36520ms step_avg:94.37ms
step:388/1750 train_time:36615ms step_avg:94.37ms
step:389/1750 train_time:36710ms step_avg:94.37ms
step:390/1750 train_time:36805ms step_avg:94.37ms
step:391/1750 train_time:36901ms step_avg:94.38ms
step:392/1750 train_time:36997ms step_avg:94.38ms
step:393/1750 train_time:37092ms step_avg:94.38ms
step:394/1750 train_time:37188ms step_avg:94.39ms
step:395/1750 train_time:37283ms step_avg:94.39ms
step:396/1750 train_time:37379ms step_avg:94.39ms
step:397/1750 train_time:37475ms step_avg:94.40ms
step:398/1750 train_time:37573ms step_avg:94.40ms
step:399/1750 train_time:37669ms step_avg:94.41ms
step:400/1750 train_time:37765ms step_avg:94.41ms
step:401/1750 train_time:37862ms step_avg:94.42ms
step:402/1750 train_time:37958ms step_avg:94.42ms
step:403/1750 train_time:38054ms step_avg:94.43ms
step:404/1750 train_time:38150ms step_avg:94.43ms
step:405/1750 train_time:38245ms step_avg:94.43ms
step:406/1750 train_time:38341ms step_avg:94.44ms
step:407/1750 train_time:38437ms step_avg:94.44ms
step:408/1750 train_time:38534ms step_avg:94.45ms
step:409/1750 train_time:38630ms step_avg:94.45ms
step:410/1750 train_time:38726ms step_avg:94.45ms
step:411/1750 train_time:38822ms step_avg:94.46ms
step:412/1750 train_time:38920ms step_avg:94.46ms
step:413/1750 train_time:39016ms step_avg:94.47ms
step:414/1750 train_time:39112ms step_avg:94.47ms
step:415/1750 train_time:39207ms step_avg:94.47ms
step:416/1750 train_time:39302ms step_avg:94.48ms
step:417/1750 train_time:39398ms step_avg:94.48ms
step:418/1750 train_time:39493ms step_avg:94.48ms
step:419/1750 train_time:39590ms step_avg:94.49ms
step:420/1750 train_time:39685ms step_avg:94.49ms
step:421/1750 train_time:39782ms step_avg:94.49ms
step:422/1750 train_time:39877ms step_avg:94.50ms
step:423/1750 train_time:39973ms step_avg:94.50ms
step:424/1750 train_time:40070ms step_avg:94.50ms
step:425/1750 train_time:40165ms step_avg:94.51ms
step:426/1750 train_time:40262ms step_avg:94.51ms
step:427/1750 train_time:40357ms step_avg:94.51ms
step:428/1750 train_time:40454ms step_avg:94.52ms
step:429/1750 train_time:40549ms step_avg:94.52ms
step:430/1750 train_time:40645ms step_avg:94.52ms
step:431/1750 train_time:40741ms step_avg:94.53ms
step:432/1750 train_time:40837ms step_avg:94.53ms
step:433/1750 train_time:40932ms step_avg:94.53ms
step:434/1750 train_time:41029ms step_avg:94.54ms
step:435/1750 train_time:41125ms step_avg:94.54ms
step:436/1750 train_time:41221ms step_avg:94.54ms
step:437/1750 train_time:41317ms step_avg:94.55ms
step:438/1750 train_time:41414ms step_avg:94.55ms
step:439/1750 train_time:41509ms step_avg:94.55ms
step:440/1750 train_time:41605ms step_avg:94.56ms
step:441/1750 train_time:41703ms step_avg:94.56ms
step:442/1750 train_time:41798ms step_avg:94.57ms
step:443/1750 train_time:41895ms step_avg:94.57ms
step:444/1750 train_time:41991ms step_avg:94.57ms
step:445/1750 train_time:42086ms step_avg:94.58ms
step:446/1750 train_time:42183ms step_avg:94.58ms
step:447/1750 train_time:42279ms step_avg:94.58ms
step:448/1750 train_time:42375ms step_avg:94.59ms
step:449/1750 train_time:42471ms step_avg:94.59ms
step:450/1750 train_time:42567ms step_avg:94.59ms
step:451/1750 train_time:42664ms step_avg:94.60ms
step:452/1750 train_time:42760ms step_avg:94.60ms
step:453/1750 train_time:42856ms step_avg:94.61ms
step:454/1750 train_time:42952ms step_avg:94.61ms
step:455/1750 train_time:43049ms step_avg:94.61ms
step:456/1750 train_time:43144ms step_avg:94.62ms
step:457/1750 train_time:43240ms step_avg:94.62ms
step:458/1750 train_time:43335ms step_avg:94.62ms
step:459/1750 train_time:43432ms step_avg:94.62ms
step:460/1750 train_time:43528ms step_avg:94.63ms
step:461/1750 train_time:43624ms step_avg:94.63ms
step:462/1750 train_time:43720ms step_avg:94.63ms
step:463/1750 train_time:43816ms step_avg:94.64ms
step:464/1750 train_time:43912ms step_avg:94.64ms
step:465/1750 train_time:44008ms step_avg:94.64ms
step:466/1750 train_time:44104ms step_avg:94.64ms
step:467/1750 train_time:44201ms step_avg:94.65ms
step:468/1750 train_time:44296ms step_avg:94.65ms
step:469/1750 train_time:44392ms step_avg:94.65ms
step:470/1750 train_time:44487ms step_avg:94.65ms
step:471/1750 train_time:44583ms step_avg:94.66ms
step:472/1750 train_time:44678ms step_avg:94.66ms
step:473/1750 train_time:44774ms step_avg:94.66ms
step:474/1750 train_time:44870ms step_avg:94.66ms
step:475/1750 train_time:44966ms step_avg:94.67ms
step:476/1750 train_time:45063ms step_avg:94.67ms
step:477/1750 train_time:45160ms step_avg:94.67ms
step:478/1750 train_time:45255ms step_avg:94.68ms
step:479/1750 train_time:45351ms step_avg:94.68ms
step:480/1750 train_time:45446ms step_avg:94.68ms
step:481/1750 train_time:45542ms step_avg:94.68ms
step:482/1750 train_time:45637ms step_avg:94.68ms
step:483/1750 train_time:45733ms step_avg:94.68ms
step:484/1750 train_time:45829ms step_avg:94.69ms
step:485/1750 train_time:45925ms step_avg:94.69ms
step:486/1750 train_time:46021ms step_avg:94.69ms
step:487/1750 train_time:46117ms step_avg:94.70ms
step:488/1750 train_time:46214ms step_avg:94.70ms
step:489/1750 train_time:46310ms step_avg:94.70ms
step:490/1750 train_time:46405ms step_avg:94.70ms
step:491/1750 train_time:46502ms step_avg:94.71ms
step:492/1750 train_time:46597ms step_avg:94.71ms
step:493/1750 train_time:46693ms step_avg:94.71ms
step:494/1750 train_time:46789ms step_avg:94.71ms
step:495/1750 train_time:46884ms step_avg:94.72ms
step:496/1750 train_time:46981ms step_avg:94.72ms
step:497/1750 train_time:47077ms step_avg:94.72ms
step:498/1750 train_time:47173ms step_avg:94.73ms
step:499/1750 train_time:47269ms step_avg:94.73ms
step:500/1750 train_time:47365ms step_avg:94.73ms
step:500/1750 val_loss:3.7453 train_time:47450ms step_avg:94.90ms
step:501/1750 train_time:47472ms step_avg:94.75ms
step:502/1750 train_time:47565ms step_avg:94.75ms
step:503/1750 train_time:47665ms step_avg:94.76ms
step:504/1750 train_time:47761ms step_avg:94.76ms
step:505/1750 train_time:47855ms step_avg:94.76ms
step:506/1750 train_time:47950ms step_avg:94.76ms
step:507/1750 train_time:48046ms step_avg:94.76ms
step:508/1750 train_time:48141ms step_avg:94.77ms
step:509/1750 train_time:48236ms step_avg:94.77ms
step:510/1750 train_time:48331ms step_avg:94.77ms
step:511/1750 train_time:48428ms step_avg:94.77ms
step:512/1750 train_time:48526ms step_avg:94.78ms
step:513/1750 train_time:48623ms step_avg:94.78ms
step:514/1750 train_time:48720ms step_avg:94.79ms
step:515/1750 train_time:48815ms step_avg:94.79ms
step:516/1750 train_time:48911ms step_avg:94.79ms
step:517/1750 train_time:49007ms step_avg:94.79ms
step:518/1750 train_time:49102ms step_avg:94.79ms
step:519/1750 train_time:49198ms step_avg:94.79ms
step:520/1750 train_time:49292ms step_avg:94.79ms
step:521/1750 train_time:49388ms step_avg:94.80ms
step:522/1750 train_time:49485ms step_avg:94.80ms
step:523/1750 train_time:49584ms step_avg:94.81ms
step:524/1750 train_time:49682ms step_avg:94.81ms
step:525/1750 train_time:49778ms step_avg:94.82ms
step:526/1750 train_time:49874ms step_avg:94.82ms
step:527/1750 train_time:49970ms step_avg:94.82ms
step:528/1750 train_time:50067ms step_avg:94.82ms
step:529/1750 train_time:50163ms step_avg:94.83ms
step:530/1750 train_time:50259ms step_avg:94.83ms
step:531/1750 train_time:50354ms step_avg:94.83ms
step:532/1750 train_time:50451ms step_avg:94.83ms
step:533/1750 train_time:50548ms step_avg:94.84ms
step:534/1750 train_time:50646ms step_avg:94.84ms
step:535/1750 train_time:50745ms step_avg:94.85ms
step:536/1750 train_time:50841ms step_avg:94.85ms
step:537/1750 train_time:50937ms step_avg:94.86ms
step:538/1750 train_time:51033ms step_avg:94.86ms
step:539/1750 train_time:51129ms step_avg:94.86ms
step:540/1750 train_time:51225ms step_avg:94.86ms
step:541/1750 train_time:51321ms step_avg:94.86ms
step:542/1750 train_time:51417ms step_avg:94.87ms
step:543/1750 train_time:51513ms step_avg:94.87ms
step:544/1750 train_time:51611ms step_avg:94.87ms
step:545/1750 train_time:51709ms step_avg:94.88ms
step:546/1750 train_time:51805ms step_avg:94.88ms
step:547/1750 train_time:51902ms step_avg:94.88ms
step:548/1750 train_time:51999ms step_avg:94.89ms
step:549/1750 train_time:52095ms step_avg:94.89ms
step:550/1750 train_time:52191ms step_avg:94.89ms
step:551/1750 train_time:52287ms step_avg:94.89ms
step:552/1750 train_time:52383ms step_avg:94.90ms
step:553/1750 train_time:52479ms step_avg:94.90ms
step:554/1750 train_time:52575ms step_avg:94.90ms
step:555/1750 train_time:52673ms step_avg:94.91ms
step:556/1750 train_time:52770ms step_avg:94.91ms
step:557/1750 train_time:52867ms step_avg:94.91ms
step:558/1750 train_time:52964ms step_avg:94.92ms
step:559/1750 train_time:53060ms step_avg:94.92ms
step:560/1750 train_time:53155ms step_avg:94.92ms
step:561/1750 train_time:53251ms step_avg:94.92ms
step:562/1750 train_time:53346ms step_avg:94.92ms
step:563/1750 train_time:53442ms step_avg:94.92ms
step:564/1750 train_time:53538ms step_avg:94.93ms
step:565/1750 train_time:53635ms step_avg:94.93ms
step:566/1750 train_time:53732ms step_avg:94.93ms
step:567/1750 train_time:53829ms step_avg:94.94ms
step:568/1750 train_time:53925ms step_avg:94.94ms
step:569/1750 train_time:54022ms step_avg:94.94ms
step:570/1750 train_time:54119ms step_avg:94.95ms
step:571/1750 train_time:54214ms step_avg:94.95ms
step:572/1750 train_time:54310ms step_avg:94.95ms
step:573/1750 train_time:54407ms step_avg:94.95ms
step:574/1750 train_time:54504ms step_avg:94.96ms
step:575/1750 train_time:54601ms step_avg:94.96ms
step:576/1750 train_time:54698ms step_avg:94.96ms
step:577/1750 train_time:54794ms step_avg:94.96ms
step:578/1750 train_time:54891ms step_avg:94.97ms
step:579/1750 train_time:54987ms step_avg:94.97ms
step:580/1750 train_time:55084ms step_avg:94.97ms
step:581/1750 train_time:55180ms step_avg:94.97ms
step:582/1750 train_time:55275ms step_avg:94.97ms
step:583/1750 train_time:55372ms step_avg:94.98ms
step:584/1750 train_time:55468ms step_avg:94.98ms
step:585/1750 train_time:55565ms step_avg:94.98ms
step:586/1750 train_time:55662ms step_avg:94.99ms
step:587/1750 train_time:55759ms step_avg:94.99ms
step:588/1750 train_time:55855ms step_avg:94.99ms
step:589/1750 train_time:55951ms step_avg:94.99ms
step:590/1750 train_time:56048ms step_avg:95.00ms
step:591/1750 train_time:56145ms step_avg:95.00ms
step:592/1750 train_time:56242ms step_avg:95.00ms
step:593/1750 train_time:56338ms step_avg:95.01ms
step:594/1750 train_time:56435ms step_avg:95.01ms
step:595/1750 train_time:56531ms step_avg:95.01ms
step:596/1750 train_time:56629ms step_avg:95.01ms
step:597/1750 train_time:56725ms step_avg:95.02ms
step:598/1750 train_time:56823ms step_avg:95.02ms
step:599/1750 train_time:56919ms step_avg:95.02ms
step:600/1750 train_time:57015ms step_avg:95.03ms
step:601/1750 train_time:57111ms step_avg:95.03ms
step:602/1750 train_time:57208ms step_avg:95.03ms
step:603/1750 train_time:57305ms step_avg:95.03ms
step:604/1750 train_time:57402ms step_avg:95.04ms
step:605/1750 train_time:57498ms step_avg:95.04ms
step:606/1750 train_time:57595ms step_avg:95.04ms
step:607/1750 train_time:57691ms step_avg:95.04ms
step:608/1750 train_time:57787ms step_avg:95.04ms
step:609/1750 train_time:57883ms step_avg:95.05ms
step:610/1750 train_time:57980ms step_avg:95.05ms
step:611/1750 train_time:58076ms step_avg:95.05ms
step:612/1750 train_time:58172ms step_avg:95.05ms
step:613/1750 train_time:58269ms step_avg:95.06ms
step:614/1750 train_time:58367ms step_avg:95.06ms
step:615/1750 train_time:58464ms step_avg:95.06ms
step:616/1750 train_time:58560ms step_avg:95.07ms
step:617/1750 train_time:58656ms step_avg:95.07ms
step:618/1750 train_time:58752ms step_avg:95.07ms
step:619/1750 train_time:58849ms step_avg:95.07ms
step:620/1750 train_time:58945ms step_avg:95.07ms
step:621/1750 train_time:59043ms step_avg:95.08ms
step:622/1750 train_time:59140ms step_avg:95.08ms
step:623/1750 train_time:59237ms step_avg:95.08ms
step:624/1750 train_time:59333ms step_avg:95.09ms
step:625/1750 train_time:59431ms step_avg:95.09ms
step:625/1750 val_loss:3.6614 train_time:59517ms step_avg:95.23ms
step:626/1750 train_time:59539ms step_avg:95.11ms
step:627/1750 train_time:59635ms step_avg:95.11ms
step:628/1750 train_time:59733ms step_avg:95.12ms
step:629/1750 train_time:59831ms step_avg:95.12ms
step:630/1750 train_time:59926ms step_avg:95.12ms
step:631/1750 train_time:60022ms step_avg:95.12ms
step:632/1750 train_time:60118ms step_avg:95.12ms
step:633/1750 train_time:60212ms step_avg:95.12ms
step:634/1750 train_time:60308ms step_avg:95.12ms
step:635/1750 train_time:60403ms step_avg:95.12ms
step:636/1750 train_time:60500ms step_avg:95.13ms
step:637/1750 train_time:60599ms step_avg:95.13ms
step:638/1750 train_time:60696ms step_avg:95.13ms
step:639/1750 train_time:60793ms step_avg:95.14ms
step:640/1750 train_time:60891ms step_avg:95.14ms
step:641/1750 train_time:60988ms step_avg:95.14ms
step:642/1750 train_time:61083ms step_avg:95.14ms
step:643/1750 train_time:61179ms step_avg:95.15ms
step:644/1750 train_time:61275ms step_avg:95.15ms
step:645/1750 train_time:61370ms step_avg:95.15ms
step:646/1750 train_time:61467ms step_avg:95.15ms
step:647/1750 train_time:61564ms step_avg:95.15ms
step:648/1750 train_time:61662ms step_avg:95.16ms
step:649/1750 train_time:61759ms step_avg:95.16ms
step:650/1750 train_time:61855ms step_avg:95.16ms
step:651/1750 train_time:61953ms step_avg:95.17ms
step:652/1750 train_time:62052ms step_avg:95.17ms
step:653/1750 train_time:62150ms step_avg:95.18ms
step:654/1750 train_time:62248ms step_avg:95.18ms
step:655/1750 train_time:62344ms step_avg:95.18ms
step:656/1750 train_time:62441ms step_avg:95.18ms
step:657/1750 train_time:62540ms step_avg:95.19ms
step:658/1750 train_time:62638ms step_avg:95.19ms
step:659/1750 train_time:62737ms step_avg:95.20ms
step:660/1750 train_time:62835ms step_avg:95.20ms
step:661/1750 train_time:62932ms step_avg:95.21ms
step:662/1750 train_time:63030ms step_avg:95.21ms
step:663/1750 train_time:63127ms step_avg:95.21ms
step:664/1750 train_time:63225ms step_avg:95.22ms
step:665/1750 train_time:63322ms step_avg:95.22ms
step:666/1750 train_time:63419ms step_avg:95.22ms
step:667/1750 train_time:63517ms step_avg:95.23ms
step:668/1750 train_time:63615ms step_avg:95.23ms
step:669/1750 train_time:63714ms step_avg:95.24ms
step:670/1750 train_time:63812ms step_avg:95.24ms
step:671/1750 train_time:63910ms step_avg:95.25ms
step:672/1750 train_time:64007ms step_avg:95.25ms
step:673/1750 train_time:64104ms step_avg:95.25ms
step:674/1750 train_time:64203ms step_avg:95.26ms
step:675/1750 train_time:64301ms step_avg:95.26ms
step:676/1750 train_time:64399ms step_avg:95.26ms
step:677/1750 train_time:64497ms step_avg:95.27ms
step:678/1750 train_time:64594ms step_avg:95.27ms
step:679/1750 train_time:64692ms step_avg:95.28ms
step:680/1750 train_time:64791ms step_avg:95.28ms
step:681/1750 train_time:64888ms step_avg:95.28ms
step:682/1750 train_time:64986ms step_avg:95.29ms
step:683/1750 train_time:65085ms step_avg:95.29ms
step:684/1750 train_time:65184ms step_avg:95.30ms
step:685/1750 train_time:65282ms step_avg:95.30ms
step:686/1750 train_time:65379ms step_avg:95.31ms
step:687/1750 train_time:65477ms step_avg:95.31ms
step:688/1750 train_time:65574ms step_avg:95.31ms
step:689/1750 train_time:65672ms step_avg:95.32ms
step:690/1750 train_time:65770ms step_avg:95.32ms
step:691/1750 train_time:65869ms step_avg:95.32ms
step:692/1750 train_time:65967ms step_avg:95.33ms
step:693/1750 train_time:66065ms step_avg:95.33ms
step:694/1750 train_time:66163ms step_avg:95.34ms
step:695/1750 train_time:66260ms step_avg:95.34ms
step:696/1750 train_time:66358ms step_avg:95.34ms
step:697/1750 train_time:66456ms step_avg:95.35ms
step:698/1750 train_time:66553ms step_avg:95.35ms
step:699/1750 train_time:66651ms step_avg:95.35ms
step:700/1750 train_time:66749ms step_avg:95.36ms
step:701/1750 train_time:66846ms step_avg:95.36ms
step:702/1750 train_time:66944ms step_avg:95.36ms
step:703/1750 train_time:67042ms step_avg:95.36ms
step:704/1750 train_time:67139ms step_avg:95.37ms
step:705/1750 train_time:67237ms step_avg:95.37ms
step:706/1750 train_time:67334ms step_avg:95.37ms
step:707/1750 train_time:67432ms step_avg:95.38ms
step:708/1750 train_time:67531ms step_avg:95.38ms
step:709/1750 train_time:67629ms step_avg:95.39ms
step:710/1750 train_time:67728ms step_avg:95.39ms
step:711/1750 train_time:67826ms step_avg:95.39ms
step:712/1750 train_time:67924ms step_avg:95.40ms
step:713/1750 train_time:68022ms step_avg:95.40ms
step:714/1750 train_time:68119ms step_avg:95.41ms
step:715/1750 train_time:68217ms step_avg:95.41ms
step:716/1750 train_time:68314ms step_avg:95.41ms
step:717/1750 train_time:68412ms step_avg:95.41ms
step:718/1750 train_time:68510ms step_avg:95.42ms
step:719/1750 train_time:68608ms step_avg:95.42ms
step:720/1750 train_time:68705ms step_avg:95.42ms
step:721/1750 train_time:68804ms step_avg:95.43ms
step:722/1750 train_time:68901ms step_avg:95.43ms
step:723/1750 train_time:68999ms step_avg:95.43ms
step:724/1750 train_time:69097ms step_avg:95.44ms
step:725/1750 train_time:69195ms step_avg:95.44ms
step:726/1750 train_time:69293ms step_avg:95.44ms
step:727/1750 train_time:69391ms step_avg:95.45ms
step:728/1750 train_time:69489ms step_avg:95.45ms
step:729/1750 train_time:69587ms step_avg:95.45ms
step:730/1750 train_time:69684ms step_avg:95.46ms
step:731/1750 train_time:69781ms step_avg:95.46ms
step:732/1750 train_time:69879ms step_avg:95.46ms
step:733/1750 train_time:69976ms step_avg:95.47ms
step:734/1750 train_time:70074ms step_avg:95.47ms
step:735/1750 train_time:70172ms step_avg:95.47ms
step:736/1750 train_time:70270ms step_avg:95.48ms
step:737/1750 train_time:70368ms step_avg:95.48ms
step:738/1750 train_time:70466ms step_avg:95.48ms
step:739/1750 train_time:70563ms step_avg:95.48ms
step:740/1750 train_time:70661ms step_avg:95.49ms
step:741/1750 train_time:70759ms step_avg:95.49ms
step:742/1750 train_time:70856ms step_avg:95.49ms
step:743/1750 train_time:70954ms step_avg:95.50ms
step:744/1750 train_time:71052ms step_avg:95.50ms
step:745/1750 train_time:71150ms step_avg:95.50ms
step:746/1750 train_time:71249ms step_avg:95.51ms
step:747/1750 train_time:71347ms step_avg:95.51ms
step:748/1750 train_time:71444ms step_avg:95.51ms
step:749/1750 train_time:71543ms step_avg:95.52ms
step:750/1750 train_time:71641ms step_avg:95.52ms
step:750/1750 val_loss:3.5986 train_time:71727ms step_avg:95.64ms
step:751/1750 train_time:71749ms step_avg:95.54ms
step:752/1750 train_time:71849ms step_avg:95.54ms
step:753/1750 train_time:71949ms step_avg:95.55ms
step:754/1750 train_time:72048ms step_avg:95.55ms
step:755/1750 train_time:72144ms step_avg:95.56ms
step:756/1750 train_time:72241ms step_avg:95.56ms
step:757/1750 train_time:72339ms step_avg:95.56ms
step:758/1750 train_time:72435ms step_avg:95.56ms
step:759/1750 train_time:72532ms step_avg:95.56ms
step:760/1750 train_time:72629ms step_avg:95.56ms
step:761/1750 train_time:72728ms step_avg:95.57ms
step:762/1750 train_time:72829ms step_avg:95.58ms
step:763/1750 train_time:72929ms step_avg:95.58ms
step:764/1750 train_time:73027ms step_avg:95.58ms
step:765/1750 train_time:73124ms step_avg:95.59ms
step:766/1750 train_time:73221ms step_avg:95.59ms
step:767/1750 train_time:73319ms step_avg:95.59ms
step:768/1750 train_time:73415ms step_avg:95.59ms
step:769/1750 train_time:73512ms step_avg:95.59ms
step:770/1750 train_time:73609ms step_avg:95.60ms
step:771/1750 train_time:73707ms step_avg:95.60ms
step:772/1750 train_time:73806ms step_avg:95.60ms
step:773/1750 train_time:73906ms step_avg:95.61ms
step:774/1750 train_time:74006ms step_avg:95.61ms
step:775/1750 train_time:74104ms step_avg:95.62ms
step:776/1750 train_time:74201ms step_avg:95.62ms
step:777/1750 train_time:74298ms step_avg:95.62ms
step:778/1750 train_time:74394ms step_avg:95.62ms
step:779/1750 train_time:74492ms step_avg:95.62ms
step:780/1750 train_time:74590ms step_avg:95.63ms
step:781/1750 train_time:74688ms step_avg:95.63ms
step:782/1750 train_time:74787ms step_avg:95.64ms
step:783/1750 train_time:74886ms step_avg:95.64ms
step:784/1750 train_time:74985ms step_avg:95.64ms
step:785/1750 train_time:75084ms step_avg:95.65ms
step:786/1750 train_time:75183ms step_avg:95.65ms
step:787/1750 train_time:75282ms step_avg:95.66ms
step:788/1750 train_time:75380ms step_avg:95.66ms
step:789/1750 train_time:75477ms step_avg:95.66ms
step:790/1750 train_time:75574ms step_avg:95.66ms
step:791/1750 train_time:75671ms step_avg:95.67ms
step:792/1750 train_time:75769ms step_avg:95.67ms
step:793/1750 train_time:75868ms step_avg:95.67ms
step:794/1750 train_time:75967ms step_avg:95.68ms
step:795/1750 train_time:76065ms step_avg:95.68ms
step:796/1750 train_time:76166ms step_avg:95.69ms
step:797/1750 train_time:76264ms step_avg:95.69ms
step:798/1750 train_time:76363ms step_avg:95.69ms
step:799/1750 train_time:76461ms step_avg:95.70ms
step:800/1750 train_time:76559ms step_avg:95.70ms
step:801/1750 train_time:76656ms step_avg:95.70ms
step:802/1750 train_time:76754ms step_avg:95.70ms
step:803/1750 train_time:76852ms step_avg:95.71ms
step:804/1750 train_time:76951ms step_avg:95.71ms
step:805/1750 train_time:77049ms step_avg:95.71ms
step:806/1750 train_time:77147ms step_avg:95.72ms
step:807/1750 train_time:77245ms step_avg:95.72ms
step:808/1750 train_time:77344ms step_avg:95.72ms
step:809/1750 train_time:77442ms step_avg:95.73ms
step:810/1750 train_time:77540ms step_avg:95.73ms
step:811/1750 train_time:77638ms step_avg:95.73ms
step:812/1750 train_time:77736ms step_avg:95.73ms
step:813/1750 train_time:77835ms step_avg:95.74ms
step:814/1750 train_time:77933ms step_avg:95.74ms
step:815/1750 train_time:78032ms step_avg:95.74ms
step:816/1750 train_time:78130ms step_avg:95.75ms
step:817/1750 train_time:78229ms step_avg:95.75ms
step:818/1750 train_time:78327ms step_avg:95.75ms
step:819/1750 train_time:78427ms step_avg:95.76ms
step:820/1750 train_time:78525ms step_avg:95.76ms
step:821/1750 train_time:78624ms step_avg:95.77ms
step:822/1750 train_time:78723ms step_avg:95.77ms
step:823/1750 train_time:78821ms step_avg:95.77ms
step:824/1750 train_time:78920ms step_avg:95.78ms
step:825/1750 train_time:79019ms step_avg:95.78ms
step:826/1750 train_time:79118ms step_avg:95.78ms
step:827/1750 train_time:79216ms step_avg:95.79ms
step:828/1750 train_time:79313ms step_avg:95.79ms
step:829/1750 train_time:79411ms step_avg:95.79ms
step:830/1750 train_time:79511ms step_avg:95.80ms
step:831/1750 train_time:79610ms step_avg:95.80ms
step:832/1750 train_time:79709ms step_avg:95.80ms
step:833/1750 train_time:79807ms step_avg:95.81ms
step:834/1750 train_time:79906ms step_avg:95.81ms
step:835/1750 train_time:80006ms step_avg:95.82ms
step:836/1750 train_time:80105ms step_avg:95.82ms
step:837/1750 train_time:80204ms step_avg:95.82ms
step:838/1750 train_time:80303ms step_avg:95.83ms
step:839/1750 train_time:80401ms step_avg:95.83ms
step:840/1750 train_time:80500ms step_avg:95.83ms
step:841/1750 train_time:80598ms step_avg:95.84ms
step:842/1750 train_time:80698ms step_avg:95.84ms
step:843/1750 train_time:80796ms step_avg:95.84ms
step:844/1750 train_time:80894ms step_avg:95.85ms
step:845/1750 train_time:80992ms step_avg:95.85ms
step:846/1750 train_time:81090ms step_avg:95.85ms
step:847/1750 train_time:81189ms step_avg:95.85ms
step:848/1750 train_time:81287ms step_avg:95.86ms
step:849/1750 train_time:81385ms step_avg:95.86ms
step:850/1750 train_time:81484ms step_avg:95.86ms
step:851/1750 train_time:81581ms step_avg:95.87ms
step:852/1750 train_time:81679ms step_avg:95.87ms
step:853/1750 train_time:81776ms step_avg:95.87ms
step:854/1750 train_time:81874ms step_avg:95.87ms
step:855/1750 train_time:81972ms step_avg:95.87ms
step:856/1750 train_time:82070ms step_avg:95.88ms
step:857/1750 train_time:82169ms step_avg:95.88ms
step:858/1750 train_time:82267ms step_avg:95.88ms
step:859/1750 train_time:82366ms step_avg:95.89ms
step:860/1750 train_time:82464ms step_avg:95.89ms
step:861/1750 train_time:82562ms step_avg:95.89ms
step:862/1750 train_time:82659ms step_avg:95.89ms
step:863/1750 train_time:82757ms step_avg:95.89ms
step:864/1750 train_time:82855ms step_avg:95.90ms
step:865/1750 train_time:82952ms step_avg:95.90ms
step:866/1750 train_time:83051ms step_avg:95.90ms
step:867/1750 train_time:83149ms step_avg:95.90ms
step:868/1750 train_time:83247ms step_avg:95.91ms
step:869/1750 train_time:83345ms step_avg:95.91ms
step:870/1750 train_time:83443ms step_avg:95.91ms
step:871/1750 train_time:83542ms step_avg:95.92ms
step:872/1750 train_time:83640ms step_avg:95.92ms
step:873/1750 train_time:83739ms step_avg:95.92ms
step:874/1750 train_time:83836ms step_avg:95.92ms
step:875/1750 train_time:83934ms step_avg:95.92ms
step:875/1750 val_loss:3.5505 train_time:84021ms step_avg:96.02ms
step:876/1750 train_time:84042ms step_avg:95.94ms
step:877/1750 train_time:84139ms step_avg:95.94ms
step:878/1750 train_time:84237ms step_avg:95.94ms
step:879/1750 train_time:84336ms step_avg:95.95ms
step:880/1750 train_time:84433ms step_avg:95.95ms
step:881/1750 train_time:84531ms step_avg:95.95ms
step:882/1750 train_time:84628ms step_avg:95.95ms
step:883/1750 train_time:84726ms step_avg:95.95ms
step:884/1750 train_time:84824ms step_avg:95.95ms
step:885/1750 train_time:84921ms step_avg:95.96ms
step:886/1750 train_time:85021ms step_avg:95.96ms
step:887/1750 train_time:85121ms step_avg:95.96ms
step:888/1750 train_time:85219ms step_avg:95.97ms
step:889/1750 train_time:85318ms step_avg:95.97ms
step:890/1750 train_time:85416ms step_avg:95.97ms
step:891/1750 train_time:85513ms step_avg:95.97ms
step:892/1750 train_time:85610ms step_avg:95.98ms
step:893/1750 train_time:85708ms step_avg:95.98ms
step:894/1750 train_time:85805ms step_avg:95.98ms
step:895/1750 train_time:85904ms step_avg:95.98ms
step:896/1750 train_time:86002ms step_avg:95.98ms
step:897/1750 train_time:86099ms step_avg:95.99ms
step:898/1750 train_time:86198ms step_avg:95.99ms
step:899/1750 train_time:86296ms step_avg:95.99ms
step:900/1750 train_time:86394ms step_avg:95.99ms
step:901/1750 train_time:86492ms step_avg:96.00ms
step:902/1750 train_time:86590ms step_avg:96.00ms
step:903/1750 train_time:86687ms step_avg:96.00ms
step:904/1750 train_time:86785ms step_avg:96.00ms
step:905/1750 train_time:86882ms step_avg:96.00ms
step:906/1750 train_time:86980ms step_avg:96.00ms
step:907/1750 train_time:87078ms step_avg:96.01ms
step:908/1750 train_time:87176ms step_avg:96.01ms
step:909/1750 train_time:87275ms step_avg:96.01ms
step:910/1750 train_time:87375ms step_avg:96.02ms
step:911/1750 train_time:87474ms step_avg:96.02ms
step:912/1750 train_time:87573ms step_avg:96.02ms
step:913/1750 train_time:87673ms step_avg:96.03ms
step:914/1750 train_time:87773ms step_avg:96.03ms
step:915/1750 train_time:87873ms step_avg:96.04ms
step:916/1750 train_time:87974ms step_avg:96.04ms
step:917/1750 train_time:88075ms step_avg:96.05ms
step:918/1750 train_time:88175ms step_avg:96.05ms
step:919/1750 train_time:88275ms step_avg:96.06ms
step:920/1750 train_time:88374ms step_avg:96.06ms
step:921/1750 train_time:88474ms step_avg:96.06ms
step:922/1750 train_time:88573ms step_avg:96.07ms
step:923/1750 train_time:88673ms step_avg:96.07ms
step:924/1750 train_time:88772ms step_avg:96.07ms
step:925/1750 train_time:88872ms step_avg:96.08ms
step:926/1750 train_time:88972ms step_avg:96.08ms
step:927/1750 train_time:89072ms step_avg:96.09ms
step:928/1750 train_time:89172ms step_avg:96.09ms
step:929/1750 train_time:89272ms step_avg:96.09ms
step:930/1750 train_time:89372ms step_avg:96.10ms
step:931/1750 train_time:89472ms step_avg:96.10ms
step:932/1750 train_time:89571ms step_avg:96.11ms
step:933/1750 train_time:89672ms step_avg:96.11ms
step:934/1750 train_time:89771ms step_avg:96.11ms
step:935/1750 train_time:89871ms step_avg:96.12ms
step:936/1750 train_time:89971ms step_avg:96.12ms
step:937/1750 train_time:90071ms step_avg:96.13ms
step:938/1750 train_time:90171ms step_avg:96.13ms
step:939/1750 train_time:90271ms step_avg:96.13ms
step:940/1750 train_time:90370ms step_avg:96.14ms
step:941/1750 train_time:90471ms step_avg:96.14ms
step:942/1750 train_time:90570ms step_avg:96.15ms
step:943/1750 train_time:90669ms step_avg:96.15ms
step:944/1750 train_time:90768ms step_avg:96.15ms
step:945/1750 train_time:90868ms step_avg:96.16ms
step:946/1750 train_time:90969ms step_avg:96.16ms
step:947/1750 train_time:91068ms step_avg:96.16ms
step:948/1750 train_time:91167ms step_avg:96.17ms
step:949/1750 train_time:91268ms step_avg:96.17ms
step:950/1750 train_time:91368ms step_avg:96.18ms
step:951/1750 train_time:91468ms step_avg:96.18ms
step:952/1750 train_time:91568ms step_avg:96.18ms
step:953/1750 train_time:91667ms step_avg:96.19ms
step:954/1750 train_time:91766ms step_avg:96.19ms
step:955/1750 train_time:91864ms step_avg:96.19ms
step:956/1750 train_time:91964ms step_avg:96.20ms
step:957/1750 train_time:92065ms step_avg:96.20ms
step:958/1750 train_time:92165ms step_avg:96.21ms
step:959/1750 train_time:92264ms step_avg:96.21ms
step:960/1750 train_time:92364ms step_avg:96.21ms
step:961/1750 train_time:92463ms step_avg:96.22ms
step:962/1750 train_time:92562ms step_avg:96.22ms
step:963/1750 train_time:92662ms step_avg:96.22ms
step:964/1750 train_time:92761ms step_avg:96.23ms
step:965/1750 train_time:92861ms step_avg:96.23ms
step:966/1750 train_time:92959ms step_avg:96.23ms
step:967/1750 train_time:93058ms step_avg:96.23ms
step:968/1750 train_time:93158ms step_avg:96.24ms
step:969/1750 train_time:93258ms step_avg:96.24ms
step:970/1750 train_time:93358ms step_avg:96.25ms
step:971/1750 train_time:93458ms step_avg:96.25ms
step:972/1750 train_time:93557ms step_avg:96.25ms
step:973/1750 train_time:93657ms step_avg:96.26ms
step:974/1750 train_time:93756ms step_avg:96.26ms
step:975/1750 train_time:93855ms step_avg:96.26ms
step:976/1750 train_time:93955ms step_avg:96.27ms
step:977/1750 train_time:94055ms step_avg:96.27ms
step:978/1750 train_time:94155ms step_avg:96.27ms
step:979/1750 train_time:94255ms step_avg:96.28ms
step:980/1750 train_time:94355ms step_avg:96.28ms
step:981/1750 train_time:94455ms step_avg:96.28ms
step:982/1750 train_time:94556ms step_avg:96.29ms
step:983/1750 train_time:94655ms step_avg:96.29ms
step:984/1750 train_time:94755ms step_avg:96.30ms
step:985/1750 train_time:94855ms step_avg:96.30ms
step:986/1750 train_time:94955ms step_avg:96.30ms
step:987/1750 train_time:95055ms step_avg:96.31ms
step:988/1750 train_time:95155ms step_avg:96.31ms
step:989/1750 train_time:95255ms step_avg:96.31ms
step:990/1750 train_time:95355ms step_avg:96.32ms
step:991/1750 train_time:95455ms step_avg:96.32ms
step:992/1750 train_time:95555ms step_avg:96.33ms
step:993/1750 train_time:95655ms step_avg:96.33ms
step:994/1750 train_time:95755ms step_avg:96.33ms
step:995/1750 train_time:95855ms step_avg:96.34ms
step:996/1750 train_time:95956ms step_avg:96.34ms
step:997/1750 train_time:96055ms step_avg:96.34ms
step:998/1750 train_time:96155ms step_avg:96.35ms
step:999/1750 train_time:96254ms step_avg:96.35ms
step:1000/1750 train_time:96354ms step_avg:96.35ms
step:1000/1750 val_loss:3.5070 train_time:96443ms step_avg:96.44ms
step:1001/1750 train_time:96464ms step_avg:96.37ms
step:1002/1750 train_time:96561ms step_avg:96.37ms
step:1003/1750 train_time:96660ms step_avg:96.37ms
step:1004/1750 train_time:96759ms step_avg:96.37ms
step:1005/1750 train_time:96858ms step_avg:96.38ms
step:1006/1750 train_time:96956ms step_avg:96.38ms
step:1007/1750 train_time:97054ms step_avg:96.38ms
step:1008/1750 train_time:97152ms step_avg:96.38ms
step:1009/1750 train_time:97251ms step_avg:96.38ms
step:1010/1750 train_time:97350ms step_avg:96.39ms
step:1011/1750 train_time:97452ms step_avg:96.39ms
step:1012/1750 train_time:97552ms step_avg:96.39ms
step:1013/1750 train_time:97651ms step_avg:96.40ms
step:1014/1750 train_time:97749ms step_avg:96.40ms
step:1015/1750 train_time:97848ms step_avg:96.40ms
step:1016/1750 train_time:97948ms step_avg:96.41ms
step:1017/1750 train_time:98048ms step_avg:96.41ms
step:1018/1750 train_time:98147ms step_avg:96.41ms
step:1019/1750 train_time:98247ms step_avg:96.41ms
step:1020/1750 train_time:98346ms step_avg:96.42ms
step:1021/1750 train_time:98447ms step_avg:96.42ms
step:1022/1750 train_time:98547ms step_avg:96.43ms
step:1023/1750 train_time:98648ms step_avg:96.43ms
step:1024/1750 train_time:98748ms step_avg:96.43ms
step:1025/1750 train_time:98848ms step_avg:96.44ms
step:1026/1750 train_time:98948ms step_avg:96.44ms
step:1027/1750 train_time:99048ms step_avg:96.44ms
step:1028/1750 train_time:99147ms step_avg:96.45ms
step:1029/1750 train_time:99246ms step_avg:96.45ms
step:1030/1750 train_time:99346ms step_avg:96.45ms
step:1031/1750 train_time:99446ms step_avg:96.46ms
step:1032/1750 train_time:99546ms step_avg:96.46ms
step:1033/1750 train_time:99646ms step_avg:96.46ms
step:1034/1750 train_time:99746ms step_avg:96.47ms
step:1035/1750 train_time:99846ms step_avg:96.47ms
step:1036/1750 train_time:99946ms step_avg:96.47ms
step:1037/1750 train_time:100045ms step_avg:96.48ms
step:1038/1750 train_time:100145ms step_avg:96.48ms
step:1039/1750 train_time:100244ms step_avg:96.48ms
step:1040/1750 train_time:100345ms step_avg:96.49ms
step:1041/1750 train_time:100445ms step_avg:96.49ms
step:1042/1750 train_time:100545ms step_avg:96.49ms
step:1043/1750 train_time:100645ms step_avg:96.50ms
step:1044/1750 train_time:100745ms step_avg:96.50ms
step:1045/1750 train_time:100845ms step_avg:96.50ms
step:1046/1750 train_time:100945ms step_avg:96.51ms
step:1047/1750 train_time:101045ms step_avg:96.51ms
step:1048/1750 train_time:101145ms step_avg:96.51ms
step:1049/1750 train_time:101245ms step_avg:96.52ms
step:1050/1750 train_time:101345ms step_avg:96.52ms
step:1051/1750 train_time:101445ms step_avg:96.52ms
step:1052/1750 train_time:101545ms step_avg:96.53ms
step:1053/1750 train_time:101645ms step_avg:96.53ms
step:1054/1750 train_time:101745ms step_avg:96.53ms
step:1055/1750 train_time:101846ms step_avg:96.54ms
step:1056/1750 train_time:101946ms step_avg:96.54ms
step:1057/1750 train_time:102046ms step_avg:96.54ms
step:1058/1750 train_time:102146ms step_avg:96.55ms
step:1059/1750 train_time:102246ms step_avg:96.55ms
step:1060/1750 train_time:102346ms step_avg:96.55ms
step:1061/1750 train_time:102446ms step_avg:96.56ms
step:1062/1750 train_time:102546ms step_avg:96.56ms
step:1063/1750 train_time:102646ms step_avg:96.56ms
step:1064/1750 train_time:102747ms step_avg:96.57ms
step:1065/1750 train_time:102848ms step_avg:96.57ms
step:1066/1750 train_time:102947ms step_avg:96.57ms
step:1067/1750 train_time:103048ms step_avg:96.58ms
step:1068/1750 train_time:103147ms step_avg:96.58ms
step:1069/1750 train_time:103247ms step_avg:96.58ms
step:1070/1750 train_time:103347ms step_avg:96.59ms
step:1071/1750 train_time:103447ms step_avg:96.59ms
step:1072/1750 train_time:103549ms step_avg:96.59ms
step:1073/1750 train_time:103648ms step_avg:96.60ms
step:1074/1750 train_time:103748ms step_avg:96.60ms
step:1075/1750 train_time:103848ms step_avg:96.60ms
step:1076/1750 train_time:103948ms step_avg:96.61ms
step:1077/1750 train_time:104049ms step_avg:96.61ms
step:1078/1750 train_time:104148ms step_avg:96.61ms
step:1079/1750 train_time:104247ms step_avg:96.61ms
step:1080/1750 train_time:104347ms step_avg:96.62ms
step:1081/1750 train_time:104447ms step_avg:96.62ms
step:1082/1750 train_time:104547ms step_avg:96.62ms
step:1083/1750 train_time:104647ms step_avg:96.63ms
step:1084/1750 train_time:104747ms step_avg:96.63ms
step:1085/1750 train_time:104846ms step_avg:96.63ms
step:1086/1750 train_time:104946ms step_avg:96.64ms
step:1087/1750 train_time:105047ms step_avg:96.64ms
step:1088/1750 train_time:105147ms step_avg:96.64ms
step:1089/1750 train_time:105246ms step_avg:96.64ms
step:1090/1750 train_time:105347ms step_avg:96.65ms
step:1091/1750 train_time:105447ms step_avg:96.65ms
step:1092/1750 train_time:105547ms step_avg:96.65ms
step:1093/1750 train_time:105646ms step_avg:96.66ms
step:1094/1750 train_time:105746ms step_avg:96.66ms
step:1095/1750 train_time:105846ms step_avg:96.66ms
step:1096/1750 train_time:105946ms step_avg:96.67ms
step:1097/1750 train_time:106047ms step_avg:96.67ms
step:1098/1750 train_time:106148ms step_avg:96.67ms
step:1099/1750 train_time:106247ms step_avg:96.68ms
step:1100/1750 train_time:106347ms step_avg:96.68ms
step:1101/1750 train_time:106446ms step_avg:96.68ms
step:1102/1750 train_time:106546ms step_avg:96.68ms
step:1103/1750 train_time:106647ms step_avg:96.69ms
step:1104/1750 train_time:106747ms step_avg:96.69ms
step:1105/1750 train_time:106847ms step_avg:96.69ms
step:1106/1750 train_time:106948ms step_avg:96.70ms
step:1107/1750 train_time:107048ms step_avg:96.70ms
step:1108/1750 train_time:107148ms step_avg:96.70ms
step:1109/1750 train_time:107247ms step_avg:96.71ms
step:1110/1750 train_time:107347ms step_avg:96.71ms
step:1111/1750 train_time:107447ms step_avg:96.71ms
step:1112/1750 train_time:107548ms step_avg:96.72ms
step:1113/1750 train_time:107648ms step_avg:96.72ms
step:1114/1750 train_time:107748ms step_avg:96.72ms
step:1115/1750 train_time:107848ms step_avg:96.72ms
step:1116/1750 train_time:107948ms step_avg:96.73ms
step:1117/1750 train_time:108048ms step_avg:96.73ms
step:1118/1750 train_time:108148ms step_avg:96.73ms
step:1119/1750 train_time:108247ms step_avg:96.74ms
step:1120/1750 train_time:108347ms step_avg:96.74ms
step:1121/1750 train_time:108448ms step_avg:96.74ms
step:1122/1750 train_time:108547ms step_avg:96.74ms
step:1123/1750 train_time:108647ms step_avg:96.75ms
step:1124/1750 train_time:108747ms step_avg:96.75ms
step:1125/1750 train_time:108847ms step_avg:96.75ms
step:1125/1750 val_loss:3.4579 train_time:108936ms step_avg:96.83ms
step:1126/1750 train_time:108957ms step_avg:96.76ms
step:1127/1750 train_time:109058ms step_avg:96.77ms
step:1128/1750 train_time:109158ms step_avg:96.77ms
step:1129/1750 train_time:109258ms step_avg:96.77ms
step:1130/1750 train_time:109358ms step_avg:96.78ms
step:1131/1750 train_time:109457ms step_avg:96.78ms
step:1132/1750 train_time:109556ms step_avg:96.78ms
step:1133/1750 train_time:109655ms step_avg:96.78ms
step:1134/1750 train_time:109754ms step_avg:96.78ms
step:1135/1750 train_time:109854ms step_avg:96.79ms
step:1136/1750 train_time:109957ms step_avg:96.79ms
step:1137/1750 train_time:110059ms step_avg:96.80ms
step:1138/1750 train_time:110159ms step_avg:96.80ms
step:1139/1750 train_time:110260ms step_avg:96.80ms
step:1140/1750 train_time:110359ms step_avg:96.81ms
step:1141/1750 train_time:110458ms step_avg:96.81ms
step:1142/1750 train_time:110557ms step_avg:96.81ms
step:1143/1750 train_time:110656ms step_avg:96.81ms
step:1144/1750 train_time:110755ms step_avg:96.81ms
step:1145/1750 train_time:110855ms step_avg:96.82ms
step:1146/1750 train_time:110957ms step_avg:96.82ms
step:1147/1750 train_time:111058ms step_avg:96.82ms
step:1148/1750 train_time:111159ms step_avg:96.83ms
step:1149/1750 train_time:111259ms step_avg:96.83ms
step:1150/1750 train_time:111359ms step_avg:96.83ms
step:1151/1750 train_time:111458ms step_avg:96.84ms
step:1152/1750 train_time:111557ms step_avg:96.84ms
step:1153/1750 train_time:111656ms step_avg:96.84ms
step:1154/1750 train_time:111755ms step_avg:96.84ms
step:1155/1750 train_time:111855ms step_avg:96.84ms
step:1156/1750 train_time:111955ms step_avg:96.85ms
step:1157/1750 train_time:112056ms step_avg:96.85ms
step:1158/1750 train_time:112456ms step_avg:97.11ms
step:1159/1750 train_time:112554ms step_avg:97.11ms
step:1160/1750 train_time:112909ms step_avg:97.34ms
step:1161/1750 train_time:113015ms step_avg:97.34ms
step:1162/1750 train_time:113112ms step_avg:97.34ms
step:1163/1750 train_time:113213ms step_avg:97.35ms
step:1164/1750 train_time:113313ms step_avg:97.35ms
step:1165/1750 train_time:113412ms step_avg:97.35ms
step:1166/1750 train_time:113511ms step_avg:97.35ms
step:1167/1750 train_time:113609ms step_avg:97.35ms
step:1168/1750 train_time:113709ms step_avg:97.35ms
step:1169/1750 train_time:113811ms step_avg:97.36ms
step:1170/1750 train_time:113915ms step_avg:97.36ms
step:1171/1750 train_time:114017ms step_avg:97.37ms
step:1172/1750 train_time:114119ms step_avg:97.37ms
step:1173/1750 train_time:114219ms step_avg:97.37ms
step:1174/1750 train_time:114319ms step_avg:97.38ms
step:1175/1750 train_time:114420ms step_avg:97.38ms
step:1176/1750 train_time:114798ms step_avg:97.62ms
step:1177/1750 train_time:114896ms step_avg:97.62ms
step:1178/1750 train_time:114995ms step_avg:97.62ms
step:1179/1750 train_time:115096ms step_avg:97.62ms
step:1180/1750 train_time:115196ms step_avg:97.62ms
step:1181/1750 train_time:115296ms step_avg:97.63ms
step:1182/1750 train_time:115397ms step_avg:97.63ms
step:1183/1750 train_time:115496ms step_avg:97.63ms
step:1184/1750 train_time:115597ms step_avg:97.63ms
step:1185/1750 train_time:115705ms step_avg:97.64ms
step:1186/1750 train_time:115807ms step_avg:97.65ms
step:1187/1750 train_time:115907ms step_avg:97.65ms
step:1188/1750 train_time:116006ms step_avg:97.65ms
step:1189/1750 train_time:116105ms step_avg:97.65ms
step:1190/1750 train_time:116204ms step_avg:97.65ms
step:1191/1750 train_time:116306ms step_avg:97.65ms
step:1192/1750 train_time:116405ms step_avg:97.66ms
step:1193/1750 train_time:116505ms step_avg:97.66ms
step:1194/1750 train_time:116607ms step_avg:97.66ms
step:1195/1750 train_time:116710ms step_avg:97.66ms
step:1196/1750 train_time:116811ms step_avg:97.67ms
step:1197/1750 train_time:116915ms step_avg:97.67ms
step:1198/1750 train_time:117014ms step_avg:97.67ms
step:1199/1750 train_time:117115ms step_avg:97.68ms
step:1200/1750 train_time:117215ms step_avg:97.68ms
step:1201/1750 train_time:117316ms step_avg:97.68ms
step:1202/1750 train_time:117715ms step_avg:97.93ms
step:1203/1750 train_time:117815ms step_avg:97.93ms
step:1204/1750 train_time:117915ms step_avg:97.94ms
step:1205/1750 train_time:118014ms step_avg:97.94ms
step:1206/1750 train_time:118113ms step_avg:97.94ms
step:1207/1750 train_time:118214ms step_avg:97.94ms
step:1208/1750 train_time:118313ms step_avg:97.94ms
step:1209/1750 train_time:118413ms step_avg:97.94ms
step:1210/1750 train_time:118513ms step_avg:97.94ms
step:1211/1750 train_time:118616ms step_avg:97.95ms
step:1212/1750 train_time:118720ms step_avg:97.95ms
step:1213/1750 train_time:118821ms step_avg:97.96ms
step:1214/1750 train_time:118921ms step_avg:97.96ms
step:1215/1750 train_time:119329ms step_avg:98.21ms
step:1216/1750 train_time:119429ms step_avg:98.21ms
step:1217/1750 train_time:119528ms step_avg:98.22ms
step:1218/1750 train_time:119628ms step_avg:98.22ms
step:1219/1750 train_time:119727ms step_avg:98.22ms
step:1220/1750 train_time:119826ms step_avg:98.22ms
step:1221/1750 train_time:119925ms step_avg:98.22ms
step:1222/1750 train_time:120025ms step_avg:98.22ms
step:1223/1750 train_time:120124ms step_avg:98.22ms
step:1224/1750 train_time:120226ms step_avg:98.22ms
step:1225/1750 train_time:120331ms step_avg:98.23ms
step:1226/1750 train_time:120432ms step_avg:98.23ms
step:1227/1750 train_time:120533ms step_avg:98.23ms
step:1228/1750 train_time:120633ms step_avg:98.24ms
step:1229/1750 train_time:120733ms step_avg:98.24ms
step:1230/1750 train_time:120832ms step_avg:98.24ms
step:1231/1750 train_time:120934ms step_avg:98.24ms
step:1232/1750 train_time:121035ms step_avg:98.24ms
step:1233/1750 train_time:121135ms step_avg:98.24ms
step:1234/1750 train_time:121576ms step_avg:98.52ms
step:1235/1750 train_time:121639ms step_avg:98.49ms
step:1236/1750 train_time:121737ms step_avg:98.49ms
step:1237/1750 train_time:121838ms step_avg:98.49ms
step:1238/1750 train_time:121937ms step_avg:98.50ms
step:1239/1750 train_time:122037ms step_avg:98.50ms
step:1240/1750 train_time:122136ms step_avg:98.50ms
step:1241/1750 train_time:122237ms step_avg:98.50ms
step:1242/1750 train_time:122338ms step_avg:98.50ms
step:1243/1750 train_time:122438ms step_avg:98.50ms
step:1244/1750 train_time:122546ms step_avg:98.51ms
step:1245/1750 train_time:122648ms step_avg:98.51ms
step:1246/1750 train_time:122750ms step_avg:98.52ms
step:1247/1750 train_time:122851ms step_avg:98.52ms
step:1248/1750 train_time:122952ms step_avg:98.52ms
step:1249/1750 train_time:123053ms step_avg:98.52ms
step:1250/1750 train_time:123153ms step_avg:98.52ms
step:1250/1750 val_loss:3.4116 train_time:123241ms step_avg:98.59ms
step:1251/1750 train_time:123263ms step_avg:98.53ms
step:1252/1750 train_time:123362ms step_avg:98.53ms
step:1253/1750 train_time:123464ms step_avg:98.53ms
step:1254/1750 train_time:123564ms step_avg:98.54ms
step:1255/1750 train_time:123666ms step_avg:98.54ms
step:1256/1750 train_time:123767ms step_avg:98.54ms
step:1257/1750 train_time:123867ms step_avg:98.54ms
step:1258/1750 train_time:123968ms step_avg:98.54ms
step:1259/1750 train_time:124068ms step_avg:98.54ms
step:1260/1750 train_time:124168ms step_avg:98.55ms
step:1261/1750 train_time:124272ms step_avg:98.55ms
step:1262/1750 train_time:124374ms step_avg:98.55ms
step:1263/1750 train_time:124474ms step_avg:98.55ms
step:1264/1750 train_time:124574ms step_avg:98.56ms
step:1265/1750 train_time:124673ms step_avg:98.56ms
step:1266/1750 train_time:124772ms step_avg:98.56ms
step:1267/1750 train_time:124872ms step_avg:98.56ms
step:1268/1750 train_time:124972ms step_avg:98.56ms
step:1269/1750 train_time:125073ms step_avg:98.56ms
step:1270/1750 train_time:125173ms step_avg:98.56ms
step:1271/1750 train_time:125276ms step_avg:98.56ms
step:1272/1750 train_time:125376ms step_avg:98.57ms
step:1273/1750 train_time:125477ms step_avg:98.57ms
step:1274/1750 train_time:125577ms step_avg:98.57ms
step:1275/1750 train_time:125677ms step_avg:98.57ms
step:1276/1750 train_time:125780ms step_avg:98.57ms
step:1277/1750 train_time:125881ms step_avg:98.58ms
step:1278/1750 train_time:125982ms step_avg:98.58ms
step:1279/1750 train_time:126082ms step_avg:98.58ms
step:1280/1750 train_time:126182ms step_avg:98.58ms
step:1281/1750 train_time:126284ms step_avg:98.58ms
step:1282/1750 train_time:126385ms step_avg:98.58ms
step:1283/1750 train_time:126486ms step_avg:98.59ms
step:1284/1750 train_time:126587ms step_avg:98.59ms
step:1285/1750 train_time:126689ms step_avg:98.59ms
step:1286/1750 train_time:126789ms step_avg:98.59ms
step:1287/1750 train_time:126889ms step_avg:98.59ms
step:1288/1750 train_time:126990ms step_avg:98.59ms
step:1289/1750 train_time:127090ms step_avg:98.60ms
step:1290/1750 train_time:127190ms step_avg:98.60ms
step:1291/1750 train_time:127291ms step_avg:98.60ms
step:1292/1750 train_time:127391ms step_avg:98.60ms
step:1293/1750 train_time:127492ms step_avg:98.60ms
step:1294/1750 train_time:127593ms step_avg:98.60ms
step:1295/1750 train_time:127694ms step_avg:98.61ms
step:1296/1750 train_time:127794ms step_avg:98.61ms
step:1297/1750 train_time:127894ms step_avg:98.61ms
step:1298/1750 train_time:127993ms step_avg:98.61ms
step:1299/1750 train_time:128093ms step_avg:98.61ms
step:1300/1750 train_time:128194ms step_avg:98.61ms
step:1301/1750 train_time:128294ms step_avg:98.61ms
step:1302/1750 train_time:128394ms step_avg:98.61ms
step:1303/1750 train_time:128495ms step_avg:98.61ms
step:1304/1750 train_time:128596ms step_avg:98.62ms
step:1305/1750 train_time:128695ms step_avg:98.62ms
step:1306/1750 train_time:128796ms step_avg:98.62ms
step:1307/1750 train_time:128896ms step_avg:98.62ms
step:1308/1750 train_time:128997ms step_avg:98.62ms
step:1309/1750 train_time:129097ms step_avg:98.62ms
step:1310/1750 train_time:129198ms step_avg:98.62ms
step:1311/1750 train_time:129300ms step_avg:98.63ms
step:1312/1750 train_time:129401ms step_avg:98.63ms
step:1313/1750 train_time:129503ms step_avg:98.63ms
step:1314/1750 train_time:129603ms step_avg:98.63ms
step:1315/1750 train_time:129705ms step_avg:98.64ms
step:1316/1750 train_time:129806ms step_avg:98.64ms
step:1317/1750 train_time:129907ms step_avg:98.64ms
step:1318/1750 train_time:130008ms step_avg:98.64ms
step:1319/1750 train_time:130110ms step_avg:98.64ms
step:1320/1750 train_time:130213ms step_avg:98.65ms
step:1321/1750 train_time:130313ms step_avg:98.65ms
step:1322/1750 train_time:130414ms step_avg:98.65ms
step:1323/1750 train_time:130514ms step_avg:98.65ms
step:1324/1750 train_time:130614ms step_avg:98.65ms
step:1325/1750 train_time:130714ms step_avg:98.65ms
step:1326/1750 train_time:130816ms step_avg:98.65ms
step:1327/1750 train_time:130917ms step_avg:98.66ms
step:1328/1750 train_time:131018ms step_avg:98.66ms
step:1329/1750 train_time:131119ms step_avg:98.66ms
step:1330/1750 train_time:131221ms step_avg:98.66ms
step:1331/1750 train_time:131322ms step_avg:98.66ms
step:1332/1750 train_time:131423ms step_avg:98.67ms
step:1333/1750 train_time:131524ms step_avg:98.67ms
step:1334/1750 train_time:131625ms step_avg:98.67ms
step:1335/1750 train_time:131727ms step_avg:98.67ms
step:1336/1750 train_time:131830ms step_avg:98.68ms
step:1337/1750 train_time:131932ms step_avg:98.68ms
step:1338/1750 train_time:132031ms step_avg:98.68ms
step:1339/1750 train_time:132132ms step_avg:98.68ms
step:1340/1750 train_time:132231ms step_avg:98.68ms
step:1341/1750 train_time:132331ms step_avg:98.68ms
step:1342/1750 train_time:132431ms step_avg:98.68ms
step:1343/1750 train_time:132530ms step_avg:98.68ms
step:1344/1750 train_time:132631ms step_avg:98.68ms
step:1345/1750 train_time:132732ms step_avg:98.69ms
step:1346/1750 train_time:132833ms step_avg:98.69ms
step:1347/1750 train_time:132934ms step_avg:98.69ms
step:1348/1750 train_time:133034ms step_avg:98.69ms
step:1349/1750 train_time:133133ms step_avg:98.69ms
step:1350/1750 train_time:133236ms step_avg:98.69ms
step:1351/1750 train_time:133336ms step_avg:98.69ms
step:1352/1750 train_time:133437ms step_avg:98.70ms
step:1353/1750 train_time:133538ms step_avg:98.70ms
step:1354/1750 train_time:133640ms step_avg:98.70ms
step:1355/1750 train_time:133741ms step_avg:98.70ms
step:1356/1750 train_time:133842ms step_avg:98.70ms
step:1357/1750 train_time:133942ms step_avg:98.70ms
step:1358/1750 train_time:134043ms step_avg:98.71ms
step:1359/1750 train_time:134144ms step_avg:98.71ms
step:1360/1750 train_time:134246ms step_avg:98.71ms
step:1361/1750 train_time:134347ms step_avg:98.71ms
step:1362/1750 train_time:134448ms step_avg:98.71ms
step:1363/1750 train_time:134550ms step_avg:98.72ms
step:1364/1750 train_time:134652ms step_avg:98.72ms
step:1365/1750 train_time:134752ms step_avg:98.72ms
step:1366/1750 train_time:134851ms step_avg:98.72ms
step:1367/1750 train_time:134950ms step_avg:98.72ms
step:1368/1750 train_time:135051ms step_avg:98.72ms
step:1369/1750 train_time:135151ms step_avg:98.72ms
step:1370/1750 train_time:135251ms step_avg:98.72ms
step:1371/1750 train_time:135352ms step_avg:98.73ms
step:1372/1750 train_time:135453ms step_avg:98.73ms
step:1373/1750 train_time:135553ms step_avg:98.73ms
step:1374/1750 train_time:135653ms step_avg:98.73ms
step:1375/1750 train_time:135754ms step_avg:98.73ms
step:1375/1750 val_loss:3.3721 train_time:135843ms step_avg:98.79ms
step:1376/1750 train_time:135864ms step_avg:98.74ms
step:1377/1750 train_time:135965ms step_avg:98.74ms
step:1378/1750 train_time:136066ms step_avg:98.74ms
step:1379/1750 train_time:136167ms step_avg:98.74ms
step:1380/1750 train_time:136268ms step_avg:98.75ms
step:1381/1750 train_time:136368ms step_avg:98.75ms
step:1382/1750 train_time:136467ms step_avg:98.75ms
step:1383/1750 train_time:136567ms step_avg:98.75ms
step:1384/1750 train_time:136667ms step_avg:98.75ms
step:1385/1750 train_time:136770ms step_avg:98.75ms
step:1386/1750 train_time:136873ms step_avg:98.75ms
step:1387/1750 train_time:136975ms step_avg:98.76ms
step:1388/1750 train_time:137074ms step_avg:98.76ms
step:1389/1750 train_time:137174ms step_avg:98.76ms
step:1390/1750 train_time:137274ms step_avg:98.76ms
step:1391/1750 train_time:137375ms step_avg:98.76ms
step:1392/1750 train_time:137477ms step_avg:98.76ms
step:1393/1750 train_time:137577ms step_avg:98.76ms
step:1394/1750 train_time:137678ms step_avg:98.76ms
step:1395/1750 train_time:137779ms step_avg:98.77ms
step:1396/1750 train_time:137882ms step_avg:98.77ms
step:1397/1750 train_time:137985ms step_avg:98.77ms
step:1398/1750 train_time:138086ms step_avg:98.77ms
step:1399/1750 train_time:138187ms step_avg:98.78ms
step:1400/1750 train_time:138288ms step_avg:98.78ms
step:1401/1750 train_time:138391ms step_avg:98.78ms
step:1402/1750 train_time:138491ms step_avg:98.78ms
step:1403/1750 train_time:138592ms step_avg:98.78ms
step:1404/1750 train_time:138692ms step_avg:98.78ms
step:1405/1750 train_time:138793ms step_avg:98.78ms
step:1406/1750 train_time:138894ms step_avg:98.79ms
step:1407/1750 train_time:138995ms step_avg:98.79ms
step:1408/1750 train_time:139096ms step_avg:98.79ms
step:1409/1750 train_time:139199ms step_avg:98.79ms
step:1410/1750 train_time:139300ms step_avg:98.79ms
step:1411/1750 train_time:139402ms step_avg:98.80ms
step:1412/1750 train_time:139504ms step_avg:98.80ms
step:1413/1750 train_time:139604ms step_avg:98.80ms
step:1414/1750 train_time:139704ms step_avg:98.80ms
step:1415/1750 train_time:139806ms step_avg:98.80ms
step:1416/1750 train_time:139908ms step_avg:98.81ms
step:1417/1750 train_time:140009ms step_avg:98.81ms
step:1418/1750 train_time:140110ms step_avg:98.81ms
step:1419/1750 train_time:140211ms step_avg:98.81ms
step:1420/1750 train_time:140312ms step_avg:98.81ms
step:1421/1750 train_time:140413ms step_avg:98.81ms
step:1422/1750 train_time:140513ms step_avg:98.81ms
step:1423/1750 train_time:140612ms step_avg:98.81ms
step:1424/1750 train_time:140713ms step_avg:98.82ms
step:1425/1750 train_time:140813ms step_avg:98.82ms
step:1426/1750 train_time:140915ms step_avg:98.82ms
step:1427/1750 train_time:141016ms step_avg:98.82ms
step:1428/1750 train_time:141120ms step_avg:98.82ms
step:1429/1750 train_time:141222ms step_avg:98.83ms
step:1430/1750 train_time:141325ms step_avg:98.83ms
step:1431/1750 train_time:141427ms step_avg:98.83ms
step:1432/1750 train_time:141528ms step_avg:98.83ms
step:1433/1750 train_time:141630ms step_avg:98.83ms
step:1434/1750 train_time:141731ms step_avg:98.84ms
step:1435/1750 train_time:141834ms step_avg:98.84ms
step:1436/1750 train_time:141937ms step_avg:98.84ms
step:1437/1750 train_time:142039ms step_avg:98.84ms
step:1438/1750 train_time:142140ms step_avg:98.85ms
step:1439/1750 train_time:142244ms step_avg:98.85ms
step:1440/1750 train_time:142347ms step_avg:98.85ms
step:1441/1750 train_time:142450ms step_avg:98.85ms
step:1442/1750 train_time:142550ms step_avg:98.86ms
step:1443/1750 train_time:142651ms step_avg:98.86ms
step:1444/1750 train_time:142753ms step_avg:98.86ms
step:1445/1750 train_time:142855ms step_avg:98.86ms
step:1446/1750 train_time:142956ms step_avg:98.86ms
step:1447/1750 train_time:143057ms step_avg:98.86ms
step:1448/1750 train_time:143159ms step_avg:98.87ms
step:1449/1750 train_time:143260ms step_avg:98.87ms
step:1450/1750 train_time:143363ms step_avg:98.87ms
step:1451/1750 train_time:143465ms step_avg:98.87ms
step:1452/1750 train_time:143568ms step_avg:98.88ms
step:1453/1750 train_time:143671ms step_avg:98.88ms
step:1454/1750 train_time:143774ms step_avg:98.88ms
step:1455/1750 train_time:143875ms step_avg:98.88ms
step:1456/1750 train_time:143976ms step_avg:98.88ms
step:1457/1750 train_time:144078ms step_avg:98.89ms
step:1458/1750 train_time:144180ms step_avg:98.89ms
step:1459/1750 train_time:144283ms step_avg:98.89ms
step:1460/1750 train_time:144384ms step_avg:98.89ms
step:1461/1750 train_time:144487ms step_avg:98.90ms
step:1462/1750 train_time:144590ms step_avg:98.90ms
step:1463/1750 train_time:144691ms step_avg:98.90ms
step:1464/1750 train_time:144792ms step_avg:98.90ms
step:1465/1750 train_time:144894ms step_avg:98.90ms
step:1466/1750 train_time:144995ms step_avg:98.91ms
step:1467/1750 train_time:145097ms step_avg:98.91ms
step:1468/1750 train_time:145199ms step_avg:98.91ms
step:1469/1750 train_time:145300ms step_avg:98.91ms
step:1470/1750 train_time:145403ms step_avg:98.91ms
step:1471/1750 train_time:145506ms step_avg:98.92ms
step:1472/1750 train_time:145608ms step_avg:98.92ms
step:1473/1750 train_time:145710ms step_avg:98.92ms
step:1474/1750 train_time:145811ms step_avg:98.92ms
step:1475/1750 train_time:145913ms step_avg:98.92ms
step:1476/1750 train_time:146015ms step_avg:98.93ms
step:1477/1750 train_time:146117ms step_avg:98.93ms
step:1478/1750 train_time:146219ms step_avg:98.93ms
step:1479/1750 train_time:146319ms step_avg:98.93ms
step:1480/1750 train_time:146422ms step_avg:98.93ms
step:1481/1750 train_time:146525ms step_avg:98.94ms
step:1482/1750 train_time:146628ms step_avg:98.94ms
step:1483/1750 train_time:146730ms step_avg:98.94ms
step:1484/1750 train_time:146833ms step_avg:98.94ms
step:1485/1750 train_time:146935ms step_avg:98.95ms
step:1486/1750 train_time:147037ms step_avg:98.95ms
step:1487/1750 train_time:147138ms step_avg:98.95ms
step:1488/1750 train_time:147241ms step_avg:98.95ms
step:1489/1750 train_time:147343ms step_avg:98.95ms
step:1490/1750 train_time:147445ms step_avg:98.96ms
step:1491/1750 train_time:147546ms step_avg:98.96ms
step:1492/1750 train_time:147648ms step_avg:98.96ms
step:1493/1750 train_time:147749ms step_avg:98.96ms
step:1494/1750 train_time:147852ms step_avg:98.96ms
step:1495/1750 train_time:147953ms step_avg:98.97ms
step:1496/1750 train_time:148054ms step_avg:98.97ms
step:1497/1750 train_time:148155ms step_avg:98.97ms
step:1498/1750 train_time:148256ms step_avg:98.97ms
step:1499/1750 train_time:148357ms step_avg:98.97ms
step:1500/1750 train_time:148459ms step_avg:98.97ms
step:1500/1750 val_loss:3.3363 train_time:148549ms step_avg:99.03ms
step:1501/1750 train_time:148570ms step_avg:98.98ms
step:1502/1750 train_time:148669ms step_avg:98.98ms
step:1503/1750 train_time:148771ms step_avg:98.98ms
step:1504/1750 train_time:148872ms step_avg:98.98ms
step:1505/1750 train_time:148972ms step_avg:98.98ms
step:1506/1750 train_time:149072ms step_avg:98.99ms
step:1507/1750 train_time:149173ms step_avg:98.99ms
step:1508/1750 train_time:149274ms step_avg:98.99ms
step:1509/1750 train_time:149378ms step_avg:98.99ms
step:1510/1750 train_time:149482ms step_avg:98.99ms
step:1511/1750 train_time:149586ms step_avg:99.00ms
step:1512/1750 train_time:149689ms step_avg:99.00ms
step:1513/1750 train_time:149790ms step_avg:99.00ms
step:1514/1750 train_time:149891ms step_avg:99.00ms
step:1515/1750 train_time:149995ms step_avg:99.01ms
step:1516/1750 train_time:150096ms step_avg:99.01ms
step:1517/1750 train_time:150196ms step_avg:99.01ms
step:1518/1750 train_time:150297ms step_avg:99.01ms
step:1519/1750 train_time:150399ms step_avg:99.01ms
step:1520/1750 train_time:150501ms step_avg:99.01ms
step:1521/1750 train_time:150603ms step_avg:99.02ms
step:1522/1750 train_time:150705ms step_avg:99.02ms
step:1523/1750 train_time:150807ms step_avg:99.02ms
step:1524/1750 train_time:150911ms step_avg:99.02ms
step:1525/1750 train_time:151013ms step_avg:99.02ms
step:1526/1750 train_time:151114ms step_avg:99.03ms
step:1527/1750 train_time:151216ms step_avg:99.03ms
step:1528/1750 train_time:151321ms step_avg:99.03ms
step:1529/1750 train_time:151423ms step_avg:99.03ms
step:1530/1750 train_time:151525ms step_avg:99.04ms
step:1531/1750 train_time:151627ms step_avg:99.04ms
step:1532/1750 train_time:151729ms step_avg:99.04ms
step:1533/1750 train_time:151829ms step_avg:99.04ms
step:1534/1750 train_time:151931ms step_avg:99.04ms
step:1535/1750 train_time:152032ms step_avg:99.04ms
step:1536/1750 train_time:152133ms step_avg:99.05ms
step:1537/1750 train_time:152235ms step_avg:99.05ms
step:1538/1750 train_time:152336ms step_avg:99.05ms
step:1539/1750 train_time:152438ms step_avg:99.05ms
step:1540/1750 train_time:152541ms step_avg:99.05ms
step:1541/1750 train_time:152644ms step_avg:99.06ms
step:1542/1750 train_time:152749ms step_avg:99.06ms
step:1543/1750 train_time:152850ms step_avg:99.06ms
step:1544/1750 train_time:152952ms step_avg:99.06ms
step:1545/1750 train_time:153053ms step_avg:99.06ms
step:1546/1750 train_time:153154ms step_avg:99.06ms
step:1547/1750 train_time:153256ms step_avg:99.07ms
step:1548/1750 train_time:153359ms step_avg:99.07ms
step:1549/1750 train_time:153461ms step_avg:99.07ms
step:1550/1750 train_time:153562ms step_avg:99.07ms
step:1551/1750 train_time:153666ms step_avg:99.08ms
step:1552/1750 train_time:153769ms step_avg:99.08ms
step:1553/1750 train_time:153871ms step_avg:99.08ms
step:1554/1750 train_time:153972ms step_avg:99.08ms
step:1555/1750 train_time:154073ms step_avg:99.08ms
step:1556/1750 train_time:154175ms step_avg:99.08ms
step:1557/1750 train_time:154277ms step_avg:99.09ms
step:1558/1750 train_time:154379ms step_avg:99.09ms
step:1559/1750 train_time:154483ms step_avg:99.09ms
step:1560/1750 train_time:154585ms step_avg:99.09ms
step:1561/1750 train_time:154687ms step_avg:99.09ms
step:1562/1750 train_time:154790ms step_avg:99.10ms
step:1563/1750 train_time:154894ms step_avg:99.10ms
step:1564/1750 train_time:154995ms step_avg:99.10ms
step:1565/1750 train_time:155095ms step_avg:99.10ms
step:1566/1750 train_time:155197ms step_avg:99.10ms
step:1567/1750 train_time:155298ms step_avg:99.11ms
step:1568/1750 train_time:155399ms step_avg:99.11ms
step:1569/1750 train_time:155501ms step_avg:99.11ms
step:1570/1750 train_time:155605ms step_avg:99.11ms
step:1571/1750 train_time:155708ms step_avg:99.11ms
step:1572/1750 train_time:155809ms step_avg:99.12ms
step:1573/1750 train_time:155911ms step_avg:99.12ms
step:1574/1750 train_time:156014ms step_avg:99.12ms
step:1575/1750 train_time:156115ms step_avg:99.12ms
step:1576/1750 train_time:156217ms step_avg:99.12ms
step:1577/1750 train_time:156320ms step_avg:99.12ms
step:1578/1750 train_time:156421ms step_avg:99.13ms
step:1579/1750 train_time:156522ms step_avg:99.13ms
step:1580/1750 train_time:156625ms step_avg:99.13ms
step:1581/1750 train_time:156729ms step_avg:99.13ms
step:1582/1750 train_time:156831ms step_avg:99.13ms
step:1583/1750 train_time:156935ms step_avg:99.14ms
step:1584/1750 train_time:157036ms step_avg:99.14ms
step:1585/1750 train_time:157137ms step_avg:99.14ms
step:1586/1750 train_time:157239ms step_avg:99.14ms
step:1587/1750 train_time:157341ms step_avg:99.14ms
step:1588/1750 train_time:157443ms step_avg:99.15ms
step:1589/1750 train_time:157545ms step_avg:99.15ms
step:1590/1750 train_time:157647ms step_avg:99.15ms
step:1591/1750 train_time:157751ms step_avg:99.15ms
step:1592/1750 train_time:157852ms step_avg:99.15ms
step:1593/1750 train_time:157954ms step_avg:99.16ms
step:1594/1750 train_time:158059ms step_avg:99.16ms
step:1595/1750 train_time:158160ms step_avg:99.16ms
step:1596/1750 train_time:158262ms step_avg:99.16ms
step:1597/1750 train_time:158363ms step_avg:99.16ms
step:1598/1750 train_time:158467ms step_avg:99.17ms
step:1599/1750 train_time:158568ms step_avg:99.17ms
step:1600/1750 train_time:158670ms step_avg:99.17ms
step:1601/1750 train_time:158772ms step_avg:99.17ms
step:1602/1750 train_time:158874ms step_avg:99.17ms
step:1603/1750 train_time:158977ms step_avg:99.17ms
step:1604/1750 train_time:159076ms step_avg:99.17ms
step:1605/1750 train_time:159179ms step_avg:99.18ms
step:1606/1750 train_time:159282ms step_avg:99.18ms
step:1607/1750 train_time:159383ms step_avg:99.18ms
step:1608/1750 train_time:159485ms step_avg:99.18ms
step:1609/1750 train_time:159587ms step_avg:99.18ms
step:1610/1750 train_time:159689ms step_avg:99.19ms
step:1611/1750 train_time:159791ms step_avg:99.19ms
step:1612/1750 train_time:159894ms step_avg:99.19ms
step:1613/1750 train_time:159996ms step_avg:99.19ms
step:1614/1750 train_time:160096ms step_avg:99.19ms
step:1615/1750 train_time:160198ms step_avg:99.19ms
step:1616/1750 train_time:160299ms step_avg:99.19ms
step:1617/1750 train_time:160401ms step_avg:99.20ms
step:1618/1750 train_time:160503ms step_avg:99.20ms
step:1619/1750 train_time:160606ms step_avg:99.20ms
step:1620/1750 train_time:160709ms step_avg:99.20ms
step:1621/1750 train_time:160811ms step_avg:99.20ms
step:1622/1750 train_time:160911ms step_avg:99.21ms
step:1623/1750 train_time:161012ms step_avg:99.21ms
step:1624/1750 train_time:161115ms step_avg:99.21ms
step:1625/1750 train_time:161219ms step_avg:99.21ms
step:1625/1750 val_loss:3.3064 train_time:161310ms step_avg:99.27ms
step:1626/1750 train_time:161332ms step_avg:99.22ms
step:1627/1750 train_time:161436ms step_avg:99.22ms
step:1628/1750 train_time:161537ms step_avg:99.22ms
step:1629/1750 train_time:161638ms step_avg:99.23ms
step:1630/1750 train_time:161738ms step_avg:99.23ms
step:1631/1750 train_time:161839ms step_avg:99.23ms
step:1632/1750 train_time:161941ms step_avg:99.23ms
step:1633/1750 train_time:162041ms step_avg:99.23ms
step:1634/1750 train_time:162144ms step_avg:99.23ms
step:1635/1750 train_time:162245ms step_avg:99.23ms
step:1636/1750 train_time:162349ms step_avg:99.24ms
step:1637/1750 train_time:162452ms step_avg:99.24ms
step:1638/1750 train_time:162555ms step_avg:99.24ms
step:1639/1750 train_time:162657ms step_avg:99.24ms
step:1640/1750 train_time:162758ms step_avg:99.24ms
step:1641/1750 train_time:162858ms step_avg:99.24ms
step:1642/1750 train_time:162959ms step_avg:99.24ms
step:1643/1750 train_time:163061ms step_avg:99.25ms
step:1644/1750 train_time:163162ms step_avg:99.25ms
step:1645/1750 train_time:163264ms step_avg:99.25ms
step:1646/1750 train_time:163365ms step_avg:99.25ms
step:1647/1750 train_time:163470ms step_avg:99.25ms
step:1648/1750 train_time:163574ms step_avg:99.26ms
step:1649/1750 train_time:163676ms step_avg:99.26ms
step:1650/1750 train_time:163778ms step_avg:99.26ms
step:1651/1750 train_time:163880ms step_avg:99.26ms
step:1652/1750 train_time:163981ms step_avg:99.26ms
step:1653/1750 train_time:164084ms step_avg:99.26ms
step:1654/1750 train_time:164185ms step_avg:99.27ms
step:1655/1750 train_time:164287ms step_avg:99.27ms
step:1656/1750 train_time:164388ms step_avg:99.27ms
step:1657/1750 train_time:164489ms step_avg:99.27ms
step:1658/1750 train_time:164591ms step_avg:99.27ms
step:1659/1750 train_time:164696ms step_avg:99.27ms
step:1660/1750 train_time:164798ms step_avg:99.28ms
step:1661/1750 train_time:164901ms step_avg:99.28ms
step:1662/1750 train_time:165004ms step_avg:99.28ms
step:1663/1750 train_time:165106ms step_avg:99.28ms
step:1664/1750 train_time:165208ms step_avg:99.28ms
step:1665/1750 train_time:165312ms step_avg:99.29ms
step:1666/1750 train_time:165415ms step_avg:99.29ms
step:1667/1750 train_time:165517ms step_avg:99.29ms
step:1668/1750 train_time:165620ms step_avg:99.29ms
step:1669/1750 train_time:165722ms step_avg:99.29ms
step:1670/1750 train_time:165823ms step_avg:99.30ms
step:1671/1750 train_time:165925ms step_avg:99.30ms
step:1672/1750 train_time:166027ms step_avg:99.30ms
step:1673/1750 train_time:166129ms step_avg:99.30ms
step:1674/1750 train_time:166230ms step_avg:99.30ms
step:1675/1750 train_time:166333ms step_avg:99.30ms
step:1676/1750 train_time:166436ms step_avg:99.31ms
step:1677/1750 train_time:166538ms step_avg:99.31ms
step:1678/1750 train_time:166641ms step_avg:99.31ms
step:1679/1750 train_time:166743ms step_avg:99.31ms
step:1680/1750 train_time:166843ms step_avg:99.31ms
step:1681/1750 train_time:166946ms step_avg:99.31ms
step:1682/1750 train_time:167050ms step_avg:99.32ms
step:1683/1750 train_time:167152ms step_avg:99.32ms
step:1684/1750 train_time:167254ms step_avg:99.32ms
step:1685/1750 train_time:167358ms step_avg:99.32ms
step:1686/1750 train_time:167459ms step_avg:99.32ms
step:1687/1750 train_time:167561ms step_avg:99.33ms
step:1688/1750 train_time:167663ms step_avg:99.33ms
step:1689/1750 train_time:167765ms step_avg:99.33ms
step:1690/1750 train_time:167867ms step_avg:99.33ms
step:1691/1750 train_time:167969ms step_avg:99.33ms
step:1692/1750 train_time:168072ms step_avg:99.33ms
step:1693/1750 train_time:168176ms step_avg:99.34ms
step:1694/1750 train_time:168279ms step_avg:99.34ms
step:1695/1750 train_time:168382ms step_avg:99.34ms
step:1696/1750 train_time:168484ms step_avg:99.34ms
step:1697/1750 train_time:168589ms step_avg:99.35ms
step:1698/1750 train_time:168693ms step_avg:99.35ms
step:1699/1750 train_time:168795ms step_avg:99.35ms
step:1700/1750 train_time:168899ms step_avg:99.35ms
step:1701/1750 train_time:169001ms step_avg:99.35ms
step:1702/1750 train_time:169106ms step_avg:99.36ms
step:1703/1750 train_time:169207ms step_avg:99.36ms
step:1704/1750 train_time:169310ms step_avg:99.36ms
step:1705/1750 train_time:169411ms step_avg:99.36ms
step:1706/1750 train_time:169514ms step_avg:99.36ms
step:1707/1750 train_time:169619ms step_avg:99.37ms
step:1708/1750 train_time:169723ms step_avg:99.37ms
step:1709/1750 train_time:169825ms step_avg:99.37ms
step:1710/1750 train_time:169928ms step_avg:99.37ms
step:1711/1750 train_time:170032ms step_avg:99.38ms
step:1712/1750 train_time:170134ms step_avg:99.38ms
step:1713/1750 train_time:170239ms step_avg:99.38ms
step:1714/1750 train_time:170341ms step_avg:99.38ms
step:1715/1750 train_time:170445ms step_avg:99.38ms
step:1716/1750 train_time:170547ms step_avg:99.39ms
step:1717/1750 train_time:170649ms step_avg:99.39ms
step:1718/1750 train_time:170752ms step_avg:99.39ms
step:1719/1750 train_time:170858ms step_avg:99.39ms
step:1720/1750 train_time:170960ms step_avg:99.40ms
step:1721/1750 train_time:171063ms step_avg:99.40ms
step:1722/1750 train_time:171165ms step_avg:99.40ms
step:1723/1750 train_time:171269ms step_avg:99.40ms
step:1724/1750 train_time:171374ms step_avg:99.41ms
step:1725/1750 train_time:171479ms step_avg:99.41ms
step:1726/1750 train_time:171581ms step_avg:99.41ms
step:1727/1750 train_time:171683ms step_avg:99.41ms
step:1728/1750 train_time:171786ms step_avg:99.41ms
step:1729/1750 train_time:171889ms step_avg:99.42ms
step:1730/1750 train_time:171992ms step_avg:99.42ms
step:1731/1750 train_time:172096ms step_avg:99.42ms
step:1732/1750 train_time:172199ms step_avg:99.42ms
step:1733/1750 train_time:172302ms step_avg:99.42ms
step:1734/1750 train_time:172406ms step_avg:99.43ms
step:1735/1750 train_time:172508ms step_avg:99.43ms
step:1736/1750 train_time:172610ms step_avg:99.43ms
step:1737/1750 train_time:172713ms step_avg:99.43ms
step:1738/1750 train_time:172815ms step_avg:99.43ms
step:1739/1750 train_time:172918ms step_avg:99.44ms
step:1740/1750 train_time:173020ms step_avg:99.44ms
step:1741/1750 train_time:173127ms step_avg:99.44ms
step:1742/1750 train_time:173230ms step_avg:99.44ms
step:1743/1750 train_time:173334ms step_avg:99.45ms
step:1744/1750 train_time:173437ms step_avg:99.45ms
step:1745/1750 train_time:173538ms step_avg:99.45ms
step:1746/1750 train_time:173641ms step_avg:99.45ms
step:1747/1750 train_time:173743ms step_avg:99.45ms
step:1748/1750 train_time:173845ms step_avg:99.45ms
step:1749/1750 train_time:173948ms step_avg:99.46ms
step:1750/1750 train_time:174052ms step_avg:99.46ms
step:1750/1750 val_loss:3.2832 train_time:174143ms step_avg:99.51ms
peak memory allocated: 33278 MiB reserved: 49174 MiB
