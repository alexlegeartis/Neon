import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X
'''
import torch.utils.dlpack as thd
from cupyx.scipy.sparse.linalg import svds as cupyx_svds

def several_sv_svds_approximation(W_torch, k, num_iter=50):
    """SVD approximation using the top k singular values and corresponding vectors."""
    # Store original device and dtype
    original_device = W_torch.device
    original_dtype = W_torch.dtype
    
    W = cp.from_dlpack(thd.to_dlpack(W_torch)).astype(cp.float32)
    U, S, Vt = cupyx_svds(W, k=min([k, W.shape[0] - 1, W.shape[1] - 1]), maxiter=num_iter, which='LM')

    # Convert back to torch tensors and ensure they're on the correct device
    approx_torch_U = thd.from_dlpack(U.toDlpack()).to(device=original_device, dtype=original_dtype)
    approx_torch_S = thd.from_dlpack(S.toDlpack()).to(device=original_device, dtype=original_dtype)
    approx_torch_Vt = thd.from_dlpack(Vt.toDlpack()).to(device=original_device, dtype=original_dtype)
    
    return approx_torch_U, approx_torch_S, approx_torch_Vt

class NormNeon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    # v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    u, s, vt = several_sv_svds_approximation(grad.bfloat16(), 5)
                    p.add_(other=u@vt, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()
'''
class FastNormMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, sgd_coeff=0):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, sgd_coeff=sgd_coeff)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        self.sgd_coeff = sgd_coeff

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            sgd_coeff = group["sgd_coeff"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    # assume: grad is a Tensor, momentum_buffer exists, momentum, sgd_coeff, eps, eff_lr are defined
                    with torch.no_grad():
                        # inplace blend with momentum (keeps original semantic: grad becomes the lerped tensor)
                        grad.lerp_(momentum_buffer, momentum)

                        # compute zeropower part from the (post-lerp) grad, in bfloat16 to match the original code
                        if sgd_coeff != 1:
                            # create update_part in bfloat16 (may allocate once per call; unavoidable if zeropower expects bfloat16)
                            update_part = zeropower_via_newtonschulz5(grad.to(torch.bfloat16), 5)

                            # upcast update_part to grad dtype to do mixed-dtype blending without extra temporaries later
                            if update_part.dtype != grad.dtype:
                                update_part = update_part.to(grad.dtype)

                        # compute inverse norm scalar efficiently (no large temp tensors)
                        # - view(-1) is cheap (no copy)
                        # - dot(view, view) is a single reduction
                        norm_sq = grad.view(-1).dot(grad.view(-1))
                        inv_norm = (norm_sq + eps).rsqrt()  # stable rsqrt on the scalar

                        # normalize grad in-place (so we avoid allocating g_normalized)
                        grad.mul_(inv_norm)   # now grad == g_normalized (in-place)

                        # build final update (reuse update_part buffer if available)
                        if sgd_coeff != 1:
                            # update_part := (1 - sgd_coeff) * update_part + sgd_coeff * grad
                            # do it in-place on update_part to avoid creating another temp
                            update_part.mul_(1 - sgd_coeff)    # in-place scale
                            update_part.lerp_(grad, sgd_coeff) # in-place linear interpolation
                            update = update_part
                        else:
                            # if sgd_coeff == 1 we can use grad directly (already normalized in-place)
                            update = grad

                        # apply update (same as original)
                        p.add_(other=update, alpha=-eff_lr)
                    '''
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    eps = 1e-12
                    g_normalized = grad / (grad.norm() + eps)
                    if sgd_coeff != 1:
                        update_part = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                        update = (1 - sgd_coeff) * update_part + sgd_coeff * g_normalized
                    else:
                        update = sgd_coeff * g_normalized
                    p.add_(other=update, alpha=-eff_lr)
                    '''
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class NormMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, sgd_coeff=0):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, sgd_coeff=sgd_coeff)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        self.sgd_coeff = sgd_coeff

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            sgd_coeff = group["sgd_coeff"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    eps = 1e-12
                    g_normalized = grad / (grad.norm() + eps)
                    if sgd_coeff != 1:
                        update_part = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                        update = (1 - sgd_coeff) * update_part + sgd_coeff * g_normalized
                    else:
                        update = sgd_coeff * g_normalized
                    p.add_(other=update, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()
class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)
                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_reduce_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 27.5
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos : pos + max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()
    starts = []
    for i in range(1, len(boundary_positions)):
        end = boundary_positions[i].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == dist.get_world_size():
                return starts, end - pos
            start = end
    assert False # increase max_batch_span if necessary

def distributed_data_generator(filename_pattern: str, batch_size: int, align_to_bos: bool):
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    max_batch_span = 2 * batch_size if align_to_bos else batch_size # provide buffer to handle samples up to length local_batch_size
    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            batch_span = batch_size
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1750 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
from datetime import datetime
if master_process:
    # run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_id = str(int(time.time() * 1000))
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0)
optimizer2 = NormMuon(hidden_matrix_params, lr=0.065, momentum=0.95, weight_decay=0.0, sgd_coeff=0.5)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.11 | packaged by conda-forge | (main, Jun  4 2025, 14:45:31) [GCC 13.3.0]
Running PyTorch 2.9.0.dev20250824+cu126 compiled for CUDA 12.6
Sun Aug 24 17:58:04 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:04:00.0 Off |                    0 |
| N/A   41C    P0            122W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:23:00.0 Off |                    0 |
| N/A   34C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:43:00.0 Off |                    0 |
| N/A   39C    P0            125W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   34C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:84:00.0 Off |                    0 |
| N/A   39C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:A3:00.0 Off |                    0 |
| N/A   34C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:C3:00.0 Off |                    0 |
| N/A   41C    P0            121W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E4:00.0 Off |                    0 |
| N/A   33C    P0            113W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.05ms
step:1/1750 train_time:140ms step_avg:140.01ms
step:2/1750 train_time:160ms step_avg:80.24ms
step:3/1750 train_time:242ms step_avg:80.56ms
step:4/1750 train_time:333ms step_avg:83.32ms
step:5/1750 train_time:426ms step_avg:85.16ms
step:6/1750 train_time:518ms step_avg:86.41ms
step:7/1750 train_time:611ms step_avg:87.22ms
step:8/1750 train_time:703ms step_avg:87.88ms
step:9/1750 train_time:795ms step_avg:88.36ms
step:10/1750 train_time:887ms step_avg:88.75ms
step:11/1750 train_time:980ms step_avg:89.07ms
step:12/1750 train_time:1075ms step_avg:89.61ms
step:13/1750 train_time:1173ms step_avg:90.20ms
step:14/1750 train_time:1269ms step_avg:90.62ms
step:15/1750 train_time:1362ms step_avg:90.82ms
step:16/1750 train_time:1455ms step_avg:90.95ms
step:17/1750 train_time:1548ms step_avg:91.03ms
step:18/1750 train_time:1641ms step_avg:91.15ms
step:19/1750 train_time:1733ms step_avg:91.23ms
step:20/1750 train_time:1826ms step_avg:91.31ms
step:21/1750 train_time:1919ms step_avg:91.38ms
step:22/1750 train_time:2013ms step_avg:91.49ms
step:23/1750 train_time:2107ms step_avg:91.61ms
step:24/1750 train_time:2203ms step_avg:91.80ms
step:25/1750 train_time:2298ms step_avg:91.91ms
step:26/1750 train_time:2391ms step_avg:91.96ms
step:27/1750 train_time:2485ms step_avg:92.02ms
step:28/1750 train_time:2578ms step_avg:92.06ms
step:29/1750 train_time:2670ms step_avg:92.08ms
step:30/1750 train_time:2763ms step_avg:92.10ms
step:31/1750 train_time:2857ms step_avg:92.15ms
step:32/1750 train_time:2949ms step_avg:92.17ms
step:33/1750 train_time:3043ms step_avg:92.21ms
step:34/1750 train_time:3137ms step_avg:92.26ms
step:35/1750 train_time:3232ms step_avg:92.35ms
step:36/1750 train_time:3326ms step_avg:92.40ms
step:37/1750 train_time:3420ms step_avg:92.44ms
step:38/1750 train_time:3513ms step_avg:92.45ms
step:39/1750 train_time:3606ms step_avg:92.45ms
step:40/1750 train_time:3699ms step_avg:92.47ms
step:41/1750 train_time:3792ms step_avg:92.49ms
step:42/1750 train_time:3885ms step_avg:92.50ms
step:43/1750 train_time:3978ms step_avg:92.51ms
step:44/1750 train_time:4071ms step_avg:92.53ms
step:45/1750 train_time:4166ms step_avg:92.57ms
step:46/1750 train_time:4259ms step_avg:92.59ms
step:47/1750 train_time:4353ms step_avg:92.62ms
step:48/1750 train_time:4446ms step_avg:92.63ms
step:49/1750 train_time:4540ms step_avg:92.65ms
step:50/1750 train_time:4632ms step_avg:92.65ms
step:51/1750 train_time:4726ms step_avg:92.66ms
step:52/1750 train_time:4819ms step_avg:92.68ms
step:53/1750 train_time:4913ms step_avg:92.70ms
step:54/1750 train_time:5005ms step_avg:92.69ms
step:55/1750 train_time:5099ms step_avg:92.71ms
step:56/1750 train_time:5193ms step_avg:92.74ms
step:57/1750 train_time:5287ms step_avg:92.75ms
step:58/1750 train_time:5381ms step_avg:92.78ms
step:59/1750 train_time:5474ms step_avg:92.79ms
step:60/1750 train_time:5568ms step_avg:92.80ms
step:61/1750 train_time:5662ms step_avg:92.81ms
step:62/1750 train_time:5755ms step_avg:92.83ms
step:63/1750 train_time:5848ms step_avg:92.83ms
step:64/1750 train_time:5942ms step_avg:92.85ms
step:65/1750 train_time:6036ms step_avg:92.86ms
step:66/1750 train_time:6129ms step_avg:92.87ms
step:67/1750 train_time:6223ms step_avg:92.88ms
step:68/1750 train_time:6316ms step_avg:92.88ms
step:69/1750 train_time:6410ms step_avg:92.89ms
step:70/1750 train_time:6503ms step_avg:92.91ms
step:71/1750 train_time:6597ms step_avg:92.91ms
step:72/1750 train_time:6691ms step_avg:92.93ms
step:73/1750 train_time:6784ms step_avg:92.93ms
step:74/1750 train_time:6877ms step_avg:92.94ms
step:75/1750 train_time:6970ms step_avg:92.94ms
step:76/1750 train_time:7065ms step_avg:92.95ms
step:77/1750 train_time:7159ms step_avg:92.97ms
step:78/1750 train_time:7252ms step_avg:92.97ms
step:79/1750 train_time:7346ms step_avg:92.98ms
step:80/1750 train_time:7439ms step_avg:92.99ms
step:81/1750 train_time:7533ms step_avg:92.99ms
step:82/1750 train_time:7626ms step_avg:92.99ms
step:83/1750 train_time:7719ms step_avg:93.00ms
step:84/1750 train_time:7811ms step_avg:92.99ms
step:85/1750 train_time:7905ms step_avg:93.00ms
step:86/1750 train_time:7998ms step_avg:93.00ms
step:87/1750 train_time:8092ms step_avg:93.01ms
step:88/1750 train_time:8185ms step_avg:93.02ms
step:89/1750 train_time:8279ms step_avg:93.02ms
step:90/1750 train_time:8373ms step_avg:93.03ms
step:91/1750 train_time:8466ms step_avg:93.03ms
step:92/1750 train_time:8559ms step_avg:93.03ms
step:93/1750 train_time:8652ms step_avg:93.03ms
step:94/1750 train_time:8746ms step_avg:93.04ms
step:95/1750 train_time:8838ms step_avg:93.03ms
step:96/1750 train_time:8932ms step_avg:93.04ms
step:97/1750 train_time:9025ms step_avg:93.04ms
step:98/1750 train_time:9118ms step_avg:93.04ms
step:99/1750 train_time:9212ms step_avg:93.05ms
step:100/1750 train_time:9306ms step_avg:93.06ms
step:101/1750 train_time:9400ms step_avg:93.06ms
step:102/1750 train_time:9493ms step_avg:93.06ms
step:103/1750 train_time:9585ms step_avg:93.06ms
step:104/1750 train_time:9678ms step_avg:93.06ms
step:105/1750 train_time:9771ms step_avg:93.06ms
step:106/1750 train_time:9865ms step_avg:93.06ms
step:107/1750 train_time:9958ms step_avg:93.07ms
step:108/1750 train_time:10051ms step_avg:93.07ms
step:109/1750 train_time:10145ms step_avg:93.08ms
step:110/1750 train_time:10238ms step_avg:93.08ms
step:111/1750 train_time:10333ms step_avg:93.09ms
step:112/1750 train_time:10426ms step_avg:93.09ms
step:113/1750 train_time:10520ms step_avg:93.09ms
step:114/1750 train_time:10613ms step_avg:93.10ms
step:115/1750 train_time:10707ms step_avg:93.10ms
step:116/1750 train_time:10800ms step_avg:93.10ms
step:117/1750 train_time:10893ms step_avg:93.10ms
step:118/1750 train_time:10986ms step_avg:93.10ms
step:119/1750 train_time:11079ms step_avg:93.10ms
step:120/1750 train_time:11172ms step_avg:93.10ms
step:121/1750 train_time:11265ms step_avg:93.10ms
step:122/1750 train_time:11359ms step_avg:93.10ms
step:123/1750 train_time:11452ms step_avg:93.11ms
step:124/1750 train_time:11545ms step_avg:93.11ms
step:125/1750 train_time:11638ms step_avg:93.11ms
step:125/1750 val_loss:4.6716 train_time:11721ms step_avg:93.77ms
step:126/1750 train_time:11743ms step_avg:93.20ms
step:127/1750 train_time:11834ms step_avg:93.18ms
step:128/1750 train_time:11935ms step_avg:93.24ms
step:129/1750 train_time:12030ms step_avg:93.25ms
step:130/1750 train_time:12123ms step_avg:93.26ms
step:131/1750 train_time:12216ms step_avg:93.25ms
step:132/1750 train_time:12309ms step_avg:93.25ms
step:133/1750 train_time:12402ms step_avg:93.25ms
step:134/1750 train_time:12495ms step_avg:93.24ms
step:135/1750 train_time:12587ms step_avg:93.24ms
step:136/1750 train_time:12680ms step_avg:93.23ms
step:137/1750 train_time:12775ms step_avg:93.24ms
step:138/1750 train_time:12870ms step_avg:93.26ms
step:139/1750 train_time:12966ms step_avg:93.28ms
step:140/1750 train_time:13061ms step_avg:93.29ms
step:141/1750 train_time:13155ms step_avg:93.30ms
step:142/1750 train_time:13248ms step_avg:93.30ms
step:143/1750 train_time:13341ms step_avg:93.29ms
step:144/1750 train_time:13434ms step_avg:93.29ms
step:145/1750 train_time:13527ms step_avg:93.29ms
step:146/1750 train_time:13621ms step_avg:93.29ms
step:147/1750 train_time:13715ms step_avg:93.30ms
step:148/1750 train_time:13810ms step_avg:93.31ms
step:149/1750 train_time:13904ms step_avg:93.32ms
step:150/1750 train_time:13999ms step_avg:93.33ms
step:151/1750 train_time:14094ms step_avg:93.34ms
step:152/1750 train_time:14189ms step_avg:93.35ms
step:153/1750 train_time:14282ms step_avg:93.35ms
step:154/1750 train_time:14375ms step_avg:93.34ms
step:155/1750 train_time:14468ms step_avg:93.34ms
step:156/1750 train_time:14560ms step_avg:93.33ms
step:157/1750 train_time:14653ms step_avg:93.33ms
step:158/1750 train_time:14747ms step_avg:93.33ms
step:159/1750 train_time:14841ms step_avg:93.34ms
step:160/1750 train_time:14936ms step_avg:93.35ms
step:161/1750 train_time:15030ms step_avg:93.35ms
step:162/1750 train_time:15123ms step_avg:93.35ms
step:163/1750 train_time:15218ms step_avg:93.36ms
step:164/1750 train_time:15311ms step_avg:93.36ms
step:165/1750 train_time:15404ms step_avg:93.36ms
step:166/1750 train_time:15497ms step_avg:93.36ms
step:167/1750 train_time:15591ms step_avg:93.36ms
step:168/1750 train_time:15685ms step_avg:93.36ms
step:169/1750 train_time:15779ms step_avg:93.37ms
step:170/1750 train_time:15872ms step_avg:93.37ms
step:171/1750 train_time:15967ms step_avg:93.37ms
step:172/1750 train_time:16061ms step_avg:93.38ms
step:173/1750 train_time:16155ms step_avg:93.38ms
step:174/1750 train_time:16249ms step_avg:93.39ms
step:175/1750 train_time:16342ms step_avg:93.39ms
step:176/1750 train_time:16436ms step_avg:93.39ms
step:177/1750 train_time:16530ms step_avg:93.39ms
step:178/1750 train_time:16623ms step_avg:93.39ms
step:179/1750 train_time:16717ms step_avg:93.39ms
step:180/1750 train_time:16811ms step_avg:93.39ms
step:181/1750 train_time:16904ms step_avg:93.39ms
step:182/1750 train_time:16998ms step_avg:93.40ms
step:183/1750 train_time:17093ms step_avg:93.40ms
step:184/1750 train_time:17187ms step_avg:93.41ms
step:185/1750 train_time:17280ms step_avg:93.41ms
step:186/1750 train_time:17373ms step_avg:93.40ms
step:187/1750 train_time:17466ms step_avg:93.40ms
step:188/1750 train_time:17559ms step_avg:93.40ms
step:189/1750 train_time:17653ms step_avg:93.40ms
step:190/1750 train_time:17746ms step_avg:93.40ms
step:191/1750 train_time:17840ms step_avg:93.40ms
step:192/1750 train_time:17934ms step_avg:93.41ms
step:193/1750 train_time:18028ms step_avg:93.41ms
step:194/1750 train_time:18122ms step_avg:93.41ms
step:195/1750 train_time:18216ms step_avg:93.42ms
step:196/1750 train_time:18310ms step_avg:93.42ms
step:197/1750 train_time:18404ms step_avg:93.42ms
step:198/1750 train_time:18498ms step_avg:93.42ms
step:199/1750 train_time:18592ms step_avg:93.43ms
step:200/1750 train_time:18685ms step_avg:93.43ms
step:201/1750 train_time:18779ms step_avg:93.43ms
step:202/1750 train_time:18873ms step_avg:93.43ms
step:203/1750 train_time:18967ms step_avg:93.43ms
step:204/1750 train_time:19061ms step_avg:93.44ms
step:205/1750 train_time:19156ms step_avg:93.44ms
step:206/1750 train_time:19250ms step_avg:93.44ms
step:207/1750 train_time:19343ms step_avg:93.45ms
step:208/1750 train_time:19437ms step_avg:93.45ms
step:209/1750 train_time:19531ms step_avg:93.45ms
step:210/1750 train_time:19624ms step_avg:93.45ms
step:211/1750 train_time:19718ms step_avg:93.45ms
step:212/1750 train_time:19812ms step_avg:93.45ms
step:213/1750 train_time:19905ms step_avg:93.45ms
step:214/1750 train_time:19999ms step_avg:93.45ms
step:215/1750 train_time:20093ms step_avg:93.46ms
step:216/1750 train_time:20187ms step_avg:93.46ms
step:217/1750 train_time:20281ms step_avg:93.46ms
step:218/1750 train_time:20375ms step_avg:93.46ms
step:219/1750 train_time:20469ms step_avg:93.47ms
step:220/1750 train_time:20562ms step_avg:93.46ms
step:221/1750 train_time:20656ms step_avg:93.47ms
step:222/1750 train_time:20750ms step_avg:93.47ms
step:223/1750 train_time:20843ms step_avg:93.47ms
step:224/1750 train_time:20937ms step_avg:93.47ms
step:225/1750 train_time:21030ms step_avg:93.47ms
step:226/1750 train_time:21124ms step_avg:93.47ms
step:227/1750 train_time:21218ms step_avg:93.47ms
step:228/1750 train_time:21312ms step_avg:93.48ms
step:229/1750 train_time:21406ms step_avg:93.48ms
step:230/1750 train_time:21500ms step_avg:93.48ms
step:231/1750 train_time:21594ms step_avg:93.48ms
step:232/1750 train_time:21688ms step_avg:93.48ms
step:233/1750 train_time:21782ms step_avg:93.48ms
step:234/1750 train_time:21876ms step_avg:93.49ms
step:235/1750 train_time:21970ms step_avg:93.49ms
step:236/1750 train_time:22064ms step_avg:93.49ms
step:237/1750 train_time:22158ms step_avg:93.50ms
step:238/1750 train_time:22253ms step_avg:93.50ms
step:239/1750 train_time:22346ms step_avg:93.50ms
step:240/1750 train_time:22440ms step_avg:93.50ms
step:241/1750 train_time:22534ms step_avg:93.50ms
step:242/1750 train_time:22628ms step_avg:93.50ms
step:243/1750 train_time:22722ms step_avg:93.50ms
step:244/1750 train_time:22815ms step_avg:93.51ms
step:245/1750 train_time:22909ms step_avg:93.51ms
step:246/1750 train_time:23002ms step_avg:93.50ms
step:247/1750 train_time:23096ms step_avg:93.50ms
step:248/1750 train_time:23190ms step_avg:93.51ms
step:249/1750 train_time:23284ms step_avg:93.51ms
step:250/1750 train_time:23378ms step_avg:93.51ms
step:250/1750 val_loss:4.0963 train_time:23461ms step_avg:93.84ms
step:251/1750 train_time:23485ms step_avg:93.57ms
step:252/1750 train_time:23573ms step_avg:93.54ms
step:253/1750 train_time:23672ms step_avg:93.57ms
step:254/1750 train_time:23769ms step_avg:93.58ms
step:255/1750 train_time:23862ms step_avg:93.58ms
step:256/1750 train_time:23955ms step_avg:93.57ms
step:257/1750 train_time:24048ms step_avg:93.57ms
step:258/1750 train_time:24141ms step_avg:93.57ms
step:259/1750 train_time:24234ms step_avg:93.57ms
step:260/1750 train_time:24327ms step_avg:93.57ms
step:261/1750 train_time:24421ms step_avg:93.57ms
step:262/1750 train_time:24517ms step_avg:93.58ms
step:263/1750 train_time:24613ms step_avg:93.59ms
step:264/1750 train_time:24708ms step_avg:93.59ms
step:265/1750 train_time:24803ms step_avg:93.59ms
step:266/1750 train_time:24897ms step_avg:93.60ms
step:267/1750 train_time:24990ms step_avg:93.60ms
step:268/1750 train_time:25084ms step_avg:93.60ms
step:269/1750 train_time:25178ms step_avg:93.60ms
step:270/1750 train_time:25272ms step_avg:93.60ms
step:271/1750 train_time:25366ms step_avg:93.60ms
step:272/1750 train_time:25461ms step_avg:93.61ms
step:273/1750 train_time:25557ms step_avg:93.61ms
step:274/1750 train_time:25652ms step_avg:93.62ms
step:275/1750 train_time:25747ms step_avg:93.62ms
step:276/1750 train_time:25842ms step_avg:93.63ms
step:277/1750 train_time:25936ms step_avg:93.63ms
step:278/1750 train_time:26030ms step_avg:93.63ms
step:279/1750 train_time:26124ms step_avg:93.63ms
step:280/1750 train_time:26218ms step_avg:93.64ms
step:281/1750 train_time:26311ms step_avg:93.63ms
step:282/1750 train_time:26405ms step_avg:93.64ms
step:283/1750 train_time:26500ms step_avg:93.64ms
step:284/1750 train_time:26594ms step_avg:93.64ms
step:285/1750 train_time:26690ms step_avg:93.65ms
step:286/1750 train_time:26784ms step_avg:93.65ms
step:287/1750 train_time:26879ms step_avg:93.66ms
step:288/1750 train_time:26973ms step_avg:93.66ms
step:289/1750 train_time:27067ms step_avg:93.66ms
step:290/1750 train_time:27161ms step_avg:93.66ms
step:291/1750 train_time:27254ms step_avg:93.66ms
step:292/1750 train_time:27348ms step_avg:93.66ms
step:293/1750 train_time:27443ms step_avg:93.66ms
step:294/1750 train_time:27537ms step_avg:93.66ms
step:295/1750 train_time:27632ms step_avg:93.67ms
step:296/1750 train_time:27726ms step_avg:93.67ms
step:297/1750 train_time:27822ms step_avg:93.68ms
step:298/1750 train_time:27917ms step_avg:93.68ms
step:299/1750 train_time:28011ms step_avg:93.68ms
step:300/1750 train_time:28105ms step_avg:93.68ms
step:301/1750 train_time:28199ms step_avg:93.68ms
step:302/1750 train_time:28292ms step_avg:93.68ms
step:303/1750 train_time:28386ms step_avg:93.68ms
step:304/1750 train_time:28481ms step_avg:93.69ms
step:305/1750 train_time:28575ms step_avg:93.69ms
step:306/1750 train_time:28670ms step_avg:93.69ms
step:307/1750 train_time:28764ms step_avg:93.69ms
step:308/1750 train_time:28859ms step_avg:93.70ms
step:309/1750 train_time:28953ms step_avg:93.70ms
step:310/1750 train_time:29047ms step_avg:93.70ms
step:311/1750 train_time:29141ms step_avg:93.70ms
step:312/1750 train_time:29234ms step_avg:93.70ms
step:313/1750 train_time:29328ms step_avg:93.70ms
step:314/1750 train_time:29422ms step_avg:93.70ms
step:315/1750 train_time:29516ms step_avg:93.70ms
step:316/1750 train_time:29610ms step_avg:93.70ms
step:317/1750 train_time:29705ms step_avg:93.71ms
step:318/1750 train_time:29799ms step_avg:93.71ms
step:319/1750 train_time:29893ms step_avg:93.71ms
step:320/1750 train_time:29987ms step_avg:93.71ms
step:321/1750 train_time:30082ms step_avg:93.71ms
step:322/1750 train_time:30176ms step_avg:93.71ms
step:323/1750 train_time:30270ms step_avg:93.71ms
step:324/1750 train_time:30364ms step_avg:93.71ms
step:325/1750 train_time:30458ms step_avg:93.72ms
step:326/1750 train_time:30552ms step_avg:93.72ms
step:327/1750 train_time:30646ms step_avg:93.72ms
step:328/1750 train_time:30741ms step_avg:93.72ms
step:329/1750 train_time:30836ms step_avg:93.73ms
step:330/1750 train_time:30930ms step_avg:93.73ms
step:331/1750 train_time:31024ms step_avg:93.73ms
step:332/1750 train_time:31119ms step_avg:93.73ms
step:333/1750 train_time:31213ms step_avg:93.73ms
step:334/1750 train_time:31307ms step_avg:93.73ms
step:335/1750 train_time:31401ms step_avg:93.74ms
step:336/1750 train_time:31495ms step_avg:93.74ms
step:337/1750 train_time:31590ms step_avg:93.74ms
step:338/1750 train_time:31685ms step_avg:93.74ms
step:339/1750 train_time:31779ms step_avg:93.74ms
step:340/1750 train_time:31873ms step_avg:93.74ms
step:341/1750 train_time:31967ms step_avg:93.75ms
step:342/1750 train_time:32061ms step_avg:93.75ms
step:343/1750 train_time:32155ms step_avg:93.75ms
step:344/1750 train_time:32249ms step_avg:93.75ms
step:345/1750 train_time:32344ms step_avg:93.75ms
step:346/1750 train_time:32437ms step_avg:93.75ms
step:347/1750 train_time:32531ms step_avg:93.75ms
step:348/1750 train_time:32625ms step_avg:93.75ms
step:349/1750 train_time:32719ms step_avg:93.75ms
step:350/1750 train_time:32813ms step_avg:93.75ms
step:351/1750 train_time:32908ms step_avg:93.75ms
step:352/1750 train_time:33003ms step_avg:93.76ms
step:353/1750 train_time:33098ms step_avg:93.76ms
step:354/1750 train_time:33191ms step_avg:93.76ms
step:355/1750 train_time:33285ms step_avg:93.76ms
step:356/1750 train_time:33379ms step_avg:93.76ms
step:357/1750 train_time:33473ms step_avg:93.76ms
step:358/1750 train_time:33567ms step_avg:93.76ms
step:359/1750 train_time:33662ms step_avg:93.77ms
step:360/1750 train_time:33755ms step_avg:93.76ms
step:361/1750 train_time:33850ms step_avg:93.77ms
step:362/1750 train_time:33944ms step_avg:93.77ms
step:363/1750 train_time:34038ms step_avg:93.77ms
step:364/1750 train_time:34132ms step_avg:93.77ms
step:365/1750 train_time:34226ms step_avg:93.77ms
step:366/1750 train_time:34321ms step_avg:93.77ms
step:367/1750 train_time:34415ms step_avg:93.77ms
step:368/1750 train_time:34509ms step_avg:93.77ms
step:369/1750 train_time:34603ms step_avg:93.77ms
step:370/1750 train_time:34696ms step_avg:93.77ms
step:371/1750 train_time:34791ms step_avg:93.78ms
step:372/1750 train_time:34885ms step_avg:93.78ms
step:373/1750 train_time:34979ms step_avg:93.78ms
step:374/1750 train_time:35073ms step_avg:93.78ms
step:375/1750 train_time:35168ms step_avg:93.78ms
step:375/1750 val_loss:3.8919 train_time:35251ms step_avg:94.00ms
step:376/1750 train_time:35274ms step_avg:93.81ms
step:377/1750 train_time:35364ms step_avg:93.80ms
step:378/1750 train_time:35461ms step_avg:93.81ms
step:379/1750 train_time:35555ms step_avg:93.81ms
step:380/1750 train_time:35648ms step_avg:93.81ms
step:381/1750 train_time:35742ms step_avg:93.81ms
step:382/1750 train_time:35835ms step_avg:93.81ms
step:383/1750 train_time:35928ms step_avg:93.81ms
step:384/1750 train_time:36021ms step_avg:93.81ms
step:385/1750 train_time:36116ms step_avg:93.81ms
step:386/1750 train_time:36210ms step_avg:93.81ms
step:387/1750 train_time:36307ms step_avg:93.82ms
step:388/1750 train_time:36403ms step_avg:93.82ms
step:389/1750 train_time:36499ms step_avg:93.83ms
step:390/1750 train_time:36593ms step_avg:93.83ms
step:391/1750 train_time:36688ms step_avg:93.83ms
step:392/1750 train_time:36784ms step_avg:93.84ms
step:393/1750 train_time:36879ms step_avg:93.84ms
step:394/1750 train_time:36974ms step_avg:93.84ms
step:395/1750 train_time:37069ms step_avg:93.85ms
step:396/1750 train_time:37165ms step_avg:93.85ms
step:397/1750 train_time:37262ms step_avg:93.86ms
step:398/1750 train_time:37358ms step_avg:93.87ms
step:399/1750 train_time:37455ms step_avg:93.87ms
step:400/1750 train_time:37553ms step_avg:93.88ms
step:401/1750 train_time:37649ms step_avg:93.89ms
step:402/1750 train_time:37745ms step_avg:93.89ms
step:403/1750 train_time:37840ms step_avg:93.90ms
step:404/1750 train_time:37936ms step_avg:93.90ms
step:405/1750 train_time:38031ms step_avg:93.90ms
step:406/1750 train_time:38126ms step_avg:93.91ms
step:407/1750 train_time:38221ms step_avg:93.91ms
step:408/1750 train_time:38318ms step_avg:93.92ms
step:409/1750 train_time:38414ms step_avg:93.92ms
step:410/1750 train_time:38510ms step_avg:93.93ms
step:411/1750 train_time:38607ms step_avg:93.93ms
step:412/1750 train_time:38703ms step_avg:93.94ms
step:413/1750 train_time:38799ms step_avg:93.95ms
step:414/1750 train_time:38896ms step_avg:93.95ms
step:415/1750 train_time:38991ms step_avg:93.95ms
step:416/1750 train_time:39087ms step_avg:93.96ms
step:417/1750 train_time:39183ms step_avg:93.96ms
step:418/1750 train_time:39279ms step_avg:93.97ms
step:419/1750 train_time:39374ms step_avg:93.97ms
step:420/1750 train_time:39470ms step_avg:93.98ms
step:421/1750 train_time:39567ms step_avg:93.98ms
step:422/1750 train_time:39664ms step_avg:93.99ms
step:423/1750 train_time:39759ms step_avg:93.99ms
step:424/1750 train_time:39855ms step_avg:94.00ms
step:425/1750 train_time:39952ms step_avg:94.00ms
step:426/1750 train_time:40047ms step_avg:94.01ms
step:427/1750 train_time:40143ms step_avg:94.01ms
step:428/1750 train_time:40238ms step_avg:94.01ms
step:429/1750 train_time:40334ms step_avg:94.02ms
step:430/1750 train_time:40430ms step_avg:94.02ms
step:431/1750 train_time:40526ms step_avg:94.03ms
step:432/1750 train_time:40623ms step_avg:94.03ms
step:433/1750 train_time:40719ms step_avg:94.04ms
step:434/1750 train_time:40815ms step_avg:94.04ms
step:435/1750 train_time:40910ms step_avg:94.05ms
step:436/1750 train_time:41006ms step_avg:94.05ms
step:437/1750 train_time:41101ms step_avg:94.05ms
step:438/1750 train_time:41198ms step_avg:94.06ms
step:439/1750 train_time:41295ms step_avg:94.06ms
step:440/1750 train_time:41391ms step_avg:94.07ms
step:441/1750 train_time:41487ms step_avg:94.07ms
step:442/1750 train_time:41582ms step_avg:94.08ms
step:443/1750 train_time:41678ms step_avg:94.08ms
step:444/1750 train_time:41774ms step_avg:94.09ms
step:445/1750 train_time:41869ms step_avg:94.09ms
step:446/1750 train_time:41966ms step_avg:94.09ms
step:447/1750 train_time:42062ms step_avg:94.10ms
step:448/1750 train_time:42157ms step_avg:94.10ms
step:449/1750 train_time:42253ms step_avg:94.11ms
step:450/1750 train_time:42349ms step_avg:94.11ms
step:451/1750 train_time:42445ms step_avg:94.11ms
step:452/1750 train_time:42541ms step_avg:94.12ms
step:453/1750 train_time:42638ms step_avg:94.12ms
step:454/1750 train_time:42734ms step_avg:94.13ms
step:455/1750 train_time:42831ms step_avg:94.13ms
step:456/1750 train_time:42927ms step_avg:94.14ms
step:457/1750 train_time:43023ms step_avg:94.14ms
step:458/1750 train_time:43119ms step_avg:94.15ms
step:459/1750 train_time:43215ms step_avg:94.15ms
step:460/1750 train_time:43311ms step_avg:94.15ms
step:461/1750 train_time:43407ms step_avg:94.16ms
step:462/1750 train_time:43504ms step_avg:94.16ms
step:463/1750 train_time:43600ms step_avg:94.17ms
step:464/1750 train_time:43697ms step_avg:94.17ms
step:465/1750 train_time:43793ms step_avg:94.18ms
step:466/1750 train_time:43889ms step_avg:94.18ms
step:467/1750 train_time:43985ms step_avg:94.19ms
step:468/1750 train_time:44082ms step_avg:94.19ms
step:469/1750 train_time:44178ms step_avg:94.20ms
step:470/1750 train_time:44274ms step_avg:94.20ms
step:471/1750 train_time:44369ms step_avg:94.20ms
step:472/1750 train_time:44465ms step_avg:94.21ms
step:473/1750 train_time:44561ms step_avg:94.21ms
step:474/1750 train_time:44657ms step_avg:94.21ms
step:475/1750 train_time:44754ms step_avg:94.22ms
step:476/1750 train_time:44850ms step_avg:94.22ms
step:477/1750 train_time:44946ms step_avg:94.23ms
step:478/1750 train_time:45043ms step_avg:94.23ms
step:479/1750 train_time:45139ms step_avg:94.24ms
step:480/1750 train_time:45235ms step_avg:94.24ms
step:481/1750 train_time:45331ms step_avg:94.24ms
step:482/1750 train_time:45427ms step_avg:94.25ms
step:483/1750 train_time:45523ms step_avg:94.25ms
step:484/1750 train_time:45619ms step_avg:94.25ms
step:485/1750 train_time:45716ms step_avg:94.26ms
step:486/1750 train_time:45813ms step_avg:94.26ms
step:487/1750 train_time:45908ms step_avg:94.27ms
step:488/1750 train_time:46003ms step_avg:94.27ms
step:489/1750 train_time:46099ms step_avg:94.27ms
step:490/1750 train_time:46195ms step_avg:94.27ms
step:491/1750 train_time:46291ms step_avg:94.28ms
step:492/1750 train_time:46387ms step_avg:94.28ms
step:493/1750 train_time:46483ms step_avg:94.29ms
step:494/1750 train_time:46578ms step_avg:94.29ms
step:495/1750 train_time:46675ms step_avg:94.29ms
step:496/1750 train_time:46771ms step_avg:94.30ms
step:497/1750 train_time:46866ms step_avg:94.30ms
step:498/1750 train_time:46962ms step_avg:94.30ms
step:499/1750 train_time:47058ms step_avg:94.30ms
step:500/1750 train_time:47154ms step_avg:94.31ms
step:500/1750 val_loss:3.7478 train_time:47238ms step_avg:94.48ms
step:501/1750 train_time:47260ms step_avg:94.33ms
step:502/1750 train_time:47352ms step_avg:94.33ms
step:503/1750 train_time:47452ms step_avg:94.34ms
step:504/1750 train_time:47549ms step_avg:94.34ms
step:505/1750 train_time:47644ms step_avg:94.34ms
step:506/1750 train_time:47739ms step_avg:94.35ms
step:507/1750 train_time:47834ms step_avg:94.35ms
step:508/1750 train_time:47929ms step_avg:94.35ms
step:509/1750 train_time:48024ms step_avg:94.35ms
step:510/1750 train_time:48119ms step_avg:94.35ms
step:511/1750 train_time:48216ms step_avg:94.36ms
step:512/1750 train_time:48313ms step_avg:94.36ms
step:513/1750 train_time:48410ms step_avg:94.37ms
step:514/1750 train_time:48507ms step_avg:94.37ms
step:515/1750 train_time:48602ms step_avg:94.37ms
step:516/1750 train_time:48698ms step_avg:94.38ms
step:517/1750 train_time:48794ms step_avg:94.38ms
step:518/1750 train_time:48889ms step_avg:94.38ms
step:519/1750 train_time:48985ms step_avg:94.38ms
step:520/1750 train_time:49080ms step_avg:94.38ms
step:521/1750 train_time:49176ms step_avg:94.39ms
step:522/1750 train_time:49273ms step_avg:94.39ms
step:523/1750 train_time:49370ms step_avg:94.40ms
step:524/1750 train_time:49467ms step_avg:94.40ms
step:525/1750 train_time:49564ms step_avg:94.41ms
step:526/1750 train_time:49661ms step_avg:94.41ms
step:527/1750 train_time:49757ms step_avg:94.41ms
step:528/1750 train_time:49852ms step_avg:94.42ms
step:529/1750 train_time:49948ms step_avg:94.42ms
step:530/1750 train_time:50044ms step_avg:94.42ms
step:531/1750 train_time:50141ms step_avg:94.43ms
step:532/1750 train_time:50238ms step_avg:94.43ms
step:533/1750 train_time:50335ms step_avg:94.44ms
step:534/1750 train_time:50432ms step_avg:94.44ms
step:535/1750 train_time:50529ms step_avg:94.45ms
step:536/1750 train_time:50625ms step_avg:94.45ms
step:537/1750 train_time:50722ms step_avg:94.45ms
step:538/1750 train_time:50817ms step_avg:94.46ms
step:539/1750 train_time:50914ms step_avg:94.46ms
step:540/1750 train_time:51009ms step_avg:94.46ms
step:541/1750 train_time:51105ms step_avg:94.46ms
step:542/1750 train_time:51202ms step_avg:94.47ms
step:543/1750 train_time:51299ms step_avg:94.47ms
step:544/1750 train_time:51396ms step_avg:94.48ms
step:545/1750 train_time:51492ms step_avg:94.48ms
step:546/1750 train_time:51588ms step_avg:94.48ms
step:547/1750 train_time:51685ms step_avg:94.49ms
step:548/1750 train_time:51781ms step_avg:94.49ms
step:549/1750 train_time:51877ms step_avg:94.49ms
step:550/1750 train_time:51973ms step_avg:94.50ms
step:551/1750 train_time:52069ms step_avg:94.50ms
step:552/1750 train_time:52166ms step_avg:94.50ms
step:553/1750 train_time:52262ms step_avg:94.51ms
step:554/1750 train_time:52360ms step_avg:94.51ms
step:555/1750 train_time:52457ms step_avg:94.52ms
step:556/1750 train_time:52553ms step_avg:94.52ms
step:557/1750 train_time:52651ms step_avg:94.53ms
step:558/1750 train_time:52747ms step_avg:94.53ms
step:559/1750 train_time:52844ms step_avg:94.53ms
step:560/1750 train_time:52941ms step_avg:94.54ms
step:561/1750 train_time:53038ms step_avg:94.54ms
step:562/1750 train_time:53135ms step_avg:94.55ms
step:563/1750 train_time:53232ms step_avg:94.55ms
step:564/1750 train_time:53329ms step_avg:94.55ms
step:565/1750 train_time:53425ms step_avg:94.56ms
step:566/1750 train_time:53522ms step_avg:94.56ms
step:567/1750 train_time:53619ms step_avg:94.57ms
step:568/1750 train_time:53716ms step_avg:94.57ms
step:569/1750 train_time:53813ms step_avg:94.57ms
step:570/1750 train_time:53910ms step_avg:94.58ms
step:571/1750 train_time:54006ms step_avg:94.58ms
step:572/1750 train_time:54102ms step_avg:94.58ms
step:573/1750 train_time:54199ms step_avg:94.59ms
step:574/1750 train_time:54295ms step_avg:94.59ms
step:575/1750 train_time:54392ms step_avg:94.59ms
step:576/1750 train_time:54488ms step_avg:94.60ms
step:577/1750 train_time:54584ms step_avg:94.60ms
step:578/1750 train_time:54682ms step_avg:94.60ms
step:579/1750 train_time:54779ms step_avg:94.61ms
step:580/1750 train_time:54876ms step_avg:94.61ms
step:581/1750 train_time:54973ms step_avg:94.62ms
step:582/1750 train_time:55069ms step_avg:94.62ms
step:583/1750 train_time:55165ms step_avg:94.62ms
step:584/1750 train_time:55261ms step_avg:94.63ms
step:585/1750 train_time:55358ms step_avg:94.63ms
step:586/1750 train_time:55454ms step_avg:94.63ms
step:587/1750 train_time:55550ms step_avg:94.63ms
step:588/1750 train_time:55646ms step_avg:94.64ms
step:589/1750 train_time:55742ms step_avg:94.64ms
step:590/1750 train_time:55839ms step_avg:94.64ms
step:591/1750 train_time:55935ms step_avg:94.65ms
step:592/1750 train_time:56032ms step_avg:94.65ms
step:593/1750 train_time:56128ms step_avg:94.65ms
step:594/1750 train_time:56224ms step_avg:94.65ms
step:595/1750 train_time:56321ms step_avg:94.66ms
step:596/1750 train_time:56417ms step_avg:94.66ms
step:597/1750 train_time:56514ms step_avg:94.66ms
step:598/1750 train_time:56610ms step_avg:94.67ms
step:599/1750 train_time:56706ms step_avg:94.67ms
step:600/1750 train_time:56803ms step_avg:94.67ms
step:601/1750 train_time:56900ms step_avg:94.68ms
step:602/1750 train_time:56996ms step_avg:94.68ms
step:603/1750 train_time:57092ms step_avg:94.68ms
step:604/1750 train_time:57189ms step_avg:94.68ms
step:605/1750 train_time:57285ms step_avg:94.69ms
step:606/1750 train_time:57381ms step_avg:94.69ms
step:607/1750 train_time:57478ms step_avg:94.69ms
step:608/1750 train_time:57575ms step_avg:94.70ms
step:609/1750 train_time:57672ms step_avg:94.70ms
step:610/1750 train_time:57768ms step_avg:94.70ms
step:611/1750 train_time:57864ms step_avg:94.70ms
step:612/1750 train_time:57961ms step_avg:94.71ms
step:613/1750 train_time:58057ms step_avg:94.71ms
step:614/1750 train_time:58154ms step_avg:94.71ms
step:615/1750 train_time:58250ms step_avg:94.72ms
step:616/1750 train_time:58347ms step_avg:94.72ms
step:617/1750 train_time:58444ms step_avg:94.72ms
step:618/1750 train_time:58541ms step_avg:94.73ms
step:619/1750 train_time:58638ms step_avg:94.73ms
step:620/1750 train_time:58734ms step_avg:94.73ms
step:621/1750 train_time:58831ms step_avg:94.74ms
step:622/1750 train_time:58927ms step_avg:94.74ms
step:623/1750 train_time:59024ms step_avg:94.74ms
step:624/1750 train_time:59121ms step_avg:94.74ms
step:625/1750 train_time:59217ms step_avg:94.75ms
step:625/1750 val_loss:3.6616 train_time:59303ms step_avg:94.88ms
step:626/1750 train_time:59325ms step_avg:94.77ms
step:627/1750 train_time:59419ms step_avg:94.77ms
step:628/1750 train_time:59520ms step_avg:94.78ms
step:629/1750 train_time:59616ms step_avg:94.78ms
step:630/1750 train_time:59713ms step_avg:94.78ms
step:631/1750 train_time:59809ms step_avg:94.78ms
step:632/1750 train_time:59905ms step_avg:94.79ms
step:633/1750 train_time:60000ms step_avg:94.79ms
step:634/1750 train_time:60095ms step_avg:94.79ms
step:635/1750 train_time:60190ms step_avg:94.79ms
step:636/1750 train_time:60286ms step_avg:94.79ms
step:637/1750 train_time:60384ms step_avg:94.79ms
step:638/1750 train_time:60482ms step_avg:94.80ms
step:639/1750 train_time:60578ms step_avg:94.80ms
step:640/1750 train_time:60675ms step_avg:94.80ms
step:641/1750 train_time:60771ms step_avg:94.81ms
step:642/1750 train_time:60867ms step_avg:94.81ms
step:643/1750 train_time:60962ms step_avg:94.81ms
step:644/1750 train_time:61058ms step_avg:94.81ms
step:645/1750 train_time:61154ms step_avg:94.81ms
step:646/1750 train_time:61250ms step_avg:94.81ms
step:647/1750 train_time:61346ms step_avg:94.82ms
step:648/1750 train_time:61444ms step_avg:94.82ms
step:649/1750 train_time:61540ms step_avg:94.82ms
step:650/1750 train_time:61637ms step_avg:94.83ms
step:651/1750 train_time:61736ms step_avg:94.83ms
step:652/1750 train_time:61834ms step_avg:94.84ms
step:653/1750 train_time:61932ms step_avg:94.84ms
step:654/1750 train_time:62029ms step_avg:94.85ms
step:655/1750 train_time:62126ms step_avg:94.85ms
step:656/1750 train_time:62223ms step_avg:94.85ms
step:657/1750 train_time:62321ms step_avg:94.86ms
step:658/1750 train_time:62420ms step_avg:94.86ms
step:659/1750 train_time:62518ms step_avg:94.87ms
step:660/1750 train_time:62617ms step_avg:94.87ms
step:661/1750 train_time:62715ms step_avg:94.88ms
step:662/1750 train_time:62813ms step_avg:94.88ms
step:663/1750 train_time:62910ms step_avg:94.89ms
step:664/1750 train_time:63007ms step_avg:94.89ms
step:665/1750 train_time:63105ms step_avg:94.89ms
step:666/1750 train_time:63203ms step_avg:94.90ms
step:667/1750 train_time:63300ms step_avg:94.90ms
step:668/1750 train_time:63398ms step_avg:94.91ms
step:669/1750 train_time:63495ms step_avg:94.91ms
step:670/1750 train_time:63594ms step_avg:94.92ms
step:671/1750 train_time:63692ms step_avg:94.92ms
step:672/1750 train_time:63790ms step_avg:94.93ms
step:673/1750 train_time:63887ms step_avg:94.93ms
step:674/1750 train_time:63985ms step_avg:94.93ms
step:675/1750 train_time:64083ms step_avg:94.94ms
step:676/1750 train_time:64180ms step_avg:94.94ms
step:677/1750 train_time:64278ms step_avg:94.95ms
step:678/1750 train_time:64376ms step_avg:94.95ms
step:679/1750 train_time:64474ms step_avg:94.95ms
step:680/1750 train_time:64571ms step_avg:94.96ms
step:681/1750 train_time:64670ms step_avg:94.96ms
step:682/1750 train_time:64767ms step_avg:94.97ms
step:683/1750 train_time:64865ms step_avg:94.97ms
step:684/1750 train_time:64963ms step_avg:94.97ms
step:685/1750 train_time:65061ms step_avg:94.98ms
step:686/1750 train_time:65159ms step_avg:94.98ms
step:687/1750 train_time:65256ms step_avg:94.99ms
step:688/1750 train_time:65353ms step_avg:94.99ms
step:689/1750 train_time:65451ms step_avg:94.99ms
step:690/1750 train_time:65549ms step_avg:95.00ms
step:691/1750 train_time:65646ms step_avg:95.00ms
step:692/1750 train_time:65745ms step_avg:95.01ms
step:693/1750 train_time:65843ms step_avg:95.01ms
step:694/1750 train_time:65941ms step_avg:95.02ms
step:695/1750 train_time:66039ms step_avg:95.02ms
step:696/1750 train_time:66137ms step_avg:95.02ms
step:697/1750 train_time:66235ms step_avg:95.03ms
step:698/1750 train_time:66333ms step_avg:95.03ms
step:699/1750 train_time:66431ms step_avg:95.04ms
step:700/1750 train_time:66529ms step_avg:95.04ms
step:701/1750 train_time:66626ms step_avg:95.04ms
step:702/1750 train_time:66726ms step_avg:95.05ms
step:703/1750 train_time:66824ms step_avg:95.06ms
step:704/1750 train_time:66923ms step_avg:95.06ms
step:705/1750 train_time:67021ms step_avg:95.06ms
step:706/1750 train_time:67118ms step_avg:95.07ms
step:707/1750 train_time:67216ms step_avg:95.07ms
step:708/1750 train_time:67313ms step_avg:95.07ms
step:709/1750 train_time:67411ms step_avg:95.08ms
step:710/1750 train_time:67508ms step_avg:95.08ms
step:711/1750 train_time:67605ms step_avg:95.08ms
step:712/1750 train_time:67703ms step_avg:95.09ms
step:713/1750 train_time:67802ms step_avg:95.09ms
step:714/1750 train_time:67900ms step_avg:95.10ms
step:715/1750 train_time:67998ms step_avg:95.10ms
step:716/1750 train_time:68096ms step_avg:95.11ms
step:717/1750 train_time:68194ms step_avg:95.11ms
step:718/1750 train_time:68292ms step_avg:95.11ms
step:719/1750 train_time:68390ms step_avg:95.12ms
step:720/1750 train_time:68487ms step_avg:95.12ms
step:721/1750 train_time:68584ms step_avg:95.12ms
step:722/1750 train_time:68682ms step_avg:95.13ms
step:723/1750 train_time:68780ms step_avg:95.13ms
step:724/1750 train_time:68878ms step_avg:95.14ms
step:725/1750 train_time:68976ms step_avg:95.14ms
step:726/1750 train_time:69074ms step_avg:95.14ms
step:727/1750 train_time:69171ms step_avg:95.15ms
step:728/1750 train_time:69268ms step_avg:95.15ms
step:729/1750 train_time:69366ms step_avg:95.15ms
step:730/1750 train_time:69464ms step_avg:95.16ms
step:731/1750 train_time:69562ms step_avg:95.16ms
step:732/1750 train_time:69660ms step_avg:95.16ms
step:733/1750 train_time:69758ms step_avg:95.17ms
step:734/1750 train_time:69856ms step_avg:95.17ms
step:735/1750 train_time:69955ms step_avg:95.18ms
step:736/1750 train_time:70053ms step_avg:95.18ms
step:737/1750 train_time:70150ms step_avg:95.18ms
step:738/1750 train_time:70248ms step_avg:95.19ms
step:739/1750 train_time:70346ms step_avg:95.19ms
step:740/1750 train_time:70443ms step_avg:95.19ms
step:741/1750 train_time:70541ms step_avg:95.20ms
step:742/1750 train_time:70639ms step_avg:95.20ms
step:743/1750 train_time:70738ms step_avg:95.21ms
step:744/1750 train_time:70837ms step_avg:95.21ms
step:745/1750 train_time:70935ms step_avg:95.22ms
step:746/1750 train_time:71033ms step_avg:95.22ms
step:747/1750 train_time:71132ms step_avg:95.22ms
step:748/1750 train_time:71229ms step_avg:95.23ms
step:749/1750 train_time:71327ms step_avg:95.23ms
step:750/1750 train_time:71425ms step_avg:95.23ms
step:750/1750 val_loss:3.5964 train_time:71511ms step_avg:95.35ms
step:751/1750 train_time:71533ms step_avg:95.25ms
step:752/1750 train_time:71626ms step_avg:95.25ms
step:753/1750 train_time:71725ms step_avg:95.25ms
step:754/1750 train_time:71823ms step_avg:95.26ms
step:755/1750 train_time:71921ms step_avg:95.26ms
step:756/1750 train_time:72018ms step_avg:95.26ms
step:757/1750 train_time:72116ms step_avg:95.27ms
step:758/1750 train_time:72212ms step_avg:95.27ms
step:759/1750 train_time:72308ms step_avg:95.27ms
step:760/1750 train_time:72406ms step_avg:95.27ms
step:761/1750 train_time:72507ms step_avg:95.28ms
step:762/1750 train_time:72607ms step_avg:95.28ms
step:763/1750 train_time:72706ms step_avg:95.29ms
step:764/1750 train_time:72804ms step_avg:95.29ms
step:765/1750 train_time:72902ms step_avg:95.30ms
step:766/1750 train_time:72999ms step_avg:95.30ms
step:767/1750 train_time:73097ms step_avg:95.30ms
step:768/1750 train_time:73194ms step_avg:95.30ms
step:769/1750 train_time:73291ms step_avg:95.31ms
step:770/1750 train_time:73389ms step_avg:95.31ms
step:771/1750 train_time:73487ms step_avg:95.31ms
step:772/1750 train_time:73586ms step_avg:95.32ms
step:773/1750 train_time:73685ms step_avg:95.32ms
step:774/1750 train_time:73784ms step_avg:95.33ms
step:775/1750 train_time:73883ms step_avg:95.33ms
step:776/1750 train_time:73980ms step_avg:95.34ms
step:777/1750 train_time:74078ms step_avg:95.34ms
step:778/1750 train_time:74175ms step_avg:95.34ms
step:779/1750 train_time:74273ms step_avg:95.34ms
step:780/1750 train_time:74371ms step_avg:95.35ms
step:781/1750 train_time:74469ms step_avg:95.35ms
step:782/1750 train_time:74568ms step_avg:95.36ms
step:783/1750 train_time:74667ms step_avg:95.36ms
step:784/1750 train_time:74765ms step_avg:95.36ms
step:785/1750 train_time:74864ms step_avg:95.37ms
step:786/1750 train_time:74962ms step_avg:95.37ms
step:787/1750 train_time:75060ms step_avg:95.38ms
step:788/1750 train_time:75159ms step_avg:95.38ms
step:789/1750 train_time:75256ms step_avg:95.38ms
step:790/1750 train_time:75354ms step_avg:95.38ms
step:791/1750 train_time:75453ms step_avg:95.39ms
step:792/1750 train_time:75551ms step_avg:95.39ms
step:793/1750 train_time:75650ms step_avg:95.40ms
step:794/1750 train_time:75748ms step_avg:95.40ms
step:795/1750 train_time:75845ms step_avg:95.40ms
step:796/1750 train_time:75943ms step_avg:95.41ms
step:797/1750 train_time:76041ms step_avg:95.41ms
step:798/1750 train_time:76139ms step_avg:95.41ms
step:799/1750 train_time:76237ms step_avg:95.42ms
step:800/1750 train_time:76335ms step_avg:95.42ms
step:801/1750 train_time:76434ms step_avg:95.42ms
step:802/1750 train_time:76533ms step_avg:95.43ms
step:803/1750 train_time:76632ms step_avg:95.43ms
step:804/1750 train_time:76731ms step_avg:95.44ms
step:805/1750 train_time:76830ms step_avg:95.44ms
step:806/1750 train_time:76927ms step_avg:95.44ms
step:807/1750 train_time:77026ms step_avg:95.45ms
step:808/1750 train_time:77125ms step_avg:95.45ms
step:809/1750 train_time:77224ms step_avg:95.46ms
step:810/1750 train_time:77323ms step_avg:95.46ms
step:811/1750 train_time:77421ms step_avg:95.46ms
step:812/1750 train_time:77519ms step_avg:95.47ms
step:813/1750 train_time:77618ms step_avg:95.47ms
step:814/1750 train_time:77716ms step_avg:95.47ms
step:815/1750 train_time:77815ms step_avg:95.48ms
step:816/1750 train_time:77914ms step_avg:95.48ms
step:817/1750 train_time:78013ms step_avg:95.49ms
step:818/1750 train_time:78110ms step_avg:95.49ms
step:819/1750 train_time:78208ms step_avg:95.49ms
step:820/1750 train_time:78306ms step_avg:95.49ms
step:821/1750 train_time:78405ms step_avg:95.50ms
step:822/1750 train_time:78504ms step_avg:95.50ms
step:823/1750 train_time:78603ms step_avg:95.51ms
step:824/1750 train_time:78701ms step_avg:95.51ms
step:825/1750 train_time:78800ms step_avg:95.51ms
step:826/1750 train_time:78898ms step_avg:95.52ms
step:827/1750 train_time:78996ms step_avg:95.52ms
step:828/1750 train_time:79095ms step_avg:95.53ms
step:829/1750 train_time:79193ms step_avg:95.53ms
step:830/1750 train_time:79291ms step_avg:95.53ms
step:831/1750 train_time:79391ms step_avg:95.54ms
step:832/1750 train_time:79489ms step_avg:95.54ms
step:833/1750 train_time:79587ms step_avg:95.54ms
step:834/1750 train_time:79685ms step_avg:95.54ms
step:835/1750 train_time:79783ms step_avg:95.55ms
step:836/1750 train_time:79881ms step_avg:95.55ms
step:837/1750 train_time:79979ms step_avg:95.55ms
step:838/1750 train_time:80077ms step_avg:95.56ms
step:839/1750 train_time:80176ms step_avg:95.56ms
step:840/1750 train_time:80274ms step_avg:95.56ms
step:841/1750 train_time:80372ms step_avg:95.57ms
step:842/1750 train_time:80470ms step_avg:95.57ms
step:843/1750 train_time:80569ms step_avg:95.57ms
step:844/1750 train_time:80667ms step_avg:95.58ms
step:845/1750 train_time:80765ms step_avg:95.58ms
step:846/1750 train_time:80863ms step_avg:95.58ms
step:847/1750 train_time:80961ms step_avg:95.59ms
step:848/1750 train_time:81059ms step_avg:95.59ms
step:849/1750 train_time:81157ms step_avg:95.59ms
step:850/1750 train_time:81256ms step_avg:95.60ms
step:851/1750 train_time:81355ms step_avg:95.60ms
step:852/1750 train_time:81454ms step_avg:95.60ms
step:853/1750 train_time:81551ms step_avg:95.61ms
step:854/1750 train_time:81649ms step_avg:95.61ms
step:855/1750 train_time:81748ms step_avg:95.61ms
step:856/1750 train_time:81846ms step_avg:95.61ms
step:857/1750 train_time:81944ms step_avg:95.62ms
step:858/1750 train_time:82042ms step_avg:95.62ms
step:859/1750 train_time:82141ms step_avg:95.62ms
step:860/1750 train_time:82240ms step_avg:95.63ms
step:861/1750 train_time:82339ms step_avg:95.63ms
step:862/1750 train_time:82437ms step_avg:95.63ms
step:863/1750 train_time:82535ms step_avg:95.64ms
step:864/1750 train_time:82633ms step_avg:95.64ms
step:865/1750 train_time:82730ms step_avg:95.64ms
step:866/1750 train_time:82828ms step_avg:95.64ms
step:867/1750 train_time:82927ms step_avg:95.65ms
step:868/1750 train_time:83025ms step_avg:95.65ms
step:869/1750 train_time:83124ms step_avg:95.65ms
step:870/1750 train_time:83222ms step_avg:95.66ms
step:871/1750 train_time:83321ms step_avg:95.66ms
step:872/1750 train_time:83419ms step_avg:95.66ms
step:873/1750 train_time:83517ms step_avg:95.67ms
step:874/1750 train_time:83615ms step_avg:95.67ms
step:875/1750 train_time:83714ms step_avg:95.67ms
step:875/1750 val_loss:3.5489 train_time:83803ms step_avg:95.77ms
step:876/1750 train_time:83826ms step_avg:95.69ms
step:877/1750 train_time:83920ms step_avg:95.69ms
step:878/1750 train_time:84019ms step_avg:95.69ms
step:879/1750 train_time:84118ms step_avg:95.70ms
step:880/1750 train_time:84216ms step_avg:95.70ms
step:881/1750 train_time:84313ms step_avg:95.70ms
step:882/1750 train_time:84410ms step_avg:95.70ms
step:883/1750 train_time:84507ms step_avg:95.70ms
step:884/1750 train_time:84604ms step_avg:95.71ms
step:885/1750 train_time:84701ms step_avg:95.71ms
step:886/1750 train_time:84802ms step_avg:95.71ms
step:887/1750 train_time:84901ms step_avg:95.72ms
step:888/1750 train_time:85001ms step_avg:95.72ms
step:889/1750 train_time:85100ms step_avg:95.73ms
step:890/1750 train_time:85198ms step_avg:95.73ms
step:891/1750 train_time:85296ms step_avg:95.73ms
step:892/1750 train_time:85394ms step_avg:95.73ms
step:893/1750 train_time:85491ms step_avg:95.73ms
step:894/1750 train_time:85588ms step_avg:95.74ms
step:895/1750 train_time:85687ms step_avg:95.74ms
step:896/1750 train_time:85785ms step_avg:95.74ms
step:897/1750 train_time:85883ms step_avg:95.74ms
step:898/1750 train_time:85982ms step_avg:95.75ms
step:899/1750 train_time:86081ms step_avg:95.75ms
step:900/1750 train_time:86180ms step_avg:95.76ms
step:901/1750 train_time:86278ms step_avg:95.76ms
step:902/1750 train_time:86377ms step_avg:95.76ms
step:903/1750 train_time:86475ms step_avg:95.76ms
step:904/1750 train_time:86573ms step_avg:95.77ms
step:905/1750 train_time:86672ms step_avg:95.77ms
step:906/1750 train_time:86770ms step_avg:95.77ms
step:907/1750 train_time:86868ms step_avg:95.77ms
step:908/1750 train_time:86966ms step_avg:95.78ms
step:909/1750 train_time:87065ms step_avg:95.78ms
step:910/1750 train_time:87164ms step_avg:95.78ms
step:911/1750 train_time:87263ms step_avg:95.79ms
step:912/1750 train_time:87364ms step_avg:95.79ms
step:913/1750 train_time:87463ms step_avg:95.80ms
step:914/1750 train_time:87563ms step_avg:95.80ms
step:915/1750 train_time:87663ms step_avg:95.81ms
step:916/1750 train_time:87763ms step_avg:95.81ms
step:917/1750 train_time:87862ms step_avg:95.81ms
step:918/1750 train_time:87962ms step_avg:95.82ms
step:919/1750 train_time:88062ms step_avg:95.82ms
step:920/1750 train_time:88163ms step_avg:95.83ms
step:921/1750 train_time:88262ms step_avg:95.83ms
step:922/1750 train_time:88362ms step_avg:95.84ms
step:923/1750 train_time:88462ms step_avg:95.84ms
step:924/1750 train_time:88563ms step_avg:95.85ms
step:925/1750 train_time:88662ms step_avg:95.85ms
step:926/1750 train_time:88762ms step_avg:95.86ms
step:927/1750 train_time:88862ms step_avg:95.86ms
step:928/1750 train_time:88961ms step_avg:95.86ms
step:929/1750 train_time:89060ms step_avg:95.87ms
step:930/1750 train_time:89159ms step_avg:95.87ms
step:931/1750 train_time:89259ms step_avg:95.87ms
step:932/1750 train_time:89360ms step_avg:95.88ms
step:933/1750 train_time:89460ms step_avg:95.88ms
step:934/1750 train_time:89560ms step_avg:95.89ms
step:935/1750 train_time:89660ms step_avg:95.89ms
step:936/1750 train_time:89760ms step_avg:95.90ms
step:937/1750 train_time:89860ms step_avg:95.90ms
step:938/1750 train_time:89960ms step_avg:95.91ms
step:939/1750 train_time:90060ms step_avg:95.91ms
step:940/1750 train_time:90160ms step_avg:95.92ms
step:941/1750 train_time:90262ms step_avg:95.92ms
step:942/1750 train_time:90361ms step_avg:95.92ms
step:943/1750 train_time:90462ms step_avg:95.93ms
step:944/1750 train_time:90561ms step_avg:95.93ms
step:945/1750 train_time:90660ms step_avg:95.94ms
step:946/1750 train_time:90760ms step_avg:95.94ms
step:947/1750 train_time:90860ms step_avg:95.95ms
step:948/1750 train_time:90960ms step_avg:95.95ms
step:949/1750 train_time:91059ms step_avg:95.95ms
step:950/1750 train_time:91159ms step_avg:95.96ms
step:951/1750 train_time:91259ms step_avg:95.96ms
step:952/1750 train_time:91358ms step_avg:95.96ms
step:953/1750 train_time:91457ms step_avg:95.97ms
step:954/1750 train_time:91556ms step_avg:95.97ms
step:955/1750 train_time:91656ms step_avg:95.98ms
step:956/1750 train_time:91756ms step_avg:95.98ms
step:957/1750 train_time:91856ms step_avg:95.98ms
step:958/1750 train_time:91955ms step_avg:95.99ms
step:959/1750 train_time:92055ms step_avg:95.99ms
step:960/1750 train_time:92155ms step_avg:95.99ms
step:961/1750 train_time:92256ms step_avg:96.00ms
step:962/1750 train_time:92356ms step_avg:96.00ms
step:963/1750 train_time:92456ms step_avg:96.01ms
step:964/1750 train_time:92556ms step_avg:96.01ms
step:965/1750 train_time:92656ms step_avg:96.02ms
step:966/1750 train_time:92756ms step_avg:96.02ms
step:967/1750 train_time:92856ms step_avg:96.03ms
step:968/1750 train_time:92957ms step_avg:96.03ms
step:969/1750 train_time:93057ms step_avg:96.03ms
step:970/1750 train_time:93157ms step_avg:96.04ms
step:971/1750 train_time:93257ms step_avg:96.04ms
step:972/1750 train_time:93357ms step_avg:96.05ms
step:973/1750 train_time:93457ms step_avg:96.05ms
step:974/1750 train_time:93556ms step_avg:96.05ms
step:975/1750 train_time:93655ms step_avg:96.06ms
step:976/1750 train_time:93755ms step_avg:96.06ms
step:977/1750 train_time:93855ms step_avg:96.06ms
step:978/1750 train_time:93955ms step_avg:96.07ms
step:979/1750 train_time:94055ms step_avg:96.07ms
step:980/1750 train_time:94155ms step_avg:96.08ms
step:981/1750 train_time:94255ms step_avg:96.08ms
step:982/1750 train_time:94355ms step_avg:96.08ms
step:983/1750 train_time:94456ms step_avg:96.09ms
step:984/1750 train_time:94554ms step_avg:96.09ms
step:985/1750 train_time:94653ms step_avg:96.09ms
step:986/1750 train_time:94753ms step_avg:96.10ms
step:987/1750 train_time:94852ms step_avg:96.10ms
step:988/1750 train_time:94951ms step_avg:96.10ms
step:989/1750 train_time:95050ms step_avg:96.11ms
step:990/1750 train_time:95150ms step_avg:96.11ms
step:991/1750 train_time:95250ms step_avg:96.12ms
step:992/1750 train_time:95349ms step_avg:96.12ms
step:993/1750 train_time:95448ms step_avg:96.12ms
step:994/1750 train_time:95547ms step_avg:96.12ms
step:995/1750 train_time:95647ms step_avg:96.13ms
step:996/1750 train_time:95747ms step_avg:96.13ms
step:997/1750 train_time:95847ms step_avg:96.14ms
step:998/1750 train_time:95946ms step_avg:96.14ms
step:999/1750 train_time:96046ms step_avg:96.14ms
step:1000/1750 train_time:96146ms step_avg:96.15ms
step:1000/1750 val_loss:3.5081 train_time:96235ms step_avg:96.23ms
step:1001/1750 train_time:96256ms step_avg:96.16ms
step:1002/1750 train_time:96358ms step_avg:96.17ms
step:1003/1750 train_time:96458ms step_avg:96.17ms
step:1004/1750 train_time:96560ms step_avg:96.17ms
step:1005/1750 train_time:96660ms step_avg:96.18ms
step:1006/1750 train_time:96758ms step_avg:96.18ms
step:1007/1750 train_time:96858ms step_avg:96.18ms
step:1008/1750 train_time:96956ms step_avg:96.19ms
step:1009/1750 train_time:97055ms step_avg:96.19ms
step:1010/1750 train_time:97154ms step_avg:96.19ms
step:1011/1750 train_time:97256ms step_avg:96.20ms
step:1012/1750 train_time:97358ms step_avg:96.20ms
step:1013/1750 train_time:97460ms step_avg:96.21ms
step:1014/1750 train_time:97560ms step_avg:96.21ms
step:1015/1750 train_time:97659ms step_avg:96.22ms
step:1016/1750 train_time:97758ms step_avg:96.22ms
step:1017/1750 train_time:97856ms step_avg:96.22ms
step:1018/1750 train_time:97955ms step_avg:96.22ms
step:1019/1750 train_time:98053ms step_avg:96.22ms
step:1020/1750 train_time:98151ms step_avg:96.23ms
step:1021/1750 train_time:98253ms step_avg:96.23ms
step:1022/1750 train_time:98353ms step_avg:96.24ms
step:1023/1750 train_time:98454ms step_avg:96.24ms
step:1024/1750 train_time:98557ms step_avg:96.25ms
step:1025/1750 train_time:98658ms step_avg:96.25ms
step:1026/1750 train_time:98757ms step_avg:96.25ms
step:1027/1750 train_time:98856ms step_avg:96.26ms
step:1028/1750 train_time:98955ms step_avg:96.26ms
step:1029/1750 train_time:99055ms step_avg:96.26ms
step:1030/1750 train_time:99154ms step_avg:96.27ms
step:1031/1750 train_time:99254ms step_avg:96.27ms
step:1032/1750 train_time:99354ms step_avg:96.27ms
step:1033/1750 train_time:99455ms step_avg:96.28ms
step:1034/1750 train_time:99556ms step_avg:96.28ms
step:1035/1750 train_time:99656ms step_avg:96.29ms
step:1036/1750 train_time:99756ms step_avg:96.29ms
step:1037/1750 train_time:99856ms step_avg:96.29ms
step:1038/1750 train_time:99956ms step_avg:96.30ms
step:1039/1750 train_time:100055ms step_avg:96.30ms
step:1040/1750 train_time:100155ms step_avg:96.30ms
step:1041/1750 train_time:100255ms step_avg:96.31ms
step:1042/1750 train_time:100606ms step_avg:96.55ms
step:1043/1750 train_time:100704ms step_avg:96.55ms
step:1044/1750 train_time:100801ms step_avg:96.55ms
step:1045/1750 train_time:100900ms step_avg:96.56ms
step:1046/1750 train_time:100998ms step_avg:96.56ms
step:1047/1750 train_time:101096ms step_avg:96.56ms
step:1048/1750 train_time:101195ms step_avg:96.56ms
step:1049/1750 train_time:101293ms step_avg:96.56ms
step:1050/1750 train_time:101392ms step_avg:96.56ms
step:1051/1750 train_time:101750ms step_avg:96.81ms
step:1052/1750 train_time:101848ms step_avg:96.81ms
step:1053/1750 train_time:101945ms step_avg:96.81ms
step:1054/1750 train_time:102043ms step_avg:96.82ms
step:1055/1750 train_time:102142ms step_avg:96.82ms
step:1056/1750 train_time:102240ms step_avg:96.82ms
step:1057/1750 train_time:102338ms step_avg:96.82ms
step:1058/1750 train_time:102437ms step_avg:96.82ms
step:1059/1750 train_time:102535ms step_avg:96.82ms
step:1060/1750 train_time:102890ms step_avg:97.07ms
step:1061/1750 train_time:102988ms step_avg:97.07ms
step:1062/1750 train_time:103086ms step_avg:97.07ms
step:1063/1750 train_time:103185ms step_avg:97.07ms
step:1064/1750 train_time:103284ms step_avg:97.07ms
step:1065/1750 train_time:103381ms step_avg:97.07ms
step:1066/1750 train_time:103480ms step_avg:97.07ms
step:1067/1750 train_time:103579ms step_avg:97.07ms
step:1068/1750 train_time:103677ms step_avg:97.08ms
step:1069/1750 train_time:103779ms step_avg:97.08ms
step:1070/1750 train_time:103883ms step_avg:97.09ms
step:1071/1750 train_time:103984ms step_avg:97.09ms
step:1072/1750 train_time:104085ms step_avg:97.09ms
step:1073/1750 train_time:104183ms step_avg:97.10ms
step:1074/1750 train_time:104282ms step_avg:97.10ms
step:1075/1750 train_time:104381ms step_avg:97.10ms
step:1076/1750 train_time:104479ms step_avg:97.10ms
step:1077/1750 train_time:104579ms step_avg:97.10ms
step:1078/1750 train_time:104962ms step_avg:97.37ms
step:1079/1750 train_time:105060ms step_avg:97.37ms
step:1080/1750 train_time:105158ms step_avg:97.37ms
step:1081/1750 train_time:105257ms step_avg:97.37ms
step:1082/1750 train_time:105356ms step_avg:97.37ms
step:1083/1750 train_time:105455ms step_avg:97.37ms
step:1084/1750 train_time:105552ms step_avg:97.37ms
step:1085/1750 train_time:105650ms step_avg:97.37ms
step:1086/1750 train_time:105749ms step_avg:97.38ms
step:1087/1750 train_time:105852ms step_avg:97.38ms
step:1088/1750 train_time:105955ms step_avg:97.39ms
step:1089/1750 train_time:106056ms step_avg:97.39ms
step:1090/1750 train_time:106156ms step_avg:97.39ms
step:1091/1750 train_time:106256ms step_avg:97.39ms
step:1092/1750 train_time:106355ms step_avg:97.39ms
step:1093/1750 train_time:106454ms step_avg:97.40ms
step:1094/1750 train_time:106552ms step_avg:97.40ms
step:1095/1750 train_time:106652ms step_avg:97.40ms
step:1096/1750 train_time:106752ms step_avg:97.40ms
step:1097/1750 train_time:106854ms step_avg:97.41ms
step:1098/1750 train_time:106956ms step_avg:97.41ms
step:1099/1750 train_time:107057ms step_avg:97.41ms
step:1100/1750 train_time:107157ms step_avg:97.42ms
step:1101/1750 train_time:107256ms step_avg:97.42ms
step:1102/1750 train_time:107356ms step_avg:97.42ms
step:1103/1750 train_time:107454ms step_avg:97.42ms
step:1104/1750 train_time:107553ms step_avg:97.42ms
step:1105/1750 train_time:107653ms step_avg:97.42ms
step:1106/1750 train_time:107753ms step_avg:97.43ms
step:1107/1750 train_time:107854ms step_avg:97.43ms
step:1108/1750 train_time:107954ms step_avg:97.43ms
step:1109/1750 train_time:108055ms step_avg:97.43ms
step:1110/1750 train_time:108156ms step_avg:97.44ms
step:1111/1750 train_time:108255ms step_avg:97.44ms
step:1112/1750 train_time:108356ms step_avg:97.44ms
step:1113/1750 train_time:108456ms step_avg:97.44ms
step:1114/1750 train_time:108555ms step_avg:97.45ms
step:1115/1750 train_time:108655ms step_avg:97.45ms
step:1116/1750 train_time:108755ms step_avg:97.45ms
step:1117/1750 train_time:108856ms step_avg:97.45ms
step:1118/1750 train_time:108956ms step_avg:97.46ms
step:1119/1750 train_time:109057ms step_avg:97.46ms
step:1120/1750 train_time:109157ms step_avg:97.46ms
step:1121/1750 train_time:109257ms step_avg:97.46ms
step:1122/1750 train_time:109357ms step_avg:97.47ms
step:1123/1750 train_time:109457ms step_avg:97.47ms
step:1124/1750 train_time:109556ms step_avg:97.47ms
step:1125/1750 train_time:109658ms step_avg:97.47ms
step:1125/1750 val_loss:3.4568 train_time:109746ms step_avg:97.55ms
step:1126/1750 train_time:109769ms step_avg:97.49ms
step:1127/1750 train_time:109866ms step_avg:97.49ms
step:1128/1750 train_time:109968ms step_avg:97.49ms
step:1129/1750 train_time:110068ms step_avg:97.49ms
step:1130/1750 train_time:110167ms step_avg:97.49ms
step:1131/1750 train_time:110266ms step_avg:97.49ms
step:1132/1750 train_time:110365ms step_avg:97.50ms
step:1133/1750 train_time:110464ms step_avg:97.50ms
step:1134/1750 train_time:110561ms step_avg:97.50ms
step:1135/1750 train_time:110660ms step_avg:97.50ms
step:1136/1750 train_time:110761ms step_avg:97.50ms
step:1137/1750 train_time:110862ms step_avg:97.50ms
step:1138/1750 train_time:110963ms step_avg:97.51ms
step:1139/1750 train_time:111063ms step_avg:97.51ms
step:1140/1750 train_time:111163ms step_avg:97.51ms
step:1141/1750 train_time:111262ms step_avg:97.51ms
step:1142/1750 train_time:111361ms step_avg:97.51ms
step:1143/1750 train_time:111460ms step_avg:97.52ms
step:1144/1750 train_time:111558ms step_avg:97.52ms
step:1145/1750 train_time:111657ms step_avg:97.52ms
step:1146/1750 train_time:111758ms step_avg:97.52ms
step:1147/1750 train_time:111858ms step_avg:97.52ms
step:1148/1750 train_time:111959ms step_avg:97.53ms
step:1149/1750 train_time:112059ms step_avg:97.53ms
step:1150/1750 train_time:112159ms step_avg:97.53ms
step:1151/1750 train_time:112259ms step_avg:97.53ms
step:1152/1750 train_time:112359ms step_avg:97.53ms
step:1153/1750 train_time:112458ms step_avg:97.53ms
step:1154/1750 train_time:112556ms step_avg:97.54ms
step:1155/1750 train_time:112655ms step_avg:97.54ms
step:1156/1750 train_time:112755ms step_avg:97.54ms
step:1157/1750 train_time:112856ms step_avg:97.54ms
step:1158/1750 train_time:112955ms step_avg:97.54ms
step:1159/1750 train_time:113057ms step_avg:97.55ms
step:1160/1750 train_time:113158ms step_avg:97.55ms
step:1161/1750 train_time:113257ms step_avg:97.55ms
step:1162/1750 train_time:113357ms step_avg:97.55ms
step:1163/1750 train_time:113457ms step_avg:97.56ms
step:1164/1750 train_time:113556ms step_avg:97.56ms
step:1165/1750 train_time:113654ms step_avg:97.56ms
step:1166/1750 train_time:113754ms step_avg:97.56ms
step:1167/1750 train_time:114114ms step_avg:97.78ms
step:1168/1750 train_time:114212ms step_avg:97.78ms
step:1169/1750 train_time:114312ms step_avg:97.79ms
step:1170/1750 train_time:114413ms step_avg:97.79ms
step:1171/1750 train_time:114512ms step_avg:97.79ms
step:1172/1750 train_time:114613ms step_avg:97.79ms
step:1173/1750 train_time:114713ms step_avg:97.79ms
step:1174/1750 train_time:114812ms step_avg:97.80ms
step:1175/1750 train_time:114912ms step_avg:97.80ms
step:1176/1750 train_time:115016ms step_avg:97.80ms
step:1177/1750 train_time:115125ms step_avg:97.81ms
step:1178/1750 train_time:115225ms step_avg:97.81ms
step:1179/1750 train_time:115328ms step_avg:97.82ms
step:1180/1750 train_time:115429ms step_avg:97.82ms
step:1181/1750 train_time:115529ms step_avg:97.82ms
step:1182/1750 train_time:115629ms step_avg:97.82ms
step:1183/1750 train_time:115728ms step_avg:97.83ms
step:1184/1750 train_time:115829ms step_avg:97.83ms
step:1185/1750 train_time:115930ms step_avg:97.83ms
step:1186/1750 train_time:116033ms step_avg:97.84ms
step:1187/1750 train_time:116135ms step_avg:97.84ms
step:1188/1750 train_time:116238ms step_avg:97.84ms
step:1189/1750 train_time:116337ms step_avg:97.84ms
step:1190/1750 train_time:116437ms step_avg:97.85ms
step:1191/1750 train_time:116538ms step_avg:97.85ms
step:1192/1750 train_time:116637ms step_avg:97.85ms
step:1193/1750 train_time:116737ms step_avg:97.85ms
step:1194/1750 train_time:116837ms step_avg:97.85ms
step:1195/1750 train_time:116938ms step_avg:97.86ms
step:1196/1750 train_time:117040ms step_avg:97.86ms
step:1197/1750 train_time:117141ms step_avg:97.86ms
step:1198/1750 train_time:117243ms step_avg:97.87ms
step:1199/1750 train_time:117344ms step_avg:97.87ms
step:1200/1750 train_time:117443ms step_avg:97.87ms
step:1201/1750 train_time:117544ms step_avg:97.87ms
step:1202/1750 train_time:117645ms step_avg:97.87ms
step:1203/1750 train_time:117746ms step_avg:97.88ms
step:1204/1750 train_time:117848ms step_avg:97.88ms
step:1205/1750 train_time:117949ms step_avg:97.88ms
step:1206/1750 train_time:118049ms step_avg:97.89ms
step:1207/1750 train_time:118151ms step_avg:97.89ms
step:1208/1750 train_time:118253ms step_avg:97.89ms
step:1209/1750 train_time:118354ms step_avg:97.89ms
step:1210/1750 train_time:118752ms step_avg:98.14ms
step:1211/1750 train_time:118851ms step_avg:98.14ms
step:1212/1750 train_time:118950ms step_avg:98.14ms
step:1213/1750 train_time:119050ms step_avg:98.14ms
step:1214/1750 train_time:119149ms step_avg:98.15ms
step:1215/1750 train_time:119249ms step_avg:98.15ms
step:1216/1750 train_time:119351ms step_avg:98.15ms
step:1217/1750 train_time:119450ms step_avg:98.15ms
step:1218/1750 train_time:119886ms step_avg:98.43ms
step:1219/1750 train_time:119948ms step_avg:98.40ms
step:1220/1750 train_time:120047ms step_avg:98.40ms
step:1221/1750 train_time:120146ms step_avg:98.40ms
step:1222/1750 train_time:120246ms step_avg:98.40ms
step:1223/1750 train_time:120346ms step_avg:98.40ms
step:1224/1750 train_time:120446ms step_avg:98.40ms
step:1225/1750 train_time:120546ms step_avg:98.40ms
step:1226/1750 train_time:120645ms step_avg:98.41ms
step:1227/1750 train_time:120745ms step_avg:98.41ms
step:1228/1750 train_time:120850ms step_avg:98.41ms
step:1229/1750 train_time:120955ms step_avg:98.42ms
step:1230/1750 train_time:121055ms step_avg:98.42ms
step:1231/1750 train_time:121156ms step_avg:98.42ms
step:1232/1750 train_time:121256ms step_avg:98.42ms
step:1233/1750 train_time:121356ms step_avg:98.42ms
step:1234/1750 train_time:121457ms step_avg:98.43ms
step:1235/1750 train_time:121558ms step_avg:98.43ms
step:1236/1750 train_time:121659ms step_avg:98.43ms
step:1237/1750 train_time:121759ms step_avg:98.43ms
step:1238/1750 train_time:121860ms step_avg:98.43ms
step:1239/1750 train_time:121962ms step_avg:98.44ms
step:1240/1750 train_time:122062ms step_avg:98.44ms
step:1241/1750 train_time:122164ms step_avg:98.44ms
step:1242/1750 train_time:122264ms step_avg:98.44ms
step:1243/1750 train_time:122364ms step_avg:98.44ms
step:1244/1750 train_time:122465ms step_avg:98.44ms
step:1245/1750 train_time:122567ms step_avg:98.45ms
step:1246/1750 train_time:122669ms step_avg:98.45ms
step:1247/1750 train_time:122770ms step_avg:98.45ms
step:1248/1750 train_time:122872ms step_avg:98.46ms
step:1249/1750 train_time:122973ms step_avg:98.46ms
step:1250/1750 train_time:123076ms step_avg:98.46ms
step:1250/1750 val_loss:3.4129 train_time:123165ms step_avg:98.53ms
step:1251/1750 train_time:123186ms step_avg:98.47ms
step:1252/1750 train_time:123287ms step_avg:98.47ms
step:1253/1750 train_time:123388ms step_avg:98.47ms
step:1254/1750 train_time:123488ms step_avg:98.48ms
step:1255/1750 train_time:123588ms step_avg:98.48ms
step:1256/1750 train_time:123687ms step_avg:98.48ms
step:1257/1750 train_time:123787ms step_avg:98.48ms
step:1258/1750 train_time:123887ms step_avg:98.48ms
step:1259/1750 train_time:123986ms step_avg:98.48ms
step:1260/1750 train_time:124086ms step_avg:98.48ms
step:1261/1750 train_time:124189ms step_avg:98.48ms
step:1262/1750 train_time:124291ms step_avg:98.49ms
step:1263/1750 train_time:124392ms step_avg:98.49ms
step:1264/1750 train_time:124493ms step_avg:98.49ms
step:1265/1750 train_time:124594ms step_avg:98.49ms
step:1266/1750 train_time:124694ms step_avg:98.49ms
step:1267/1750 train_time:124794ms step_avg:98.50ms
step:1268/1750 train_time:124896ms step_avg:98.50ms
step:1269/1750 train_time:124997ms step_avg:98.50ms
step:1270/1750 train_time:125099ms step_avg:98.50ms
step:1271/1750 train_time:125201ms step_avg:98.51ms
step:1272/1750 train_time:125301ms step_avg:98.51ms
step:1273/1750 train_time:125402ms step_avg:98.51ms
step:1274/1750 train_time:125503ms step_avg:98.51ms
step:1275/1750 train_time:125604ms step_avg:98.51ms
step:1276/1750 train_time:125704ms step_avg:98.51ms
step:1277/1750 train_time:125805ms step_avg:98.52ms
step:1278/1750 train_time:125905ms step_avg:98.52ms
step:1279/1750 train_time:126005ms step_avg:98.52ms
step:1280/1750 train_time:126105ms step_avg:98.52ms
step:1281/1750 train_time:126205ms step_avg:98.52ms
step:1282/1750 train_time:126305ms step_avg:98.52ms
step:1283/1750 train_time:126406ms step_avg:98.52ms
step:1284/1750 train_time:126507ms step_avg:98.53ms
step:1285/1750 train_time:126608ms step_avg:98.53ms
step:1286/1750 train_time:126707ms step_avg:98.53ms
step:1287/1750 train_time:126808ms step_avg:98.53ms
step:1288/1750 train_time:126909ms step_avg:98.53ms
step:1289/1750 train_time:127009ms step_avg:98.53ms
step:1290/1750 train_time:127110ms step_avg:98.53ms
step:1291/1750 train_time:127210ms step_avg:98.54ms
step:1292/1750 train_time:127312ms step_avg:98.54ms
step:1293/1750 train_time:127414ms step_avg:98.54ms
step:1294/1750 train_time:127516ms step_avg:98.54ms
step:1295/1750 train_time:127618ms step_avg:98.55ms
step:1296/1750 train_time:127718ms step_avg:98.55ms
step:1297/1750 train_time:127819ms step_avg:98.55ms
step:1298/1750 train_time:127920ms step_avg:98.55ms
step:1299/1750 train_time:128021ms step_avg:98.55ms
step:1300/1750 train_time:128123ms step_avg:98.56ms
step:1301/1750 train_time:128224ms step_avg:98.56ms
step:1302/1750 train_time:128324ms step_avg:98.56ms
step:1303/1750 train_time:128426ms step_avg:98.56ms
step:1304/1750 train_time:128527ms step_avg:98.56ms
step:1305/1750 train_time:128626ms step_avg:98.56ms
step:1306/1750 train_time:128726ms step_avg:98.57ms
step:1307/1750 train_time:128827ms step_avg:98.57ms
step:1308/1750 train_time:128929ms step_avg:98.57ms
step:1309/1750 train_time:129029ms step_avg:98.57ms
step:1310/1750 train_time:129130ms step_avg:98.57ms
step:1311/1750 train_time:129232ms step_avg:98.58ms
step:1312/1750 train_time:129334ms step_avg:98.58ms
step:1313/1750 train_time:129436ms step_avg:98.58ms
step:1314/1750 train_time:129538ms step_avg:98.58ms
step:1315/1750 train_time:129639ms step_avg:98.58ms
step:1316/1750 train_time:129740ms step_avg:98.59ms
step:1317/1750 train_time:129840ms step_avg:98.59ms
step:1318/1750 train_time:129941ms step_avg:98.59ms
step:1319/1750 train_time:130043ms step_avg:98.59ms
step:1320/1750 train_time:130145ms step_avg:98.59ms
step:1321/1750 train_time:130247ms step_avg:98.60ms
step:1322/1750 train_time:130347ms step_avg:98.60ms
step:1323/1750 train_time:130446ms step_avg:98.60ms
step:1324/1750 train_time:130547ms step_avg:98.60ms
step:1325/1750 train_time:130647ms step_avg:98.60ms
step:1326/1750 train_time:130749ms step_avg:98.60ms
step:1327/1750 train_time:130851ms step_avg:98.61ms
step:1328/1750 train_time:130952ms step_avg:98.61ms
step:1329/1750 train_time:131053ms step_avg:98.61ms
step:1330/1750 train_time:131154ms step_avg:98.61ms
step:1331/1750 train_time:131258ms step_avg:98.62ms
step:1332/1750 train_time:131359ms step_avg:98.62ms
step:1333/1750 train_time:131461ms step_avg:98.62ms
step:1334/1750 train_time:131562ms step_avg:98.62ms
step:1335/1750 train_time:131663ms step_avg:98.62ms
step:1336/1750 train_time:131764ms step_avg:98.63ms
step:1337/1750 train_time:131866ms step_avg:98.63ms
step:1338/1750 train_time:131965ms step_avg:98.63ms
step:1339/1750 train_time:132066ms step_avg:98.63ms
step:1340/1750 train_time:132167ms step_avg:98.63ms
step:1341/1750 train_time:132267ms step_avg:98.63ms
step:1342/1750 train_time:132367ms step_avg:98.63ms
step:1343/1750 train_time:132468ms step_avg:98.64ms
step:1344/1750 train_time:132569ms step_avg:98.64ms
step:1345/1750 train_time:132670ms step_avg:98.64ms
step:1346/1750 train_time:132773ms step_avg:98.64ms
step:1347/1750 train_time:132875ms step_avg:98.64ms
step:1348/1750 train_time:132975ms step_avg:98.65ms
step:1349/1750 train_time:133077ms step_avg:98.65ms
step:1350/1750 train_time:133178ms step_avg:98.65ms
step:1351/1750 train_time:133279ms step_avg:98.65ms
step:1352/1750 train_time:133380ms step_avg:98.65ms
step:1353/1750 train_time:133482ms step_avg:98.66ms
step:1354/1750 train_time:133583ms step_avg:98.66ms
step:1355/1750 train_time:133685ms step_avg:98.66ms
step:1356/1750 train_time:133786ms step_avg:98.66ms
step:1357/1750 train_time:133885ms step_avg:98.66ms
step:1358/1750 train_time:133986ms step_avg:98.66ms
step:1359/1750 train_time:134087ms step_avg:98.67ms
step:1360/1750 train_time:134188ms step_avg:98.67ms
step:1361/1750 train_time:134288ms step_avg:98.67ms
step:1362/1750 train_time:134389ms step_avg:98.67ms
step:1363/1750 train_time:134492ms step_avg:98.67ms
step:1364/1750 train_time:134593ms step_avg:98.68ms
step:1365/1750 train_time:134694ms step_avg:98.68ms
step:1366/1750 train_time:134795ms step_avg:98.68ms
step:1367/1750 train_time:134895ms step_avg:98.68ms
step:1368/1750 train_time:134998ms step_avg:98.68ms
step:1369/1750 train_time:135100ms step_avg:98.68ms
step:1370/1750 train_time:135200ms step_avg:98.69ms
step:1371/1750 train_time:135301ms step_avg:98.69ms
step:1372/1750 train_time:135402ms step_avg:98.69ms
step:1373/1750 train_time:135503ms step_avg:98.69ms
step:1374/1750 train_time:135604ms step_avg:98.69ms
step:1375/1750 train_time:135705ms step_avg:98.69ms
step:1375/1750 val_loss:3.3730 train_time:135794ms step_avg:98.76ms
step:1376/1750 train_time:135815ms step_avg:98.70ms
step:1377/1750 train_time:135916ms step_avg:98.70ms
step:1378/1750 train_time:136017ms step_avg:98.71ms
step:1379/1750 train_time:136117ms step_avg:98.71ms
step:1380/1750 train_time:136219ms step_avg:98.71ms
step:1381/1750 train_time:136319ms step_avg:98.71ms
step:1382/1750 train_time:136418ms step_avg:98.71ms
step:1383/1750 train_time:136518ms step_avg:98.71ms
step:1384/1750 train_time:136619ms step_avg:98.71ms
step:1385/1750 train_time:136721ms step_avg:98.72ms
step:1386/1750 train_time:136825ms step_avg:98.72ms
step:1387/1750 train_time:136927ms step_avg:98.72ms
step:1388/1750 train_time:137027ms step_avg:98.72ms
step:1389/1750 train_time:137127ms step_avg:98.72ms
step:1390/1750 train_time:137227ms step_avg:98.72ms
step:1391/1750 train_time:137328ms step_avg:98.73ms
step:1392/1750 train_time:137429ms step_avg:98.73ms
step:1393/1750 train_time:137530ms step_avg:98.73ms
step:1394/1750 train_time:137631ms step_avg:98.73ms
step:1395/1750 train_time:137732ms step_avg:98.73ms
step:1396/1750 train_time:137835ms step_avg:98.74ms
step:1397/1750 train_time:137937ms step_avg:98.74ms
step:1398/1750 train_time:138038ms step_avg:98.74ms
step:1399/1750 train_time:138139ms step_avg:98.74ms
step:1400/1750 train_time:138241ms step_avg:98.74ms
step:1401/1750 train_time:138343ms step_avg:98.75ms
step:1402/1750 train_time:138443ms step_avg:98.75ms
step:1403/1750 train_time:138544ms step_avg:98.75ms
step:1404/1750 train_time:138644ms step_avg:98.75ms
step:1405/1750 train_time:138745ms step_avg:98.75ms
step:1406/1750 train_time:138846ms step_avg:98.75ms
step:1407/1750 train_time:138947ms step_avg:98.75ms
step:1408/1750 train_time:139048ms step_avg:98.76ms
step:1409/1750 train_time:139150ms step_avg:98.76ms
step:1410/1750 train_time:139251ms step_avg:98.76ms
step:1411/1750 train_time:139352ms step_avg:98.76ms
step:1412/1750 train_time:139455ms step_avg:98.76ms
step:1413/1750 train_time:139555ms step_avg:98.77ms
step:1414/1750 train_time:139656ms step_avg:98.77ms
step:1415/1750 train_time:139758ms step_avg:98.77ms
step:1416/1750 train_time:139861ms step_avg:98.77ms
step:1417/1750 train_time:139963ms step_avg:98.77ms
step:1418/1750 train_time:140063ms step_avg:98.78ms
step:1419/1750 train_time:140164ms step_avg:98.78ms
step:1420/1750 train_time:140264ms step_avg:98.78ms
step:1421/1750 train_time:140365ms step_avg:98.78ms
step:1422/1750 train_time:140466ms step_avg:98.78ms
step:1423/1750 train_time:140565ms step_avg:98.78ms
step:1424/1750 train_time:140667ms step_avg:98.78ms
step:1425/1750 train_time:140768ms step_avg:98.78ms
step:1426/1750 train_time:140869ms step_avg:98.79ms
step:1427/1750 train_time:140971ms step_avg:98.79ms
step:1428/1750 train_time:141073ms step_avg:98.79ms
step:1429/1750 train_time:141177ms step_avg:98.79ms
step:1430/1750 train_time:141281ms step_avg:98.80ms
step:1431/1750 train_time:141384ms step_avg:98.80ms
step:1432/1750 train_time:141484ms step_avg:98.80ms
step:1433/1750 train_time:141585ms step_avg:98.80ms
step:1434/1750 train_time:141686ms step_avg:98.80ms
step:1435/1750 train_time:141788ms step_avg:98.81ms
step:1436/1750 train_time:141890ms step_avg:98.81ms
step:1437/1750 train_time:141993ms step_avg:98.81ms
step:1438/1750 train_time:142095ms step_avg:98.81ms
step:1439/1750 train_time:142199ms step_avg:98.82ms
step:1440/1750 train_time:142302ms step_avg:98.82ms
step:1441/1750 train_time:142404ms step_avg:98.82ms
step:1442/1750 train_time:142505ms step_avg:98.82ms
step:1443/1750 train_time:142605ms step_avg:98.83ms
step:1444/1750 train_time:142707ms step_avg:98.83ms
step:1445/1750 train_time:142808ms step_avg:98.83ms
step:1446/1750 train_time:142908ms step_avg:98.83ms
step:1447/1750 train_time:143011ms step_avg:98.83ms
step:1448/1750 train_time:143115ms step_avg:98.84ms
step:1449/1750 train_time:143217ms step_avg:98.84ms
step:1450/1750 train_time:143320ms step_avg:98.84ms
step:1451/1750 train_time:143422ms step_avg:98.84ms
step:1452/1750 train_time:143523ms step_avg:98.85ms
step:1453/1750 train_time:143625ms step_avg:98.85ms
step:1454/1750 train_time:143729ms step_avg:98.85ms
step:1455/1750 train_time:143829ms step_avg:98.85ms
step:1456/1750 train_time:143930ms step_avg:98.85ms
step:1457/1750 train_time:144032ms step_avg:98.85ms
step:1458/1750 train_time:144135ms step_avg:98.86ms
step:1459/1750 train_time:144238ms step_avg:98.86ms
step:1460/1750 train_time:144341ms step_avg:98.86ms
step:1461/1750 train_time:144443ms step_avg:98.87ms
step:1462/1750 train_time:144545ms step_avg:98.87ms
step:1463/1750 train_time:144647ms step_avg:98.87ms
step:1464/1750 train_time:144748ms step_avg:98.87ms
step:1465/1750 train_time:144849ms step_avg:98.87ms
step:1466/1750 train_time:144950ms step_avg:98.87ms
step:1467/1750 train_time:145050ms step_avg:98.88ms
step:1468/1750 train_time:145153ms step_avg:98.88ms
step:1469/1750 train_time:145256ms step_avg:98.88ms
step:1470/1750 train_time:145358ms step_avg:98.88ms
step:1471/1750 train_time:145460ms step_avg:98.88ms
step:1472/1750 train_time:145563ms step_avg:98.89ms
step:1473/1750 train_time:145663ms step_avg:98.89ms
step:1474/1750 train_time:145765ms step_avg:98.89ms
step:1475/1750 train_time:145866ms step_avg:98.89ms
step:1476/1750 train_time:145969ms step_avg:98.89ms
step:1477/1750 train_time:146071ms step_avg:98.90ms
step:1478/1750 train_time:146174ms step_avg:98.90ms
step:1479/1750 train_time:146275ms step_avg:98.90ms
step:1480/1750 train_time:146378ms step_avg:98.90ms
step:1481/1750 train_time:146481ms step_avg:98.91ms
step:1482/1750 train_time:146583ms step_avg:98.91ms
step:1483/1750 train_time:146686ms step_avg:98.91ms
step:1484/1750 train_time:146788ms step_avg:98.91ms
step:1485/1750 train_time:146890ms step_avg:98.92ms
step:1486/1750 train_time:146991ms step_avg:98.92ms
step:1487/1750 train_time:147092ms step_avg:98.92ms
step:1488/1750 train_time:147195ms step_avg:98.92ms
step:1489/1750 train_time:147298ms step_avg:98.92ms
step:1490/1750 train_time:147401ms step_avg:98.93ms
step:1491/1750 train_time:147502ms step_avg:98.93ms
step:1492/1750 train_time:147604ms step_avg:98.93ms
step:1493/1750 train_time:147705ms step_avg:98.93ms
step:1494/1750 train_time:147807ms step_avg:98.93ms
step:1495/1750 train_time:147908ms step_avg:98.94ms
step:1496/1750 train_time:148010ms step_avg:98.94ms
step:1497/1750 train_time:148110ms step_avg:98.94ms
step:1498/1750 train_time:148212ms step_avg:98.94ms
step:1499/1750 train_time:148314ms step_avg:98.94ms
step:1500/1750 train_time:148417ms step_avg:98.94ms
step:1500/1750 val_loss:3.3383 train_time:148507ms step_avg:99.00ms
step:1501/1750 train_time:148529ms step_avg:98.95ms
step:1502/1750 train_time:148629ms step_avg:98.95ms
step:1503/1750 train_time:148731ms step_avg:98.96ms
step:1504/1750 train_time:148833ms step_avg:98.96ms
step:1505/1750 train_time:148934ms step_avg:98.96ms
step:1506/1750 train_time:149034ms step_avg:98.96ms
step:1507/1750 train_time:149135ms step_avg:98.96ms
step:1508/1750 train_time:149236ms step_avg:98.96ms
step:1509/1750 train_time:149338ms step_avg:98.96ms
step:1510/1750 train_time:149440ms step_avg:98.97ms
step:1511/1750 train_time:149546ms step_avg:98.97ms
step:1512/1750 train_time:149650ms step_avg:98.97ms
step:1513/1750 train_time:149751ms step_avg:98.98ms
step:1514/1750 train_time:149853ms step_avg:98.98ms
step:1515/1750 train_time:149957ms step_avg:98.98ms
step:1516/1750 train_time:150057ms step_avg:98.98ms
step:1517/1750 train_time:150157ms step_avg:98.98ms
step:1518/1750 train_time:150258ms step_avg:98.98ms
step:1519/1750 train_time:150360ms step_avg:98.99ms
step:1520/1750 train_time:150463ms step_avg:98.99ms
step:1521/1750 train_time:150565ms step_avg:98.99ms
step:1522/1750 train_time:150667ms step_avg:98.99ms
step:1523/1750 train_time:150768ms step_avg:98.99ms
step:1524/1750 train_time:150872ms step_avg:99.00ms
step:1525/1750 train_time:150977ms step_avg:99.00ms
step:1526/1750 train_time:151078ms step_avg:99.00ms
step:1527/1750 train_time:151180ms step_avg:99.00ms
step:1528/1750 train_time:151284ms step_avg:99.01ms
step:1529/1750 train_time:151385ms step_avg:99.01ms
step:1530/1750 train_time:151489ms step_avg:99.01ms
step:1531/1750 train_time:151590ms step_avg:99.01ms
step:1532/1750 train_time:151692ms step_avg:99.02ms
step:1533/1750 train_time:151793ms step_avg:99.02ms
step:1534/1750 train_time:151895ms step_avg:99.02ms
step:1535/1750 train_time:151997ms step_avg:99.02ms
step:1536/1750 train_time:152099ms step_avg:99.02ms
step:1537/1750 train_time:152200ms step_avg:99.02ms
step:1538/1750 train_time:152301ms step_avg:99.03ms
step:1539/1750 train_time:152403ms step_avg:99.03ms
step:1540/1750 train_time:152505ms step_avg:99.03ms
step:1541/1750 train_time:152606ms step_avg:99.03ms
step:1542/1750 train_time:152709ms step_avg:99.03ms
step:1543/1750 train_time:152811ms step_avg:99.04ms
step:1544/1750 train_time:152914ms step_avg:99.04ms
step:1545/1750 train_time:153016ms step_avg:99.04ms
step:1546/1750 train_time:153117ms step_avg:99.04ms
step:1547/1750 train_time:153219ms step_avg:99.04ms
step:1548/1750 train_time:153321ms step_avg:99.04ms
step:1549/1750 train_time:153423ms step_avg:99.05ms
step:1550/1750 train_time:153524ms step_avg:99.05ms
step:1551/1750 train_time:153626ms step_avg:99.05ms
step:1552/1750 train_time:153727ms step_avg:99.05ms
step:1553/1750 train_time:153828ms step_avg:99.05ms
step:1554/1750 train_time:153930ms step_avg:99.05ms
step:1555/1750 train_time:154033ms step_avg:99.06ms
step:1556/1750 train_time:154135ms step_avg:99.06ms
step:1557/1750 train_time:154238ms step_avg:99.06ms
step:1558/1750 train_time:154341ms step_avg:99.06ms
step:1559/1750 train_time:154443ms step_avg:99.07ms
step:1560/1750 train_time:154544ms step_avg:99.07ms
step:1561/1750 train_time:154645ms step_avg:99.07ms
step:1562/1750 train_time:154747ms step_avg:99.07ms
step:1563/1750 train_time:154851ms step_avg:99.07ms
step:1564/1750 train_time:154952ms step_avg:99.07ms
step:1565/1750 train_time:155053ms step_avg:99.08ms
step:1566/1750 train_time:155156ms step_avg:99.08ms
step:1567/1750 train_time:155258ms step_avg:99.08ms
step:1568/1750 train_time:155360ms step_avg:99.08ms
step:1569/1750 train_time:155462ms step_avg:99.08ms
step:1570/1750 train_time:155565ms step_avg:99.09ms
step:1571/1750 train_time:155666ms step_avg:99.09ms
step:1572/1750 train_time:155768ms step_avg:99.09ms
step:1573/1750 train_time:155869ms step_avg:99.09ms
step:1574/1750 train_time:155971ms step_avg:99.09ms
step:1575/1750 train_time:156072ms step_avg:99.09ms
step:1576/1750 train_time:156176ms step_avg:99.10ms
step:1577/1750 train_time:156279ms step_avg:99.10ms
step:1578/1750 train_time:156380ms step_avg:99.10ms
step:1579/1750 train_time:156482ms step_avg:99.10ms
step:1580/1750 train_time:156585ms step_avg:99.10ms
step:1581/1750 train_time:156686ms step_avg:99.11ms
step:1582/1750 train_time:156787ms step_avg:99.11ms
step:1583/1750 train_time:156890ms step_avg:99.11ms
step:1584/1750 train_time:156993ms step_avg:99.11ms
step:1585/1750 train_time:157095ms step_avg:99.11ms
step:1586/1750 train_time:157198ms step_avg:99.12ms
step:1587/1750 train_time:157300ms step_avg:99.12ms
step:1588/1750 train_time:157401ms step_avg:99.12ms
step:1589/1750 train_time:157503ms step_avg:99.12ms
step:1590/1750 train_time:157604ms step_avg:99.12ms
step:1591/1750 train_time:157707ms step_avg:99.12ms
step:1592/1750 train_time:157808ms step_avg:99.13ms
step:1593/1750 train_time:157909ms step_avg:99.13ms
step:1594/1750 train_time:158014ms step_avg:99.13ms
step:1595/1750 train_time:158117ms step_avg:99.13ms
step:1596/1750 train_time:158219ms step_avg:99.13ms
step:1597/1750 train_time:158321ms step_avg:99.14ms
step:1598/1750 train_time:158424ms step_avg:99.14ms
step:1599/1750 train_time:158524ms step_avg:99.14ms
step:1600/1750 train_time:158626ms step_avg:99.14ms
step:1601/1750 train_time:158728ms step_avg:99.14ms
step:1602/1750 train_time:158830ms step_avg:99.14ms
step:1603/1750 train_time:158931ms step_avg:99.15ms
step:1604/1750 train_time:159033ms step_avg:99.15ms
step:1605/1750 train_time:159136ms step_avg:99.15ms
step:1606/1750 train_time:159239ms step_avg:99.15ms
step:1607/1750 train_time:159341ms step_avg:99.15ms
step:1608/1750 train_time:159441ms step_avg:99.16ms
step:1609/1750 train_time:159543ms step_avg:99.16ms
step:1610/1750 train_time:159645ms step_avg:99.16ms
step:1611/1750 train_time:159748ms step_avg:99.16ms
step:1612/1750 train_time:159850ms step_avg:99.16ms
step:1613/1750 train_time:159952ms step_avg:99.16ms
step:1614/1750 train_time:160052ms step_avg:99.16ms
step:1615/1750 train_time:160154ms step_avg:99.17ms
step:1616/1750 train_time:160257ms step_avg:99.17ms
step:1617/1750 train_time:160359ms step_avg:99.17ms
step:1618/1750 train_time:160460ms step_avg:99.17ms
step:1619/1750 train_time:160563ms step_avg:99.17ms
step:1620/1750 train_time:160666ms step_avg:99.18ms
step:1621/1750 train_time:160767ms step_avg:99.18ms
step:1622/1750 train_time:160867ms step_avg:99.18ms
step:1623/1750 train_time:160969ms step_avg:99.18ms
step:1624/1750 train_time:161072ms step_avg:99.18ms
step:1625/1750 train_time:161177ms step_avg:99.19ms
step:1625/1750 val_loss:3.3072 train_time:161267ms step_avg:99.24ms
step:1626/1750 train_time:161289ms step_avg:99.19ms
step:1627/1750 train_time:161394ms step_avg:99.20ms
step:1628/1750 train_time:161495ms step_avg:99.20ms
step:1629/1750 train_time:161598ms step_avg:99.20ms
step:1630/1750 train_time:161699ms step_avg:99.20ms
step:1631/1750 train_time:161800ms step_avg:99.20ms
step:1632/1750 train_time:161901ms step_avg:99.20ms
step:1633/1750 train_time:162002ms step_avg:99.21ms
step:1634/1750 train_time:162105ms step_avg:99.21ms
step:1635/1750 train_time:162207ms step_avg:99.21ms
step:1636/1750 train_time:162310ms step_avg:99.21ms
step:1637/1750 train_time:162412ms step_avg:99.21ms
step:1638/1750 train_time:162515ms step_avg:99.22ms
step:1639/1750 train_time:162617ms step_avg:99.22ms
step:1640/1750 train_time:162719ms step_avg:99.22ms
step:1641/1750 train_time:162820ms step_avg:99.22ms
step:1642/1750 train_time:162921ms step_avg:99.22ms
step:1643/1750 train_time:163022ms step_avg:99.22ms
step:1644/1750 train_time:163124ms step_avg:99.22ms
step:1645/1750 train_time:163226ms step_avg:99.23ms
step:1646/1750 train_time:163330ms step_avg:99.23ms
step:1647/1750 train_time:163433ms step_avg:99.23ms
step:1648/1750 train_time:163537ms step_avg:99.23ms
step:1649/1750 train_time:163639ms step_avg:99.24ms
step:1650/1750 train_time:163740ms step_avg:99.24ms
step:1651/1750 train_time:163842ms step_avg:99.24ms
step:1652/1750 train_time:163944ms step_avg:99.24ms
step:1653/1750 train_time:164046ms step_avg:99.24ms
step:1654/1750 train_time:164147ms step_avg:99.24ms
step:1655/1750 train_time:164249ms step_avg:99.24ms
step:1656/1750 train_time:164352ms step_avg:99.25ms
step:1657/1750 train_time:164453ms step_avg:99.25ms
step:1658/1750 train_time:164554ms step_avg:99.25ms
step:1659/1750 train_time:164659ms step_avg:99.25ms
step:1660/1750 train_time:164760ms step_avg:99.25ms
step:1661/1750 train_time:164864ms step_avg:99.26ms
step:1662/1750 train_time:164967ms step_avg:99.26ms
step:1663/1750 train_time:165069ms step_avg:99.26ms
step:1664/1750 train_time:165170ms step_avg:99.26ms
step:1665/1750 train_time:165274ms step_avg:99.26ms
step:1666/1750 train_time:165377ms step_avg:99.27ms
step:1667/1750 train_time:165478ms step_avg:99.27ms
step:1668/1750 train_time:165581ms step_avg:99.27ms
step:1669/1750 train_time:165683ms step_avg:99.27ms
step:1670/1750 train_time:165784ms step_avg:99.27ms
step:1671/1750 train_time:165885ms step_avg:99.27ms
step:1672/1750 train_time:165988ms step_avg:99.27ms
step:1673/1750 train_time:166089ms step_avg:99.28ms
step:1674/1750 train_time:166190ms step_avg:99.28ms
step:1675/1750 train_time:166294ms step_avg:99.28ms
step:1676/1750 train_time:166398ms step_avg:99.28ms
step:1677/1750 train_time:166499ms step_avg:99.28ms
step:1678/1750 train_time:166601ms step_avg:99.29ms
step:1679/1750 train_time:166703ms step_avg:99.29ms
step:1680/1750 train_time:166804ms step_avg:99.29ms
step:1681/1750 train_time:166907ms step_avg:99.29ms
step:1682/1750 train_time:167010ms step_avg:99.29ms
step:1683/1750 train_time:167112ms step_avg:99.29ms
step:1684/1750 train_time:167214ms step_avg:99.30ms
step:1685/1750 train_time:167318ms step_avg:99.30ms
step:1686/1750 train_time:167420ms step_avg:99.30ms
step:1687/1750 train_time:167522ms step_avg:99.30ms
step:1688/1750 train_time:167624ms step_avg:99.30ms
step:1689/1750 train_time:167726ms step_avg:99.30ms
step:1690/1750 train_time:167828ms step_avg:99.31ms
step:1691/1750 train_time:167930ms step_avg:99.31ms
step:1692/1750 train_time:168034ms step_avg:99.31ms
step:1693/1750 train_time:168137ms step_avg:99.31ms
step:1694/1750 train_time:168241ms step_avg:99.32ms
step:1695/1750 train_time:168344ms step_avg:99.32ms
step:1696/1750 train_time:168446ms step_avg:99.32ms
step:1697/1750 train_time:168551ms step_avg:99.32ms
step:1698/1750 train_time:168655ms step_avg:99.33ms
step:1699/1750 train_time:168757ms step_avg:99.33ms
step:1700/1750 train_time:168860ms step_avg:99.33ms
step:1701/1750 train_time:168963ms step_avg:99.33ms
step:1702/1750 train_time:169067ms step_avg:99.33ms
step:1703/1750 train_time:169169ms step_avg:99.34ms
step:1704/1750 train_time:169272ms step_avg:99.34ms
step:1705/1750 train_time:169374ms step_avg:99.34ms
step:1706/1750 train_time:169476ms step_avg:99.34ms
step:1707/1750 train_time:169581ms step_avg:99.34ms
step:1708/1750 train_time:169685ms step_avg:99.35ms
step:1709/1750 train_time:169787ms step_avg:99.35ms
step:1710/1750 train_time:169889ms step_avg:99.35ms
step:1711/1750 train_time:169993ms step_avg:99.35ms
step:1712/1750 train_time:170096ms step_avg:99.36ms
step:1713/1750 train_time:170200ms step_avg:99.36ms
step:1714/1750 train_time:170303ms step_avg:99.36ms
step:1715/1750 train_time:170407ms step_avg:99.36ms
step:1716/1750 train_time:170510ms step_avg:99.36ms
step:1717/1750 train_time:170612ms step_avg:99.37ms
step:1718/1750 train_time:170715ms step_avg:99.37ms
step:1719/1750 train_time:170821ms step_avg:99.37ms
step:1720/1750 train_time:170923ms step_avg:99.37ms
step:1721/1750 train_time:171026ms step_avg:99.38ms
step:1722/1750 train_time:171128ms step_avg:99.38ms
step:1723/1750 train_time:171231ms step_avg:99.38ms
step:1724/1750 train_time:171336ms step_avg:99.38ms
step:1725/1750 train_time:171439ms step_avg:99.38ms
step:1726/1750 train_time:171541ms step_avg:99.39ms
step:1727/1750 train_time:171645ms step_avg:99.39ms
step:1728/1750 train_time:171748ms step_avg:99.39ms
step:1729/1750 train_time:171850ms step_avg:99.39ms
step:1730/1750 train_time:171952ms step_avg:99.39ms
step:1731/1750 train_time:172056ms step_avg:99.40ms
step:1732/1750 train_time:172160ms step_avg:99.40ms
step:1733/1750 train_time:172264ms step_avg:99.40ms
step:1734/1750 train_time:172367ms step_avg:99.40ms
step:1735/1750 train_time:172469ms step_avg:99.41ms
step:1736/1750 train_time:172571ms step_avg:99.41ms
step:1737/1750 train_time:172675ms step_avg:99.41ms
step:1738/1750 train_time:172779ms step_avg:99.41ms
step:1739/1750 train_time:172881ms step_avg:99.41ms
step:1740/1750 train_time:172983ms step_avg:99.42ms
step:1741/1750 train_time:173090ms step_avg:99.42ms
step:1742/1750 train_time:173193ms step_avg:99.42ms
step:1743/1750 train_time:173297ms step_avg:99.42ms
step:1744/1750 train_time:173400ms step_avg:99.43ms
step:1745/1750 train_time:173502ms step_avg:99.43ms
step:1746/1750 train_time:173604ms step_avg:99.43ms
step:1747/1750 train_time:173707ms step_avg:99.43ms
step:1748/1750 train_time:173810ms step_avg:99.43ms
step:1749/1750 train_time:173912ms step_avg:99.44ms
step:1750/1750 train_time:174016ms step_avg:99.44ms
step:1750/1750 val_loss:3.2839 train_time:174107ms step_avg:99.49ms
peak memory allocated: 33278 MiB reserved: 49174 MiB
