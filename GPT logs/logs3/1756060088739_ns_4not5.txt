import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X
'''
import torch.utils.dlpack as thd
from cupyx.scipy.sparse.linalg import svds as cupyx_svds

def several_sv_svds_approximation(W_torch, k, num_iter=50):
    """SVD approximation using the top k singular values and corresponding vectors."""
    # Store original device and dtype
    original_device = W_torch.device
    original_dtype = W_torch.dtype
    
    W = cp.from_dlpack(thd.to_dlpack(W_torch)).astype(cp.float32)
    U, S, Vt = cupyx_svds(W, k=min([k, W.shape[0] - 1, W.shape[1] - 1]), maxiter=num_iter, which='LM')

    # Convert back to torch tensors and ensure they're on the correct device
    approx_torch_U = thd.from_dlpack(U.toDlpack()).to(device=original_device, dtype=original_dtype)
    approx_torch_S = thd.from_dlpack(S.toDlpack()).to(device=original_device, dtype=original_dtype)
    approx_torch_Vt = thd.from_dlpack(Vt.toDlpack()).to(device=original_device, dtype=original_dtype)
    
    return approx_torch_U, approx_torch_S, approx_torch_Vt

class NormNeon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    # v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    u, s, vt = several_sv_svds_approximation(grad.bfloat16(), 5)
                    p.add_(other=u@vt, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()
'''
class FastNormMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, sgd_coeff=0):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, sgd_coeff=sgd_coeff)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        self.sgd_coeff = sgd_coeff

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            sgd_coeff = group["sgd_coeff"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    # assume: grad is a Tensor, momentum_buffer exists, momentum, sgd_coeff, eps, eff_lr are defined
                    with torch.no_grad():
                        # inplace blend with momentum (keeps original semantic: grad becomes the lerped tensor)
                        grad.lerp_(momentum_buffer, momentum)

                        # compute zeropower part from the (post-lerp) grad, in bfloat16 to match the original code
                        if sgd_coeff != 1:
                            # create update_part in bfloat16 (may allocate once per call; unavoidable if zeropower expects bfloat16)
                            update_part = zeropower_via_newtonschulz5(grad.to(torch.bfloat16), 5)

                            # upcast update_part to grad dtype to do mixed-dtype blending without extra temporaries later
                            if update_part.dtype != grad.dtype:
                                update_part = update_part.to(grad.dtype)

                        # compute inverse norm scalar efficiently (no large temp tensors)
                        # - view(-1) is cheap (no copy)
                        # - dot(view, view) is a single reduction
                        norm_sq = grad.view(-1).dot(grad.view(-1))
                        inv_norm = (norm_sq + eps).rsqrt()  # stable rsqrt on the scalar

                        # normalize grad in-place (so we avoid allocating g_normalized)
                        grad.mul_(inv_norm)   # now grad == g_normalized (in-place)

                        # build final update (reuse update_part buffer if available)
                        if sgd_coeff != 1:
                            # update_part := (1 - sgd_coeff) * update_part + sgd_coeff * grad
                            # do it in-place on update_part to avoid creating another temp
                            update_part.mul_(1 - sgd_coeff)    # in-place scale
                            update_part.lerp_(grad, sgd_coeff) # in-place linear interpolation
                            update = update_part
                        else:
                            # if sgd_coeff == 1 we can use grad directly (already normalized in-place)
                            update = grad

                        # apply update (same as original)
                        p.add_(other=update, alpha=-eff_lr)
                    '''
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    eps = 1e-12
                    g_normalized = grad / (grad.norm() + eps)
                    if sgd_coeff != 1:
                        update_part = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                        update = (1 - sgd_coeff) * update_part + sgd_coeff * g_normalized
                    else:
                        update = sgd_coeff * g_normalized
                    p.add_(other=update, alpha=-eff_lr)
                    '''
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class NormMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, sgd_coeff=0):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, sgd_coeff=sgd_coeff)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        self.sgd_coeff = sgd_coeff

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            sgd_coeff = group["sgd_coeff"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    eps = 1e-12
                    g_normalized = grad / (grad.norm() + eps)
                    if sgd_coeff != 1:
                        update_part = zeropower_via_newtonschulz5(grad.bfloat16(), 4)
                        update = (1 - sgd_coeff) * update_part + sgd_coeff * g_normalized
                    else:
                        update = sgd_coeff * g_normalized
                    p.add_(other=update, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()
class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)
                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_reduce_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 27.5
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos : pos + max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()
    starts = []
    for i in range(1, len(boundary_positions)):
        end = boundary_positions[i].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == dist.get_world_size():
                return starts, end - pos
            start = end
    assert False # increase max_batch_span if necessary

def distributed_data_generator(filename_pattern: str, batch_size: int, align_to_bos: bool):
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    max_batch_span = 2 * batch_size if align_to_bos else batch_size # provide buffer to handle samples up to length local_batch_size
    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            batch_span = batch_size
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1750 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
from datetime import datetime
if master_process:
    # run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_id = str(int(time.time() * 1000))
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0)
optimizer2 = NormMuon(hidden_matrix_params, lr=0.05, momentum=0.95, weight_decay=0, sgd_coeff=0.5)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.11 | packaged by conda-forge | (main, Jun  4 2025, 14:45:31) [GCC 13.3.0]
Running PyTorch 2.9.0.dev20250824+cu126 compiled for CUDA 12.6
Sun Aug 24 18:28:09 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:04:00.0 Off |                    0 |
| N/A   43C    P0            124W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:23:00.0 Off |                    0 |
| N/A   35C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:43:00.0 Off |                    0 |
| N/A   41C    P0            125W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   35C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:84:00.0 Off |                    0 |
| N/A   41C    P0            123W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:A3:00.0 Off |                    0 |
| N/A   35C    P0            123W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:C3:00.0 Off |                    0 |
| N/A   42C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E4:00.0 Off |                    0 |
| N/A   34C    P0            114W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.07ms
step:1/1750 train_time:139ms step_avg:138.69ms
step:2/1750 train_time:160ms step_avg:80.00ms
step:3/1750 train_time:235ms step_avg:78.20ms
step:4/1750 train_time:326ms step_avg:81.52ms
step:5/1750 train_time:418ms step_avg:83.67ms
step:6/1750 train_time:511ms step_avg:85.13ms
step:7/1750 train_time:603ms step_avg:86.15ms
step:8/1750 train_time:695ms step_avg:86.93ms
step:9/1750 train_time:788ms step_avg:87.53ms
step:10/1750 train_time:880ms step_avg:87.99ms
step:11/1750 train_time:973ms step_avg:88.43ms
step:12/1750 train_time:1068ms step_avg:88.97ms
step:13/1750 train_time:1164ms step_avg:89.56ms
step:14/1750 train_time:1261ms step_avg:90.09ms
step:15/1750 train_time:1355ms step_avg:90.32ms
step:16/1750 train_time:1448ms step_avg:90.49ms
step:17/1750 train_time:1541ms step_avg:90.67ms
step:18/1750 train_time:1634ms step_avg:90.77ms
step:19/1750 train_time:1727ms step_avg:90.87ms
step:20/1750 train_time:1819ms step_avg:90.96ms
step:21/1750 train_time:1912ms step_avg:91.06ms
step:22/1750 train_time:2006ms step_avg:91.16ms
step:23/1750 train_time:2100ms step_avg:91.31ms
step:24/1750 train_time:2194ms step_avg:91.41ms
step:25/1750 train_time:2288ms step_avg:91.51ms
step:26/1750 train_time:2382ms step_avg:91.60ms
step:27/1750 train_time:2476ms step_avg:91.69ms
step:28/1750 train_time:2568ms step_avg:91.72ms
step:29/1750 train_time:2662ms step_avg:91.80ms
step:30/1750 train_time:2756ms step_avg:91.85ms
step:31/1750 train_time:2848ms step_avg:91.88ms
step:32/1750 train_time:2941ms step_avg:91.91ms
step:33/1750 train_time:3035ms step_avg:91.96ms
step:34/1750 train_time:3128ms step_avg:92.01ms
step:35/1750 train_time:3222ms step_avg:92.05ms
step:36/1750 train_time:3316ms step_avg:92.11ms
step:37/1750 train_time:3410ms step_avg:92.15ms
step:38/1750 train_time:3503ms step_avg:92.18ms
step:39/1750 train_time:3596ms step_avg:92.21ms
step:40/1750 train_time:3689ms step_avg:92.23ms
step:41/1750 train_time:3782ms step_avg:92.25ms
step:42/1750 train_time:3876ms step_avg:92.28ms
step:43/1750 train_time:3969ms step_avg:92.30ms
step:44/1750 train_time:4062ms step_avg:92.31ms
step:45/1750 train_time:4155ms step_avg:92.33ms
step:46/1750 train_time:4249ms step_avg:92.38ms
step:47/1750 train_time:4343ms step_avg:92.41ms
step:48/1750 train_time:4437ms step_avg:92.44ms
step:49/1750 train_time:4531ms step_avg:92.46ms
step:50/1750 train_time:4624ms step_avg:92.48ms
step:51/1750 train_time:4718ms step_avg:92.50ms
step:52/1750 train_time:4811ms step_avg:92.52ms
step:53/1750 train_time:4904ms step_avg:92.53ms
step:54/1750 train_time:4998ms step_avg:92.55ms
step:55/1750 train_time:5091ms step_avg:92.56ms
step:56/1750 train_time:5185ms step_avg:92.58ms
step:57/1750 train_time:5279ms step_avg:92.61ms
step:58/1750 train_time:5372ms step_avg:92.63ms
step:59/1750 train_time:5465ms step_avg:92.64ms
step:60/1750 train_time:5559ms step_avg:92.65ms
step:61/1750 train_time:5652ms step_avg:92.66ms
step:62/1750 train_time:5746ms step_avg:92.68ms
step:63/1750 train_time:5840ms step_avg:92.69ms
step:64/1750 train_time:5933ms step_avg:92.70ms
step:65/1750 train_time:6026ms step_avg:92.70ms
step:66/1750 train_time:6119ms step_avg:92.72ms
step:67/1750 train_time:6212ms step_avg:92.72ms
step:68/1750 train_time:6305ms step_avg:92.72ms
step:69/1750 train_time:6399ms step_avg:92.75ms
step:70/1750 train_time:6493ms step_avg:92.75ms
step:71/1750 train_time:6586ms step_avg:92.76ms
step:72/1750 train_time:6679ms step_avg:92.77ms
step:73/1750 train_time:6772ms step_avg:92.77ms
step:74/1750 train_time:6865ms step_avg:92.77ms
step:75/1750 train_time:6958ms step_avg:92.77ms
step:76/1750 train_time:7051ms step_avg:92.77ms
step:77/1750 train_time:7144ms step_avg:92.78ms
step:78/1750 train_time:7238ms step_avg:92.79ms
step:79/1750 train_time:7331ms step_avg:92.80ms
step:80/1750 train_time:7425ms step_avg:92.81ms
step:81/1750 train_time:7519ms step_avg:92.82ms
step:82/1750 train_time:7612ms step_avg:92.83ms
step:83/1750 train_time:7705ms step_avg:92.84ms
step:84/1750 train_time:7800ms step_avg:92.86ms
step:85/1750 train_time:7894ms step_avg:92.87ms
step:86/1750 train_time:7986ms step_avg:92.87ms
step:87/1750 train_time:8081ms step_avg:92.89ms
step:88/1750 train_time:8175ms step_avg:92.89ms
step:89/1750 train_time:8267ms step_avg:92.89ms
step:90/1750 train_time:8361ms step_avg:92.90ms
step:91/1750 train_time:8454ms step_avg:92.91ms
step:92/1750 train_time:8548ms step_avg:92.91ms
step:93/1750 train_time:8641ms step_avg:92.92ms
step:94/1750 train_time:8735ms step_avg:92.92ms
step:95/1750 train_time:8828ms step_avg:92.93ms
step:96/1750 train_time:8921ms step_avg:92.93ms
step:97/1750 train_time:9015ms step_avg:92.94ms
step:98/1750 train_time:9108ms step_avg:92.94ms
step:99/1750 train_time:9202ms step_avg:92.95ms
step:100/1750 train_time:9295ms step_avg:92.95ms
step:101/1750 train_time:9388ms step_avg:92.95ms
step:102/1750 train_time:9481ms step_avg:92.95ms
step:103/1750 train_time:9576ms step_avg:92.97ms
step:104/1750 train_time:9669ms step_avg:92.97ms
step:105/1750 train_time:9762ms step_avg:92.97ms
step:106/1750 train_time:9855ms step_avg:92.97ms
step:107/1750 train_time:9948ms step_avg:92.97ms
step:108/1750 train_time:10042ms step_avg:92.98ms
step:109/1750 train_time:10134ms step_avg:92.98ms
step:110/1750 train_time:10228ms step_avg:92.98ms
step:111/1750 train_time:10321ms step_avg:92.98ms
step:112/1750 train_time:10414ms step_avg:92.98ms
step:113/1750 train_time:10507ms step_avg:92.99ms
step:114/1750 train_time:10601ms step_avg:92.99ms
step:115/1750 train_time:10694ms step_avg:92.99ms
step:116/1750 train_time:10787ms step_avg:92.99ms
step:117/1750 train_time:10882ms step_avg:93.00ms
step:118/1750 train_time:10976ms step_avg:93.02ms
step:119/1750 train_time:11069ms step_avg:93.01ms
step:120/1750 train_time:11163ms step_avg:93.02ms
step:121/1750 train_time:11255ms step_avg:93.02ms
step:122/1750 train_time:11348ms step_avg:93.02ms
step:123/1750 train_time:11442ms step_avg:93.03ms
step:124/1750 train_time:11536ms step_avg:93.04ms
step:125/1750 train_time:11629ms step_avg:93.04ms
step:125/1750 val_loss:4.7191 train_time:11718ms step_avg:93.74ms
step:126/1750 train_time:11741ms step_avg:93.18ms
step:127/1750 train_time:11827ms step_avg:93.13ms
step:128/1750 train_time:11931ms step_avg:93.21ms
step:129/1750 train_time:12024ms step_avg:93.21ms
step:130/1750 train_time:12118ms step_avg:93.21ms
step:131/1750 train_time:12210ms step_avg:93.21ms
step:132/1750 train_time:12303ms step_avg:93.21ms
step:133/1750 train_time:12396ms step_avg:93.20ms
step:134/1750 train_time:12489ms step_avg:93.20ms
step:135/1750 train_time:12581ms step_avg:93.19ms
step:136/1750 train_time:12674ms step_avg:93.19ms
step:137/1750 train_time:12770ms step_avg:93.21ms
step:138/1750 train_time:12866ms step_avg:93.23ms
step:139/1750 train_time:12961ms step_avg:93.25ms
step:140/1750 train_time:13056ms step_avg:93.26ms
step:141/1750 train_time:13150ms step_avg:93.26ms
step:142/1750 train_time:13243ms step_avg:93.26ms
step:143/1750 train_time:13336ms step_avg:93.26ms
step:144/1750 train_time:13430ms step_avg:93.26ms
step:145/1750 train_time:13523ms step_avg:93.26ms
step:146/1750 train_time:13616ms step_avg:93.26ms
step:147/1750 train_time:13709ms step_avg:93.26ms
step:148/1750 train_time:13804ms step_avg:93.27ms
step:149/1750 train_time:13899ms step_avg:93.28ms
step:150/1750 train_time:13994ms step_avg:93.29ms
step:151/1750 train_time:14088ms step_avg:93.30ms
step:152/1750 train_time:14181ms step_avg:93.29ms
step:153/1750 train_time:14274ms step_avg:93.30ms
step:154/1750 train_time:14368ms step_avg:93.30ms
step:155/1750 train_time:14461ms step_avg:93.30ms
step:156/1750 train_time:14555ms step_avg:93.30ms
step:157/1750 train_time:14648ms step_avg:93.30ms
step:158/1750 train_time:14741ms step_avg:93.30ms
step:159/1750 train_time:14836ms step_avg:93.31ms
step:160/1750 train_time:14931ms step_avg:93.32ms
step:161/1750 train_time:15025ms step_avg:93.32ms
step:162/1750 train_time:15119ms step_avg:93.33ms
step:163/1750 train_time:15212ms step_avg:93.33ms
step:164/1750 train_time:15306ms step_avg:93.33ms
step:165/1750 train_time:15400ms step_avg:93.33ms
step:166/1750 train_time:15493ms step_avg:93.33ms
step:167/1750 train_time:15586ms step_avg:93.33ms
step:168/1750 train_time:15679ms step_avg:93.33ms
step:169/1750 train_time:15773ms step_avg:93.33ms
step:170/1750 train_time:15867ms step_avg:93.34ms
step:171/1750 train_time:15961ms step_avg:93.34ms
step:172/1750 train_time:16054ms step_avg:93.34ms
step:173/1750 train_time:16148ms step_avg:93.34ms
step:174/1750 train_time:16241ms step_avg:93.34ms
step:175/1750 train_time:16335ms step_avg:93.34ms
step:176/1750 train_time:16429ms step_avg:93.35ms
step:177/1750 train_time:16522ms step_avg:93.35ms
step:178/1750 train_time:16616ms step_avg:93.35ms
step:179/1750 train_time:16710ms step_avg:93.35ms
step:180/1750 train_time:16803ms step_avg:93.35ms
step:181/1750 train_time:16898ms step_avg:93.36ms
step:182/1750 train_time:16993ms step_avg:93.37ms
step:183/1750 train_time:17086ms step_avg:93.37ms
step:184/1750 train_time:17180ms step_avg:93.37ms
step:185/1750 train_time:17274ms step_avg:93.38ms
step:186/1750 train_time:17368ms step_avg:93.38ms
step:187/1750 train_time:17462ms step_avg:93.38ms
step:188/1750 train_time:17555ms step_avg:93.38ms
step:189/1750 train_time:17649ms step_avg:93.38ms
step:190/1750 train_time:17742ms step_avg:93.38ms
step:191/1750 train_time:17837ms step_avg:93.39ms
step:192/1750 train_time:17931ms step_avg:93.39ms
step:193/1750 train_time:18025ms step_avg:93.39ms
step:194/1750 train_time:18119ms step_avg:93.39ms
step:195/1750 train_time:18213ms step_avg:93.40ms
step:196/1750 train_time:18307ms step_avg:93.40ms
step:197/1750 train_time:18401ms step_avg:93.41ms
step:198/1750 train_time:18494ms step_avg:93.41ms
step:199/1750 train_time:18588ms step_avg:93.41ms
step:200/1750 train_time:18681ms step_avg:93.41ms
step:201/1750 train_time:18775ms step_avg:93.41ms
step:202/1750 train_time:18869ms step_avg:93.41ms
step:203/1750 train_time:18962ms step_avg:93.41ms
step:204/1750 train_time:19056ms step_avg:93.41ms
step:205/1750 train_time:19149ms step_avg:93.41ms
step:206/1750 train_time:19244ms step_avg:93.42ms
step:207/1750 train_time:19337ms step_avg:93.42ms
step:208/1750 train_time:19432ms step_avg:93.42ms
step:209/1750 train_time:19525ms step_avg:93.42ms
step:210/1750 train_time:19619ms step_avg:93.42ms
step:211/1750 train_time:19713ms step_avg:93.42ms
step:212/1750 train_time:19805ms step_avg:93.42ms
step:213/1750 train_time:19900ms step_avg:93.43ms
step:214/1750 train_time:19993ms step_avg:93.43ms
step:215/1750 train_time:20087ms step_avg:93.43ms
step:216/1750 train_time:20181ms step_avg:93.43ms
step:217/1750 train_time:20275ms step_avg:93.43ms
step:218/1750 train_time:20370ms step_avg:93.44ms
step:219/1750 train_time:20463ms step_avg:93.44ms
step:220/1750 train_time:20558ms step_avg:93.44ms
step:221/1750 train_time:20651ms step_avg:93.44ms
step:222/1750 train_time:20744ms step_avg:93.44ms
step:223/1750 train_time:20838ms step_avg:93.45ms
step:224/1750 train_time:20932ms step_avg:93.45ms
step:225/1750 train_time:21026ms step_avg:93.45ms
step:226/1750 train_time:21119ms step_avg:93.45ms
step:227/1750 train_time:21214ms step_avg:93.45ms
step:228/1750 train_time:21307ms step_avg:93.45ms
step:229/1750 train_time:21401ms step_avg:93.46ms
step:230/1750 train_time:21495ms step_avg:93.46ms
step:231/1750 train_time:21589ms step_avg:93.46ms
step:232/1750 train_time:21682ms step_avg:93.46ms
step:233/1750 train_time:21775ms step_avg:93.46ms
step:234/1750 train_time:21870ms step_avg:93.46ms
step:235/1750 train_time:21964ms step_avg:93.46ms
step:236/1750 train_time:22057ms step_avg:93.46ms
step:237/1750 train_time:22151ms step_avg:93.47ms
step:238/1750 train_time:22245ms step_avg:93.47ms
step:239/1750 train_time:22339ms step_avg:93.47ms
step:240/1750 train_time:22433ms step_avg:93.47ms
step:241/1750 train_time:22527ms step_avg:93.47ms
step:242/1750 train_time:22620ms step_avg:93.47ms
step:243/1750 train_time:22714ms step_avg:93.47ms
step:244/1750 train_time:22808ms step_avg:93.48ms
step:245/1750 train_time:22903ms step_avg:93.48ms
step:246/1750 train_time:22997ms step_avg:93.48ms
step:247/1750 train_time:23091ms step_avg:93.48ms
step:248/1750 train_time:23184ms step_avg:93.48ms
step:249/1750 train_time:23278ms step_avg:93.49ms
step:250/1750 train_time:23372ms step_avg:93.49ms
step:250/1750 val_loss:4.1375 train_time:23462ms step_avg:93.85ms
step:251/1750 train_time:23484ms step_avg:93.56ms
step:252/1750 train_time:23569ms step_avg:93.53ms
step:253/1750 train_time:23667ms step_avg:93.54ms
step:254/1750 train_time:23761ms step_avg:93.55ms
step:255/1750 train_time:23854ms step_avg:93.54ms
step:256/1750 train_time:23947ms step_avg:93.54ms
step:257/1750 train_time:24039ms step_avg:93.54ms
step:258/1750 train_time:24132ms step_avg:93.53ms
step:259/1750 train_time:24225ms step_avg:93.53ms
step:260/1750 train_time:24318ms step_avg:93.53ms
step:261/1750 train_time:24411ms step_avg:93.53ms
step:262/1750 train_time:24507ms step_avg:93.54ms
step:263/1750 train_time:24604ms step_avg:93.55ms
step:264/1750 train_time:24700ms step_avg:93.56ms
step:265/1750 train_time:24794ms step_avg:93.56ms
step:266/1750 train_time:24888ms step_avg:93.56ms
step:267/1750 train_time:24982ms step_avg:93.57ms
step:268/1750 train_time:25076ms step_avg:93.57ms
step:269/1750 train_time:25170ms step_avg:93.57ms
step:270/1750 train_time:25264ms step_avg:93.57ms
step:271/1750 train_time:25357ms step_avg:93.57ms
step:272/1750 train_time:25452ms step_avg:93.57ms
step:273/1750 train_time:25547ms step_avg:93.58ms
step:274/1750 train_time:25642ms step_avg:93.58ms
step:275/1750 train_time:25736ms step_avg:93.59ms
step:276/1750 train_time:25830ms step_avg:93.59ms
step:277/1750 train_time:25925ms step_avg:93.59ms
step:278/1750 train_time:26019ms step_avg:93.59ms
step:279/1750 train_time:26113ms step_avg:93.60ms
step:280/1750 train_time:26207ms step_avg:93.60ms
step:281/1750 train_time:26301ms step_avg:93.60ms
step:282/1750 train_time:26394ms step_avg:93.60ms
step:283/1750 train_time:26490ms step_avg:93.60ms
step:284/1750 train_time:26584ms step_avg:93.61ms
step:285/1750 train_time:26679ms step_avg:93.61ms
step:286/1750 train_time:26774ms step_avg:93.62ms
step:287/1750 train_time:26868ms step_avg:93.62ms
step:288/1750 train_time:26963ms step_avg:93.62ms
step:289/1750 train_time:27057ms step_avg:93.62ms
step:290/1750 train_time:27150ms step_avg:93.62ms
step:291/1750 train_time:27244ms step_avg:93.62ms
step:292/1750 train_time:27338ms step_avg:93.62ms
step:293/1750 train_time:27432ms step_avg:93.62ms
step:294/1750 train_time:27526ms step_avg:93.63ms
step:295/1750 train_time:27621ms step_avg:93.63ms
step:296/1750 train_time:27715ms step_avg:93.63ms
step:297/1750 train_time:27810ms step_avg:93.63ms
step:298/1750 train_time:27905ms step_avg:93.64ms
step:299/1750 train_time:27999ms step_avg:93.64ms
step:300/1750 train_time:28093ms step_avg:93.64ms
step:301/1750 train_time:28186ms step_avg:93.64ms
step:302/1750 train_time:28281ms step_avg:93.64ms
step:303/1750 train_time:28374ms step_avg:93.64ms
step:304/1750 train_time:28469ms step_avg:93.65ms
step:305/1750 train_time:28563ms step_avg:93.65ms
step:306/1750 train_time:28658ms step_avg:93.65ms
step:307/1750 train_time:28752ms step_avg:93.66ms
step:308/1750 train_time:28847ms step_avg:93.66ms
step:309/1750 train_time:28941ms step_avg:93.66ms
step:310/1750 train_time:29036ms step_avg:93.66ms
step:311/1750 train_time:29130ms step_avg:93.67ms
step:312/1750 train_time:29225ms step_avg:93.67ms
step:313/1750 train_time:29319ms step_avg:93.67ms
step:314/1750 train_time:29412ms step_avg:93.67ms
step:315/1750 train_time:29507ms step_avg:93.67ms
step:316/1750 train_time:29603ms step_avg:93.68ms
step:317/1750 train_time:29698ms step_avg:93.68ms
step:318/1750 train_time:29792ms step_avg:93.69ms
step:319/1750 train_time:29886ms step_avg:93.69ms
step:320/1750 train_time:29981ms step_avg:93.69ms
step:321/1750 train_time:30075ms step_avg:93.69ms
step:322/1750 train_time:30170ms step_avg:93.69ms
step:323/1750 train_time:30264ms step_avg:93.70ms
step:324/1750 train_time:30358ms step_avg:93.70ms
step:325/1750 train_time:30452ms step_avg:93.70ms
step:326/1750 train_time:30547ms step_avg:93.70ms
step:327/1750 train_time:30642ms step_avg:93.71ms
step:328/1750 train_time:30736ms step_avg:93.71ms
step:329/1750 train_time:30830ms step_avg:93.71ms
step:330/1750 train_time:30925ms step_avg:93.71ms
step:331/1750 train_time:31020ms step_avg:93.71ms
step:332/1750 train_time:31114ms step_avg:93.72ms
step:333/1750 train_time:31208ms step_avg:93.72ms
step:334/1750 train_time:31302ms step_avg:93.72ms
step:335/1750 train_time:31395ms step_avg:93.72ms
step:336/1750 train_time:31489ms step_avg:93.72ms
step:337/1750 train_time:31584ms step_avg:93.72ms
step:338/1750 train_time:31677ms step_avg:93.72ms
step:339/1750 train_time:31772ms step_avg:93.72ms
step:340/1750 train_time:31867ms step_avg:93.73ms
step:341/1750 train_time:31961ms step_avg:93.73ms
step:342/1750 train_time:32055ms step_avg:93.73ms
step:343/1750 train_time:32150ms step_avg:93.73ms
step:344/1750 train_time:32244ms step_avg:93.73ms
step:345/1750 train_time:32338ms step_avg:93.73ms
step:346/1750 train_time:32433ms step_avg:93.74ms
step:347/1750 train_time:32527ms step_avg:93.74ms
step:348/1750 train_time:32622ms step_avg:93.74ms
step:349/1750 train_time:32716ms step_avg:93.74ms
step:350/1750 train_time:32810ms step_avg:93.74ms
step:351/1750 train_time:32904ms step_avg:93.74ms
step:352/1750 train_time:32999ms step_avg:93.75ms
step:353/1750 train_time:33093ms step_avg:93.75ms
step:354/1750 train_time:33188ms step_avg:93.75ms
step:355/1750 train_time:33282ms step_avg:93.75ms
step:356/1750 train_time:33376ms step_avg:93.75ms
step:357/1750 train_time:33470ms step_avg:93.75ms
step:358/1750 train_time:33565ms step_avg:93.76ms
step:359/1750 train_time:33659ms step_avg:93.76ms
step:360/1750 train_time:33753ms step_avg:93.76ms
step:361/1750 train_time:33848ms step_avg:93.76ms
step:362/1750 train_time:33941ms step_avg:93.76ms
step:363/1750 train_time:34036ms step_avg:93.76ms
step:364/1750 train_time:34130ms step_avg:93.76ms
step:365/1750 train_time:34225ms step_avg:93.77ms
step:366/1750 train_time:34319ms step_avg:93.77ms
step:367/1750 train_time:34413ms step_avg:93.77ms
step:368/1750 train_time:34507ms step_avg:93.77ms
step:369/1750 train_time:34601ms step_avg:93.77ms
step:370/1750 train_time:34695ms step_avg:93.77ms
step:371/1750 train_time:34790ms step_avg:93.77ms
step:372/1750 train_time:34885ms step_avg:93.78ms
step:373/1750 train_time:34980ms step_avg:93.78ms
step:374/1750 train_time:35074ms step_avg:93.78ms
step:375/1750 train_time:35169ms step_avg:93.78ms
step:375/1750 val_loss:3.9111 train_time:35258ms step_avg:94.02ms
step:376/1750 train_time:35280ms step_avg:93.83ms
step:377/1750 train_time:35365ms step_avg:93.81ms
step:378/1750 train_time:35463ms step_avg:93.82ms
step:379/1750 train_time:35557ms step_avg:93.82ms
step:380/1750 train_time:35650ms step_avg:93.82ms
step:381/1750 train_time:35743ms step_avg:93.81ms
step:382/1750 train_time:35836ms step_avg:93.81ms
step:383/1750 train_time:35930ms step_avg:93.81ms
step:384/1750 train_time:36023ms step_avg:93.81ms
step:385/1750 train_time:36116ms step_avg:93.81ms
step:386/1750 train_time:36210ms step_avg:93.81ms
step:387/1750 train_time:36305ms step_avg:93.81ms
step:388/1750 train_time:36402ms step_avg:93.82ms
step:389/1750 train_time:36497ms step_avg:93.82ms
step:390/1750 train_time:36591ms step_avg:93.82ms
step:391/1750 train_time:36687ms step_avg:93.83ms
step:392/1750 train_time:36783ms step_avg:93.83ms
step:393/1750 train_time:36879ms step_avg:93.84ms
step:394/1750 train_time:36975ms step_avg:93.84ms
step:395/1750 train_time:37070ms step_avg:93.85ms
step:396/1750 train_time:37165ms step_avg:93.85ms
step:397/1750 train_time:37262ms step_avg:93.86ms
step:398/1750 train_time:37359ms step_avg:93.87ms
step:399/1750 train_time:37456ms step_avg:93.87ms
step:400/1750 train_time:37552ms step_avg:93.88ms
step:401/1750 train_time:37648ms step_avg:93.88ms
step:402/1750 train_time:37744ms step_avg:93.89ms
step:403/1750 train_time:37839ms step_avg:93.89ms
step:404/1750 train_time:37935ms step_avg:93.90ms
step:405/1750 train_time:38031ms step_avg:93.90ms
step:406/1750 train_time:38127ms step_avg:93.91ms
step:407/1750 train_time:38223ms step_avg:93.91ms
step:408/1750 train_time:38320ms step_avg:93.92ms
step:409/1750 train_time:38416ms step_avg:93.93ms
step:410/1750 train_time:38513ms step_avg:93.93ms
step:411/1750 train_time:38609ms step_avg:93.94ms
step:412/1750 train_time:38705ms step_avg:93.94ms
step:413/1750 train_time:38802ms step_avg:93.95ms
step:414/1750 train_time:38898ms step_avg:93.96ms
step:415/1750 train_time:38993ms step_avg:93.96ms
step:416/1750 train_time:39089ms step_avg:93.96ms
step:417/1750 train_time:39184ms step_avg:93.97ms
step:418/1750 train_time:39281ms step_avg:93.97ms
step:419/1750 train_time:39378ms step_avg:93.98ms
step:420/1750 train_time:39475ms step_avg:93.99ms
step:421/1750 train_time:39572ms step_avg:94.00ms
step:422/1750 train_time:39669ms step_avg:94.00ms
step:423/1750 train_time:39764ms step_avg:94.01ms
step:424/1750 train_time:39860ms step_avg:94.01ms
step:425/1750 train_time:39956ms step_avg:94.02ms
step:426/1750 train_time:40051ms step_avg:94.02ms
step:427/1750 train_time:40148ms step_avg:94.02ms
step:428/1750 train_time:40244ms step_avg:94.03ms
step:429/1750 train_time:40341ms step_avg:94.03ms
step:430/1750 train_time:40436ms step_avg:94.04ms
step:431/1750 train_time:40534ms step_avg:94.05ms
step:432/1750 train_time:40630ms step_avg:94.05ms
step:433/1750 train_time:40726ms step_avg:94.05ms
step:434/1750 train_time:40822ms step_avg:94.06ms
step:435/1750 train_time:40919ms step_avg:94.07ms
step:436/1750 train_time:41015ms step_avg:94.07ms
step:437/1750 train_time:41111ms step_avg:94.08ms
step:438/1750 train_time:41207ms step_avg:94.08ms
step:439/1750 train_time:41304ms step_avg:94.09ms
step:440/1750 train_time:41400ms step_avg:94.09ms
step:441/1750 train_time:41496ms step_avg:94.10ms
step:442/1750 train_time:41592ms step_avg:94.10ms
step:443/1750 train_time:41688ms step_avg:94.10ms
step:444/1750 train_time:41784ms step_avg:94.11ms
step:445/1750 train_time:41880ms step_avg:94.11ms
step:446/1750 train_time:41976ms step_avg:94.12ms
step:447/1750 train_time:42073ms step_avg:94.12ms
step:448/1750 train_time:42169ms step_avg:94.13ms
step:449/1750 train_time:42266ms step_avg:94.13ms
step:450/1750 train_time:42362ms step_avg:94.14ms
step:451/1750 train_time:42458ms step_avg:94.14ms
step:452/1750 train_time:42554ms step_avg:94.15ms
step:453/1750 train_time:42650ms step_avg:94.15ms
step:454/1750 train_time:42746ms step_avg:94.15ms
step:455/1750 train_time:42842ms step_avg:94.16ms
step:456/1750 train_time:42940ms step_avg:94.17ms
step:457/1750 train_time:43036ms step_avg:94.17ms
step:458/1750 train_time:43132ms step_avg:94.17ms
step:459/1750 train_time:43228ms step_avg:94.18ms
step:460/1750 train_time:43324ms step_avg:94.18ms
step:461/1750 train_time:43421ms step_avg:94.19ms
step:462/1750 train_time:43517ms step_avg:94.19ms
step:463/1750 train_time:43612ms step_avg:94.19ms
step:464/1750 train_time:43707ms step_avg:94.20ms
step:465/1750 train_time:43803ms step_avg:94.20ms
step:466/1750 train_time:43900ms step_avg:94.21ms
step:467/1750 train_time:43997ms step_avg:94.21ms
step:468/1750 train_time:44093ms step_avg:94.22ms
step:469/1750 train_time:44189ms step_avg:94.22ms
step:470/1750 train_time:44285ms step_avg:94.22ms
step:471/1750 train_time:44381ms step_avg:94.23ms
step:472/1750 train_time:44478ms step_avg:94.23ms
step:473/1750 train_time:44574ms step_avg:94.24ms
step:474/1750 train_time:44671ms step_avg:94.24ms
step:475/1750 train_time:44767ms step_avg:94.25ms
step:476/1750 train_time:44863ms step_avg:94.25ms
step:477/1750 train_time:44960ms step_avg:94.26ms
step:478/1750 train_time:45055ms step_avg:94.26ms
step:479/1750 train_time:45151ms step_avg:94.26ms
step:480/1750 train_time:45248ms step_avg:94.27ms
step:481/1750 train_time:45344ms step_avg:94.27ms
step:482/1750 train_time:45441ms step_avg:94.28ms
step:483/1750 train_time:45539ms step_avg:94.28ms
step:484/1750 train_time:45636ms step_avg:94.29ms
step:485/1750 train_time:45732ms step_avg:94.29ms
step:486/1750 train_time:45828ms step_avg:94.30ms
step:487/1750 train_time:45925ms step_avg:94.30ms
step:488/1750 train_time:46021ms step_avg:94.31ms
step:489/1750 train_time:46117ms step_avg:94.31ms
step:490/1750 train_time:46213ms step_avg:94.31ms
step:491/1750 train_time:46310ms step_avg:94.32ms
step:492/1750 train_time:46406ms step_avg:94.32ms
step:493/1750 train_time:46502ms step_avg:94.33ms
step:494/1750 train_time:46600ms step_avg:94.33ms
step:495/1750 train_time:46697ms step_avg:94.34ms
step:496/1750 train_time:46793ms step_avg:94.34ms
step:497/1750 train_time:46890ms step_avg:94.35ms
step:498/1750 train_time:46987ms step_avg:94.35ms
step:499/1750 train_time:47083ms step_avg:94.35ms
step:500/1750 train_time:47179ms step_avg:94.36ms
step:500/1750 val_loss:3.7618 train_time:47271ms step_avg:94.54ms
step:501/1750 train_time:47293ms step_avg:94.40ms
step:502/1750 train_time:47380ms step_avg:94.38ms
step:503/1750 train_time:47476ms step_avg:94.39ms
step:504/1750 train_time:47573ms step_avg:94.39ms
step:505/1750 train_time:47668ms step_avg:94.39ms
step:506/1750 train_time:47763ms step_avg:94.39ms
step:507/1750 train_time:47859ms step_avg:94.40ms
step:508/1750 train_time:47954ms step_avg:94.40ms
step:509/1750 train_time:48049ms step_avg:94.40ms
step:510/1750 train_time:48144ms step_avg:94.40ms
step:511/1750 train_time:48241ms step_avg:94.41ms
step:512/1750 train_time:48339ms step_avg:94.41ms
step:513/1750 train_time:48436ms step_avg:94.42ms
step:514/1750 train_time:48533ms step_avg:94.42ms
step:515/1750 train_time:48628ms step_avg:94.42ms
step:516/1750 train_time:48724ms step_avg:94.43ms
step:517/1750 train_time:48819ms step_avg:94.43ms
step:518/1750 train_time:48915ms step_avg:94.43ms
step:519/1750 train_time:49011ms step_avg:94.43ms
step:520/1750 train_time:49106ms step_avg:94.43ms
step:521/1750 train_time:49202ms step_avg:94.44ms
step:522/1750 train_time:49300ms step_avg:94.44ms
step:523/1750 train_time:49397ms step_avg:94.45ms
step:524/1750 train_time:49493ms step_avg:94.45ms
step:525/1750 train_time:49590ms step_avg:94.46ms
step:526/1750 train_time:49686ms step_avg:94.46ms
step:527/1750 train_time:49781ms step_avg:94.46ms
step:528/1750 train_time:49877ms step_avg:94.46ms
step:529/1750 train_time:49974ms step_avg:94.47ms
step:530/1750 train_time:50070ms step_avg:94.47ms
step:531/1750 train_time:50165ms step_avg:94.47ms
step:532/1750 train_time:50262ms step_avg:94.48ms
step:533/1750 train_time:50359ms step_avg:94.48ms
step:534/1750 train_time:50456ms step_avg:94.49ms
step:535/1750 train_time:50553ms step_avg:94.49ms
step:536/1750 train_time:50650ms step_avg:94.50ms
step:537/1750 train_time:50746ms step_avg:94.50ms
step:538/1750 train_time:50841ms step_avg:94.50ms
step:539/1750 train_time:50936ms step_avg:94.50ms
step:540/1750 train_time:51033ms step_avg:94.51ms
step:541/1750 train_time:51129ms step_avg:94.51ms
step:542/1750 train_time:51225ms step_avg:94.51ms
step:543/1750 train_time:51322ms step_avg:94.52ms
step:544/1750 train_time:51419ms step_avg:94.52ms
step:545/1750 train_time:51516ms step_avg:94.52ms
step:546/1750 train_time:51613ms step_avg:94.53ms
step:547/1750 train_time:51710ms step_avg:94.53ms
step:548/1750 train_time:51806ms step_avg:94.54ms
step:549/1750 train_time:51901ms step_avg:94.54ms
step:550/1750 train_time:51997ms step_avg:94.54ms
step:551/1750 train_time:52094ms step_avg:94.55ms
step:552/1750 train_time:52190ms step_avg:94.55ms
step:553/1750 train_time:52287ms step_avg:94.55ms
step:554/1750 train_time:52383ms step_avg:94.55ms
step:555/1750 train_time:52480ms step_avg:94.56ms
step:556/1750 train_time:52577ms step_avg:94.56ms
step:557/1750 train_time:52675ms step_avg:94.57ms
step:558/1750 train_time:52772ms step_avg:94.57ms
step:559/1750 train_time:52868ms step_avg:94.58ms
step:560/1750 train_time:52963ms step_avg:94.58ms
step:561/1750 train_time:53059ms step_avg:94.58ms
step:562/1750 train_time:53155ms step_avg:94.58ms
step:563/1750 train_time:53252ms step_avg:94.59ms
step:564/1750 train_time:53348ms step_avg:94.59ms
step:565/1750 train_time:53445ms step_avg:94.59ms
step:566/1750 train_time:53542ms step_avg:94.60ms
step:567/1750 train_time:53639ms step_avg:94.60ms
step:568/1750 train_time:53735ms step_avg:94.60ms
step:569/1750 train_time:53832ms step_avg:94.61ms
step:570/1750 train_time:53928ms step_avg:94.61ms
step:571/1750 train_time:54024ms step_avg:94.61ms
step:572/1750 train_time:54121ms step_avg:94.62ms
step:573/1750 train_time:54218ms step_avg:94.62ms
step:574/1750 train_time:54314ms step_avg:94.62ms
step:575/1750 train_time:54411ms step_avg:94.63ms
step:576/1750 train_time:54508ms step_avg:94.63ms
step:577/1750 train_time:54604ms step_avg:94.64ms
step:578/1750 train_time:54701ms step_avg:94.64ms
step:579/1750 train_time:54798ms step_avg:94.64ms
step:580/1750 train_time:54894ms step_avg:94.64ms
step:581/1750 train_time:54991ms step_avg:94.65ms
step:582/1750 train_time:55088ms step_avg:94.65ms
step:583/1750 train_time:55185ms step_avg:94.66ms
step:584/1750 train_time:55283ms step_avg:94.66ms
step:585/1750 train_time:55380ms step_avg:94.67ms
step:586/1750 train_time:55476ms step_avg:94.67ms
step:587/1750 train_time:55573ms step_avg:94.67ms
step:588/1750 train_time:55670ms step_avg:94.68ms
step:589/1750 train_time:55766ms step_avg:94.68ms
step:590/1750 train_time:55864ms step_avg:94.69ms
step:591/1750 train_time:55960ms step_avg:94.69ms
step:592/1750 train_time:56058ms step_avg:94.69ms
step:593/1750 train_time:56154ms step_avg:94.70ms
step:594/1750 train_time:56252ms step_avg:94.70ms
step:595/1750 train_time:56349ms step_avg:94.70ms
step:596/1750 train_time:56445ms step_avg:94.71ms
step:597/1750 train_time:56542ms step_avg:94.71ms
step:598/1750 train_time:56638ms step_avg:94.71ms
step:599/1750 train_time:56735ms step_avg:94.72ms
step:600/1750 train_time:56831ms step_avg:94.72ms
step:601/1750 train_time:56928ms step_avg:94.72ms
step:602/1750 train_time:57024ms step_avg:94.72ms
step:603/1750 train_time:57121ms step_avg:94.73ms
step:604/1750 train_time:57218ms step_avg:94.73ms
step:605/1750 train_time:57314ms step_avg:94.73ms
step:606/1750 train_time:57411ms step_avg:94.74ms
step:607/1750 train_time:57507ms step_avg:94.74ms
step:608/1750 train_time:57604ms step_avg:94.74ms
step:609/1750 train_time:57700ms step_avg:94.75ms
step:610/1750 train_time:57796ms step_avg:94.75ms
step:611/1750 train_time:57893ms step_avg:94.75ms
step:612/1750 train_time:57990ms step_avg:94.75ms
step:613/1750 train_time:58087ms step_avg:94.76ms
step:614/1750 train_time:58183ms step_avg:94.76ms
step:615/1750 train_time:58280ms step_avg:94.76ms
step:616/1750 train_time:58376ms step_avg:94.77ms
step:617/1750 train_time:58473ms step_avg:94.77ms
step:618/1750 train_time:58569ms step_avg:94.77ms
step:619/1750 train_time:58665ms step_avg:94.77ms
step:620/1750 train_time:58761ms step_avg:94.78ms
step:621/1750 train_time:58858ms step_avg:94.78ms
step:622/1750 train_time:58954ms step_avg:94.78ms
step:623/1750 train_time:59051ms step_avg:94.78ms
step:624/1750 train_time:59147ms step_avg:94.79ms
step:625/1750 train_time:59243ms step_avg:94.79ms
step:625/1750 val_loss:3.6714 train_time:59335ms step_avg:94.94ms
step:626/1750 train_time:59357ms step_avg:94.82ms
step:627/1750 train_time:59449ms step_avg:94.81ms
step:628/1750 train_time:59547ms step_avg:94.82ms
step:629/1750 train_time:59644ms step_avg:94.82ms
step:630/1750 train_time:59739ms step_avg:94.82ms
step:631/1750 train_time:59835ms step_avg:94.83ms
step:632/1750 train_time:59931ms step_avg:94.83ms
step:633/1750 train_time:60026ms step_avg:94.83ms
step:634/1750 train_time:60121ms step_avg:94.83ms
step:635/1750 train_time:60217ms step_avg:94.83ms
step:636/1750 train_time:60316ms step_avg:94.84ms
step:637/1750 train_time:60414ms step_avg:94.84ms
step:638/1750 train_time:60512ms step_avg:94.85ms
step:639/1750 train_time:60610ms step_avg:94.85ms
step:640/1750 train_time:60707ms step_avg:94.85ms
step:641/1750 train_time:60803ms step_avg:94.86ms
step:642/1750 train_time:60899ms step_avg:94.86ms
step:643/1750 train_time:60995ms step_avg:94.86ms
step:644/1750 train_time:61091ms step_avg:94.86ms
step:645/1750 train_time:61187ms step_avg:94.86ms
step:646/1750 train_time:61283ms step_avg:94.87ms
step:647/1750 train_time:61381ms step_avg:94.87ms
step:648/1750 train_time:61478ms step_avg:94.87ms
step:649/1750 train_time:61576ms step_avg:94.88ms
step:650/1750 train_time:61673ms step_avg:94.88ms
step:651/1750 train_time:61772ms step_avg:94.89ms
step:652/1750 train_time:61870ms step_avg:94.89ms
step:653/1750 train_time:61968ms step_avg:94.90ms
step:654/1750 train_time:62065ms step_avg:94.90ms
step:655/1750 train_time:62162ms step_avg:94.90ms
step:656/1750 train_time:62260ms step_avg:94.91ms
step:657/1750 train_time:62358ms step_avg:94.91ms
step:658/1750 train_time:62456ms step_avg:94.92ms
step:659/1750 train_time:62554ms step_avg:94.92ms
step:660/1750 train_time:62653ms step_avg:94.93ms
step:661/1750 train_time:62751ms step_avg:94.93ms
step:662/1750 train_time:62849ms step_avg:94.94ms
step:663/1750 train_time:62947ms step_avg:94.94ms
step:664/1750 train_time:63043ms step_avg:94.94ms
step:665/1750 train_time:63141ms step_avg:94.95ms
step:666/1750 train_time:63238ms step_avg:94.95ms
step:667/1750 train_time:63336ms step_avg:94.96ms
step:668/1750 train_time:63434ms step_avg:94.96ms
step:669/1750 train_time:63531ms step_avg:94.96ms
step:670/1750 train_time:63629ms step_avg:94.97ms
step:671/1750 train_time:63727ms step_avg:94.97ms
step:672/1750 train_time:63825ms step_avg:94.98ms
step:673/1750 train_time:63923ms step_avg:94.98ms
step:674/1750 train_time:64020ms step_avg:94.99ms
step:675/1750 train_time:64118ms step_avg:94.99ms
step:676/1750 train_time:64216ms step_avg:94.99ms
step:677/1750 train_time:64314ms step_avg:95.00ms
step:678/1750 train_time:64413ms step_avg:95.00ms
step:679/1750 train_time:64512ms step_avg:95.01ms
step:680/1750 train_time:64610ms step_avg:95.01ms
step:681/1750 train_time:64707ms step_avg:95.02ms
step:682/1750 train_time:64806ms step_avg:95.02ms
step:683/1750 train_time:64904ms step_avg:95.03ms
step:684/1750 train_time:65002ms step_avg:95.03ms
step:685/1750 train_time:65099ms step_avg:95.04ms
step:686/1750 train_time:65197ms step_avg:95.04ms
step:687/1750 train_time:65295ms step_avg:95.04ms
step:688/1750 train_time:65393ms step_avg:95.05ms
step:689/1750 train_time:65491ms step_avg:95.05ms
step:690/1750 train_time:65589ms step_avg:95.06ms
step:691/1750 train_time:65687ms step_avg:95.06ms
step:692/1750 train_time:65785ms step_avg:95.06ms
step:693/1750 train_time:65882ms step_avg:95.07ms
step:694/1750 train_time:65980ms step_avg:95.07ms
step:695/1750 train_time:66078ms step_avg:95.08ms
step:696/1750 train_time:66176ms step_avg:95.08ms
step:697/1750 train_time:66274ms step_avg:95.08ms
step:698/1750 train_time:66372ms step_avg:95.09ms
step:699/1750 train_time:66470ms step_avg:95.09ms
step:700/1750 train_time:66568ms step_avg:95.10ms
step:701/1750 train_time:66665ms step_avg:95.10ms
step:702/1750 train_time:66762ms step_avg:95.10ms
step:703/1750 train_time:66861ms step_avg:95.11ms
step:704/1750 train_time:66958ms step_avg:95.11ms
step:705/1750 train_time:67056ms step_avg:95.11ms
step:706/1750 train_time:67154ms step_avg:95.12ms
step:707/1750 train_time:67252ms step_avg:95.12ms
step:708/1750 train_time:67351ms step_avg:95.13ms
step:709/1750 train_time:67449ms step_avg:95.13ms
step:710/1750 train_time:67548ms step_avg:95.14ms
step:711/1750 train_time:67646ms step_avg:95.14ms
step:712/1750 train_time:67745ms step_avg:95.15ms
step:713/1750 train_time:67844ms step_avg:95.15ms
step:714/1750 train_time:67941ms step_avg:95.16ms
step:715/1750 train_time:68038ms step_avg:95.16ms
step:716/1750 train_time:68136ms step_avg:95.16ms
step:717/1750 train_time:68234ms step_avg:95.17ms
step:718/1750 train_time:68332ms step_avg:95.17ms
step:719/1750 train_time:68430ms step_avg:95.17ms
step:720/1750 train_time:68529ms step_avg:95.18ms
step:721/1750 train_time:68626ms step_avg:95.18ms
step:722/1750 train_time:68724ms step_avg:95.19ms
step:723/1750 train_time:68822ms step_avg:95.19ms
step:724/1750 train_time:68920ms step_avg:95.19ms
step:725/1750 train_time:69017ms step_avg:95.20ms
step:726/1750 train_time:69114ms step_avg:95.20ms
step:727/1750 train_time:69211ms step_avg:95.20ms
step:728/1750 train_time:69310ms step_avg:95.21ms
step:729/1750 train_time:69408ms step_avg:95.21ms
step:730/1750 train_time:69506ms step_avg:95.21ms
step:731/1750 train_time:69603ms step_avg:95.22ms
step:732/1750 train_time:69701ms step_avg:95.22ms
step:733/1750 train_time:69799ms step_avg:95.22ms
step:734/1750 train_time:69897ms step_avg:95.23ms
step:735/1750 train_time:69995ms step_avg:95.23ms
step:736/1750 train_time:70093ms step_avg:95.24ms
step:737/1750 train_time:70191ms step_avg:95.24ms
step:738/1750 train_time:70289ms step_avg:95.24ms
step:739/1750 train_time:70386ms step_avg:95.25ms
step:740/1750 train_time:70484ms step_avg:95.25ms
step:741/1750 train_time:70582ms step_avg:95.25ms
step:742/1750 train_time:70680ms step_avg:95.26ms
step:743/1750 train_time:70778ms step_avg:95.26ms
step:744/1750 train_time:70875ms step_avg:95.26ms
step:745/1750 train_time:70973ms step_avg:95.27ms
step:746/1750 train_time:71071ms step_avg:95.27ms
step:747/1750 train_time:71169ms step_avg:95.27ms
step:748/1750 train_time:71267ms step_avg:95.28ms
step:749/1750 train_time:71365ms step_avg:95.28ms
step:750/1750 train_time:71463ms step_avg:95.28ms
step:750/1750 val_loss:3.6082 train_time:71554ms step_avg:95.41ms
step:751/1750 train_time:71577ms step_avg:95.31ms
step:752/1750 train_time:71667ms step_avg:95.30ms
step:753/1750 train_time:71768ms step_avg:95.31ms
step:754/1750 train_time:71867ms step_avg:95.31ms
step:755/1750 train_time:71965ms step_avg:95.32ms
step:756/1750 train_time:72064ms step_avg:95.32ms
step:757/1750 train_time:72160ms step_avg:95.32ms
step:758/1750 train_time:72257ms step_avg:95.33ms
step:759/1750 train_time:72354ms step_avg:95.33ms
step:760/1750 train_time:72451ms step_avg:95.33ms
step:761/1750 train_time:72550ms step_avg:95.34ms
step:762/1750 train_time:72649ms step_avg:95.34ms
step:763/1750 train_time:72748ms step_avg:95.35ms
step:764/1750 train_time:72847ms step_avg:95.35ms
step:765/1750 train_time:72945ms step_avg:95.35ms
step:766/1750 train_time:73043ms step_avg:95.36ms
step:767/1750 train_time:73141ms step_avg:95.36ms
step:768/1750 train_time:73238ms step_avg:95.36ms
step:769/1750 train_time:73335ms step_avg:95.36ms
step:770/1750 train_time:73432ms step_avg:95.37ms
step:771/1750 train_time:73529ms step_avg:95.37ms
step:772/1750 train_time:73628ms step_avg:95.37ms
step:773/1750 train_time:73727ms step_avg:95.38ms
step:774/1750 train_time:73826ms step_avg:95.38ms
step:775/1750 train_time:73926ms step_avg:95.39ms
step:776/1750 train_time:74024ms step_avg:95.39ms
step:777/1750 train_time:74121ms step_avg:95.39ms
step:778/1750 train_time:74218ms step_avg:95.40ms
step:779/1750 train_time:74315ms step_avg:95.40ms
step:780/1750 train_time:74414ms step_avg:95.40ms
step:781/1750 train_time:74512ms step_avg:95.41ms
step:782/1750 train_time:74610ms step_avg:95.41ms
step:783/1750 train_time:74709ms step_avg:95.41ms
step:784/1750 train_time:74809ms step_avg:95.42ms
step:785/1750 train_time:74907ms step_avg:95.42ms
step:786/1750 train_time:75006ms step_avg:95.43ms
step:787/1750 train_time:75104ms step_avg:95.43ms
step:788/1750 train_time:75202ms step_avg:95.43ms
step:789/1750 train_time:75299ms step_avg:95.44ms
step:790/1750 train_time:75398ms step_avg:95.44ms
step:791/1750 train_time:75495ms step_avg:95.44ms
step:792/1750 train_time:75594ms step_avg:95.45ms
step:793/1750 train_time:75692ms step_avg:95.45ms
step:794/1750 train_time:75790ms step_avg:95.45ms
step:795/1750 train_time:75888ms step_avg:95.46ms
step:796/1750 train_time:75987ms step_avg:95.46ms
step:797/1750 train_time:76086ms step_avg:95.46ms
step:798/1750 train_time:76184ms step_avg:95.47ms
step:799/1750 train_time:76282ms step_avg:95.47ms
step:800/1750 train_time:76381ms step_avg:95.48ms
step:801/1750 train_time:76479ms step_avg:95.48ms
step:802/1750 train_time:76578ms step_avg:95.48ms
step:803/1750 train_time:76677ms step_avg:95.49ms
step:804/1750 train_time:76778ms step_avg:95.49ms
step:805/1750 train_time:76877ms step_avg:95.50ms
step:806/1750 train_time:76977ms step_avg:95.51ms
step:807/1750 train_time:77077ms step_avg:95.51ms
step:808/1750 train_time:77175ms step_avg:95.51ms
step:809/1750 train_time:77273ms step_avg:95.52ms
step:810/1750 train_time:77371ms step_avg:95.52ms
step:811/1750 train_time:77469ms step_avg:95.52ms
step:812/1750 train_time:77568ms step_avg:95.53ms
step:813/1750 train_time:77666ms step_avg:95.53ms
step:814/1750 train_time:77765ms step_avg:95.53ms
step:815/1750 train_time:77865ms step_avg:95.54ms
step:816/1750 train_time:77964ms step_avg:95.54ms
step:817/1750 train_time:78063ms step_avg:95.55ms
step:818/1750 train_time:78161ms step_avg:95.55ms
step:819/1750 train_time:78259ms step_avg:95.55ms
step:820/1750 train_time:78357ms step_avg:95.56ms
step:821/1750 train_time:78456ms step_avg:95.56ms
step:822/1750 train_time:78554ms step_avg:95.56ms
step:823/1750 train_time:78652ms step_avg:95.57ms
step:824/1750 train_time:78750ms step_avg:95.57ms
step:825/1750 train_time:78848ms step_avg:95.57ms
step:826/1750 train_time:78947ms step_avg:95.58ms
step:827/1750 train_time:79046ms step_avg:95.58ms
step:828/1750 train_time:79144ms step_avg:95.59ms
step:829/1750 train_time:79243ms step_avg:95.59ms
step:830/1750 train_time:79342ms step_avg:95.59ms
step:831/1750 train_time:79440ms step_avg:95.60ms
step:832/1750 train_time:79539ms step_avg:95.60ms
step:833/1750 train_time:79638ms step_avg:95.60ms
step:834/1750 train_time:79737ms step_avg:95.61ms
step:835/1750 train_time:79836ms step_avg:95.61ms
step:836/1750 train_time:79935ms step_avg:95.62ms
step:837/1750 train_time:80033ms step_avg:95.62ms
step:838/1750 train_time:80131ms step_avg:95.62ms
step:839/1750 train_time:80230ms step_avg:95.63ms
step:840/1750 train_time:80329ms step_avg:95.63ms
step:841/1750 train_time:80428ms step_avg:95.63ms
step:842/1750 train_time:80526ms step_avg:95.64ms
step:843/1750 train_time:80625ms step_avg:95.64ms
step:844/1750 train_time:80723ms step_avg:95.64ms
step:845/1750 train_time:80821ms step_avg:95.65ms
step:846/1750 train_time:80919ms step_avg:95.65ms
step:847/1750 train_time:81017ms step_avg:95.65ms
step:848/1750 train_time:81115ms step_avg:95.65ms
step:849/1750 train_time:81213ms step_avg:95.66ms
step:850/1750 train_time:81310ms step_avg:95.66ms
step:851/1750 train_time:81408ms step_avg:95.66ms
step:852/1750 train_time:81506ms step_avg:95.66ms
step:853/1750 train_time:81604ms step_avg:95.67ms
step:854/1750 train_time:81703ms step_avg:95.67ms
step:855/1750 train_time:81802ms step_avg:95.67ms
step:856/1750 train_time:81900ms step_avg:95.68ms
step:857/1750 train_time:81998ms step_avg:95.68ms
step:858/1750 train_time:82097ms step_avg:95.68ms
step:859/1750 train_time:82195ms step_avg:95.69ms
step:860/1750 train_time:82293ms step_avg:95.69ms
step:861/1750 train_time:82390ms step_avg:95.69ms
step:862/1750 train_time:82488ms step_avg:95.69ms
step:863/1750 train_time:82587ms step_avg:95.70ms
step:864/1750 train_time:82685ms step_avg:95.70ms
step:865/1750 train_time:82783ms step_avg:95.70ms
step:866/1750 train_time:82881ms step_avg:95.71ms
step:867/1750 train_time:82979ms step_avg:95.71ms
step:868/1750 train_time:83078ms step_avg:95.71ms
step:869/1750 train_time:83176ms step_avg:95.72ms
step:870/1750 train_time:83275ms step_avg:95.72ms
step:871/1750 train_time:83374ms step_avg:95.72ms
step:872/1750 train_time:83473ms step_avg:95.73ms
step:873/1750 train_time:83570ms step_avg:95.73ms
step:874/1750 train_time:83668ms step_avg:95.73ms
step:875/1750 train_time:83766ms step_avg:95.73ms
