import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X
'''
import torch.utils.dlpack as thd
from cupyx.scipy.sparse.linalg import svds as cupyx_svds

def several_sv_svds_approximation(W_torch, k, num_iter=50):
    """SVD approximation using the top k singular values and corresponding vectors."""
    # Store original device and dtype
    original_device = W_torch.device
    original_dtype = W_torch.dtype
    
    W = cp.from_dlpack(thd.to_dlpack(W_torch)).astype(cp.float32)
    U, S, Vt = cupyx_svds(W, k=min([k, W.shape[0] - 1, W.shape[1] - 1]), maxiter=num_iter, which='LM')

    # Convert back to torch tensors and ensure they're on the correct device
    approx_torch_U = thd.from_dlpack(U.toDlpack()).to(device=original_device, dtype=original_dtype)
    approx_torch_S = thd.from_dlpack(S.toDlpack()).to(device=original_device, dtype=original_dtype)
    approx_torch_Vt = thd.from_dlpack(Vt.toDlpack()).to(device=original_device, dtype=original_dtype)
    
    return approx_torch_U, approx_torch_S, approx_torch_Vt

class NormNeon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    # v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    u, s, vt = several_sv_svds_approximation(grad.bfloat16(), 5)
                    p.add_(other=u@vt, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()
'''
class FastNormMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, sgd_coeff=0):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, sgd_coeff=sgd_coeff)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        self.sgd_coeff = sgd_coeff

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            sgd_coeff = group["sgd_coeff"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    # assume: grad is a Tensor, momentum_buffer exists, momentum, sgd_coeff, eps, eff_lr are defined
                    with torch.no_grad():
                        # inplace blend with momentum (keeps original semantic: grad becomes the lerped tensor)
                        grad.lerp_(momentum_buffer, momentum)

                        # compute zeropower part from the (post-lerp) grad, in bfloat16 to match the original code
                        if sgd_coeff != 1:
                            # create update_part in bfloat16 (may allocate once per call; unavoidable if zeropower expects bfloat16)
                            update_part = zeropower_via_newtonschulz5(grad.to(torch.bfloat16), 5)

                            # upcast update_part to grad dtype to do mixed-dtype blending without extra temporaries later
                            if update_part.dtype != grad.dtype:
                                update_part = update_part.to(grad.dtype)

                        # compute inverse norm scalar efficiently (no large temp tensors)
                        # - view(-1) is cheap (no copy)
                        # - dot(view, view) is a single reduction
                        norm_sq = grad.view(-1).dot(grad.view(-1))
                        inv_norm = (norm_sq + eps).rsqrt()  # stable rsqrt on the scalar

                        # normalize grad in-place (so we avoid allocating g_normalized)
                        grad.mul_(inv_norm)   # now grad == g_normalized (in-place)

                        # build final update (reuse update_part buffer if available)
                        if sgd_coeff != 1:
                            # update_part := (1 - sgd_coeff) * update_part + sgd_coeff * grad
                            # do it in-place on update_part to avoid creating another temp
                            update_part.mul_(1 - sgd_coeff)    # in-place scale
                            update_part.lerp_(grad, sgd_coeff) # in-place linear interpolation
                            update = update_part
                        else:
                            # if sgd_coeff == 1 we can use grad directly (already normalized in-place)
                            update = grad

                        # apply update (same as original)
                        p.add_(other=update, alpha=-eff_lr)
                    '''
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    eps = 1e-12
                    g_normalized = grad / (grad.norm() + eps)
                    if sgd_coeff != 1:
                        update_part = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                        update = (1 - sgd_coeff) * update_part + sgd_coeff * g_normalized
                    else:
                        update = sgd_coeff * g_normalized
                    p.add_(other=update, alpha=-eff_lr)
                    '''
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class NormMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, sgd_coeff=0):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, sgd_coeff=sgd_coeff)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        self.sgd_coeff = sgd_coeff

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            sgd_coeff = group["sgd_coeff"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    eps = 1e-12
                    g_normalized = grad / (grad.norm() + eps)
                    if sgd_coeff != 1:
                        update_part = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                        update = (1 - sgd_coeff) * update_part + sgd_coeff * g_normalized
                    else:
                        update = sgd_coeff * g_normalized
                    p.add_(other=update, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()
class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)
                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_reduce_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 27.5
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos : pos + max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()
    starts = []
    for i in range(1, len(boundary_positions)):
        end = boundary_positions[i].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == dist.get_world_size():
                return starts, end - pos
            start = end
    assert False # increase max_batch_span if necessary

def distributed_data_generator(filename_pattern: str, batch_size: int, align_to_bos: bool):
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    max_batch_span = 2 * batch_size if align_to_bos else batch_size # provide buffer to handle samples up to length local_batch_size
    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            batch_span = batch_size
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1750 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
from datetime import datetime
if master_process:
    # run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_id = str(int(time.time() * 1000))
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0)
optimizer2 = NormMuon(hidden_matrix_params, lr=0.07, momentum=0.96, weight_decay=0, sgd_coeff=0.5)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.11 | packaged by conda-forge | (main, Jun  4 2025, 14:45:31) [GCC 13.3.0]
Running PyTorch 2.9.0.dev20250824+cu126 compiled for CUDA 12.6
Sun Aug 24 18:11:05 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:04:00.0 Off |                    0 |
| N/A   41C    P0            123W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:23:00.0 Off |                    0 |
| N/A   34C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:43:00.0 Off |                    0 |
| N/A   39C    P0            124W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   34C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:84:00.0 Off |                    0 |
| N/A   39C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:A3:00.0 Off |                    0 |
| N/A   34C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:C3:00.0 Off |                    0 |
| N/A   41C    P0            121W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E4:00.0 Off |                    0 |
| N/A   33C    P0            113W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.07ms
step:1/1750 train_time:158ms step_avg:157.77ms
step:2/1750 train_time:179ms step_avg:89.34ms
step:3/1750 train_time:259ms step_avg:86.46ms
step:4/1750 train_time:351ms step_avg:87.70ms
step:5/1750 train_time:443ms step_avg:88.68ms
step:6/1750 train_time:536ms step_avg:89.33ms
step:7/1750 train_time:629ms step_avg:89.85ms
step:8/1750 train_time:722ms step_avg:90.19ms
step:9/1750 train_time:814ms step_avg:90.45ms
step:10/1750 train_time:907ms step_avg:90.67ms
step:11/1750 train_time:999ms step_avg:90.84ms
step:12/1750 train_time:1095ms step_avg:91.23ms
step:13/1750 train_time:1191ms step_avg:91.61ms
step:14/1750 train_time:1286ms step_avg:91.84ms
step:15/1750 train_time:1381ms step_avg:92.05ms
step:16/1750 train_time:1473ms step_avg:92.08ms
step:17/1750 train_time:1567ms step_avg:92.16ms
step:18/1750 train_time:1660ms step_avg:92.20ms
step:19/1750 train_time:1752ms step_avg:92.20ms
step:20/1750 train_time:1845ms step_avg:92.25ms
step:21/1750 train_time:1937ms step_avg:92.26ms
step:22/1750 train_time:2031ms step_avg:92.34ms
step:23/1750 train_time:2125ms step_avg:92.41ms
step:24/1750 train_time:2220ms step_avg:92.51ms
step:25/1750 train_time:2315ms step_avg:92.58ms
step:26/1750 train_time:2408ms step_avg:92.63ms
step:27/1750 train_time:2502ms step_avg:92.66ms
step:28/1750 train_time:2595ms step_avg:92.69ms
step:29/1750 train_time:2688ms step_avg:92.69ms
step:30/1750 train_time:2782ms step_avg:92.72ms
step:31/1750 train_time:2874ms step_avg:92.72ms
step:32/1750 train_time:2967ms step_avg:92.73ms
step:33/1750 train_time:3060ms step_avg:92.73ms
step:34/1750 train_time:3154ms step_avg:92.77ms
step:35/1750 train_time:3248ms step_avg:92.80ms
step:36/1750 train_time:3342ms step_avg:92.84ms
step:37/1750 train_time:3436ms step_avg:92.87ms
step:38/1750 train_time:3530ms step_avg:92.89ms
step:39/1750 train_time:3623ms step_avg:92.90ms
step:40/1750 train_time:3716ms step_avg:92.91ms
step:41/1750 train_time:3809ms step_avg:92.91ms
step:42/1750 train_time:3902ms step_avg:92.91ms
step:43/1750 train_time:3995ms step_avg:92.91ms
step:44/1750 train_time:4089ms step_avg:92.94ms
step:45/1750 train_time:4184ms step_avg:92.97ms
step:46/1750 train_time:4277ms step_avg:92.98ms
step:47/1750 train_time:4371ms step_avg:93.00ms
step:48/1750 train_time:4465ms step_avg:93.01ms
step:49/1750 train_time:4559ms step_avg:93.04ms
step:50/1750 train_time:4652ms step_avg:93.04ms
step:51/1750 train_time:4745ms step_avg:93.03ms
step:52/1750 train_time:4838ms step_avg:93.03ms
step:53/1750 train_time:4930ms step_avg:93.02ms
step:54/1750 train_time:5023ms step_avg:93.02ms
step:55/1750 train_time:5117ms step_avg:93.03ms
step:56/1750 train_time:5211ms step_avg:93.05ms
step:57/1750 train_time:5304ms step_avg:93.06ms
step:58/1750 train_time:5398ms step_avg:93.06ms
step:59/1750 train_time:5491ms step_avg:93.07ms
step:60/1750 train_time:5585ms step_avg:93.08ms
step:61/1750 train_time:5678ms step_avg:93.08ms
step:62/1750 train_time:5772ms step_avg:93.09ms
step:63/1750 train_time:5864ms step_avg:93.08ms
step:64/1750 train_time:5958ms step_avg:93.09ms
step:65/1750 train_time:6051ms step_avg:93.10ms
step:66/1750 train_time:6145ms step_avg:93.11ms
step:67/1750 train_time:6239ms step_avg:93.11ms
step:68/1750 train_time:6331ms step_avg:93.11ms
step:69/1750 train_time:6425ms step_avg:93.11ms
step:70/1750 train_time:6518ms step_avg:93.11ms
step:71/1750 train_time:6611ms step_avg:93.11ms
step:72/1750 train_time:6704ms step_avg:93.11ms
step:73/1750 train_time:6796ms step_avg:93.10ms
step:74/1750 train_time:6890ms step_avg:93.10ms
step:75/1750 train_time:6983ms step_avg:93.10ms
step:76/1750 train_time:7076ms step_avg:93.11ms
step:77/1750 train_time:7170ms step_avg:93.11ms
step:78/1750 train_time:7263ms step_avg:93.11ms
step:79/1750 train_time:7356ms step_avg:93.12ms
step:80/1750 train_time:7450ms step_avg:93.12ms
step:81/1750 train_time:7543ms step_avg:93.12ms
step:82/1750 train_time:7636ms step_avg:93.12ms
step:83/1750 train_time:7729ms step_avg:93.12ms
step:84/1750 train_time:7824ms step_avg:93.14ms
step:85/1750 train_time:7917ms step_avg:93.15ms
step:86/1750 train_time:8011ms step_avg:93.15ms
step:87/1750 train_time:8103ms step_avg:93.14ms
step:88/1750 train_time:8196ms step_avg:93.14ms
step:89/1750 train_time:8289ms step_avg:93.14ms
step:90/1750 train_time:8383ms step_avg:93.14ms
step:91/1750 train_time:8475ms step_avg:93.14ms
step:92/1750 train_time:8569ms step_avg:93.14ms
step:93/1750 train_time:8662ms step_avg:93.14ms
step:94/1750 train_time:8755ms step_avg:93.14ms
step:95/1750 train_time:8849ms step_avg:93.15ms
step:96/1750 train_time:8942ms step_avg:93.15ms
step:97/1750 train_time:9035ms step_avg:93.14ms
step:98/1750 train_time:9128ms step_avg:93.14ms
step:99/1750 train_time:9221ms step_avg:93.14ms
step:100/1750 train_time:9315ms step_avg:93.15ms
step:101/1750 train_time:9408ms step_avg:93.15ms
step:102/1750 train_time:9502ms step_avg:93.16ms
step:103/1750 train_time:9595ms step_avg:93.15ms
step:104/1750 train_time:9688ms step_avg:93.16ms
step:105/1750 train_time:9781ms step_avg:93.15ms
step:106/1750 train_time:9874ms step_avg:93.15ms
step:107/1750 train_time:9968ms step_avg:93.16ms
step:108/1750 train_time:10061ms step_avg:93.15ms
step:109/1750 train_time:10153ms step_avg:93.15ms
step:110/1750 train_time:10246ms step_avg:93.15ms
step:111/1750 train_time:10340ms step_avg:93.15ms
step:112/1750 train_time:10433ms step_avg:93.15ms
step:113/1750 train_time:10526ms step_avg:93.15ms
step:114/1750 train_time:10619ms step_avg:93.15ms
step:115/1750 train_time:10712ms step_avg:93.15ms
step:116/1750 train_time:10806ms step_avg:93.16ms
step:117/1750 train_time:10900ms step_avg:93.16ms
step:118/1750 train_time:10993ms step_avg:93.16ms
step:119/1750 train_time:11086ms step_avg:93.16ms
step:120/1750 train_time:11179ms step_avg:93.15ms
step:121/1750 train_time:11272ms step_avg:93.16ms
step:122/1750 train_time:11366ms step_avg:93.16ms
step:123/1750 train_time:11459ms step_avg:93.16ms
step:124/1750 train_time:11552ms step_avg:93.17ms
step:125/1750 train_time:11646ms step_avg:93.17ms
step:125/1750 val_loss:4.6487 train_time:11729ms step_avg:93.83ms
step:126/1750 train_time:11751ms step_avg:93.26ms
step:127/1750 train_time:11840ms step_avg:93.23ms
step:128/1750 train_time:11942ms step_avg:93.29ms
step:129/1750 train_time:12036ms step_avg:93.30ms
step:130/1750 train_time:12130ms step_avg:93.31ms
step:131/1750 train_time:12224ms step_avg:93.31ms
step:132/1750 train_time:12316ms step_avg:93.31ms
step:133/1750 train_time:12409ms step_avg:93.30ms
step:134/1750 train_time:12501ms step_avg:93.29ms
step:135/1750 train_time:12594ms step_avg:93.29ms
step:136/1750 train_time:12687ms step_avg:93.29ms
step:137/1750 train_time:12782ms step_avg:93.30ms
step:138/1750 train_time:12878ms step_avg:93.32ms
step:139/1750 train_time:12973ms step_avg:93.33ms
step:140/1750 train_time:13067ms step_avg:93.34ms
step:141/1750 train_time:13162ms step_avg:93.35ms
step:142/1750 train_time:13255ms step_avg:93.35ms
step:143/1750 train_time:13349ms step_avg:93.35ms
step:144/1750 train_time:13442ms step_avg:93.35ms
step:145/1750 train_time:13536ms step_avg:93.35ms
step:146/1750 train_time:13628ms step_avg:93.34ms
step:147/1750 train_time:13722ms step_avg:93.35ms
step:148/1750 train_time:13816ms step_avg:93.35ms
step:149/1750 train_time:13911ms step_avg:93.36ms
step:150/1750 train_time:14005ms step_avg:93.37ms
step:151/1750 train_time:14099ms step_avg:93.37ms
step:152/1750 train_time:14193ms step_avg:93.37ms
step:153/1750 train_time:14287ms step_avg:93.38ms
step:154/1750 train_time:14381ms step_avg:93.38ms
step:155/1750 train_time:14474ms step_avg:93.38ms
step:156/1750 train_time:14568ms step_avg:93.39ms
step:157/1750 train_time:14662ms step_avg:93.39ms
step:158/1750 train_time:14755ms step_avg:93.39ms
step:159/1750 train_time:14849ms step_avg:93.39ms
step:160/1750 train_time:14943ms step_avg:93.40ms
step:161/1750 train_time:15037ms step_avg:93.40ms
step:162/1750 train_time:15131ms step_avg:93.40ms
step:163/1750 train_time:15225ms step_avg:93.40ms
step:164/1750 train_time:15319ms step_avg:93.41ms
step:165/1750 train_time:15412ms step_avg:93.41ms
step:166/1750 train_time:15506ms step_avg:93.41ms
step:167/1750 train_time:15599ms step_avg:93.41ms
step:168/1750 train_time:15692ms step_avg:93.41ms
step:169/1750 train_time:15786ms step_avg:93.41ms
step:170/1750 train_time:15880ms step_avg:93.41ms
step:171/1750 train_time:15974ms step_avg:93.41ms
step:172/1750 train_time:16068ms step_avg:93.42ms
step:173/1750 train_time:16162ms step_avg:93.42ms
step:174/1750 train_time:16255ms step_avg:93.42ms
step:175/1750 train_time:16350ms step_avg:93.43ms
step:176/1750 train_time:16444ms step_avg:93.43ms
step:177/1750 train_time:16538ms step_avg:93.43ms
step:178/1750 train_time:16631ms step_avg:93.43ms
step:179/1750 train_time:16724ms step_avg:93.43ms
step:180/1750 train_time:16818ms step_avg:93.43ms
step:181/1750 train_time:16911ms step_avg:93.43ms
step:182/1750 train_time:17005ms step_avg:93.43ms
step:183/1750 train_time:17099ms step_avg:93.44ms
step:184/1750 train_time:17192ms step_avg:93.43ms
step:185/1750 train_time:17286ms step_avg:93.44ms
step:186/1750 train_time:17380ms step_avg:93.44ms
step:187/1750 train_time:17474ms step_avg:93.44ms
step:188/1750 train_time:17567ms step_avg:93.44ms
step:189/1750 train_time:17661ms step_avg:93.44ms
step:190/1750 train_time:17754ms step_avg:93.44ms
step:191/1750 train_time:17848ms step_avg:93.44ms
step:192/1750 train_time:17942ms step_avg:93.45ms
step:193/1750 train_time:18035ms step_avg:93.45ms
step:194/1750 train_time:18129ms step_avg:93.45ms
step:195/1750 train_time:18224ms step_avg:93.45ms
step:196/1750 train_time:18317ms step_avg:93.46ms
step:197/1750 train_time:18411ms step_avg:93.46ms
step:198/1750 train_time:18505ms step_avg:93.46ms
step:199/1750 train_time:18598ms step_avg:93.46ms
step:200/1750 train_time:18692ms step_avg:93.46ms
step:201/1750 train_time:18786ms step_avg:93.46ms
step:202/1750 train_time:18881ms step_avg:93.47ms
step:203/1750 train_time:18974ms step_avg:93.47ms
step:204/1750 train_time:19068ms step_avg:93.47ms
step:205/1750 train_time:19162ms step_avg:93.48ms
step:206/1750 train_time:19256ms step_avg:93.48ms
step:207/1750 train_time:19350ms step_avg:93.48ms
step:208/1750 train_time:19444ms step_avg:93.48ms
step:209/1750 train_time:19538ms step_avg:93.48ms
step:210/1750 train_time:19631ms step_avg:93.48ms
step:211/1750 train_time:19725ms step_avg:93.48ms
step:212/1750 train_time:19819ms step_avg:93.49ms
step:213/1750 train_time:19913ms step_avg:93.49ms
step:214/1750 train_time:20007ms step_avg:93.49ms
step:215/1750 train_time:20101ms step_avg:93.49ms
step:216/1750 train_time:20195ms step_avg:93.49ms
step:217/1750 train_time:20288ms step_avg:93.49ms
step:218/1750 train_time:20382ms step_avg:93.50ms
step:219/1750 train_time:20476ms step_avg:93.50ms
step:220/1750 train_time:20570ms step_avg:93.50ms
step:221/1750 train_time:20664ms step_avg:93.50ms
step:222/1750 train_time:20757ms step_avg:93.50ms
step:223/1750 train_time:20851ms step_avg:93.50ms
step:224/1750 train_time:20945ms step_avg:93.50ms
step:225/1750 train_time:21039ms step_avg:93.51ms
step:226/1750 train_time:21132ms step_avg:93.51ms
step:227/1750 train_time:21226ms step_avg:93.51ms
step:228/1750 train_time:21320ms step_avg:93.51ms
step:229/1750 train_time:21414ms step_avg:93.51ms
step:230/1750 train_time:21508ms step_avg:93.51ms
step:231/1750 train_time:21602ms step_avg:93.51ms
step:232/1750 train_time:21696ms step_avg:93.52ms
step:233/1750 train_time:21789ms step_avg:93.52ms
step:234/1750 train_time:21884ms step_avg:93.52ms
step:235/1750 train_time:21977ms step_avg:93.52ms
step:236/1750 train_time:22071ms step_avg:93.52ms
step:237/1750 train_time:22165ms step_avg:93.52ms
step:238/1750 train_time:22259ms step_avg:93.52ms
step:239/1750 train_time:22352ms step_avg:93.53ms
step:240/1750 train_time:22446ms step_avg:93.53ms
step:241/1750 train_time:22540ms step_avg:93.53ms
step:242/1750 train_time:22633ms step_avg:93.52ms
step:243/1750 train_time:22727ms step_avg:93.53ms
step:244/1750 train_time:22821ms step_avg:93.53ms
step:245/1750 train_time:22914ms step_avg:93.53ms
step:246/1750 train_time:23008ms step_avg:93.53ms
step:247/1750 train_time:23102ms step_avg:93.53ms
step:248/1750 train_time:23196ms step_avg:93.53ms
step:249/1750 train_time:23290ms step_avg:93.53ms
step:250/1750 train_time:23384ms step_avg:93.54ms
step:250/1750 val_loss:4.1050 train_time:23467ms step_avg:93.87ms
step:251/1750 train_time:23489ms step_avg:93.58ms
step:252/1750 train_time:23579ms step_avg:93.57ms
step:253/1750 train_time:23676ms step_avg:93.58ms
step:254/1750 train_time:23771ms step_avg:93.59ms
step:255/1750 train_time:23863ms step_avg:93.58ms
step:256/1750 train_time:23956ms step_avg:93.58ms
step:257/1750 train_time:24048ms step_avg:93.57ms
step:258/1750 train_time:24141ms step_avg:93.57ms
step:259/1750 train_time:24234ms step_avg:93.57ms
step:260/1750 train_time:24327ms step_avg:93.56ms
step:261/1750 train_time:24421ms step_avg:93.57ms
step:262/1750 train_time:24517ms step_avg:93.58ms
step:263/1750 train_time:24613ms step_avg:93.58ms
step:264/1750 train_time:24708ms step_avg:93.59ms
step:265/1750 train_time:24803ms step_avg:93.60ms
step:266/1750 train_time:24896ms step_avg:93.59ms
step:267/1750 train_time:24990ms step_avg:93.59ms
step:268/1750 train_time:25083ms step_avg:93.59ms
step:269/1750 train_time:25177ms step_avg:93.59ms
step:270/1750 train_time:25270ms step_avg:93.59ms
step:271/1750 train_time:25364ms step_avg:93.59ms
step:272/1750 train_time:25458ms step_avg:93.60ms
step:273/1750 train_time:25553ms step_avg:93.60ms
step:274/1750 train_time:25648ms step_avg:93.61ms
step:275/1750 train_time:25743ms step_avg:93.61ms
step:276/1750 train_time:25838ms step_avg:93.61ms
step:277/1750 train_time:25931ms step_avg:93.61ms
step:278/1750 train_time:26025ms step_avg:93.62ms
step:279/1750 train_time:26119ms step_avg:93.62ms
step:280/1750 train_time:26212ms step_avg:93.62ms
step:281/1750 train_time:26306ms step_avg:93.62ms
step:282/1750 train_time:26400ms step_avg:93.62ms
step:283/1750 train_time:26494ms step_avg:93.62ms
step:284/1750 train_time:26588ms step_avg:93.62ms
step:285/1750 train_time:26683ms step_avg:93.62ms
step:286/1750 train_time:26778ms step_avg:93.63ms
step:287/1750 train_time:26872ms step_avg:93.63ms
step:288/1750 train_time:26967ms step_avg:93.63ms
step:289/1750 train_time:27061ms step_avg:93.64ms
step:290/1750 train_time:27155ms step_avg:93.64ms
step:291/1750 train_time:27248ms step_avg:93.64ms
step:292/1750 train_time:27342ms step_avg:93.64ms
step:293/1750 train_time:27437ms step_avg:93.64ms
step:294/1750 train_time:27531ms step_avg:93.64ms
step:295/1750 train_time:27626ms step_avg:93.65ms
step:296/1750 train_time:27721ms step_avg:93.65ms
step:297/1750 train_time:27816ms step_avg:93.66ms
step:298/1750 train_time:27910ms step_avg:93.66ms
step:299/1750 train_time:28005ms step_avg:93.66ms
step:300/1750 train_time:28099ms step_avg:93.66ms
step:301/1750 train_time:28192ms step_avg:93.66ms
step:302/1750 train_time:28286ms step_avg:93.66ms
step:303/1750 train_time:28380ms step_avg:93.66ms
step:304/1750 train_time:28474ms step_avg:93.67ms
step:305/1750 train_time:28568ms step_avg:93.67ms
step:306/1750 train_time:28663ms step_avg:93.67ms
step:307/1750 train_time:28757ms step_avg:93.67ms
step:308/1750 train_time:28851ms step_avg:93.67ms
step:309/1750 train_time:28946ms step_avg:93.68ms
step:310/1750 train_time:29041ms step_avg:93.68ms
step:311/1750 train_time:29134ms step_avg:93.68ms
step:312/1750 train_time:29229ms step_avg:93.68ms
step:313/1750 train_time:29323ms step_avg:93.68ms
step:314/1750 train_time:29418ms step_avg:93.69ms
step:315/1750 train_time:29512ms step_avg:93.69ms
step:316/1750 train_time:29606ms step_avg:93.69ms
step:317/1750 train_time:29700ms step_avg:93.69ms
step:318/1750 train_time:29794ms step_avg:93.69ms
step:319/1750 train_time:29889ms step_avg:93.69ms
step:320/1750 train_time:29983ms step_avg:93.70ms
step:321/1750 train_time:30078ms step_avg:93.70ms
step:322/1750 train_time:30172ms step_avg:93.70ms
step:323/1750 train_time:30266ms step_avg:93.70ms
step:324/1750 train_time:30360ms step_avg:93.70ms
step:325/1750 train_time:30454ms step_avg:93.71ms
step:326/1750 train_time:30548ms step_avg:93.71ms
step:327/1750 train_time:30643ms step_avg:93.71ms
step:328/1750 train_time:30737ms step_avg:93.71ms
step:329/1750 train_time:30831ms step_avg:93.71ms
step:330/1750 train_time:30925ms step_avg:93.71ms
step:331/1750 train_time:31020ms step_avg:93.72ms
step:332/1750 train_time:31114ms step_avg:93.72ms
step:333/1750 train_time:31208ms step_avg:93.72ms
step:334/1750 train_time:31303ms step_avg:93.72ms
step:335/1750 train_time:31397ms step_avg:93.72ms
step:336/1750 train_time:31491ms step_avg:93.72ms
step:337/1750 train_time:31585ms step_avg:93.72ms
step:338/1750 train_time:31680ms step_avg:93.73ms
step:339/1750 train_time:31773ms step_avg:93.73ms
step:340/1750 train_time:31868ms step_avg:93.73ms
step:341/1750 train_time:31962ms step_avg:93.73ms
step:342/1750 train_time:32056ms step_avg:93.73ms
step:343/1750 train_time:32150ms step_avg:93.73ms
step:344/1750 train_time:32245ms step_avg:93.73ms
step:345/1750 train_time:32340ms step_avg:93.74ms
step:346/1750 train_time:32434ms step_avg:93.74ms
step:347/1750 train_time:32529ms step_avg:93.74ms
step:348/1750 train_time:32623ms step_avg:93.74ms
step:349/1750 train_time:32717ms step_avg:93.75ms
step:350/1750 train_time:32811ms step_avg:93.75ms
step:351/1750 train_time:32905ms step_avg:93.75ms
step:352/1750 train_time:33000ms step_avg:93.75ms
step:353/1750 train_time:33094ms step_avg:93.75ms
step:354/1750 train_time:33188ms step_avg:93.75ms
step:355/1750 train_time:33282ms step_avg:93.75ms
step:356/1750 train_time:33376ms step_avg:93.75ms
step:357/1750 train_time:33470ms step_avg:93.75ms
step:358/1750 train_time:33564ms step_avg:93.76ms
step:359/1750 train_time:33659ms step_avg:93.76ms
step:360/1750 train_time:33753ms step_avg:93.76ms
step:361/1750 train_time:33848ms step_avg:93.76ms
step:362/1750 train_time:33943ms step_avg:93.76ms
step:363/1750 train_time:34037ms step_avg:93.77ms
step:364/1750 train_time:34130ms step_avg:93.76ms
step:365/1750 train_time:34225ms step_avg:93.77ms
step:366/1750 train_time:34319ms step_avg:93.77ms
step:367/1750 train_time:34413ms step_avg:93.77ms
step:368/1750 train_time:34507ms step_avg:93.77ms
step:369/1750 train_time:34602ms step_avg:93.77ms
step:370/1750 train_time:34696ms step_avg:93.77ms
step:371/1750 train_time:34790ms step_avg:93.77ms
step:372/1750 train_time:34884ms step_avg:93.77ms
step:373/1750 train_time:34979ms step_avg:93.78ms
step:374/1750 train_time:35073ms step_avg:93.78ms
step:375/1750 train_time:35167ms step_avg:93.78ms
step:375/1750 val_loss:3.9040 train_time:35251ms step_avg:94.00ms
step:376/1750 train_time:35274ms step_avg:93.81ms
step:377/1750 train_time:35362ms step_avg:93.80ms
step:378/1750 train_time:35462ms step_avg:93.81ms
step:379/1750 train_time:35557ms step_avg:93.82ms
step:380/1750 train_time:35650ms step_avg:93.82ms
step:381/1750 train_time:35744ms step_avg:93.82ms
step:382/1750 train_time:35837ms step_avg:93.81ms
step:383/1750 train_time:35930ms step_avg:93.81ms
step:384/1750 train_time:36024ms step_avg:93.81ms
step:385/1750 train_time:36117ms step_avg:93.81ms
step:386/1750 train_time:36211ms step_avg:93.81ms
step:387/1750 train_time:36308ms step_avg:93.82ms
step:388/1750 train_time:36404ms step_avg:93.82ms
step:389/1750 train_time:36499ms step_avg:93.83ms
step:390/1750 train_time:36594ms step_avg:93.83ms
step:391/1750 train_time:36690ms step_avg:93.84ms
step:392/1750 train_time:36786ms step_avg:93.84ms
step:393/1750 train_time:36881ms step_avg:93.85ms
step:394/1750 train_time:36976ms step_avg:93.85ms
step:395/1750 train_time:37072ms step_avg:93.85ms
step:396/1750 train_time:37168ms step_avg:93.86ms
step:397/1750 train_time:37266ms step_avg:93.87ms
step:398/1750 train_time:37362ms step_avg:93.87ms
step:399/1750 train_time:37460ms step_avg:93.88ms
step:400/1750 train_time:37556ms step_avg:93.89ms
step:401/1750 train_time:37653ms step_avg:93.90ms
step:402/1750 train_time:37750ms step_avg:93.91ms
step:403/1750 train_time:37846ms step_avg:93.91ms
step:404/1750 train_time:37942ms step_avg:93.92ms
step:405/1750 train_time:38037ms step_avg:93.92ms
step:406/1750 train_time:38133ms step_avg:93.92ms
step:407/1750 train_time:38229ms step_avg:93.93ms
step:408/1750 train_time:38326ms step_avg:93.94ms
step:409/1750 train_time:38421ms step_avg:93.94ms
step:410/1750 train_time:38518ms step_avg:93.95ms
step:411/1750 train_time:38615ms step_avg:93.95ms
step:412/1750 train_time:38711ms step_avg:93.96ms
step:413/1750 train_time:38808ms step_avg:93.97ms
step:414/1750 train_time:38904ms step_avg:93.97ms
step:415/1750 train_time:39000ms step_avg:93.98ms
step:416/1750 train_time:39095ms step_avg:93.98ms
step:417/1750 train_time:39191ms step_avg:93.98ms
step:418/1750 train_time:39287ms step_avg:93.99ms
step:419/1750 train_time:39384ms step_avg:93.99ms
step:420/1750 train_time:39480ms step_avg:94.00ms
step:421/1750 train_time:39576ms step_avg:94.01ms
step:422/1750 train_time:39673ms step_avg:94.01ms
step:423/1750 train_time:39770ms step_avg:94.02ms
step:424/1750 train_time:39866ms step_avg:94.02ms
step:425/1750 train_time:39962ms step_avg:94.03ms
step:426/1750 train_time:40058ms step_avg:94.03ms
step:427/1750 train_time:40154ms step_avg:94.04ms
step:428/1750 train_time:40249ms step_avg:94.04ms
step:429/1750 train_time:40347ms step_avg:94.05ms
step:430/1750 train_time:40444ms step_avg:94.05ms
step:431/1750 train_time:40541ms step_avg:94.06ms
step:432/1750 train_time:40636ms step_avg:94.07ms
step:433/1750 train_time:40733ms step_avg:94.07ms
step:434/1750 train_time:40829ms step_avg:94.08ms
step:435/1750 train_time:40925ms step_avg:94.08ms
step:436/1750 train_time:41021ms step_avg:94.09ms
step:437/1750 train_time:41117ms step_avg:94.09ms
step:438/1750 train_time:41213ms step_avg:94.09ms
step:439/1750 train_time:41309ms step_avg:94.10ms
step:440/1750 train_time:41405ms step_avg:94.10ms
step:441/1750 train_time:41501ms step_avg:94.11ms
step:442/1750 train_time:41597ms step_avg:94.11ms
step:443/1750 train_time:41693ms step_avg:94.12ms
step:444/1750 train_time:41790ms step_avg:94.12ms
step:445/1750 train_time:41886ms step_avg:94.12ms
step:446/1750 train_time:41981ms step_avg:94.13ms
step:447/1750 train_time:42076ms step_avg:94.13ms
step:448/1750 train_time:42173ms step_avg:94.14ms
step:449/1750 train_time:42269ms step_avg:94.14ms
step:450/1750 train_time:42367ms step_avg:94.15ms
step:451/1750 train_time:42463ms step_avg:94.15ms
step:452/1750 train_time:42558ms step_avg:94.16ms
step:453/1750 train_time:42654ms step_avg:94.16ms
step:454/1750 train_time:42751ms step_avg:94.16ms
step:455/1750 train_time:42847ms step_avg:94.17ms
step:456/1750 train_time:42943ms step_avg:94.17ms
step:457/1750 train_time:43039ms step_avg:94.18ms
step:458/1750 train_time:43135ms step_avg:94.18ms
step:459/1750 train_time:43231ms step_avg:94.19ms
step:460/1750 train_time:43327ms step_avg:94.19ms
step:461/1750 train_time:43423ms step_avg:94.19ms
step:462/1750 train_time:43519ms step_avg:94.20ms
step:463/1750 train_time:43615ms step_avg:94.20ms
step:464/1750 train_time:43711ms step_avg:94.20ms
step:465/1750 train_time:43808ms step_avg:94.21ms
step:466/1750 train_time:43905ms step_avg:94.22ms
step:467/1750 train_time:44001ms step_avg:94.22ms
step:468/1750 train_time:44096ms step_avg:94.22ms
step:469/1750 train_time:44193ms step_avg:94.23ms
step:470/1750 train_time:44289ms step_avg:94.23ms
step:471/1750 train_time:44385ms step_avg:94.24ms
step:472/1750 train_time:44482ms step_avg:94.24ms
step:473/1750 train_time:44578ms step_avg:94.25ms
step:474/1750 train_time:44674ms step_avg:94.25ms
step:475/1750 train_time:44770ms step_avg:94.25ms
step:476/1750 train_time:44866ms step_avg:94.26ms
step:477/1750 train_time:44963ms step_avg:94.26ms
step:478/1750 train_time:45059ms step_avg:94.27ms
step:479/1750 train_time:45155ms step_avg:94.27ms
step:480/1750 train_time:45251ms step_avg:94.27ms
step:481/1750 train_time:45348ms step_avg:94.28ms
step:482/1750 train_time:45444ms step_avg:94.28ms
step:483/1750 train_time:45539ms step_avg:94.28ms
step:484/1750 train_time:45635ms step_avg:94.29ms
step:485/1750 train_time:45731ms step_avg:94.29ms
step:486/1750 train_time:45828ms step_avg:94.30ms
step:487/1750 train_time:45923ms step_avg:94.30ms
step:488/1750 train_time:46020ms step_avg:94.30ms
step:489/1750 train_time:46117ms step_avg:94.31ms
step:490/1750 train_time:46213ms step_avg:94.31ms
step:491/1750 train_time:46309ms step_avg:94.32ms
step:492/1750 train_time:46405ms step_avg:94.32ms
step:493/1750 train_time:46500ms step_avg:94.32ms
step:494/1750 train_time:46596ms step_avg:94.32ms
step:495/1750 train_time:46691ms step_avg:94.33ms
step:496/1750 train_time:46788ms step_avg:94.33ms
step:497/1750 train_time:46885ms step_avg:94.34ms
step:498/1750 train_time:46981ms step_avg:94.34ms
step:499/1750 train_time:47078ms step_avg:94.34ms
step:500/1750 train_time:47173ms step_avg:94.35ms
step:500/1750 val_loss:3.7554 train_time:47259ms step_avg:94.52ms
step:501/1750 train_time:47281ms step_avg:94.37ms
step:502/1750 train_time:47378ms step_avg:94.38ms
step:503/1750 train_time:47477ms step_avg:94.39ms
step:504/1750 train_time:47573ms step_avg:94.39ms
step:505/1750 train_time:47668ms step_avg:94.39ms
step:506/1750 train_time:47763ms step_avg:94.39ms
step:507/1750 train_time:47858ms step_avg:94.39ms
step:508/1750 train_time:47953ms step_avg:94.40ms
step:509/1750 train_time:48049ms step_avg:94.40ms
step:510/1750 train_time:48144ms step_avg:94.40ms
step:511/1750 train_time:48241ms step_avg:94.40ms
step:512/1750 train_time:48339ms step_avg:94.41ms
step:513/1750 train_time:48438ms step_avg:94.42ms
step:514/1750 train_time:48535ms step_avg:94.43ms
step:515/1750 train_time:48631ms step_avg:94.43ms
step:516/1750 train_time:48727ms step_avg:94.43ms
step:517/1750 train_time:48822ms step_avg:94.43ms
step:518/1750 train_time:48917ms step_avg:94.43ms
step:519/1750 train_time:49012ms step_avg:94.44ms
step:520/1750 train_time:49108ms step_avg:94.44ms
step:521/1750 train_time:49204ms step_avg:94.44ms
step:522/1750 train_time:49300ms step_avg:94.44ms
step:523/1750 train_time:49397ms step_avg:94.45ms
step:524/1750 train_time:49495ms step_avg:94.46ms
step:525/1750 train_time:49592ms step_avg:94.46ms
step:526/1750 train_time:49688ms step_avg:94.46ms
step:527/1750 train_time:49785ms step_avg:94.47ms
step:528/1750 train_time:49881ms step_avg:94.47ms
step:529/1750 train_time:49978ms step_avg:94.48ms
step:530/1750 train_time:50073ms step_avg:94.48ms
step:531/1750 train_time:50169ms step_avg:94.48ms
step:532/1750 train_time:50266ms step_avg:94.48ms
step:533/1750 train_time:50363ms step_avg:94.49ms
step:534/1750 train_time:50460ms step_avg:94.49ms
step:535/1750 train_time:50557ms step_avg:94.50ms
step:536/1750 train_time:50655ms step_avg:94.50ms
step:537/1750 train_time:50751ms step_avg:94.51ms
step:538/1750 train_time:50848ms step_avg:94.51ms
step:539/1750 train_time:50944ms step_avg:94.51ms
step:540/1750 train_time:51040ms step_avg:94.52ms
step:541/1750 train_time:51136ms step_avg:94.52ms
step:542/1750 train_time:51232ms step_avg:94.52ms
step:543/1750 train_time:51328ms step_avg:94.53ms
step:544/1750 train_time:51424ms step_avg:94.53ms
step:545/1750 train_time:51521ms step_avg:94.53ms
step:546/1750 train_time:51619ms step_avg:94.54ms
step:547/1750 train_time:51716ms step_avg:94.55ms
step:548/1750 train_time:51813ms step_avg:94.55ms
step:549/1750 train_time:51909ms step_avg:94.55ms
step:550/1750 train_time:52005ms step_avg:94.55ms
step:551/1750 train_time:52102ms step_avg:94.56ms
step:552/1750 train_time:52198ms step_avg:94.56ms
step:553/1750 train_time:52294ms step_avg:94.57ms
step:554/1750 train_time:52391ms step_avg:94.57ms
step:555/1750 train_time:52487ms step_avg:94.57ms
step:556/1750 train_time:52584ms step_avg:94.58ms
step:557/1750 train_time:52681ms step_avg:94.58ms
step:558/1750 train_time:52779ms step_avg:94.59ms
step:559/1750 train_time:52875ms step_avg:94.59ms
step:560/1750 train_time:52970ms step_avg:94.59ms
step:561/1750 train_time:53068ms step_avg:94.59ms
step:562/1750 train_time:53165ms step_avg:94.60ms
step:563/1750 train_time:53262ms step_avg:94.60ms
step:564/1750 train_time:53358ms step_avg:94.61ms
step:565/1750 train_time:53454ms step_avg:94.61ms
step:566/1750 train_time:53550ms step_avg:94.61ms
step:567/1750 train_time:53648ms step_avg:94.62ms
step:568/1750 train_time:53745ms step_avg:94.62ms
step:569/1750 train_time:53842ms step_avg:94.63ms
step:570/1750 train_time:53939ms step_avg:94.63ms
step:571/1750 train_time:54035ms step_avg:94.63ms
step:572/1750 train_time:54131ms step_avg:94.63ms
step:573/1750 train_time:54228ms step_avg:94.64ms
step:574/1750 train_time:54324ms step_avg:94.64ms
step:575/1750 train_time:54421ms step_avg:94.65ms
step:576/1750 train_time:54518ms step_avg:94.65ms
step:577/1750 train_time:54614ms step_avg:94.65ms
step:578/1750 train_time:54710ms step_avg:94.65ms
step:579/1750 train_time:54807ms step_avg:94.66ms
step:580/1750 train_time:54903ms step_avg:94.66ms
step:581/1750 train_time:55000ms step_avg:94.66ms
step:582/1750 train_time:55098ms step_avg:94.67ms
step:583/1750 train_time:55194ms step_avg:94.67ms
step:584/1750 train_time:55290ms step_avg:94.67ms
step:585/1750 train_time:55387ms step_avg:94.68ms
step:586/1750 train_time:55483ms step_avg:94.68ms
step:587/1750 train_time:55581ms step_avg:94.69ms
step:588/1750 train_time:55677ms step_avg:94.69ms
step:589/1750 train_time:55774ms step_avg:94.69ms
step:590/1750 train_time:55870ms step_avg:94.69ms
step:591/1750 train_time:55967ms step_avg:94.70ms
step:592/1750 train_time:56063ms step_avg:94.70ms
step:593/1750 train_time:56160ms step_avg:94.70ms
step:594/1750 train_time:56256ms step_avg:94.71ms
step:595/1750 train_time:56352ms step_avg:94.71ms
step:596/1750 train_time:56449ms step_avg:94.71ms
step:597/1750 train_time:56545ms step_avg:94.72ms
step:598/1750 train_time:56642ms step_avg:94.72ms
step:599/1750 train_time:56739ms step_avg:94.72ms
step:600/1750 train_time:56834ms step_avg:94.72ms
step:601/1750 train_time:56931ms step_avg:94.73ms
step:602/1750 train_time:57028ms step_avg:94.73ms
step:603/1750 train_time:57124ms step_avg:94.73ms
step:604/1750 train_time:57221ms step_avg:94.74ms
step:605/1750 train_time:57318ms step_avg:94.74ms
step:606/1750 train_time:57414ms step_avg:94.74ms
step:607/1750 train_time:57510ms step_avg:94.74ms
step:608/1750 train_time:57607ms step_avg:94.75ms
step:609/1750 train_time:57703ms step_avg:94.75ms
step:610/1750 train_time:57800ms step_avg:94.75ms
step:611/1750 train_time:57897ms step_avg:94.76ms
step:612/1750 train_time:57994ms step_avg:94.76ms
step:613/1750 train_time:58090ms step_avg:94.76ms
step:614/1750 train_time:58186ms step_avg:94.77ms
step:615/1750 train_time:58283ms step_avg:94.77ms
step:616/1750 train_time:58380ms step_avg:94.77ms
step:617/1750 train_time:58477ms step_avg:94.78ms
step:618/1750 train_time:58573ms step_avg:94.78ms
step:619/1750 train_time:58669ms step_avg:94.78ms
step:620/1750 train_time:58765ms step_avg:94.78ms
step:621/1750 train_time:58862ms step_avg:94.79ms
step:622/1750 train_time:58959ms step_avg:94.79ms
step:623/1750 train_time:59055ms step_avg:94.79ms
step:624/1750 train_time:59151ms step_avg:94.79ms
step:625/1750 train_time:59248ms step_avg:94.80ms
step:625/1750 val_loss:3.6684 train_time:59334ms step_avg:94.93ms
step:626/1750 train_time:59356ms step_avg:94.82ms
step:627/1750 train_time:59450ms step_avg:94.82ms
step:628/1750 train_time:59548ms step_avg:94.82ms
step:629/1750 train_time:59645ms step_avg:94.83ms
step:630/1750 train_time:59742ms step_avg:94.83ms
step:631/1750 train_time:59837ms step_avg:94.83ms
step:632/1750 train_time:59932ms step_avg:94.83ms
step:633/1750 train_time:60028ms step_avg:94.83ms
step:634/1750 train_time:60124ms step_avg:94.83ms
step:635/1750 train_time:60219ms step_avg:94.83ms
step:636/1750 train_time:60317ms step_avg:94.84ms
step:637/1750 train_time:60415ms step_avg:94.84ms
step:638/1750 train_time:60512ms step_avg:94.85ms
step:639/1750 train_time:60609ms step_avg:94.85ms
step:640/1750 train_time:60705ms step_avg:94.85ms
step:641/1750 train_time:60801ms step_avg:94.85ms
step:642/1750 train_time:60897ms step_avg:94.86ms
step:643/1750 train_time:60993ms step_avg:94.86ms
step:644/1750 train_time:61089ms step_avg:94.86ms
step:645/1750 train_time:61185ms step_avg:94.86ms
step:646/1750 train_time:61281ms step_avg:94.86ms
step:647/1750 train_time:61379ms step_avg:94.87ms
step:648/1750 train_time:61476ms step_avg:94.87ms
step:649/1750 train_time:61573ms step_avg:94.87ms
step:650/1750 train_time:61670ms step_avg:94.88ms
step:651/1750 train_time:61767ms step_avg:94.88ms
step:652/1750 train_time:61865ms step_avg:94.89ms
step:653/1750 train_time:61963ms step_avg:94.89ms
step:654/1750 train_time:62060ms step_avg:94.89ms
step:655/1750 train_time:62159ms step_avg:94.90ms
step:656/1750 train_time:62257ms step_avg:94.90ms
step:657/1750 train_time:62355ms step_avg:94.91ms
step:658/1750 train_time:62453ms step_avg:94.91ms
step:659/1750 train_time:62551ms step_avg:94.92ms
step:660/1750 train_time:62649ms step_avg:94.92ms
step:661/1750 train_time:62747ms step_avg:94.93ms
step:662/1750 train_time:62845ms step_avg:94.93ms
step:663/1750 train_time:62943ms step_avg:94.94ms
step:664/1750 train_time:63040ms step_avg:94.94ms
step:665/1750 train_time:63138ms step_avg:94.94ms
step:666/1750 train_time:63236ms step_avg:94.95ms
step:667/1750 train_time:63334ms step_avg:94.95ms
step:668/1750 train_time:63432ms step_avg:94.96ms
step:669/1750 train_time:63530ms step_avg:94.96ms
step:670/1750 train_time:63629ms step_avg:94.97ms
step:671/1750 train_time:63727ms step_avg:94.97ms
step:672/1750 train_time:63825ms step_avg:94.98ms
step:673/1750 train_time:63922ms step_avg:94.98ms
step:674/1750 train_time:64020ms step_avg:94.99ms
step:675/1750 train_time:64119ms step_avg:94.99ms
step:676/1750 train_time:64217ms step_avg:95.00ms
step:677/1750 train_time:64314ms step_avg:95.00ms
step:678/1750 train_time:64412ms step_avg:95.00ms
step:679/1750 train_time:64509ms step_avg:95.01ms
step:680/1750 train_time:64608ms step_avg:95.01ms
step:681/1750 train_time:64706ms step_avg:95.02ms
step:682/1750 train_time:64803ms step_avg:95.02ms
step:683/1750 train_time:64900ms step_avg:95.02ms
step:684/1750 train_time:64998ms step_avg:95.03ms
step:685/1750 train_time:65096ms step_avg:95.03ms
step:686/1750 train_time:65194ms step_avg:95.03ms
step:687/1750 train_time:65291ms step_avg:95.04ms
step:688/1750 train_time:65389ms step_avg:95.04ms
step:689/1750 train_time:65487ms step_avg:95.05ms
step:690/1750 train_time:65585ms step_avg:95.05ms
step:691/1750 train_time:65683ms step_avg:95.05ms
step:692/1750 train_time:65780ms step_avg:95.06ms
step:693/1750 train_time:65878ms step_avg:95.06ms
step:694/1750 train_time:65976ms step_avg:95.07ms
step:695/1750 train_time:66074ms step_avg:95.07ms
step:696/1750 train_time:66171ms step_avg:95.07ms
step:697/1750 train_time:66269ms step_avg:95.08ms
step:698/1750 train_time:66368ms step_avg:95.08ms
step:699/1750 train_time:66465ms step_avg:95.09ms
step:700/1750 train_time:66563ms step_avg:95.09ms
step:701/1750 train_time:66661ms step_avg:95.09ms
step:702/1750 train_time:66760ms step_avg:95.10ms
step:703/1750 train_time:66859ms step_avg:95.10ms
step:704/1750 train_time:66956ms step_avg:95.11ms
step:705/1750 train_time:67054ms step_avg:95.11ms
step:706/1750 train_time:67151ms step_avg:95.11ms
step:707/1750 train_time:67249ms step_avg:95.12ms
step:708/1750 train_time:67347ms step_avg:95.12ms
step:709/1750 train_time:67446ms step_avg:95.13ms
step:710/1750 train_time:67543ms step_avg:95.13ms
step:711/1750 train_time:67641ms step_avg:95.13ms
step:712/1750 train_time:67739ms step_avg:95.14ms
step:713/1750 train_time:67836ms step_avg:95.14ms
step:714/1750 train_time:67934ms step_avg:95.15ms
step:715/1750 train_time:68032ms step_avg:95.15ms
step:716/1750 train_time:68129ms step_avg:95.15ms
step:717/1750 train_time:68227ms step_avg:95.16ms
step:718/1750 train_time:68326ms step_avg:95.16ms
step:719/1750 train_time:68423ms step_avg:95.16ms
step:720/1750 train_time:68522ms step_avg:95.17ms
step:721/1750 train_time:68620ms step_avg:95.17ms
step:722/1750 train_time:68719ms step_avg:95.18ms
step:723/1750 train_time:68816ms step_avg:95.18ms
step:724/1750 train_time:68914ms step_avg:95.18ms
step:725/1750 train_time:69012ms step_avg:95.19ms
step:726/1750 train_time:69109ms step_avg:95.19ms
step:727/1750 train_time:69207ms step_avg:95.19ms
step:728/1750 train_time:69305ms step_avg:95.20ms
step:729/1750 train_time:69402ms step_avg:95.20ms
step:730/1750 train_time:69500ms step_avg:95.21ms
step:731/1750 train_time:69598ms step_avg:95.21ms
step:732/1750 train_time:69696ms step_avg:95.21ms
step:733/1750 train_time:69793ms step_avg:95.22ms
step:734/1750 train_time:69891ms step_avg:95.22ms
step:735/1750 train_time:69989ms step_avg:95.22ms
step:736/1750 train_time:70087ms step_avg:95.23ms
step:737/1750 train_time:70184ms step_avg:95.23ms
step:738/1750 train_time:70282ms step_avg:95.23ms
step:739/1750 train_time:70380ms step_avg:95.24ms
step:740/1750 train_time:70479ms step_avg:95.24ms
step:741/1750 train_time:70576ms step_avg:95.24ms
step:742/1750 train_time:70673ms step_avg:95.25ms
step:743/1750 train_time:70772ms step_avg:95.25ms
step:744/1750 train_time:70869ms step_avg:95.25ms
step:745/1750 train_time:70967ms step_avg:95.26ms
step:746/1750 train_time:71065ms step_avg:95.26ms
step:747/1750 train_time:71163ms step_avg:95.27ms
step:748/1750 train_time:71261ms step_avg:95.27ms
step:749/1750 train_time:71359ms step_avg:95.27ms
step:750/1750 train_time:71458ms step_avg:95.28ms
step:750/1750 val_loss:3.6039 train_time:71545ms step_avg:95.39ms
step:751/1750 train_time:71566ms step_avg:95.29ms
step:752/1750 train_time:71660ms step_avg:95.29ms
step:753/1750 train_time:71760ms step_avg:95.30ms
step:754/1750 train_time:71857ms step_avg:95.30ms
step:755/1750 train_time:71954ms step_avg:95.30ms
step:756/1750 train_time:72051ms step_avg:95.31ms
step:757/1750 train_time:72149ms step_avg:95.31ms
step:758/1750 train_time:72246ms step_avg:95.31ms
step:759/1750 train_time:72343ms step_avg:95.31ms
step:760/1750 train_time:72441ms step_avg:95.32ms
step:761/1750 train_time:72541ms step_avg:95.32ms
step:762/1750 train_time:72640ms step_avg:95.33ms
step:763/1750 train_time:72739ms step_avg:95.33ms
step:764/1750 train_time:72837ms step_avg:95.34ms
step:765/1750 train_time:72934ms step_avg:95.34ms
step:766/1750 train_time:73031ms step_avg:95.34ms
step:767/1750 train_time:73129ms step_avg:95.34ms
step:768/1750 train_time:73225ms step_avg:95.35ms
step:769/1750 train_time:73323ms step_avg:95.35ms
step:770/1750 train_time:73420ms step_avg:95.35ms
step:771/1750 train_time:73519ms step_avg:95.36ms
step:772/1750 train_time:73619ms step_avg:95.36ms
step:773/1750 train_time:73718ms step_avg:95.37ms
step:774/1750 train_time:73816ms step_avg:95.37ms
step:775/1750 train_time:73914ms step_avg:95.37ms
step:776/1750 train_time:74011ms step_avg:95.37ms
step:777/1750 train_time:74109ms step_avg:95.38ms
step:778/1750 train_time:74206ms step_avg:95.38ms
step:779/1750 train_time:74303ms step_avg:95.38ms
step:780/1750 train_time:74401ms step_avg:95.39ms
step:781/1750 train_time:74500ms step_avg:95.39ms
step:782/1750 train_time:74599ms step_avg:95.40ms
step:783/1750 train_time:74699ms step_avg:95.40ms
step:784/1750 train_time:74797ms step_avg:95.40ms
step:785/1750 train_time:74896ms step_avg:95.41ms
step:786/1750 train_time:74995ms step_avg:95.41ms
step:787/1750 train_time:75094ms step_avg:95.42ms
step:788/1750 train_time:75193ms step_avg:95.42ms
step:789/1750 train_time:75290ms step_avg:95.42ms
step:790/1750 train_time:75387ms step_avg:95.43ms
step:791/1750 train_time:75485ms step_avg:95.43ms
step:792/1750 train_time:75584ms step_avg:95.43ms
step:793/1750 train_time:75684ms step_avg:95.44ms
step:794/1750 train_time:75783ms step_avg:95.44ms
step:795/1750 train_time:75881ms step_avg:95.45ms
step:796/1750 train_time:75980ms step_avg:95.45ms
step:797/1750 train_time:76079ms step_avg:95.46ms
step:798/1750 train_time:76177ms step_avg:95.46ms
step:799/1750 train_time:76274ms step_avg:95.46ms
step:800/1750 train_time:76373ms step_avg:95.47ms
step:801/1750 train_time:76470ms step_avg:95.47ms
step:802/1750 train_time:76568ms step_avg:95.47ms
step:803/1750 train_time:76665ms step_avg:95.47ms
step:804/1750 train_time:76765ms step_avg:95.48ms
step:805/1750 train_time:76864ms step_avg:95.48ms
step:806/1750 train_time:76963ms step_avg:95.49ms
step:807/1750 train_time:77061ms step_avg:95.49ms
step:808/1750 train_time:77161ms step_avg:95.50ms
step:809/1750 train_time:77260ms step_avg:95.50ms
step:810/1750 train_time:77360ms step_avg:95.51ms
step:811/1750 train_time:77459ms step_avg:95.51ms
step:812/1750 train_time:77557ms step_avg:95.51ms
step:813/1750 train_time:77656ms step_avg:95.52ms
step:814/1750 train_time:77754ms step_avg:95.52ms
step:815/1750 train_time:77852ms step_avg:95.52ms
step:816/1750 train_time:77951ms step_avg:95.53ms
step:817/1750 train_time:78048ms step_avg:95.53ms
step:818/1750 train_time:78146ms step_avg:95.53ms
step:819/1750 train_time:78245ms step_avg:95.54ms
step:820/1750 train_time:78343ms step_avg:95.54ms
step:821/1750 train_time:78442ms step_avg:95.54ms
step:822/1750 train_time:78539ms step_avg:95.55ms
step:823/1750 train_time:78637ms step_avg:95.55ms
step:824/1750 train_time:78736ms step_avg:95.55ms
step:825/1750 train_time:78835ms step_avg:95.56ms
step:826/1750 train_time:78933ms step_avg:95.56ms
step:827/1750 train_time:79030ms step_avg:95.56ms
step:828/1750 train_time:79128ms step_avg:95.56ms
step:829/1750 train_time:79226ms step_avg:95.57ms
step:830/1750 train_time:79324ms step_avg:95.57ms
step:831/1750 train_time:79422ms step_avg:95.57ms
step:832/1750 train_time:79520ms step_avg:95.58ms
step:833/1750 train_time:79619ms step_avg:95.58ms
step:834/1750 train_time:79717ms step_avg:95.58ms
step:835/1750 train_time:79815ms step_avg:95.59ms
step:836/1750 train_time:79913ms step_avg:95.59ms
step:837/1750 train_time:80011ms step_avg:95.59ms
step:838/1750 train_time:80110ms step_avg:95.60ms
step:839/1750 train_time:80208ms step_avg:95.60ms
step:840/1750 train_time:80305ms step_avg:95.60ms
step:841/1750 train_time:80403ms step_avg:95.60ms
step:842/1750 train_time:80501ms step_avg:95.61ms
step:843/1750 train_time:80600ms step_avg:95.61ms
step:844/1750 train_time:80699ms step_avg:95.61ms
step:845/1750 train_time:80797ms step_avg:95.62ms
step:846/1750 train_time:80895ms step_avg:95.62ms
step:847/1750 train_time:80994ms step_avg:95.62ms
step:848/1750 train_time:81093ms step_avg:95.63ms
step:849/1750 train_time:81191ms step_avg:95.63ms
step:850/1750 train_time:81289ms step_avg:95.63ms
step:851/1750 train_time:81387ms step_avg:95.64ms
step:852/1750 train_time:81484ms step_avg:95.64ms
step:853/1750 train_time:81582ms step_avg:95.64ms
step:854/1750 train_time:81681ms step_avg:95.65ms
step:855/1750 train_time:81779ms step_avg:95.65ms
step:856/1750 train_time:81877ms step_avg:95.65ms
step:857/1750 train_time:81976ms step_avg:95.65ms
step:858/1750 train_time:82074ms step_avg:95.66ms
step:859/1750 train_time:82172ms step_avg:95.66ms
step:860/1750 train_time:82270ms step_avg:95.66ms
step:861/1750 train_time:82367ms step_avg:95.66ms
step:862/1750 train_time:82466ms step_avg:95.67ms
step:863/1750 train_time:82563ms step_avg:95.67ms
step:864/1750 train_time:82661ms step_avg:95.67ms
step:865/1750 train_time:82759ms step_avg:95.68ms
step:866/1750 train_time:82858ms step_avg:95.68ms
step:867/1750 train_time:82956ms step_avg:95.68ms
step:868/1750 train_time:83054ms step_avg:95.68ms
step:869/1750 train_time:83151ms step_avg:95.69ms
step:870/1750 train_time:83250ms step_avg:95.69ms
step:871/1750 train_time:83348ms step_avg:95.69ms
step:872/1750 train_time:83447ms step_avg:95.70ms
step:873/1750 train_time:83545ms step_avg:95.70ms
step:874/1750 train_time:83643ms step_avg:95.70ms
step:875/1750 train_time:83741ms step_avg:95.70ms
step:875/1750 val_loss:3.5541 train_time:83828ms step_avg:95.80ms
step:876/1750 train_time:83850ms step_avg:95.72ms
step:877/1750 train_time:83947ms step_avg:95.72ms
step:878/1750 train_time:84048ms step_avg:95.73ms
step:879/1750 train_time:84146ms step_avg:95.73ms
step:880/1750 train_time:84243ms step_avg:95.73ms
step:881/1750 train_time:84341ms step_avg:95.73ms
step:882/1750 train_time:84438ms step_avg:95.74ms
step:883/1750 train_time:84536ms step_avg:95.74ms
step:884/1750 train_time:84633ms step_avg:95.74ms
step:885/1750 train_time:84730ms step_avg:95.74ms
step:886/1750 train_time:84829ms step_avg:95.74ms
step:887/1750 train_time:84929ms step_avg:95.75ms
step:888/1750 train_time:85028ms step_avg:95.75ms
step:889/1750 train_time:85127ms step_avg:95.76ms
step:890/1750 train_time:85226ms step_avg:95.76ms
step:891/1750 train_time:85324ms step_avg:95.76ms
step:892/1750 train_time:85422ms step_avg:95.76ms
step:893/1750 train_time:85520ms step_avg:95.77ms
step:894/1750 train_time:85617ms step_avg:95.77ms
step:895/1750 train_time:85714ms step_avg:95.77ms
step:896/1750 train_time:85813ms step_avg:95.77ms
step:897/1750 train_time:85912ms step_avg:95.78ms
step:898/1750 train_time:86011ms step_avg:95.78ms
step:899/1750 train_time:86110ms step_avg:95.78ms
step:900/1750 train_time:86209ms step_avg:95.79ms
step:901/1750 train_time:86307ms step_avg:95.79ms
step:902/1750 train_time:86406ms step_avg:95.79ms
step:903/1750 train_time:86504ms step_avg:95.80ms
step:904/1750 train_time:86603ms step_avg:95.80ms
step:905/1750 train_time:86701ms step_avg:95.80ms
step:906/1750 train_time:86799ms step_avg:95.80ms
step:907/1750 train_time:86897ms step_avg:95.81ms
step:908/1750 train_time:86995ms step_avg:95.81ms
step:909/1750 train_time:87094ms step_avg:95.81ms
step:910/1750 train_time:87193ms step_avg:95.82ms
step:911/1750 train_time:87293ms step_avg:95.82ms
step:912/1750 train_time:87394ms step_avg:95.83ms
step:913/1750 train_time:87494ms step_avg:95.83ms
step:914/1750 train_time:87594ms step_avg:95.84ms
step:915/1750 train_time:87694ms step_avg:95.84ms
step:916/1750 train_time:87794ms step_avg:95.84ms
step:917/1750 train_time:87894ms step_avg:95.85ms
step:918/1750 train_time:87994ms step_avg:95.85ms
step:919/1750 train_time:88094ms step_avg:95.86ms
step:920/1750 train_time:88194ms step_avg:95.86ms
step:921/1750 train_time:88294ms step_avg:95.87ms
step:922/1750 train_time:88394ms step_avg:95.87ms
step:923/1750 train_time:88494ms step_avg:95.88ms
step:924/1750 train_time:88594ms step_avg:95.88ms
step:925/1750 train_time:88693ms step_avg:95.88ms
step:926/1750 train_time:88793ms step_avg:95.89ms
step:927/1750 train_time:88893ms step_avg:95.89ms
step:928/1750 train_time:88992ms step_avg:95.90ms
step:929/1750 train_time:89091ms step_avg:95.90ms
step:930/1750 train_time:89191ms step_avg:95.90ms
step:931/1750 train_time:89291ms step_avg:95.91ms
step:932/1750 train_time:89391ms step_avg:95.91ms
step:933/1750 train_time:89491ms step_avg:95.92ms
step:934/1750 train_time:89590ms step_avg:95.92ms
step:935/1750 train_time:89691ms step_avg:95.93ms
step:936/1750 train_time:89791ms step_avg:95.93ms
step:937/1750 train_time:89891ms step_avg:95.93ms
step:938/1750 train_time:89991ms step_avg:95.94ms
step:939/1750 train_time:90091ms step_avg:95.94ms
step:940/1750 train_time:90190ms step_avg:95.95ms
step:941/1750 train_time:90292ms step_avg:95.95ms
step:942/1750 train_time:90391ms step_avg:95.96ms
step:943/1750 train_time:90491ms step_avg:95.96ms
step:944/1750 train_time:90591ms step_avg:95.97ms
step:945/1750 train_time:90692ms step_avg:95.97ms
step:946/1750 train_time:90792ms step_avg:95.97ms
step:947/1750 train_time:90892ms step_avg:95.98ms
step:948/1750 train_time:90992ms step_avg:95.98ms
step:949/1750 train_time:91092ms step_avg:95.99ms
step:950/1750 train_time:91192ms step_avg:95.99ms
step:951/1750 train_time:91292ms step_avg:96.00ms
step:952/1750 train_time:91392ms step_avg:96.00ms
step:953/1750 train_time:91492ms step_avg:96.00ms
step:954/1750 train_time:91592ms step_avg:96.01ms
step:955/1750 train_time:91692ms step_avg:96.01ms
step:956/1750 train_time:91791ms step_avg:96.02ms
step:957/1750 train_time:91892ms step_avg:96.02ms
step:958/1750 train_time:91991ms step_avg:96.02ms
step:959/1750 train_time:92092ms step_avg:96.03ms
step:960/1750 train_time:92192ms step_avg:96.03ms
step:961/1750 train_time:92292ms step_avg:96.04ms
step:962/1750 train_time:92391ms step_avg:96.04ms
step:963/1750 train_time:92491ms step_avg:96.04ms
step:964/1750 train_time:92591ms step_avg:96.05ms
step:965/1750 train_time:92690ms step_avg:96.05ms
step:966/1750 train_time:92789ms step_avg:96.05ms
step:967/1750 train_time:92889ms step_avg:96.06ms
step:968/1750 train_time:92990ms step_avg:96.06ms
step:969/1750 train_time:93090ms step_avg:96.07ms
step:970/1750 train_time:93190ms step_avg:96.07ms
step:971/1750 train_time:93290ms step_avg:96.08ms
step:972/1750 train_time:93389ms step_avg:96.08ms
step:973/1750 train_time:93489ms step_avg:96.08ms
step:974/1750 train_time:93588ms step_avg:96.09ms
step:975/1750 train_time:93687ms step_avg:96.09ms
step:976/1750 train_time:93788ms step_avg:96.09ms
step:977/1750 train_time:93887ms step_avg:96.10ms
step:978/1750 train_time:93987ms step_avg:96.10ms
step:979/1750 train_time:94087ms step_avg:96.10ms
step:980/1750 train_time:94186ms step_avg:96.11ms
step:981/1750 train_time:94286ms step_avg:96.11ms
step:982/1750 train_time:94385ms step_avg:96.12ms
step:983/1750 train_time:94484ms step_avg:96.12ms
step:984/1750 train_time:94584ms step_avg:96.12ms
step:985/1750 train_time:94683ms step_avg:96.12ms
step:986/1750 train_time:94782ms step_avg:96.13ms
step:987/1750 train_time:94882ms step_avg:96.13ms
step:988/1750 train_time:94982ms step_avg:96.14ms
step:989/1750 train_time:95082ms step_avg:96.14ms
step:990/1750 train_time:95181ms step_avg:96.14ms
step:991/1750 train_time:95281ms step_avg:96.15ms
step:992/1750 train_time:95382ms step_avg:96.15ms
step:993/1750 train_time:95480ms step_avg:96.15ms
step:994/1750 train_time:95579ms step_avg:96.16ms
step:995/1750 train_time:95677ms step_avg:96.16ms
step:996/1750 train_time:95776ms step_avg:96.16ms
step:997/1750 train_time:95875ms step_avg:96.16ms
step:998/1750 train_time:95975ms step_avg:96.17ms
step:999/1750 train_time:96074ms step_avg:96.17ms
step:1000/1750 train_time:96175ms step_avg:96.17ms
step:1000/1750 val_loss:3.5135 train_time:96264ms step_avg:96.26ms
step:1001/1750 train_time:96286ms step_avg:96.19ms
step:1002/1750 train_time:96382ms step_avg:96.19ms
step:1003/1750 train_time:96484ms step_avg:96.20ms
step:1004/1750 train_time:96584ms step_avg:96.20ms
step:1005/1750 train_time:96683ms step_avg:96.20ms
step:1006/1750 train_time:96782ms step_avg:96.20ms
step:1007/1750 train_time:96881ms step_avg:96.21ms
step:1008/1750 train_time:96980ms step_avg:96.21ms
step:1009/1750 train_time:97078ms step_avg:96.21ms
step:1010/1750 train_time:97177ms step_avg:96.22ms
step:1011/1750 train_time:97278ms step_avg:96.22ms
step:1012/1750 train_time:97381ms step_avg:96.23ms
step:1013/1750 train_time:97483ms step_avg:96.23ms
step:1014/1750 train_time:97584ms step_avg:96.24ms
step:1015/1750 train_time:97683ms step_avg:96.24ms
step:1016/1750 train_time:97783ms step_avg:96.24ms
step:1017/1750 train_time:97882ms step_avg:96.25ms
step:1018/1750 train_time:97980ms step_avg:96.25ms
step:1019/1750 train_time:98079ms step_avg:96.25ms
step:1020/1750 train_time:98179ms step_avg:96.25ms
step:1021/1750 train_time:98279ms step_avg:96.26ms
step:1022/1750 train_time:98380ms step_avg:96.26ms
step:1023/1750 train_time:98481ms step_avg:96.27ms
step:1024/1750 train_time:98584ms step_avg:96.27ms
step:1025/1750 train_time:98684ms step_avg:96.28ms
step:1026/1750 train_time:98785ms step_avg:96.28ms
step:1027/1750 train_time:98885ms step_avg:96.29ms
step:1028/1750 train_time:98984ms step_avg:96.29ms
step:1029/1750 train_time:99083ms step_avg:96.29ms
step:1030/1750 train_time:99183ms step_avg:96.29ms
step:1031/1750 train_time:99284ms step_avg:96.30ms
step:1032/1750 train_time:99383ms step_avg:96.30ms
step:1033/1750 train_time:99483ms step_avg:96.31ms
step:1034/1750 train_time:99583ms step_avg:96.31ms
step:1035/1750 train_time:99683ms step_avg:96.31ms
step:1036/1750 train_time:99782ms step_avg:96.31ms
step:1037/1750 train_time:99882ms step_avg:96.32ms
step:1038/1750 train_time:99982ms step_avg:96.32ms
step:1039/1750 train_time:100081ms step_avg:96.32ms
step:1040/1750 train_time:100181ms step_avg:96.33ms
step:1041/1750 train_time:100281ms step_avg:96.33ms
step:1042/1750 train_time:100380ms step_avg:96.33ms
step:1043/1750 train_time:100480ms step_avg:96.34ms
step:1044/1750 train_time:100581ms step_avg:96.34ms
step:1045/1750 train_time:100681ms step_avg:96.35ms
step:1046/1750 train_time:100783ms step_avg:96.35ms
step:1047/1750 train_time:100882ms step_avg:96.35ms
step:1048/1750 train_time:100982ms step_avg:96.36ms
step:1049/1750 train_time:101082ms step_avg:96.36ms
step:1050/1750 train_time:101182ms step_avg:96.36ms
step:1051/1750 train_time:101282ms step_avg:96.37ms
step:1052/1750 train_time:101382ms step_avg:96.37ms
step:1053/1750 train_time:101482ms step_avg:96.37ms
step:1054/1750 train_time:101582ms step_avg:96.38ms
step:1055/1750 train_time:101681ms step_avg:96.38ms
step:1056/1750 train_time:101781ms step_avg:96.38ms
step:1057/1750 train_time:101881ms step_avg:96.39ms
step:1058/1750 train_time:101981ms step_avg:96.39ms
step:1059/1750 train_time:102081ms step_avg:96.39ms
step:1060/1750 train_time:102181ms step_avg:96.40ms
step:1061/1750 train_time:102280ms step_avg:96.40ms
step:1062/1750 train_time:102638ms step_avg:96.65ms
step:1063/1750 train_time:102778ms step_avg:96.69ms
step:1064/1750 train_time:102878ms step_avg:96.69ms
step:1065/1750 train_time:102976ms step_avg:96.69ms
step:1066/1750 train_time:103073ms step_avg:96.69ms
step:1067/1750 train_time:103172ms step_avg:96.69ms
step:1068/1750 train_time:103270ms step_avg:96.69ms
step:1069/1750 train_time:103626ms step_avg:96.94ms
step:1070/1750 train_time:103724ms step_avg:96.94ms
step:1071/1750 train_time:103822ms step_avg:96.94ms
step:1072/1750 train_time:103921ms step_avg:96.94ms
step:1073/1750 train_time:104019ms step_avg:96.94ms
step:1074/1750 train_time:104117ms step_avg:96.94ms
step:1075/1750 train_time:104215ms step_avg:96.94ms
step:1076/1750 train_time:104313ms step_avg:96.94ms
step:1077/1750 train_time:104697ms step_avg:97.21ms
step:1078/1750 train_time:104794ms step_avg:97.21ms
step:1079/1750 train_time:104893ms step_avg:97.21ms
step:1080/1750 train_time:104991ms step_avg:97.21ms
step:1081/1750 train_time:105089ms step_avg:97.21ms
step:1082/1750 train_time:105188ms step_avg:97.22ms
step:1083/1750 train_time:105286ms step_avg:97.22ms
step:1084/1750 train_time:105385ms step_avg:97.22ms
step:1085/1750 train_time:105484ms step_avg:97.22ms
step:1086/1750 train_time:105585ms step_avg:97.22ms
step:1087/1750 train_time:105690ms step_avg:97.23ms
step:1088/1750 train_time:105791ms step_avg:97.23ms
step:1089/1750 train_time:105890ms step_avg:97.24ms
step:1090/1750 train_time:105990ms step_avg:97.24ms
step:1091/1750 train_time:106090ms step_avg:97.24ms
step:1092/1750 train_time:106189ms step_avg:97.24ms
step:1093/1750 train_time:106288ms step_avg:97.24ms
step:1094/1750 train_time:106388ms step_avg:97.25ms
step:1095/1750 train_time:106487ms step_avg:97.25ms
step:1096/1750 train_time:106588ms step_avg:97.25ms
step:1097/1750 train_time:106690ms step_avg:97.26ms
step:1098/1750 train_time:106791ms step_avg:97.26ms
step:1099/1750 train_time:106891ms step_avg:97.26ms
step:1100/1750 train_time:106990ms step_avg:97.26ms
step:1101/1750 train_time:107090ms step_avg:97.27ms
step:1102/1750 train_time:107189ms step_avg:97.27ms
step:1103/1750 train_time:107288ms step_avg:97.27ms
step:1104/1750 train_time:107387ms step_avg:97.27ms
step:1105/1750 train_time:107486ms step_avg:97.27ms
step:1106/1750 train_time:107587ms step_avg:97.28ms
step:1107/1750 train_time:107688ms step_avg:97.28ms
step:1108/1750 train_time:107789ms step_avg:97.28ms
step:1109/1750 train_time:107889ms step_avg:97.28ms
step:1110/1750 train_time:107990ms step_avg:97.29ms
step:1111/1750 train_time:108090ms step_avg:97.29ms
step:1112/1750 train_time:108190ms step_avg:97.29ms
step:1113/1750 train_time:108290ms step_avg:97.30ms
step:1114/1750 train_time:108389ms step_avg:97.30ms
step:1115/1750 train_time:108488ms step_avg:97.30ms
step:1116/1750 train_time:108589ms step_avg:97.30ms
step:1117/1750 train_time:108689ms step_avg:97.30ms
step:1118/1750 train_time:108789ms step_avg:97.31ms
step:1119/1750 train_time:108889ms step_avg:97.31ms
step:1120/1750 train_time:108990ms step_avg:97.31ms
step:1121/1750 train_time:109091ms step_avg:97.32ms
step:1122/1750 train_time:109190ms step_avg:97.32ms
step:1123/1750 train_time:109289ms step_avg:97.32ms
step:1124/1750 train_time:109389ms step_avg:97.32ms
step:1125/1750 train_time:109490ms step_avg:97.32ms
step:1125/1750 val_loss:3.4612 train_time:109578ms step_avg:97.40ms
step:1126/1750 train_time:109600ms step_avg:97.34ms
step:1127/1750 train_time:109696ms step_avg:97.33ms
step:1128/1750 train_time:109797ms step_avg:97.34ms
step:1129/1750 train_time:109897ms step_avg:97.34ms
step:1130/1750 train_time:109997ms step_avg:97.34ms
step:1131/1750 train_time:110096ms step_avg:97.34ms
step:1132/1750 train_time:110195ms step_avg:97.35ms
step:1133/1750 train_time:110294ms step_avg:97.35ms
step:1134/1750 train_time:110393ms step_avg:97.35ms
step:1135/1750 train_time:110492ms step_avg:97.35ms
step:1136/1750 train_time:110595ms step_avg:97.35ms
step:1137/1750 train_time:110697ms step_avg:97.36ms
step:1138/1750 train_time:110798ms step_avg:97.36ms
step:1139/1750 train_time:110898ms step_avg:97.36ms
step:1140/1750 train_time:110996ms step_avg:97.37ms
step:1141/1750 train_time:111096ms step_avg:97.37ms
step:1142/1750 train_time:111196ms step_avg:97.37ms
step:1143/1750 train_time:111294ms step_avg:97.37ms
step:1144/1750 train_time:111392ms step_avg:97.37ms
step:1145/1750 train_time:111491ms step_avg:97.37ms
step:1146/1750 train_time:111592ms step_avg:97.37ms
step:1147/1750 train_time:111693ms step_avg:97.38ms
step:1148/1750 train_time:111794ms step_avg:97.38ms
step:1149/1750 train_time:111895ms step_avg:97.39ms
step:1150/1750 train_time:111997ms step_avg:97.39ms
step:1151/1750 train_time:112097ms step_avg:97.39ms
step:1152/1750 train_time:112196ms step_avg:97.39ms
step:1153/1750 train_time:112296ms step_avg:97.39ms
step:1154/1750 train_time:112395ms step_avg:97.40ms
step:1155/1750 train_time:112494ms step_avg:97.40ms
step:1156/1750 train_time:112595ms step_avg:97.40ms
step:1157/1750 train_time:112696ms step_avg:97.40ms
step:1158/1750 train_time:113042ms step_avg:97.62ms
step:1159/1750 train_time:113140ms step_avg:97.62ms
step:1160/1750 train_time:113238ms step_avg:97.62ms
step:1161/1750 train_time:113336ms step_avg:97.62ms
step:1162/1750 train_time:113435ms step_avg:97.62ms
step:1163/1750 train_time:113534ms step_avg:97.62ms
step:1164/1750 train_time:113633ms step_avg:97.62ms
step:1165/1750 train_time:113731ms step_avg:97.62ms
step:1166/1750 train_time:113829ms step_avg:97.62ms
step:1167/1750 train_time:113932ms step_avg:97.63ms
step:1168/1750 train_time:114035ms step_avg:97.63ms
step:1169/1750 train_time:114137ms step_avg:97.64ms
step:1170/1750 train_time:114238ms step_avg:97.64ms
step:1171/1750 train_time:114338ms step_avg:97.64ms
step:1172/1750 train_time:114439ms step_avg:97.64ms
step:1173/1750 train_time:114538ms step_avg:97.65ms
step:1174/1750 train_time:114638ms step_avg:97.65ms
step:1175/1750 train_time:114738ms step_avg:97.65ms
step:1176/1750 train_time:114839ms step_avg:97.65ms
step:1177/1750 train_time:114940ms step_avg:97.66ms
step:1178/1750 train_time:115041ms step_avg:97.66ms
step:1179/1750 train_time:115144ms step_avg:97.66ms
step:1180/1750 train_time:115244ms step_avg:97.66ms
step:1181/1750 train_time:115344ms step_avg:97.67ms
step:1182/1750 train_time:115445ms step_avg:97.67ms
step:1183/1750 train_time:115545ms step_avg:97.67ms
step:1184/1750 train_time:115646ms step_avg:97.67ms
step:1185/1750 train_time:115747ms step_avg:97.68ms
step:1186/1750 train_time:115848ms step_avg:97.68ms
step:1187/1750 train_time:115949ms step_avg:97.68ms
step:1188/1750 train_time:116050ms step_avg:97.69ms
step:1189/1750 train_time:116151ms step_avg:97.69ms
step:1190/1750 train_time:116252ms step_avg:97.69ms
step:1191/1750 train_time:116353ms step_avg:97.69ms
step:1192/1750 train_time:116454ms step_avg:97.70ms
step:1193/1750 train_time:116554ms step_avg:97.70ms
step:1194/1750 train_time:116654ms step_avg:97.70ms
step:1195/1750 train_time:116756ms step_avg:97.70ms
step:1196/1750 train_time:116857ms step_avg:97.71ms
step:1197/1750 train_time:116959ms step_avg:97.71ms
step:1198/1750 train_time:117059ms step_avg:97.71ms
step:1199/1750 train_time:117160ms step_avg:97.72ms
step:1200/1750 train_time:117261ms step_avg:97.72ms
step:1201/1750 train_time:117361ms step_avg:97.72ms
step:1202/1750 train_time:117462ms step_avg:97.72ms
step:1203/1750 train_time:117562ms step_avg:97.72ms
step:1204/1750 train_time:117662ms step_avg:97.73ms
step:1205/1750 train_time:117762ms step_avg:97.73ms
step:1206/1750 train_time:117862ms step_avg:97.73ms
step:1207/1750 train_time:117964ms step_avg:97.73ms
step:1208/1750 train_time:118064ms step_avg:97.74ms
step:1209/1750 train_time:118165ms step_avg:97.74ms
step:1210/1750 train_time:118265ms step_avg:97.74ms
step:1211/1750 train_time:118365ms step_avg:97.74ms
step:1212/1750 train_time:118466ms step_avg:97.74ms
step:1213/1750 train_time:118567ms step_avg:97.75ms
step:1214/1750 train_time:118667ms step_avg:97.75ms
step:1215/1750 train_time:118768ms step_avg:97.75ms
step:1216/1750 train_time:119163ms step_avg:98.00ms
step:1217/1750 train_time:119262ms step_avg:98.00ms
step:1218/1750 train_time:119362ms step_avg:98.00ms
step:1219/1750 train_time:119463ms step_avg:98.00ms
step:1220/1750 train_time:119562ms step_avg:98.00ms
step:1221/1750 train_time:119661ms step_avg:98.00ms
step:1222/1750 train_time:119761ms step_avg:98.00ms
step:1223/1750 train_time:119861ms step_avg:98.01ms
step:1224/1750 train_time:119961ms step_avg:98.01ms
step:1225/1750 train_time:120064ms step_avg:98.01ms
step:1226/1750 train_time:120170ms step_avg:98.02ms
step:1227/1750 train_time:120608ms step_avg:98.30ms
step:1228/1750 train_time:120671ms step_avg:98.27ms
step:1229/1750 train_time:120770ms step_avg:98.27ms
step:1230/1750 train_time:120869ms step_avg:98.27ms
step:1231/1750 train_time:120969ms step_avg:98.27ms
step:1232/1750 train_time:121369ms step_avg:98.51ms
step:1233/1750 train_time:121467ms step_avg:98.51ms
step:1234/1750 train_time:121568ms step_avg:98.52ms
step:1235/1750 train_time:121667ms step_avg:98.52ms
step:1236/1750 train_time:121767ms step_avg:98.52ms
step:1237/1750 train_time:121867ms step_avg:98.52ms
step:1238/1750 train_time:121967ms step_avg:98.52ms
step:1239/1750 train_time:122067ms step_avg:98.52ms
step:1240/1750 train_time:122166ms step_avg:98.52ms
step:1241/1750 train_time:122268ms step_avg:98.52ms
step:1242/1750 train_time:122375ms step_avg:98.53ms
step:1243/1750 train_time:122478ms step_avg:98.53ms
step:1244/1750 train_time:122578ms step_avg:98.53ms
step:1245/1750 train_time:122678ms step_avg:98.54ms
step:1246/1750 train_time:122779ms step_avg:98.54ms
step:1247/1750 train_time:122880ms step_avg:98.54ms
step:1248/1750 train_time:122981ms step_avg:98.54ms
step:1249/1750 train_time:123081ms step_avg:98.54ms
step:1250/1750 train_time:123181ms step_avg:98.54ms
step:1250/1750 val_loss:3.4152 train_time:123270ms step_avg:98.62ms
step:1251/1750 train_time:123293ms step_avg:98.56ms
step:1252/1750 train_time:123393ms step_avg:98.56ms
step:1253/1750 train_time:123493ms step_avg:98.56ms
step:1254/1750 train_time:123593ms step_avg:98.56ms
step:1255/1750 train_time:123693ms step_avg:98.56ms
step:1256/1750 train_time:123792ms step_avg:98.56ms
step:1257/1750 train_time:123892ms step_avg:98.56ms
step:1258/1750 train_time:123991ms step_avg:98.56ms
step:1259/1750 train_time:124090ms step_avg:98.56ms
step:1260/1750 train_time:124190ms step_avg:98.56ms
step:1261/1750 train_time:124292ms step_avg:98.57ms
step:1262/1750 train_time:124394ms step_avg:98.57ms
step:1263/1750 train_time:124495ms step_avg:98.57ms
step:1264/1750 train_time:124595ms step_avg:98.57ms
step:1265/1750 train_time:124696ms step_avg:98.57ms
step:1266/1750 train_time:124796ms step_avg:98.57ms
step:1267/1750 train_time:124896ms step_avg:98.58ms
step:1268/1750 train_time:124996ms step_avg:98.58ms
step:1269/1750 train_time:125096ms step_avg:98.58ms
step:1270/1750 train_time:125198ms step_avg:98.58ms
step:1271/1750 train_time:125301ms step_avg:98.58ms
step:1272/1750 train_time:125402ms step_avg:98.59ms
step:1273/1750 train_time:125502ms step_avg:98.59ms
step:1274/1750 train_time:125603ms step_avg:98.59ms
step:1275/1750 train_time:125705ms step_avg:98.59ms
step:1276/1750 train_time:125806ms step_avg:98.59ms
step:1277/1750 train_time:125907ms step_avg:98.60ms
step:1278/1750 train_time:126007ms step_avg:98.60ms
step:1279/1750 train_time:126108ms step_avg:98.60ms
step:1280/1750 train_time:126209ms step_avg:98.60ms
step:1281/1750 train_time:126310ms step_avg:98.60ms
step:1282/1750 train_time:126409ms step_avg:98.60ms
step:1283/1750 train_time:126509ms step_avg:98.60ms
step:1284/1750 train_time:126609ms step_avg:98.61ms
step:1285/1750 train_time:126709ms step_avg:98.61ms
step:1286/1750 train_time:126809ms step_avg:98.61ms
step:1287/1750 train_time:126909ms step_avg:98.61ms
step:1288/1750 train_time:127010ms step_avg:98.61ms
step:1289/1750 train_time:127111ms step_avg:98.61ms
step:1290/1750 train_time:127211ms step_avg:98.61ms
step:1291/1750 train_time:127311ms step_avg:98.61ms
step:1292/1750 train_time:127411ms step_avg:98.62ms
step:1293/1750 train_time:127512ms step_avg:98.62ms
step:1294/1750 train_time:127614ms step_avg:98.62ms
step:1295/1750 train_time:127715ms step_avg:98.62ms
step:1296/1750 train_time:127816ms step_avg:98.62ms
step:1297/1750 train_time:127917ms step_avg:98.63ms
step:1298/1750 train_time:128019ms step_avg:98.63ms
step:1299/1750 train_time:128120ms step_avg:98.63ms
step:1300/1750 train_time:128221ms step_avg:98.63ms
step:1301/1750 train_time:128323ms step_avg:98.63ms
step:1302/1750 train_time:128424ms step_avg:98.64ms
step:1303/1750 train_time:128527ms step_avg:98.64ms
step:1304/1750 train_time:128629ms step_avg:98.64ms
step:1305/1750 train_time:128730ms step_avg:98.64ms
step:1306/1750 train_time:128830ms step_avg:98.64ms
step:1307/1750 train_time:128930ms step_avg:98.65ms
step:1308/1750 train_time:129030ms step_avg:98.65ms
step:1309/1750 train_time:129131ms step_avg:98.65ms
step:1310/1750 train_time:129231ms step_avg:98.65ms
step:1311/1750 train_time:129333ms step_avg:98.65ms
step:1312/1750 train_time:129434ms step_avg:98.65ms
step:1313/1750 train_time:129537ms step_avg:98.66ms
step:1314/1750 train_time:129638ms step_avg:98.66ms
step:1315/1750 train_time:129738ms step_avg:98.66ms
step:1316/1750 train_time:129840ms step_avg:98.66ms
step:1317/1750 train_time:129939ms step_avg:98.66ms
step:1318/1750 train_time:130041ms step_avg:98.67ms
step:1319/1750 train_time:130142ms step_avg:98.67ms
step:1320/1750 train_time:130244ms step_avg:98.67ms
step:1321/1750 train_time:130346ms step_avg:98.67ms
step:1322/1750 train_time:130448ms step_avg:98.67ms
step:1323/1750 train_time:130549ms step_avg:98.68ms
step:1324/1750 train_time:130650ms step_avg:98.68ms
step:1325/1750 train_time:130751ms step_avg:98.68ms
step:1326/1750 train_time:130851ms step_avg:98.68ms
step:1327/1750 train_time:130951ms step_avg:98.68ms
step:1328/1750 train_time:131051ms step_avg:98.68ms
step:1329/1750 train_time:131151ms step_avg:98.68ms
step:1330/1750 train_time:131252ms step_avg:98.69ms
step:1331/1750 train_time:131353ms step_avg:98.69ms
step:1332/1750 train_time:131455ms step_avg:98.69ms
step:1333/1750 train_time:131557ms step_avg:98.69ms
step:1334/1750 train_time:131658ms step_avg:98.69ms
step:1335/1750 train_time:131760ms step_avg:98.70ms
step:1336/1750 train_time:131861ms step_avg:98.70ms
step:1337/1750 train_time:131962ms step_avg:98.70ms
step:1338/1750 train_time:132063ms step_avg:98.70ms
step:1339/1750 train_time:132164ms step_avg:98.70ms
step:1340/1750 train_time:132266ms step_avg:98.71ms
step:1341/1750 train_time:132367ms step_avg:98.71ms
step:1342/1750 train_time:132469ms step_avg:98.71ms
step:1343/1750 train_time:132569ms step_avg:98.71ms
step:1344/1750 train_time:132670ms step_avg:98.71ms
step:1345/1750 train_time:132770ms step_avg:98.71ms
step:1346/1750 train_time:132871ms step_avg:98.72ms
step:1347/1750 train_time:132971ms step_avg:98.72ms
step:1348/1750 train_time:133072ms step_avg:98.72ms
step:1349/1750 train_time:133173ms step_avg:98.72ms
step:1350/1750 train_time:133274ms step_avg:98.72ms
step:1351/1750 train_time:133375ms step_avg:98.72ms
step:1352/1750 train_time:133476ms step_avg:98.73ms
step:1353/1750 train_time:133579ms step_avg:98.73ms
step:1354/1750 train_time:133680ms step_avg:98.73ms
step:1355/1750 train_time:133781ms step_avg:98.73ms
step:1356/1750 train_time:133883ms step_avg:98.73ms
step:1357/1750 train_time:133983ms step_avg:98.73ms
step:1358/1750 train_time:134085ms step_avg:98.74ms
step:1359/1750 train_time:134187ms step_avg:98.74ms
step:1360/1750 train_time:134288ms step_avg:98.74ms
step:1361/1750 train_time:134389ms step_avg:98.74ms
step:1362/1750 train_time:134490ms step_avg:98.74ms
step:1363/1750 train_time:134592ms step_avg:98.75ms
step:1364/1750 train_time:134692ms step_avg:98.75ms
step:1365/1750 train_time:134792ms step_avg:98.75ms
step:1366/1750 train_time:134892ms step_avg:98.75ms
step:1367/1750 train_time:134992ms step_avg:98.75ms
step:1368/1750 train_time:135094ms step_avg:98.75ms
step:1369/1750 train_time:135194ms step_avg:98.75ms
step:1370/1750 train_time:135296ms step_avg:98.76ms
step:1371/1750 train_time:135397ms step_avg:98.76ms
step:1372/1750 train_time:135499ms step_avg:98.76ms
step:1373/1750 train_time:135599ms step_avg:98.76ms
step:1374/1750 train_time:135701ms step_avg:98.76ms
step:1375/1750 train_time:135801ms step_avg:98.76ms
step:1375/1750 val_loss:3.3754 train_time:135891ms step_avg:98.83ms
step:1376/1750 train_time:135912ms step_avg:98.77ms
step:1377/1750 train_time:136009ms step_avg:98.77ms
step:1378/1750 train_time:136109ms step_avg:98.77ms
step:1379/1750 train_time:136209ms step_avg:98.77ms
step:1380/1750 train_time:136311ms step_avg:98.78ms
step:1381/1750 train_time:136411ms step_avg:98.78ms
step:1382/1750 train_time:136511ms step_avg:98.78ms
step:1383/1750 train_time:136610ms step_avg:98.78ms
step:1384/1750 train_time:136710ms step_avg:98.78ms
step:1385/1750 train_time:136811ms step_avg:98.78ms
step:1386/1750 train_time:136915ms step_avg:98.78ms
step:1387/1750 train_time:137018ms step_avg:98.79ms
step:1388/1750 train_time:137119ms step_avg:98.79ms
step:1389/1750 train_time:137221ms step_avg:98.79ms
step:1390/1750 train_time:137323ms step_avg:98.79ms
step:1391/1750 train_time:137425ms step_avg:98.80ms
step:1392/1750 train_time:137527ms step_avg:98.80ms
step:1393/1750 train_time:137627ms step_avg:98.80ms
step:1394/1750 train_time:137727ms step_avg:98.80ms
step:1395/1750 train_time:137828ms step_avg:98.80ms
step:1396/1750 train_time:137928ms step_avg:98.80ms
step:1397/1750 train_time:138029ms step_avg:98.80ms
step:1398/1750 train_time:138130ms step_avg:98.81ms
step:1399/1750 train_time:138232ms step_avg:98.81ms
step:1400/1750 train_time:138332ms step_avg:98.81ms
step:1401/1750 train_time:138433ms step_avg:98.81ms
step:1402/1750 train_time:138536ms step_avg:98.81ms
step:1403/1750 train_time:138638ms step_avg:98.82ms
step:1404/1750 train_time:138740ms step_avg:98.82ms
step:1405/1750 train_time:138841ms step_avg:98.82ms
step:1406/1750 train_time:138943ms step_avg:98.82ms
step:1407/1750 train_time:139044ms step_avg:98.82ms
step:1408/1750 train_time:139146ms step_avg:98.83ms
step:1409/1750 train_time:139249ms step_avg:98.83ms
step:1410/1750 train_time:139349ms step_avg:98.83ms
step:1411/1750 train_time:139449ms step_avg:98.83ms
step:1412/1750 train_time:139550ms step_avg:98.83ms
step:1413/1750 train_time:139650ms step_avg:98.83ms
step:1414/1750 train_time:139751ms step_avg:98.83ms
step:1415/1750 train_time:139852ms step_avg:98.84ms
step:1416/1750 train_time:139953ms step_avg:98.84ms
step:1417/1750 train_time:140054ms step_avg:98.84ms
step:1418/1750 train_time:140155ms step_avg:98.84ms
step:1419/1750 train_time:140257ms step_avg:98.84ms
step:1420/1750 train_time:140357ms step_avg:98.84ms
step:1421/1750 train_time:140458ms step_avg:98.84ms
step:1422/1750 train_time:140559ms step_avg:98.85ms
step:1423/1750 train_time:140660ms step_avg:98.85ms
step:1424/1750 train_time:140764ms step_avg:98.85ms
step:1425/1750 train_time:140865ms step_avg:98.85ms
step:1426/1750 train_time:140967ms step_avg:98.85ms
step:1427/1750 train_time:141067ms step_avg:98.86ms
step:1428/1750 train_time:141168ms step_avg:98.86ms
step:1429/1750 train_time:141269ms step_avg:98.86ms
step:1430/1750 train_time:141371ms step_avg:98.86ms
step:1431/1750 train_time:141472ms step_avg:98.86ms
step:1432/1750 train_time:141574ms step_avg:98.86ms
step:1433/1750 train_time:141675ms step_avg:98.87ms
step:1434/1750 train_time:141777ms step_avg:98.87ms
step:1435/1750 train_time:141880ms step_avg:98.87ms
step:1436/1750 train_time:141984ms step_avg:98.87ms
step:1437/1750 train_time:142087ms step_avg:98.88ms
step:1438/1750 train_time:142188ms step_avg:98.88ms
step:1439/1750 train_time:142291ms step_avg:98.88ms
step:1440/1750 train_time:142393ms step_avg:98.88ms
step:1441/1750 train_time:142496ms step_avg:98.89ms
step:1442/1750 train_time:142596ms step_avg:98.89ms
step:1443/1750 train_time:142698ms step_avg:98.89ms
step:1444/1750 train_time:142800ms step_avg:98.89ms
step:1445/1750 train_time:142903ms step_avg:98.89ms
step:1446/1750 train_time:143005ms step_avg:98.90ms
step:1447/1750 train_time:143107ms step_avg:98.90ms
step:1448/1750 train_time:143210ms step_avg:98.90ms
step:1449/1750 train_time:143311ms step_avg:98.90ms
step:1450/1750 train_time:143412ms step_avg:98.91ms
step:1451/1750 train_time:143513ms step_avg:98.91ms
step:1452/1750 train_time:143614ms step_avg:98.91ms
step:1453/1750 train_time:143716ms step_avg:98.91ms
step:1454/1750 train_time:143820ms step_avg:98.91ms
step:1455/1750 train_time:143923ms step_avg:98.92ms
step:1456/1750 train_time:144026ms step_avg:98.92ms
step:1457/1750 train_time:144129ms step_avg:98.92ms
step:1458/1750 train_time:144231ms step_avg:98.92ms
step:1459/1750 train_time:144332ms step_avg:98.93ms
step:1460/1750 train_time:144433ms step_avg:98.93ms
step:1461/1750 train_time:144536ms step_avg:98.93ms
step:1462/1750 train_time:144637ms step_avg:98.93ms
step:1463/1750 train_time:144739ms step_avg:98.93ms
step:1464/1750 train_time:144841ms step_avg:98.94ms
step:1465/1750 train_time:144943ms step_avg:98.94ms
step:1466/1750 train_time:145045ms step_avg:98.94ms
step:1467/1750 train_time:145147ms step_avg:98.94ms
step:1468/1750 train_time:145251ms step_avg:98.94ms
step:1469/1750 train_time:145353ms step_avg:98.95ms
step:1470/1750 train_time:145454ms step_avg:98.95ms
step:1471/1750 train_time:145555ms step_avg:98.95ms
step:1472/1750 train_time:145656ms step_avg:98.95ms
step:1473/1750 train_time:145757ms step_avg:98.95ms
step:1474/1750 train_time:145859ms step_avg:98.95ms
step:1475/1750 train_time:145961ms step_avg:98.96ms
step:1476/1750 train_time:146066ms step_avg:98.96ms
step:1477/1750 train_time:146167ms step_avg:98.96ms
step:1478/1750 train_time:146270ms step_avg:98.96ms
step:1479/1750 train_time:146371ms step_avg:98.97ms
step:1480/1750 train_time:146473ms step_avg:98.97ms
step:1481/1750 train_time:146574ms step_avg:98.97ms
step:1482/1750 train_time:146677ms step_avg:98.97ms
step:1483/1750 train_time:146778ms step_avg:98.97ms
step:1484/1750 train_time:146879ms step_avg:98.98ms
step:1485/1750 train_time:146984ms step_avg:98.98ms
step:1486/1750 train_time:147086ms step_avg:98.98ms
step:1487/1750 train_time:147188ms step_avg:98.98ms
step:1488/1750 train_time:147291ms step_avg:98.99ms
step:1489/1750 train_time:147392ms step_avg:98.99ms
step:1490/1750 train_time:147493ms step_avg:98.99ms
step:1491/1750 train_time:147595ms step_avg:98.99ms
step:1492/1750 train_time:147696ms step_avg:98.99ms
step:1493/1750 train_time:147797ms step_avg:98.99ms
step:1494/1750 train_time:147900ms step_avg:99.00ms
step:1495/1750 train_time:148003ms step_avg:99.00ms
step:1496/1750 train_time:148106ms step_avg:99.00ms
step:1497/1750 train_time:148207ms step_avg:99.00ms
step:1498/1750 train_time:148309ms step_avg:99.00ms
step:1499/1750 train_time:148410ms step_avg:99.01ms
step:1500/1750 train_time:148512ms step_avg:99.01ms
step:1500/1750 val_loss:3.3393 train_time:148602ms step_avg:99.07ms
step:1501/1750 train_time:148624ms step_avg:99.02ms
step:1502/1750 train_time:148727ms step_avg:99.02ms
step:1503/1750 train_time:148829ms step_avg:99.02ms
step:1504/1750 train_time:148930ms step_avg:99.02ms
step:1505/1750 train_time:149032ms step_avg:99.02ms
step:1506/1750 train_time:149134ms step_avg:99.03ms
step:1507/1750 train_time:149235ms step_avg:99.03ms
step:1508/1750 train_time:149337ms step_avg:99.03ms
step:1509/1750 train_time:149438ms step_avg:99.03ms
step:1510/1750 train_time:149539ms step_avg:99.03ms
step:1511/1750 train_time:149644ms step_avg:99.04ms
step:1512/1750 train_time:149747ms step_avg:99.04ms
step:1513/1750 train_time:149849ms step_avg:99.04ms
step:1514/1750 train_time:149949ms step_avg:99.04ms
step:1515/1750 train_time:150055ms step_avg:99.05ms
step:1516/1750 train_time:150157ms step_avg:99.05ms
step:1517/1750 train_time:150257ms step_avg:99.05ms
step:1518/1750 train_time:150358ms step_avg:99.05ms
step:1519/1750 train_time:150459ms step_avg:99.05ms
step:1520/1750 train_time:150561ms step_avg:99.05ms
step:1521/1750 train_time:150662ms step_avg:99.05ms
step:1522/1750 train_time:150764ms step_avg:99.06ms
step:1523/1750 train_time:150866ms step_avg:99.06ms
step:1524/1750 train_time:150971ms step_avg:99.06ms
step:1525/1750 train_time:151074ms step_avg:99.06ms
step:1526/1750 train_time:151176ms step_avg:99.07ms
step:1527/1750 train_time:151277ms step_avg:99.07ms
step:1528/1750 train_time:151382ms step_avg:99.07ms
step:1529/1750 train_time:151483ms step_avg:99.07ms
step:1530/1750 train_time:151585ms step_avg:99.08ms
step:1531/1750 train_time:151687ms step_avg:99.08ms
step:1532/1750 train_time:151789ms step_avg:99.08ms
step:1533/1750 train_time:151891ms step_avg:99.08ms
step:1534/1750 train_time:151993ms step_avg:99.08ms
step:1535/1750 train_time:152096ms step_avg:99.09ms
step:1536/1750 train_time:152196ms step_avg:99.09ms
step:1537/1750 train_time:152298ms step_avg:99.09ms
step:1538/1750 train_time:152399ms step_avg:99.09ms
step:1539/1750 train_time:152500ms step_avg:99.09ms
step:1540/1750 train_time:152602ms step_avg:99.09ms
step:1541/1750 train_time:152705ms step_avg:99.09ms
step:1542/1750 train_time:152808ms step_avg:99.10ms
step:1543/1750 train_time:152910ms step_avg:99.10ms
step:1544/1750 train_time:153013ms step_avg:99.10ms
step:1545/1750 train_time:153115ms step_avg:99.10ms
step:1546/1750 train_time:153216ms step_avg:99.10ms
step:1547/1750 train_time:153318ms step_avg:99.11ms
step:1548/1750 train_time:153420ms step_avg:99.11ms
step:1549/1750 train_time:153521ms step_avg:99.11ms
step:1550/1750 train_time:153622ms step_avg:99.11ms
step:1551/1750 train_time:153725ms step_avg:99.11ms
step:1552/1750 train_time:153827ms step_avg:99.12ms
step:1553/1750 train_time:153928ms step_avg:99.12ms
step:1554/1750 train_time:154030ms step_avg:99.12ms
step:1555/1750 train_time:154132ms step_avg:99.12ms
step:1556/1750 train_time:154235ms step_avg:99.12ms
step:1557/1750 train_time:154338ms step_avg:99.12ms
step:1558/1750 train_time:154440ms step_avg:99.13ms
step:1559/1750 train_time:154542ms step_avg:99.13ms
step:1560/1750 train_time:154643ms step_avg:99.13ms
step:1561/1750 train_time:154744ms step_avg:99.13ms
step:1562/1750 train_time:154847ms step_avg:99.13ms
step:1563/1750 train_time:154951ms step_avg:99.14ms
step:1564/1750 train_time:155052ms step_avg:99.14ms
step:1565/1750 train_time:155153ms step_avg:99.14ms
step:1566/1750 train_time:155256ms step_avg:99.14ms
step:1567/1750 train_time:155358ms step_avg:99.14ms
step:1568/1750 train_time:155459ms step_avg:99.14ms
step:1569/1750 train_time:155561ms step_avg:99.15ms
step:1570/1750 train_time:155664ms step_avg:99.15ms
step:1571/1750 train_time:155766ms step_avg:99.15ms
step:1572/1750 train_time:155867ms step_avg:99.15ms
step:1573/1750 train_time:155968ms step_avg:99.15ms
step:1574/1750 train_time:156070ms step_avg:99.16ms
step:1575/1750 train_time:156172ms step_avg:99.16ms
step:1576/1750 train_time:156276ms step_avg:99.16ms
step:1577/1750 train_time:156378ms step_avg:99.16ms
step:1578/1750 train_time:156480ms step_avg:99.16ms
step:1579/1750 train_time:156582ms step_avg:99.17ms
step:1580/1750 train_time:156685ms step_avg:99.17ms
step:1581/1750 train_time:156787ms step_avg:99.17ms
step:1582/1750 train_time:156888ms step_avg:99.17ms
step:1583/1750 train_time:156992ms step_avg:99.17ms
step:1584/1750 train_time:157095ms step_avg:99.18ms
step:1585/1750 train_time:157196ms step_avg:99.18ms
step:1586/1750 train_time:157299ms step_avg:99.18ms
step:1587/1750 train_time:157401ms step_avg:99.18ms
step:1588/1750 train_time:157503ms step_avg:99.18ms
step:1589/1750 train_time:157605ms step_avg:99.18ms
step:1590/1750 train_time:157706ms step_avg:99.19ms
step:1591/1750 train_time:157809ms step_avg:99.19ms
step:1592/1750 train_time:157912ms step_avg:99.19ms
step:1593/1750 train_time:158013ms step_avg:99.19ms
step:1594/1750 train_time:158118ms step_avg:99.20ms
step:1595/1750 train_time:158220ms step_avg:99.20ms
step:1596/1750 train_time:158321ms step_avg:99.20ms
step:1597/1750 train_time:158422ms step_avg:99.20ms
step:1598/1750 train_time:158525ms step_avg:99.20ms
step:1599/1750 train_time:158625ms step_avg:99.20ms
step:1600/1750 train_time:158728ms step_avg:99.20ms
step:1601/1750 train_time:158830ms step_avg:99.21ms
step:1602/1750 train_time:158932ms step_avg:99.21ms
step:1603/1750 train_time:159034ms step_avg:99.21ms
step:1604/1750 train_time:159136ms step_avg:99.21ms
step:1605/1750 train_time:159238ms step_avg:99.21ms
step:1606/1750 train_time:159340ms step_avg:99.22ms
step:1607/1750 train_time:159442ms step_avg:99.22ms
step:1608/1750 train_time:159542ms step_avg:99.22ms
step:1609/1750 train_time:159643ms step_avg:99.22ms
step:1610/1750 train_time:159747ms step_avg:99.22ms
step:1611/1750 train_time:159850ms step_avg:99.22ms
step:1612/1750 train_time:159953ms step_avg:99.23ms
step:1613/1750 train_time:160054ms step_avg:99.23ms
step:1614/1750 train_time:160156ms step_avg:99.23ms
step:1615/1750 train_time:160257ms step_avg:99.23ms
step:1616/1750 train_time:160360ms step_avg:99.23ms
step:1617/1750 train_time:160461ms step_avg:99.23ms
step:1618/1750 train_time:160562ms step_avg:99.24ms
step:1619/1750 train_time:160663ms step_avg:99.24ms
step:1620/1750 train_time:160765ms step_avg:99.24ms
step:1621/1750 train_time:160868ms step_avg:99.24ms
step:1622/1750 train_time:160970ms step_avg:99.24ms
step:1623/1750 train_time:161072ms step_avg:99.24ms
step:1624/1750 train_time:161177ms step_avg:99.25ms
step:1625/1750 train_time:161281ms step_avg:99.25ms
step:1625/1750 val_loss:3.3089 train_time:161371ms step_avg:99.31ms
step:1626/1750 train_time:161394ms step_avg:99.26ms
step:1627/1750 train_time:161491ms step_avg:99.26ms
step:1628/1750 train_time:161593ms step_avg:99.26ms
step:1629/1750 train_time:161697ms step_avg:99.26ms
step:1630/1750 train_time:161797ms step_avg:99.26ms
step:1631/1750 train_time:161898ms step_avg:99.26ms
step:1632/1750 train_time:161999ms step_avg:99.26ms
step:1633/1750 train_time:162100ms step_avg:99.27ms
step:1634/1750 train_time:162204ms step_avg:99.27ms
step:1635/1750 train_time:162308ms step_avg:99.27ms
step:1636/1750 train_time:162412ms step_avg:99.27ms
step:1637/1750 train_time:162514ms step_avg:99.28ms
step:1638/1750 train_time:162617ms step_avg:99.28ms
step:1639/1750 train_time:162718ms step_avg:99.28ms
step:1640/1750 train_time:162819ms step_avg:99.28ms
step:1641/1750 train_time:162920ms step_avg:99.28ms
step:1642/1750 train_time:163021ms step_avg:99.28ms
step:1643/1750 train_time:163123ms step_avg:99.28ms
step:1644/1750 train_time:163227ms step_avg:99.29ms
step:1645/1750 train_time:163330ms step_avg:99.29ms
step:1646/1750 train_time:163432ms step_avg:99.29ms
step:1647/1750 train_time:163536ms step_avg:99.29ms
step:1648/1750 train_time:163640ms step_avg:99.30ms
step:1649/1750 train_time:163741ms step_avg:99.30ms
step:1650/1750 train_time:163842ms step_avg:99.30ms
step:1651/1750 train_time:163945ms step_avg:99.30ms
step:1652/1750 train_time:164046ms step_avg:99.30ms
step:1653/1750 train_time:164148ms step_avg:99.30ms
step:1654/1750 train_time:164248ms step_avg:99.30ms
step:1655/1750 train_time:164351ms step_avg:99.31ms
step:1656/1750 train_time:164453ms step_avg:99.31ms
step:1657/1750 train_time:164555ms step_avg:99.31ms
step:1658/1750 train_time:164657ms step_avg:99.31ms
step:1659/1750 train_time:164761ms step_avg:99.31ms
step:1660/1750 train_time:164863ms step_avg:99.31ms
step:1661/1750 train_time:164966ms step_avg:99.32ms
step:1662/1750 train_time:165069ms step_avg:99.32ms
step:1663/1750 train_time:165171ms step_avg:99.32ms
step:1664/1750 train_time:165273ms step_avg:99.32ms
step:1665/1750 train_time:165377ms step_avg:99.33ms
step:1666/1750 train_time:165479ms step_avg:99.33ms
step:1667/1750 train_time:165581ms step_avg:99.33ms
step:1668/1750 train_time:165684ms step_avg:99.33ms
step:1669/1750 train_time:165786ms step_avg:99.33ms
step:1670/1750 train_time:165887ms step_avg:99.33ms
step:1671/1750 train_time:165989ms step_avg:99.34ms
step:1672/1750 train_time:166091ms step_avg:99.34ms
step:1673/1750 train_time:166193ms step_avg:99.34ms
step:1674/1750 train_time:166294ms step_avg:99.34ms
step:1675/1750 train_time:166397ms step_avg:99.34ms
step:1676/1750 train_time:166501ms step_avg:99.34ms
step:1677/1750 train_time:166603ms step_avg:99.35ms
step:1678/1750 train_time:166705ms step_avg:99.35ms
step:1679/1750 train_time:166807ms step_avg:99.35ms
step:1680/1750 train_time:166909ms step_avg:99.35ms
step:1681/1750 train_time:167011ms step_avg:99.35ms
step:1682/1750 train_time:167116ms step_avg:99.36ms
step:1683/1750 train_time:167217ms step_avg:99.36ms
step:1684/1750 train_time:167319ms step_avg:99.36ms
step:1685/1750 train_time:167423ms step_avg:99.36ms
step:1686/1750 train_time:167524ms step_avg:99.36ms
step:1687/1750 train_time:167626ms step_avg:99.36ms
step:1688/1750 train_time:167728ms step_avg:99.37ms
step:1689/1750 train_time:167830ms step_avg:99.37ms
step:1690/1750 train_time:167932ms step_avg:99.37ms
step:1691/1750 train_time:168035ms step_avg:99.37ms
step:1692/1750 train_time:168138ms step_avg:99.37ms
step:1693/1750 train_time:168241ms step_avg:99.37ms
step:1694/1750 train_time:168345ms step_avg:99.38ms
step:1695/1750 train_time:168448ms step_avg:99.38ms
step:1696/1750 train_time:168550ms step_avg:99.38ms
step:1697/1750 train_time:168654ms step_avg:99.38ms
step:1698/1750 train_time:168759ms step_avg:99.39ms
step:1699/1750 train_time:168861ms step_avg:99.39ms
step:1700/1750 train_time:168965ms step_avg:99.39ms
step:1701/1750 train_time:169067ms step_avg:99.39ms
step:1702/1750 train_time:169172ms step_avg:99.40ms
step:1703/1750 train_time:169275ms step_avg:99.40ms
step:1704/1750 train_time:169378ms step_avg:99.40ms
step:1705/1750 train_time:169480ms step_avg:99.40ms
step:1706/1750 train_time:169583ms step_avg:99.40ms
step:1707/1750 train_time:169687ms step_avg:99.41ms
step:1708/1750 train_time:169790ms step_avg:99.41ms
step:1709/1750 train_time:169892ms step_avg:99.41ms
step:1710/1750 train_time:169993ms step_avg:99.41ms
step:1711/1750 train_time:170097ms step_avg:99.41ms
step:1712/1750 train_time:170200ms step_avg:99.42ms
step:1713/1750 train_time:170304ms step_avg:99.42ms
step:1714/1750 train_time:170406ms step_avg:99.42ms
step:1715/1750 train_time:170510ms step_avg:99.42ms
step:1716/1750 train_time:170613ms step_avg:99.42ms
step:1717/1750 train_time:170716ms step_avg:99.43ms
step:1718/1750 train_time:170819ms step_avg:99.43ms
step:1719/1750 train_time:170925ms step_avg:99.43ms
step:1720/1750 train_time:171027ms step_avg:99.43ms
step:1721/1750 train_time:171130ms step_avg:99.44ms
step:1722/1750 train_time:171233ms step_avg:99.44ms
step:1723/1750 train_time:171337ms step_avg:99.44ms
step:1724/1750 train_time:171442ms step_avg:99.44ms
step:1725/1750 train_time:171546ms step_avg:99.45ms
step:1726/1750 train_time:171648ms step_avg:99.45ms
step:1727/1750 train_time:171750ms step_avg:99.45ms
step:1728/1750 train_time:171852ms step_avg:99.45ms
step:1729/1750 train_time:171956ms step_avg:99.45ms
step:1730/1750 train_time:172058ms step_avg:99.46ms
step:1731/1750 train_time:172163ms step_avg:99.46ms
step:1732/1750 train_time:172267ms step_avg:99.46ms
step:1733/1750 train_time:172369ms step_avg:99.46ms
step:1734/1750 train_time:172474ms step_avg:99.47ms
step:1735/1750 train_time:172576ms step_avg:99.47ms
step:1736/1750 train_time:172679ms step_avg:99.47ms
step:1737/1750 train_time:172782ms step_avg:99.47ms
step:1738/1750 train_time:172885ms step_avg:99.47ms
step:1739/1750 train_time:172986ms step_avg:99.47ms
step:1740/1750 train_time:173088ms step_avg:99.48ms
step:1741/1750 train_time:173194ms step_avg:99.48ms
step:1742/1750 train_time:173296ms step_avg:99.48ms
step:1743/1750 train_time:173400ms step_avg:99.48ms
step:1744/1750 train_time:173504ms step_avg:99.49ms
step:1745/1750 train_time:173605ms step_avg:99.49ms
step:1746/1750 train_time:173708ms step_avg:99.49ms
step:1747/1750 train_time:173810ms step_avg:99.49ms
step:1748/1750 train_time:173913ms step_avg:99.49ms
step:1749/1750 train_time:174015ms step_avg:99.49ms
step:1750/1750 train_time:174119ms step_avg:99.50ms
step:1750/1750 val_loss:3.2851 train_time:174210ms step_avg:99.55ms
peak memory allocated: 33278 MiB reserved: 49134 MiB
