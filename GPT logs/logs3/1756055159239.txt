import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class FastNormMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, sgd_coeff=0):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, sgd_coeff=sgd_coeff)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        self.sgd_coeff = sgd_coeff

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            sgd_coeff = group["sgd_coeff"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    # assume: grad is a Tensor, momentum_buffer exists, momentum, sgd_coeff, eps, eff_lr are defined
                    with torch.no_grad():
                        # inplace blend with momentum (keeps original semantic: grad becomes the lerped tensor)
                        grad.lerp_(momentum_buffer, momentum)

                        # compute zeropower part from the (post-lerp) grad, in bfloat16 to match the original code
                        if sgd_coeff != 1:
                            # create update_part in bfloat16 (may allocate once per call; unavoidable if zeropower expects bfloat16)
                            update_part = zeropower_via_newtonschulz5(grad.to(torch.bfloat16), 5)

                            # upcast update_part to grad dtype to do mixed-dtype blending without extra temporaries later
                            if update_part.dtype != grad.dtype:
                                update_part = update_part.to(grad.dtype)

                        # compute inverse norm scalar efficiently (no large temp tensors)
                        # - view(-1) is cheap (no copy)
                        # - dot(view, view) is a single reduction
                        norm_sq = grad.view(-1).dot(grad.view(-1))
                        inv_norm = (norm_sq + eps).rsqrt()  # stable rsqrt on the scalar

                        # normalize grad in-place (so we avoid allocating g_normalized)
                        grad.mul_(inv_norm)   # now grad == g_normalized (in-place)

                        # build final update (reuse update_part buffer if available)
                        if sgd_coeff != 1:
                            # update_part := (1 - sgd_coeff) * update_part + sgd_coeff * grad
                            # do it in-place on update_part to avoid creating another temp
                            update_part.mul_(1 - sgd_coeff)    # in-place scale
                            update_part.lerp_(grad, sgd_coeff) # in-place linear interpolation
                            update = update_part
                        else:
                            # if sgd_coeff == 1 we can use grad directly (already normalized in-place)
                            update = grad

                        # apply update (same as original)
                        p.add_(other=update, alpha=-eff_lr)
                    '''
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    eps = 1e-12
                    g_normalized = grad / (grad.norm() + eps)
                    if sgd_coeff != 1:
                        update_part = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                        update = (1 - sgd_coeff) * update_part + sgd_coeff * g_normalized
                    else:
                        update = sgd_coeff * g_normalized
                    p.add_(other=update, alpha=-eff_lr)
                    '''
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class NormMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, sgd_coeff=0):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, sgd_coeff=sgd_coeff)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        self.sgd_coeff = sgd_coeff

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            sgd_coeff = group["sgd_coeff"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    eps = 1e-12
                    g_normalized = grad / (grad.norm() + eps)
                    if sgd_coeff != 1:
                        update_part = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                        update = (1 - sgd_coeff) * update_part + sgd_coeff * g_normalized
                    else:
                        update = sgd_coeff * g_normalized
                    p.add_(other=update, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()
class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)
                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_reduce_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 27.5
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos : pos + max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()
    starts = []
    for i in range(1, len(boundary_positions)):
        end = boundary_positions[i].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == dist.get_world_size():
                return starts, end - pos
            start = end
    assert False # increase max_batch_span if necessary

def distributed_data_generator(filename_pattern: str, batch_size: int, align_to_bos: bool):
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    max_batch_span = 2 * batch_size if align_to_bos else batch_size # provide buffer to handle samples up to length local_batch_size
    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            batch_span = batch_size
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1750 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
from datetime import datetime
if master_process:
    # run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_id = str(int(time.time() * 1000))
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0)
optimizer2 = NormMuon(hidden_matrix_params, lr=0.07, momentum=0.96, weight_decay=0.0, sgd_coeff=0.5)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.11 | packaged by conda-forge | (main, Jun  4 2025, 14:45:31) [GCC 13.3.0]
Running PyTorch 2.9.0.dev20250824+cu126 compiled for CUDA 12.6
Sun Aug 24 17:05:59 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:04:00.0 Off |                    0 |
| N/A   36C    P0            119W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:23:00.0 Off |                    0 |
| N/A   30C    P0            114W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:43:00.0 Off |                    0 |
| N/A   34C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   30C    P0            115W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:84:00.0 Off |                    0 |
| N/A   35C    P0            121W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:A3:00.0 Off |                    0 |
| N/A   31C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:C3:00.0 Off |                    0 |
| N/A   35C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E4:00.0 Off |                    0 |
| N/A   29C    P0            112W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.07ms
step:1/1750 train_time:143ms step_avg:143.50ms
step:2/1750 train_time:164ms step_avg:82.04ms
step:3/1750 train_time:245ms step_avg:81.66ms
step:4/1750 train_time:336ms step_avg:84.08ms
step:5/1750 train_time:428ms step_avg:85.66ms
step:6/1750 train_time:521ms step_avg:86.77ms
step:7/1750 train_time:613ms step_avg:87.55ms
step:8/1750 train_time:705ms step_avg:88.13ms
step:9/1750 train_time:797ms step_avg:88.61ms
step:10/1750 train_time:890ms step_avg:88.97ms
step:11/1750 train_time:982ms step_avg:89.24ms
step:12/1750 train_time:1077ms step_avg:89.72ms
step:13/1750 train_time:1172ms step_avg:90.12ms
step:14/1750 train_time:1267ms step_avg:90.53ms
step:15/1750 train_time:1361ms step_avg:90.73ms
step:16/1750 train_time:1454ms step_avg:90.87ms
step:17/1750 train_time:1547ms step_avg:91.00ms
step:18/1750 train_time:1639ms step_avg:91.08ms
step:19/1750 train_time:1732ms step_avg:91.17ms
step:20/1750 train_time:1825ms step_avg:91.24ms
step:21/1750 train_time:1917ms step_avg:91.30ms
step:22/1750 train_time:2011ms step_avg:91.40ms
step:23/1750 train_time:2105ms step_avg:91.52ms
step:24/1750 train_time:2199ms step_avg:91.63ms
step:25/1750 train_time:2293ms step_avg:91.72ms
step:26/1750 train_time:2386ms step_avg:91.77ms
step:27/1750 train_time:2479ms step_avg:91.81ms
step:28/1750 train_time:2572ms step_avg:91.86ms
step:29/1750 train_time:2666ms step_avg:91.93ms
step:30/1750 train_time:2759ms step_avg:91.97ms
step:31/1750 train_time:2852ms step_avg:91.99ms
step:32/1750 train_time:2945ms step_avg:92.02ms
step:33/1750 train_time:3037ms step_avg:92.02ms
step:34/1750 train_time:3130ms step_avg:92.06ms
step:35/1750 train_time:3225ms step_avg:92.13ms
step:36/1750 train_time:3318ms step_avg:92.17ms
step:37/1750 train_time:3411ms step_avg:92.20ms
step:38/1750 train_time:3504ms step_avg:92.22ms
step:39/1750 train_time:3597ms step_avg:92.23ms
step:40/1750 train_time:3691ms step_avg:92.27ms
step:41/1750 train_time:3784ms step_avg:92.28ms
step:42/1750 train_time:3877ms step_avg:92.30ms
step:43/1750 train_time:3970ms step_avg:92.32ms
step:44/1750 train_time:4064ms step_avg:92.36ms
step:45/1750 train_time:4157ms step_avg:92.38ms
step:46/1750 train_time:4250ms step_avg:92.39ms
step:47/1750 train_time:4344ms step_avg:92.42ms
step:48/1750 train_time:4438ms step_avg:92.45ms
step:49/1750 train_time:4531ms step_avg:92.47ms
step:50/1750 train_time:4625ms step_avg:92.50ms
step:51/1750 train_time:4718ms step_avg:92.50ms
step:52/1750 train_time:4811ms step_avg:92.52ms
step:53/1750 train_time:4904ms step_avg:92.53ms
step:54/1750 train_time:4997ms step_avg:92.53ms
step:55/1750 train_time:5090ms step_avg:92.55ms
step:56/1750 train_time:5184ms step_avg:92.56ms
step:57/1750 train_time:5277ms step_avg:92.57ms
step:58/1750 train_time:5371ms step_avg:92.60ms
step:59/1750 train_time:5465ms step_avg:92.62ms
step:60/1750 train_time:5558ms step_avg:92.63ms
step:61/1750 train_time:5651ms step_avg:92.64ms
step:62/1750 train_time:5744ms step_avg:92.65ms
step:63/1750 train_time:5837ms step_avg:92.65ms
step:64/1750 train_time:5930ms step_avg:92.66ms
step:65/1750 train_time:6024ms step_avg:92.67ms
step:66/1750 train_time:6117ms step_avg:92.68ms
step:67/1750 train_time:6210ms step_avg:92.69ms
step:68/1750 train_time:6303ms step_avg:92.69ms
step:69/1750 train_time:6396ms step_avg:92.70ms
step:70/1750 train_time:6490ms step_avg:92.71ms
step:71/1750 train_time:6583ms step_avg:92.71ms
step:72/1750 train_time:6676ms step_avg:92.73ms
step:73/1750 train_time:6769ms step_avg:92.73ms
step:74/1750 train_time:6863ms step_avg:92.74ms
step:75/1750 train_time:6956ms step_avg:92.74ms
step:76/1750 train_time:7049ms step_avg:92.74ms
step:77/1750 train_time:7141ms step_avg:92.75ms
step:78/1750 train_time:7235ms step_avg:92.75ms
step:79/1750 train_time:7328ms step_avg:92.76ms
step:80/1750 train_time:7422ms step_avg:92.77ms
step:81/1750 train_time:7516ms step_avg:92.79ms
step:82/1750 train_time:7609ms step_avg:92.79ms
step:83/1750 train_time:7702ms step_avg:92.79ms
step:84/1750 train_time:7796ms step_avg:92.81ms
step:85/1750 train_time:7889ms step_avg:92.81ms
step:86/1750 train_time:7982ms step_avg:92.81ms
step:87/1750 train_time:8075ms step_avg:92.82ms
step:88/1750 train_time:8168ms step_avg:92.82ms
step:89/1750 train_time:8262ms step_avg:92.83ms
step:90/1750 train_time:8355ms step_avg:92.84ms
step:91/1750 train_time:8449ms step_avg:92.84ms
step:92/1750 train_time:8542ms step_avg:92.85ms
step:93/1750 train_time:8635ms step_avg:92.85ms
step:94/1750 train_time:8728ms step_avg:92.86ms
step:95/1750 train_time:8822ms step_avg:92.87ms
step:96/1750 train_time:8915ms step_avg:92.87ms
step:97/1750 train_time:9009ms step_avg:92.87ms
step:98/1750 train_time:9102ms step_avg:92.88ms
step:99/1750 train_time:9195ms step_avg:92.88ms
step:100/1750 train_time:9288ms step_avg:92.88ms
step:101/1750 train_time:9381ms step_avg:92.88ms
step:102/1750 train_time:9475ms step_avg:92.89ms
step:103/1750 train_time:9568ms step_avg:92.89ms
step:104/1750 train_time:9661ms step_avg:92.90ms
step:105/1750 train_time:9755ms step_avg:92.90ms
step:106/1750 train_time:9848ms step_avg:92.91ms
step:107/1750 train_time:9941ms step_avg:92.91ms
step:108/1750 train_time:10034ms step_avg:92.91ms
step:109/1750 train_time:10127ms step_avg:92.91ms
step:110/1750 train_time:10221ms step_avg:92.92ms
step:111/1750 train_time:10314ms step_avg:92.92ms
step:112/1750 train_time:10407ms step_avg:92.92ms
step:113/1750 train_time:10501ms step_avg:92.92ms
step:114/1750 train_time:10594ms step_avg:92.93ms
step:115/1750 train_time:10687ms step_avg:92.93ms
step:116/1750 train_time:10781ms step_avg:92.94ms
step:117/1750 train_time:10874ms step_avg:92.94ms
step:118/1750 train_time:10968ms step_avg:92.95ms
step:119/1750 train_time:11061ms step_avg:92.95ms
step:120/1750 train_time:11154ms step_avg:92.95ms
step:121/1750 train_time:11247ms step_avg:92.95ms
step:122/1750 train_time:11340ms step_avg:92.95ms
step:123/1750 train_time:11434ms step_avg:92.96ms
step:124/1750 train_time:11527ms step_avg:92.96ms
step:125/1750 train_time:11621ms step_avg:92.96ms
step:125/1750 val_loss:4.6630 train_time:11704ms step_avg:93.63ms
step:126/1750 train_time:11725ms step_avg:93.06ms
step:127/1750 train_time:11813ms step_avg:93.01ms
step:128/1750 train_time:11916ms step_avg:93.09ms
step:129/1750 train_time:12011ms step_avg:93.11ms
step:130/1750 train_time:12103ms step_avg:93.10ms
step:131/1750 train_time:12196ms step_avg:93.10ms
step:132/1750 train_time:12288ms step_avg:93.09ms
step:133/1750 train_time:12381ms step_avg:93.09ms
step:134/1750 train_time:12473ms step_avg:93.08ms
step:135/1750 train_time:12565ms step_avg:93.08ms
step:136/1750 train_time:12659ms step_avg:93.08ms
step:137/1750 train_time:12753ms step_avg:93.09ms
step:138/1750 train_time:12851ms step_avg:93.12ms
step:139/1750 train_time:12947ms step_avg:93.14ms
step:140/1750 train_time:13041ms step_avg:93.15ms
step:141/1750 train_time:13134ms step_avg:93.15ms
step:142/1750 train_time:13226ms step_avg:93.14ms
step:143/1750 train_time:13319ms step_avg:93.14ms
step:144/1750 train_time:13413ms step_avg:93.15ms
step:145/1750 train_time:13507ms step_avg:93.15ms
step:146/1750 train_time:13600ms step_avg:93.15ms
step:147/1750 train_time:13693ms step_avg:93.15ms
step:148/1750 train_time:13786ms step_avg:93.15ms
step:149/1750 train_time:13882ms step_avg:93.16ms
step:150/1750 train_time:13976ms step_avg:93.18ms
step:151/1750 train_time:14070ms step_avg:93.18ms
step:152/1750 train_time:14164ms step_avg:93.19ms
step:153/1750 train_time:14257ms step_avg:93.18ms
step:154/1750 train_time:14351ms step_avg:93.19ms
step:155/1750 train_time:14445ms step_avg:93.19ms
step:156/1750 train_time:14538ms step_avg:93.19ms
step:157/1750 train_time:14631ms step_avg:93.19ms
step:158/1750 train_time:14725ms step_avg:93.20ms
step:159/1750 train_time:14819ms step_avg:93.20ms
step:160/1750 train_time:14914ms step_avg:93.21ms
step:161/1750 train_time:15008ms step_avg:93.22ms
step:162/1750 train_time:15102ms step_avg:93.22ms
step:163/1750 train_time:15195ms step_avg:93.22ms
step:164/1750 train_time:15289ms step_avg:93.22ms
step:165/1750 train_time:15382ms step_avg:93.23ms
step:166/1750 train_time:15475ms step_avg:93.22ms
step:167/1750 train_time:15569ms step_avg:93.23ms
step:168/1750 train_time:15662ms step_avg:93.23ms
step:169/1750 train_time:15756ms step_avg:93.23ms
step:170/1750 train_time:15851ms step_avg:93.24ms
step:171/1750 train_time:15945ms step_avg:93.25ms
step:172/1750 train_time:16039ms step_avg:93.25ms
step:173/1750 train_time:16132ms step_avg:93.25ms
step:174/1750 train_time:16226ms step_avg:93.25ms
step:175/1750 train_time:16319ms step_avg:93.25ms
step:176/1750 train_time:16412ms step_avg:93.25ms
step:177/1750 train_time:16506ms step_avg:93.26ms
step:178/1750 train_time:16600ms step_avg:93.26ms
step:179/1750 train_time:16694ms step_avg:93.26ms
step:180/1750 train_time:16787ms step_avg:93.26ms
step:181/1750 train_time:16881ms step_avg:93.27ms
step:182/1750 train_time:16975ms step_avg:93.27ms
step:183/1750 train_time:17068ms step_avg:93.27ms
step:184/1750 train_time:17162ms step_avg:93.27ms
step:185/1750 train_time:17255ms step_avg:93.27ms
step:186/1750 train_time:17348ms step_avg:93.27ms
step:187/1750 train_time:17441ms step_avg:93.27ms
step:188/1750 train_time:17535ms step_avg:93.27ms
step:189/1750 train_time:17628ms step_avg:93.27ms
step:190/1750 train_time:17722ms step_avg:93.27ms
step:191/1750 train_time:17815ms step_avg:93.27ms
step:192/1750 train_time:17908ms step_avg:93.27ms
step:193/1750 train_time:18003ms step_avg:93.28ms
step:194/1750 train_time:18096ms step_avg:93.28ms
step:195/1750 train_time:18189ms step_avg:93.28ms
step:196/1750 train_time:18282ms step_avg:93.28ms
step:197/1750 train_time:18376ms step_avg:93.28ms
step:198/1750 train_time:18469ms step_avg:93.28ms
step:199/1750 train_time:18562ms step_avg:93.28ms
step:200/1750 train_time:18657ms step_avg:93.28ms
step:201/1750 train_time:18750ms step_avg:93.29ms
step:202/1750 train_time:18845ms step_avg:93.29ms
step:203/1750 train_time:18939ms step_avg:93.29ms
step:204/1750 train_time:19032ms step_avg:93.30ms
step:205/1750 train_time:19126ms step_avg:93.30ms
step:206/1750 train_time:19219ms step_avg:93.30ms
step:207/1750 train_time:19313ms step_avg:93.30ms
step:208/1750 train_time:19407ms step_avg:93.30ms
step:209/1750 train_time:19501ms step_avg:93.30ms
step:210/1750 train_time:19594ms step_avg:93.30ms
step:211/1750 train_time:19687ms step_avg:93.30ms
step:212/1750 train_time:19781ms step_avg:93.31ms
step:213/1750 train_time:19874ms step_avg:93.31ms
step:214/1750 train_time:19968ms step_avg:93.31ms
step:215/1750 train_time:20061ms step_avg:93.31ms
step:216/1750 train_time:20155ms step_avg:93.31ms
step:217/1750 train_time:20248ms step_avg:93.31ms
step:218/1750 train_time:20342ms step_avg:93.31ms
step:219/1750 train_time:20435ms step_avg:93.31ms
step:220/1750 train_time:20529ms step_avg:93.31ms
step:221/1750 train_time:20623ms step_avg:93.32ms
step:222/1750 train_time:20717ms step_avg:93.32ms
step:223/1750 train_time:20810ms step_avg:93.32ms
step:224/1750 train_time:20904ms step_avg:93.32ms
step:225/1750 train_time:20998ms step_avg:93.32ms
step:226/1750 train_time:21091ms step_avg:93.32ms
step:227/1750 train_time:21185ms step_avg:93.32ms
step:228/1750 train_time:21278ms step_avg:93.32ms
step:229/1750 train_time:21372ms step_avg:93.33ms
step:230/1750 train_time:21465ms step_avg:93.33ms
step:231/1750 train_time:21558ms step_avg:93.33ms
step:232/1750 train_time:21652ms step_avg:93.33ms
step:233/1750 train_time:21746ms step_avg:93.33ms
step:234/1750 train_time:21839ms step_avg:93.33ms
step:235/1750 train_time:21933ms step_avg:93.33ms
step:236/1750 train_time:22026ms step_avg:93.33ms
step:237/1750 train_time:22120ms step_avg:93.34ms
step:238/1750 train_time:22214ms step_avg:93.34ms
step:239/1750 train_time:22308ms step_avg:93.34ms
step:240/1750 train_time:22402ms step_avg:93.34ms
step:241/1750 train_time:22495ms step_avg:93.34ms
step:242/1750 train_time:22589ms step_avg:93.34ms
step:243/1750 train_time:22682ms step_avg:93.34ms
step:244/1750 train_time:22777ms step_avg:93.35ms
step:245/1750 train_time:22870ms step_avg:93.35ms
step:246/1750 train_time:22964ms step_avg:93.35ms
step:247/1750 train_time:23057ms step_avg:93.35ms
step:248/1750 train_time:23151ms step_avg:93.35ms
step:249/1750 train_time:23245ms step_avg:93.35ms
step:250/1750 train_time:23339ms step_avg:93.36ms
step:250/1750 val_loss:4.1077 train_time:23422ms step_avg:93.69ms
step:251/1750 train_time:23444ms step_avg:93.40ms
step:252/1750 train_time:23531ms step_avg:93.38ms
step:253/1750 train_time:23627ms step_avg:93.39ms
step:254/1750 train_time:23721ms step_avg:93.39ms
step:255/1750 train_time:23814ms step_avg:93.39ms
step:256/1750 train_time:23908ms step_avg:93.39ms
step:257/1750 train_time:24000ms step_avg:93.39ms
step:258/1750 train_time:24094ms step_avg:93.39ms
step:259/1750 train_time:24187ms step_avg:93.39ms
step:260/1750 train_time:24279ms step_avg:93.38ms
step:261/1750 train_time:24373ms step_avg:93.38ms
step:262/1750 train_time:24468ms step_avg:93.39ms
step:263/1750 train_time:24563ms step_avg:93.40ms
step:264/1750 train_time:24660ms step_avg:93.41ms
step:265/1750 train_time:24755ms step_avg:93.41ms
step:266/1750 train_time:24849ms step_avg:93.42ms
step:267/1750 train_time:24943ms step_avg:93.42ms
step:268/1750 train_time:25036ms step_avg:93.42ms
step:269/1750 train_time:25130ms step_avg:93.42ms
step:270/1750 train_time:25223ms step_avg:93.42ms
step:271/1750 train_time:25317ms step_avg:93.42ms
step:272/1750 train_time:25412ms step_avg:93.43ms
step:273/1750 train_time:25506ms step_avg:93.43ms
step:274/1750 train_time:25601ms step_avg:93.43ms
step:275/1750 train_time:25697ms step_avg:93.44ms
step:276/1750 train_time:25791ms step_avg:93.45ms
step:277/1750 train_time:25885ms step_avg:93.45ms
step:278/1750 train_time:25978ms step_avg:93.45ms
step:279/1750 train_time:26072ms step_avg:93.45ms
step:280/1750 train_time:26166ms step_avg:93.45ms
step:281/1750 train_time:26260ms step_avg:93.45ms
step:282/1750 train_time:26353ms step_avg:93.45ms
step:283/1750 train_time:26447ms step_avg:93.45ms
step:284/1750 train_time:26542ms step_avg:93.46ms
step:285/1750 train_time:26637ms step_avg:93.46ms
step:286/1750 train_time:26733ms step_avg:93.47ms
step:287/1750 train_time:26827ms step_avg:93.47ms
step:288/1750 train_time:26921ms step_avg:93.48ms
step:289/1750 train_time:27016ms step_avg:93.48ms
step:290/1750 train_time:27109ms step_avg:93.48ms
step:291/1750 train_time:27202ms step_avg:93.48ms
step:292/1750 train_time:27296ms step_avg:93.48ms
step:293/1750 train_time:27390ms step_avg:93.48ms
step:294/1750 train_time:27483ms step_avg:93.48ms
step:295/1750 train_time:27578ms step_avg:93.48ms
step:296/1750 train_time:27673ms step_avg:93.49ms
step:297/1750 train_time:27768ms step_avg:93.49ms
step:298/1750 train_time:27863ms step_avg:93.50ms
step:299/1750 train_time:27957ms step_avg:93.50ms
step:300/1750 train_time:28051ms step_avg:93.50ms
step:301/1750 train_time:28145ms step_avg:93.50ms
step:302/1750 train_time:28239ms step_avg:93.51ms
step:303/1750 train_time:28333ms step_avg:93.51ms
step:304/1750 train_time:28426ms step_avg:93.51ms
step:305/1750 train_time:28521ms step_avg:93.51ms
step:306/1750 train_time:28615ms step_avg:93.51ms
step:307/1750 train_time:28711ms step_avg:93.52ms
step:308/1750 train_time:28805ms step_avg:93.52ms
step:309/1750 train_time:28899ms step_avg:93.52ms
step:310/1750 train_time:28993ms step_avg:93.53ms
step:311/1750 train_time:29088ms step_avg:93.53ms
step:312/1750 train_time:29181ms step_avg:93.53ms
step:313/1750 train_time:29275ms step_avg:93.53ms
step:314/1750 train_time:29369ms step_avg:93.53ms
step:315/1750 train_time:29463ms step_avg:93.53ms
step:316/1750 train_time:29557ms step_avg:93.54ms
step:317/1750 train_time:29652ms step_avg:93.54ms
step:318/1750 train_time:29747ms step_avg:93.54ms
step:319/1750 train_time:29842ms step_avg:93.55ms
step:320/1750 train_time:29935ms step_avg:93.55ms
step:321/1750 train_time:30029ms step_avg:93.55ms
step:322/1750 train_time:30123ms step_avg:93.55ms
step:323/1750 train_time:30217ms step_avg:93.55ms
step:324/1750 train_time:30312ms step_avg:93.55ms
step:325/1750 train_time:30406ms step_avg:93.56ms
step:326/1750 train_time:30500ms step_avg:93.56ms
step:327/1750 train_time:30594ms step_avg:93.56ms
step:328/1750 train_time:30688ms step_avg:93.56ms
step:329/1750 train_time:30782ms step_avg:93.56ms
step:330/1750 train_time:30876ms step_avg:93.56ms
step:331/1750 train_time:30970ms step_avg:93.57ms
step:332/1750 train_time:31064ms step_avg:93.57ms
step:333/1750 train_time:31157ms step_avg:93.57ms
step:334/1750 train_time:31251ms step_avg:93.57ms
step:335/1750 train_time:31345ms step_avg:93.57ms
step:336/1750 train_time:31439ms step_avg:93.57ms
step:337/1750 train_time:31533ms step_avg:93.57ms
step:338/1750 train_time:31628ms step_avg:93.57ms
step:339/1750 train_time:31722ms step_avg:93.57ms
step:340/1750 train_time:31816ms step_avg:93.58ms
step:341/1750 train_time:31911ms step_avg:93.58ms
step:342/1750 train_time:32004ms step_avg:93.58ms
step:343/1750 train_time:32098ms step_avg:93.58ms
step:344/1750 train_time:32192ms step_avg:93.58ms
step:345/1750 train_time:32286ms step_avg:93.58ms
step:346/1750 train_time:32380ms step_avg:93.58ms
step:347/1750 train_time:32474ms step_avg:93.58ms
step:348/1750 train_time:32567ms step_avg:93.58ms
step:349/1750 train_time:32661ms step_avg:93.59ms
step:350/1750 train_time:32755ms step_avg:93.59ms
step:351/1750 train_time:32851ms step_avg:93.59ms
step:352/1750 train_time:32945ms step_avg:93.59ms
step:353/1750 train_time:33038ms step_avg:93.59ms
step:354/1750 train_time:33133ms step_avg:93.59ms
step:355/1750 train_time:33227ms step_avg:93.60ms
step:356/1750 train_time:33322ms step_avg:93.60ms
step:357/1750 train_time:33416ms step_avg:93.60ms
step:358/1750 train_time:33510ms step_avg:93.60ms
step:359/1750 train_time:33604ms step_avg:93.61ms
step:360/1750 train_time:33698ms step_avg:93.61ms
step:361/1750 train_time:33792ms step_avg:93.61ms
step:362/1750 train_time:33887ms step_avg:93.61ms
step:363/1750 train_time:33981ms step_avg:93.61ms
step:364/1750 train_time:34076ms step_avg:93.61ms
step:365/1750 train_time:34171ms step_avg:93.62ms
step:366/1750 train_time:34265ms step_avg:93.62ms
step:367/1750 train_time:34359ms step_avg:93.62ms
step:368/1750 train_time:34453ms step_avg:93.62ms
step:369/1750 train_time:34548ms step_avg:93.63ms
step:370/1750 train_time:34642ms step_avg:93.63ms
step:371/1750 train_time:34737ms step_avg:93.63ms
step:372/1750 train_time:34831ms step_avg:93.63ms
step:373/1750 train_time:34925ms step_avg:93.63ms
step:374/1750 train_time:35020ms step_avg:93.64ms
step:375/1750 train_time:35114ms step_avg:93.64ms
step:375/1750 val_loss:3.9022 train_time:35198ms step_avg:93.86ms
step:376/1750 train_time:35219ms step_avg:93.67ms
step:377/1750 train_time:35310ms step_avg:93.66ms
step:378/1750 train_time:35406ms step_avg:93.67ms
step:379/1750 train_time:35501ms step_avg:93.67ms
step:380/1750 train_time:35595ms step_avg:93.67ms
step:381/1750 train_time:35688ms step_avg:93.67ms
step:382/1750 train_time:35782ms step_avg:93.67ms
step:383/1750 train_time:35875ms step_avg:93.67ms
step:384/1750 train_time:35968ms step_avg:93.67ms
step:385/1750 train_time:36061ms step_avg:93.67ms
step:386/1750 train_time:36156ms step_avg:93.67ms
step:387/1750 train_time:36251ms step_avg:93.67ms
step:388/1750 train_time:36348ms step_avg:93.68ms
step:389/1750 train_time:36443ms step_avg:93.68ms
step:390/1750 train_time:36538ms step_avg:93.69ms
step:391/1750 train_time:36633ms step_avg:93.69ms
step:392/1750 train_time:36729ms step_avg:93.70ms
step:393/1750 train_time:36825ms step_avg:93.70ms
step:394/1750 train_time:36920ms step_avg:93.71ms
step:395/1750 train_time:37014ms step_avg:93.71ms
step:396/1750 train_time:37109ms step_avg:93.71ms
step:397/1750 train_time:37205ms step_avg:93.72ms
step:398/1750 train_time:37302ms step_avg:93.72ms
step:399/1750 train_time:37400ms step_avg:93.73ms
step:400/1750 train_time:37496ms step_avg:93.74ms
step:401/1750 train_time:37592ms step_avg:93.75ms
step:402/1750 train_time:37688ms step_avg:93.75ms
step:403/1750 train_time:37784ms step_avg:93.76ms
step:404/1750 train_time:37879ms step_avg:93.76ms
step:405/1750 train_time:37975ms step_avg:93.76ms
step:406/1750 train_time:38070ms step_avg:93.77ms
step:407/1750 train_time:38166ms step_avg:93.77ms
step:408/1750 train_time:38262ms step_avg:93.78ms
step:409/1750 train_time:38358ms step_avg:93.78ms
step:410/1750 train_time:38454ms step_avg:93.79ms
step:411/1750 train_time:38550ms step_avg:93.80ms
step:412/1750 train_time:38647ms step_avg:93.80ms
step:413/1750 train_time:38743ms step_avg:93.81ms
step:414/1750 train_time:38839ms step_avg:93.81ms
step:415/1750 train_time:38934ms step_avg:93.82ms
step:416/1750 train_time:39030ms step_avg:93.82ms
step:417/1750 train_time:39125ms step_avg:93.83ms
step:418/1750 train_time:39221ms step_avg:93.83ms
step:419/1750 train_time:39318ms step_avg:93.84ms
step:420/1750 train_time:39413ms step_avg:93.84ms
step:421/1750 train_time:39509ms step_avg:93.85ms
step:422/1750 train_time:39605ms step_avg:93.85ms
step:423/1750 train_time:39702ms step_avg:93.86ms
step:424/1750 train_time:39798ms step_avg:93.86ms
step:425/1750 train_time:39893ms step_avg:93.87ms
step:426/1750 train_time:39988ms step_avg:93.87ms
step:427/1750 train_time:40084ms step_avg:93.87ms
step:428/1750 train_time:40180ms step_avg:93.88ms
step:429/1750 train_time:40276ms step_avg:93.88ms
step:430/1750 train_time:40373ms step_avg:93.89ms
step:431/1750 train_time:40469ms step_avg:93.89ms
step:432/1750 train_time:40564ms step_avg:93.90ms
step:433/1750 train_time:40660ms step_avg:93.90ms
step:434/1750 train_time:40756ms step_avg:93.91ms
step:435/1750 train_time:40853ms step_avg:93.91ms
step:436/1750 train_time:40949ms step_avg:93.92ms
step:437/1750 train_time:41044ms step_avg:93.92ms
step:438/1750 train_time:41140ms step_avg:93.93ms
step:439/1750 train_time:41236ms step_avg:93.93ms
step:440/1750 train_time:41332ms step_avg:93.94ms
step:441/1750 train_time:41428ms step_avg:93.94ms
step:442/1750 train_time:41524ms step_avg:93.95ms
step:443/1750 train_time:41620ms step_avg:93.95ms
step:444/1750 train_time:41715ms step_avg:93.95ms
step:445/1750 train_time:41811ms step_avg:93.96ms
step:446/1750 train_time:41907ms step_avg:93.96ms
step:447/1750 train_time:42002ms step_avg:93.96ms
step:448/1750 train_time:42099ms step_avg:93.97ms
step:449/1750 train_time:42195ms step_avg:93.97ms
step:450/1750 train_time:42290ms step_avg:93.98ms
step:451/1750 train_time:42387ms step_avg:93.98ms
step:452/1750 train_time:42482ms step_avg:93.99ms
step:453/1750 train_time:42579ms step_avg:93.99ms
step:454/1750 train_time:42675ms step_avg:94.00ms
step:455/1750 train_time:42770ms step_avg:94.00ms
step:456/1750 train_time:42866ms step_avg:94.00ms
step:457/1750 train_time:42962ms step_avg:94.01ms
step:458/1750 train_time:43057ms step_avg:94.01ms
step:459/1750 train_time:43154ms step_avg:94.02ms
step:460/1750 train_time:43250ms step_avg:94.02ms
step:461/1750 train_time:43346ms step_avg:94.03ms
step:462/1750 train_time:43442ms step_avg:94.03ms
step:463/1750 train_time:43538ms step_avg:94.03ms
step:464/1750 train_time:43633ms step_avg:94.04ms
step:465/1750 train_time:43729ms step_avg:94.04ms
step:466/1750 train_time:43825ms step_avg:94.04ms
step:467/1750 train_time:43921ms step_avg:94.05ms
step:468/1750 train_time:44016ms step_avg:94.05ms
step:469/1750 train_time:44112ms step_avg:94.05ms
step:470/1750 train_time:44207ms step_avg:94.06ms
step:471/1750 train_time:44303ms step_avg:94.06ms
step:472/1750 train_time:44399ms step_avg:94.07ms
step:473/1750 train_time:44495ms step_avg:94.07ms
step:474/1750 train_time:44592ms step_avg:94.07ms
step:475/1750 train_time:44687ms step_avg:94.08ms
step:476/1750 train_time:44784ms step_avg:94.08ms
step:477/1750 train_time:44880ms step_avg:94.09ms
step:478/1750 train_time:44975ms step_avg:94.09ms
step:479/1750 train_time:45071ms step_avg:94.09ms
step:480/1750 train_time:45166ms step_avg:94.10ms
step:481/1750 train_time:45262ms step_avg:94.10ms
step:482/1750 train_time:45358ms step_avg:94.10ms
step:483/1750 train_time:45454ms step_avg:94.11ms
step:484/1750 train_time:45550ms step_avg:94.11ms
step:485/1750 train_time:45645ms step_avg:94.11ms
step:486/1750 train_time:45741ms step_avg:94.12ms
step:487/1750 train_time:45838ms step_avg:94.12ms
step:488/1750 train_time:45933ms step_avg:94.13ms
step:489/1750 train_time:46029ms step_avg:94.13ms
step:490/1750 train_time:46125ms step_avg:94.13ms
step:491/1750 train_time:46221ms step_avg:94.14ms
step:492/1750 train_time:46317ms step_avg:94.14ms
step:493/1750 train_time:46413ms step_avg:94.14ms
step:494/1750 train_time:46509ms step_avg:94.15ms
step:495/1750 train_time:46605ms step_avg:94.15ms
step:496/1750 train_time:46700ms step_avg:94.15ms
step:497/1750 train_time:46796ms step_avg:94.16ms
step:498/1750 train_time:46892ms step_avg:94.16ms
step:499/1750 train_time:46988ms step_avg:94.16ms
step:500/1750 train_time:47084ms step_avg:94.17ms
step:500/1750 val_loss:3.7516 train_time:47169ms step_avg:94.34ms
step:501/1750 train_time:47190ms step_avg:94.19ms
step:502/1750 train_time:47282ms step_avg:94.19ms
step:503/1750 train_time:47379ms step_avg:94.19ms
step:504/1750 train_time:47476ms step_avg:94.20ms
step:505/1750 train_time:47572ms step_avg:94.20ms
step:506/1750 train_time:47667ms step_avg:94.20ms
step:507/1750 train_time:47763ms step_avg:94.21ms
step:508/1750 train_time:47858ms step_avg:94.21ms
step:509/1750 train_time:47953ms step_avg:94.21ms
step:510/1750 train_time:48048ms step_avg:94.21ms
step:511/1750 train_time:48144ms step_avg:94.22ms
step:512/1750 train_time:48241ms step_avg:94.22ms
step:513/1750 train_time:48338ms step_avg:94.23ms
step:514/1750 train_time:48434ms step_avg:94.23ms
step:515/1750 train_time:48530ms step_avg:94.23ms
step:516/1750 train_time:48627ms step_avg:94.24ms
step:517/1750 train_time:48722ms step_avg:94.24ms
step:518/1750 train_time:48817ms step_avg:94.24ms
step:519/1750 train_time:48912ms step_avg:94.24ms
step:520/1750 train_time:49008ms step_avg:94.25ms
step:521/1750 train_time:49104ms step_avg:94.25ms
step:522/1750 train_time:49201ms step_avg:94.25ms
step:523/1750 train_time:49297ms step_avg:94.26ms
step:524/1750 train_time:49394ms step_avg:94.26ms
step:525/1750 train_time:49491ms step_avg:94.27ms
step:526/1750 train_time:49587ms step_avg:94.27ms
step:527/1750 train_time:49683ms step_avg:94.28ms
step:528/1750 train_time:49780ms step_avg:94.28ms
step:529/1750 train_time:49875ms step_avg:94.28ms
step:530/1750 train_time:49971ms step_avg:94.29ms
step:531/1750 train_time:50067ms step_avg:94.29ms
step:532/1750 train_time:50164ms step_avg:94.29ms
step:533/1750 train_time:50261ms step_avg:94.30ms
step:534/1750 train_time:50358ms step_avg:94.30ms
step:535/1750 train_time:50454ms step_avg:94.31ms
step:536/1750 train_time:50551ms step_avg:94.31ms
step:537/1750 train_time:50647ms step_avg:94.31ms
step:538/1750 train_time:50743ms step_avg:94.32ms
step:539/1750 train_time:50840ms step_avg:94.32ms
step:540/1750 train_time:50936ms step_avg:94.33ms
step:541/1750 train_time:51031ms step_avg:94.33ms
step:542/1750 train_time:51128ms step_avg:94.33ms
step:543/1750 train_time:51224ms step_avg:94.34ms
step:544/1750 train_time:51321ms step_avg:94.34ms
step:545/1750 train_time:51417ms step_avg:94.34ms
step:546/1750 train_time:51514ms step_avg:94.35ms
step:547/1750 train_time:51611ms step_avg:94.35ms
step:548/1750 train_time:51707ms step_avg:94.36ms
step:549/1750 train_time:51803ms step_avg:94.36ms
step:550/1750 train_time:51900ms step_avg:94.36ms
step:551/1750 train_time:51995ms step_avg:94.37ms
step:552/1750 train_time:52092ms step_avg:94.37ms
step:553/1750 train_time:52188ms step_avg:94.37ms
step:554/1750 train_time:52285ms step_avg:94.38ms
step:555/1750 train_time:52382ms step_avg:94.38ms
step:556/1750 train_time:52479ms step_avg:94.39ms
step:557/1750 train_time:52575ms step_avg:94.39ms
step:558/1750 train_time:52672ms step_avg:94.39ms
step:559/1750 train_time:52768ms step_avg:94.40ms
step:560/1750 train_time:52864ms step_avg:94.40ms
step:561/1750 train_time:52960ms step_avg:94.40ms
step:562/1750 train_time:53055ms step_avg:94.40ms
step:563/1750 train_time:53151ms step_avg:94.41ms
step:564/1750 train_time:53248ms step_avg:94.41ms
step:565/1750 train_time:53346ms step_avg:94.42ms
step:566/1750 train_time:53442ms step_avg:94.42ms
step:567/1750 train_time:53539ms step_avg:94.43ms
step:568/1750 train_time:53635ms step_avg:94.43ms
step:569/1750 train_time:53732ms step_avg:94.43ms
step:570/1750 train_time:53829ms step_avg:94.44ms
step:571/1750 train_time:53925ms step_avg:94.44ms
step:572/1750 train_time:54022ms step_avg:94.44ms
step:573/1750 train_time:54118ms step_avg:94.45ms
step:574/1750 train_time:54215ms step_avg:94.45ms
step:575/1750 train_time:54312ms step_avg:94.45ms
step:576/1750 train_time:54408ms step_avg:94.46ms
step:577/1750 train_time:54504ms step_avg:94.46ms
step:578/1750 train_time:54601ms step_avg:94.46ms
step:579/1750 train_time:54697ms step_avg:94.47ms
step:580/1750 train_time:54793ms step_avg:94.47ms
step:581/1750 train_time:54890ms step_avg:94.47ms
step:582/1750 train_time:54986ms step_avg:94.48ms
step:583/1750 train_time:55082ms step_avg:94.48ms
step:584/1750 train_time:55178ms step_avg:94.48ms
step:585/1750 train_time:55275ms step_avg:94.49ms
step:586/1750 train_time:55373ms step_avg:94.49ms
step:587/1750 train_time:55470ms step_avg:94.50ms
step:588/1750 train_time:55566ms step_avg:94.50ms
step:589/1750 train_time:55663ms step_avg:94.50ms
step:590/1750 train_time:55760ms step_avg:94.51ms
step:591/1750 train_time:55856ms step_avg:94.51ms
step:592/1750 train_time:55953ms step_avg:94.51ms
step:593/1750 train_time:56049ms step_avg:94.52ms
step:594/1750 train_time:56145ms step_avg:94.52ms
step:595/1750 train_time:56241ms step_avg:94.52ms
step:596/1750 train_time:56337ms step_avg:94.53ms
step:597/1750 train_time:56434ms step_avg:94.53ms
step:598/1750 train_time:56530ms step_avg:94.53ms
step:599/1750 train_time:56626ms step_avg:94.53ms
step:600/1750 train_time:56723ms step_avg:94.54ms
step:601/1750 train_time:56819ms step_avg:94.54ms
step:602/1750 train_time:56915ms step_avg:94.54ms
step:603/1750 train_time:57011ms step_avg:94.55ms
step:604/1750 train_time:57107ms step_avg:94.55ms
step:605/1750 train_time:57203ms step_avg:94.55ms
step:606/1750 train_time:57300ms step_avg:94.55ms
step:607/1750 train_time:57396ms step_avg:94.56ms
step:608/1750 train_time:57491ms step_avg:94.56ms
step:609/1750 train_time:57587ms step_avg:94.56ms
step:610/1750 train_time:57684ms step_avg:94.56ms
step:611/1750 train_time:57780ms step_avg:94.57ms
step:612/1750 train_time:57877ms step_avg:94.57ms
step:613/1750 train_time:57974ms step_avg:94.57ms
step:614/1750 train_time:58070ms step_avg:94.58ms
step:615/1750 train_time:58165ms step_avg:94.58ms
step:616/1750 train_time:58263ms step_avg:94.58ms
step:617/1750 train_time:58360ms step_avg:94.59ms
step:618/1750 train_time:58456ms step_avg:94.59ms
step:619/1750 train_time:58553ms step_avg:94.59ms
step:620/1750 train_time:58650ms step_avg:94.60ms
step:621/1750 train_time:58747ms step_avg:94.60ms
step:622/1750 train_time:58843ms step_avg:94.60ms
step:623/1750 train_time:58939ms step_avg:94.61ms
step:624/1750 train_time:59036ms step_avg:94.61ms
step:625/1750 train_time:59132ms step_avg:94.61ms
step:625/1750 val_loss:3.6615 train_time:59217ms step_avg:94.75ms
step:626/1750 train_time:59238ms step_avg:94.63ms
step:627/1750 train_time:59331ms step_avg:94.63ms
step:628/1750 train_time:59430ms step_avg:94.63ms
step:629/1750 train_time:59527ms step_avg:94.64ms
step:630/1750 train_time:59622ms step_avg:94.64ms
step:631/1750 train_time:59718ms step_avg:94.64ms
step:632/1750 train_time:59813ms step_avg:94.64ms
step:633/1750 train_time:59909ms step_avg:94.64ms
step:634/1750 train_time:60004ms step_avg:94.64ms
step:635/1750 train_time:60099ms step_avg:94.64ms
step:636/1750 train_time:60197ms step_avg:94.65ms
step:637/1750 train_time:60294ms step_avg:94.65ms
step:638/1750 train_time:60392ms step_avg:94.66ms
step:639/1750 train_time:60489ms step_avg:94.66ms
step:640/1750 train_time:60585ms step_avg:94.66ms
step:641/1750 train_time:60681ms step_avg:94.67ms
step:642/1750 train_time:60777ms step_avg:94.67ms
step:643/1750 train_time:60872ms step_avg:94.67ms
step:644/1750 train_time:60968ms step_avg:94.67ms
step:645/1750 train_time:61064ms step_avg:94.67ms
step:646/1750 train_time:61160ms step_avg:94.67ms
step:647/1750 train_time:61257ms step_avg:94.68ms
step:648/1750 train_time:61354ms step_avg:94.68ms
step:649/1750 train_time:61451ms step_avg:94.69ms
step:650/1750 train_time:61549ms step_avg:94.69ms
step:651/1750 train_time:61646ms step_avg:94.69ms
step:652/1750 train_time:61744ms step_avg:94.70ms
step:653/1750 train_time:61841ms step_avg:94.70ms
step:654/1750 train_time:61939ms step_avg:94.71ms
step:655/1750 train_time:62036ms step_avg:94.71ms
step:656/1750 train_time:62133ms step_avg:94.72ms
step:657/1750 train_time:62232ms step_avg:94.72ms
step:658/1750 train_time:62330ms step_avg:94.73ms
step:659/1750 train_time:62428ms step_avg:94.73ms
step:660/1750 train_time:62525ms step_avg:94.74ms
step:661/1750 train_time:62623ms step_avg:94.74ms
step:662/1750 train_time:62720ms step_avg:94.74ms
step:663/1750 train_time:62818ms step_avg:94.75ms
step:664/1750 train_time:62917ms step_avg:94.75ms
step:665/1750 train_time:63014ms step_avg:94.76ms
step:666/1750 train_time:63112ms step_avg:94.76ms
step:667/1750 train_time:63210ms step_avg:94.77ms
step:668/1750 train_time:63308ms step_avg:94.77ms
step:669/1750 train_time:63406ms step_avg:94.78ms
step:670/1750 train_time:63503ms step_avg:94.78ms
step:671/1750 train_time:63601ms step_avg:94.79ms
step:672/1750 train_time:63699ms step_avg:94.79ms
step:673/1750 train_time:63797ms step_avg:94.79ms
step:674/1750 train_time:63894ms step_avg:94.80ms
step:675/1750 train_time:63992ms step_avg:94.80ms
step:676/1750 train_time:64090ms step_avg:94.81ms
step:677/1750 train_time:64187ms step_avg:94.81ms
step:678/1750 train_time:64285ms step_avg:94.82ms
step:679/1750 train_time:64383ms step_avg:94.82ms
step:680/1750 train_time:64481ms step_avg:94.83ms
step:681/1750 train_time:64579ms step_avg:94.83ms
step:682/1750 train_time:64676ms step_avg:94.83ms
step:683/1750 train_time:64774ms step_avg:94.84ms
step:684/1750 train_time:64872ms step_avg:94.84ms
step:685/1750 train_time:64970ms step_avg:94.85ms
step:686/1750 train_time:65067ms step_avg:94.85ms
step:687/1750 train_time:65164ms step_avg:94.85ms
step:688/1750 train_time:65263ms step_avg:94.86ms
step:689/1750 train_time:65360ms step_avg:94.86ms
step:690/1750 train_time:65457ms step_avg:94.87ms
step:691/1750 train_time:65555ms step_avg:94.87ms
step:692/1750 train_time:65653ms step_avg:94.87ms
step:693/1750 train_time:65751ms step_avg:94.88ms
step:694/1750 train_time:65849ms step_avg:94.88ms
step:695/1750 train_time:65947ms step_avg:94.89ms
step:696/1750 train_time:66044ms step_avg:94.89ms
step:697/1750 train_time:66143ms step_avg:94.90ms
step:698/1750 train_time:66241ms step_avg:94.90ms
step:699/1750 train_time:66339ms step_avg:94.91ms
step:700/1750 train_time:66438ms step_avg:94.91ms
step:701/1750 train_time:66535ms step_avg:94.91ms
step:702/1750 train_time:66633ms step_avg:94.92ms
step:703/1750 train_time:66731ms step_avg:94.92ms
step:704/1750 train_time:66829ms step_avg:94.93ms
step:705/1750 train_time:66926ms step_avg:94.93ms
step:706/1750 train_time:67024ms step_avg:94.93ms
step:707/1750 train_time:67122ms step_avg:94.94ms
step:708/1750 train_time:67219ms step_avg:94.94ms
step:709/1750 train_time:67317ms step_avg:94.95ms
step:710/1750 train_time:67414ms step_avg:94.95ms
step:711/1750 train_time:67512ms step_avg:94.95ms
step:712/1750 train_time:67610ms step_avg:94.96ms
step:713/1750 train_time:67708ms step_avg:94.96ms
step:714/1750 train_time:67806ms step_avg:94.97ms
step:715/1750 train_time:67904ms step_avg:94.97ms
step:716/1750 train_time:68002ms step_avg:94.97ms
step:717/1750 train_time:68100ms step_avg:94.98ms
step:718/1750 train_time:68197ms step_avg:94.98ms
step:719/1750 train_time:68294ms step_avg:94.99ms
step:720/1750 train_time:68392ms step_avg:94.99ms
step:721/1750 train_time:68490ms step_avg:94.99ms
step:722/1750 train_time:68587ms step_avg:95.00ms
step:723/1750 train_time:68684ms step_avg:95.00ms
step:724/1750 train_time:68783ms step_avg:95.00ms
step:725/1750 train_time:68881ms step_avg:95.01ms
step:726/1750 train_time:68980ms step_avg:95.01ms
step:727/1750 train_time:69078ms step_avg:95.02ms
step:728/1750 train_time:69176ms step_avg:95.02ms
step:729/1750 train_time:69273ms step_avg:95.03ms
step:730/1750 train_time:69371ms step_avg:95.03ms
step:731/1750 train_time:69469ms step_avg:95.03ms
step:732/1750 train_time:69566ms step_avg:95.04ms
step:733/1750 train_time:69664ms step_avg:95.04ms
step:734/1750 train_time:69763ms step_avg:95.04ms
step:735/1750 train_time:69860ms step_avg:95.05ms
step:736/1750 train_time:69957ms step_avg:95.05ms
step:737/1750 train_time:70055ms step_avg:95.05ms
step:738/1750 train_time:70153ms step_avg:95.06ms
step:739/1750 train_time:70251ms step_avg:95.06ms
step:740/1750 train_time:70348ms step_avg:95.06ms
step:741/1750 train_time:70446ms step_avg:95.07ms
step:742/1750 train_time:70544ms step_avg:95.07ms
step:743/1750 train_time:70642ms step_avg:95.08ms
step:744/1750 train_time:70740ms step_avg:95.08ms
step:745/1750 train_time:70838ms step_avg:95.08ms
step:746/1750 train_time:70936ms step_avg:95.09ms
step:747/1750 train_time:71034ms step_avg:95.09ms
step:748/1750 train_time:71132ms step_avg:95.10ms
step:749/1750 train_time:71230ms step_avg:95.10ms
step:750/1750 train_time:71327ms step_avg:95.10ms
step:750/1750 val_loss:3.5977 train_time:71413ms step_avg:95.22ms
step:751/1750 train_time:71433ms step_avg:95.12ms
step:752/1750 train_time:71532ms step_avg:95.12ms
step:753/1750 train_time:71630ms step_avg:95.13ms
step:754/1750 train_time:71728ms step_avg:95.13ms
step:755/1750 train_time:71826ms step_avg:95.13ms
step:756/1750 train_time:71922ms step_avg:95.14ms
step:757/1750 train_time:72020ms step_avg:95.14ms
step:758/1750 train_time:72117ms step_avg:95.14ms
step:759/1750 train_time:72213ms step_avg:95.14ms
step:760/1750 train_time:72310ms step_avg:95.15ms
step:761/1750 train_time:72410ms step_avg:95.15ms
step:762/1750 train_time:72510ms step_avg:95.16ms
step:763/1750 train_time:72609ms step_avg:95.16ms
step:764/1750 train_time:72707ms step_avg:95.17ms
step:765/1750 train_time:72805ms step_avg:95.17ms
step:766/1750 train_time:72903ms step_avg:95.17ms
step:767/1750 train_time:72999ms step_avg:95.18ms
step:768/1750 train_time:73096ms step_avg:95.18ms
step:769/1750 train_time:73193ms step_avg:95.18ms
step:770/1750 train_time:73290ms step_avg:95.18ms
step:771/1750 train_time:73388ms step_avg:95.19ms
step:772/1750 train_time:73487ms step_avg:95.19ms
step:773/1750 train_time:73586ms step_avg:95.20ms
step:774/1750 train_time:73685ms step_avg:95.20ms
step:775/1750 train_time:73783ms step_avg:95.20ms
step:776/1750 train_time:73880ms step_avg:95.21ms
step:777/1750 train_time:73977ms step_avg:95.21ms
step:778/1750 train_time:74075ms step_avg:95.21ms
step:779/1750 train_time:74172ms step_avg:95.21ms
step:780/1750 train_time:74269ms step_avg:95.22ms
step:781/1750 train_time:74368ms step_avg:95.22ms
step:782/1750 train_time:74466ms step_avg:95.23ms
step:783/1750 train_time:74565ms step_avg:95.23ms
step:784/1750 train_time:74663ms step_avg:95.23ms
step:785/1750 train_time:74762ms step_avg:95.24ms
step:786/1750 train_time:74861ms step_avg:95.24ms
step:787/1750 train_time:74959ms step_avg:95.25ms
step:788/1750 train_time:75057ms step_avg:95.25ms
step:789/1750 train_time:75154ms step_avg:95.25ms
step:790/1750 train_time:75252ms step_avg:95.26ms
step:791/1750 train_time:75349ms step_avg:95.26ms
step:792/1750 train_time:75447ms step_avg:95.26ms
step:793/1750 train_time:75545ms step_avg:95.27ms
step:794/1750 train_time:75643ms step_avg:95.27ms
step:795/1750 train_time:75741ms step_avg:95.27ms
step:796/1750 train_time:75839ms step_avg:95.28ms
step:797/1750 train_time:75937ms step_avg:95.28ms
step:798/1750 train_time:76036ms step_avg:95.28ms
step:799/1750 train_time:76133ms step_avg:95.29ms
step:800/1750 train_time:76231ms step_avg:95.29ms
step:801/1750 train_time:76328ms step_avg:95.29ms
step:802/1750 train_time:76426ms step_avg:95.29ms
step:803/1750 train_time:76524ms step_avg:95.30ms
step:804/1750 train_time:76622ms step_avg:95.30ms
step:805/1750 train_time:76720ms step_avg:95.30ms
step:806/1750 train_time:76819ms step_avg:95.31ms
step:807/1750 train_time:76918ms step_avg:95.31ms
step:808/1750 train_time:77016ms step_avg:95.32ms
step:809/1750 train_time:77114ms step_avg:95.32ms
step:810/1750 train_time:77212ms step_avg:95.32ms
step:811/1750 train_time:77310ms step_avg:95.33ms
step:812/1750 train_time:77408ms step_avg:95.33ms
step:813/1750 train_time:77505ms step_avg:95.33ms
step:814/1750 train_time:77603ms step_avg:95.34ms
step:815/1750 train_time:77700ms step_avg:95.34ms
step:816/1750 train_time:77798ms step_avg:95.34ms
step:817/1750 train_time:77896ms step_avg:95.34ms
step:818/1750 train_time:77994ms step_avg:95.35ms
step:819/1750 train_time:78092ms step_avg:95.35ms
step:820/1750 train_time:78190ms step_avg:95.35ms
step:821/1750 train_time:78288ms step_avg:95.36ms
step:822/1750 train_time:78386ms step_avg:95.36ms
step:823/1750 train_time:78484ms step_avg:95.36ms
step:824/1750 train_time:78583ms step_avg:95.37ms
step:825/1750 train_time:78681ms step_avg:95.37ms
step:826/1750 train_time:78779ms step_avg:95.37ms
step:827/1750 train_time:78877ms step_avg:95.38ms
step:828/1750 train_time:78975ms step_avg:95.38ms
step:829/1750 train_time:79073ms step_avg:95.38ms
step:830/1750 train_time:79170ms step_avg:95.39ms
step:831/1750 train_time:79268ms step_avg:95.39ms
step:832/1750 train_time:79367ms step_avg:95.39ms
step:833/1750 train_time:79465ms step_avg:95.40ms
step:834/1750 train_time:79563ms step_avg:95.40ms
step:835/1750 train_time:79661ms step_avg:95.40ms
step:836/1750 train_time:79759ms step_avg:95.41ms
step:837/1750 train_time:79858ms step_avg:95.41ms
step:838/1750 train_time:79955ms step_avg:95.41ms
step:839/1750 train_time:80054ms step_avg:95.42ms
step:840/1750 train_time:80152ms step_avg:95.42ms
step:841/1750 train_time:80249ms step_avg:95.42ms
step:842/1750 train_time:80348ms step_avg:95.43ms
step:843/1750 train_time:80446ms step_avg:95.43ms
step:844/1750 train_time:80544ms step_avg:95.43ms
step:845/1750 train_time:80641ms step_avg:95.43ms
step:846/1750 train_time:80739ms step_avg:95.44ms
step:847/1750 train_time:80837ms step_avg:95.44ms
step:848/1750 train_time:80936ms step_avg:95.44ms
step:849/1750 train_time:81034ms step_avg:95.45ms
step:850/1750 train_time:81132ms step_avg:95.45ms
step:851/1750 train_time:81229ms step_avg:95.45ms
step:852/1750 train_time:81328ms step_avg:95.45ms
step:853/1750 train_time:81425ms step_avg:95.46ms
step:854/1750 train_time:81523ms step_avg:95.46ms
step:855/1750 train_time:81621ms step_avg:95.46ms
step:856/1750 train_time:81719ms step_avg:95.47ms
step:857/1750 train_time:81818ms step_avg:95.47ms
step:858/1750 train_time:81916ms step_avg:95.47ms
step:859/1750 train_time:82015ms step_avg:95.48ms
step:860/1750 train_time:82113ms step_avg:95.48ms
step:861/1750 train_time:82210ms step_avg:95.48ms
step:862/1750 train_time:82308ms step_avg:95.48ms
step:863/1750 train_time:82406ms step_avg:95.49ms
step:864/1750 train_time:82504ms step_avg:95.49ms
step:865/1750 train_time:82602ms step_avg:95.49ms
step:866/1750 train_time:82700ms step_avg:95.50ms
step:867/1750 train_time:82797ms step_avg:95.50ms
step:868/1750 train_time:82896ms step_avg:95.50ms
step:869/1750 train_time:82994ms step_avg:95.50ms
step:870/1750 train_time:83091ms step_avg:95.51ms
step:871/1750 train_time:83190ms step_avg:95.51ms
step:872/1750 train_time:83287ms step_avg:95.51ms
step:873/1750 train_time:83386ms step_avg:95.52ms
step:874/1750 train_time:83484ms step_avg:95.52ms
step:875/1750 train_time:83582ms step_avg:95.52ms
step:875/1750 val_loss:3.5483 train_time:83669ms step_avg:95.62ms
step:876/1750 train_time:83690ms step_avg:95.54ms
step:877/1750 train_time:83787ms step_avg:95.54ms
step:878/1750 train_time:83885ms step_avg:95.54ms
step:879/1750 train_time:83984ms step_avg:95.54ms
step:880/1750 train_time:84082ms step_avg:95.55ms
step:881/1750 train_time:84179ms step_avg:95.55ms
step:882/1750 train_time:84276ms step_avg:95.55ms
step:883/1750 train_time:84373ms step_avg:95.55ms
step:884/1750 train_time:84471ms step_avg:95.55ms
step:885/1750 train_time:84568ms step_avg:95.56ms
step:886/1750 train_time:84667ms step_avg:95.56ms
step:887/1750 train_time:84766ms step_avg:95.56ms
step:888/1750 train_time:84865ms step_avg:95.57ms
step:889/1750 train_time:84964ms step_avg:95.57ms
step:890/1750 train_time:85063ms step_avg:95.58ms
step:891/1750 train_time:85161ms step_avg:95.58ms
step:892/1750 train_time:85258ms step_avg:95.58ms
step:893/1750 train_time:85356ms step_avg:95.58ms
step:894/1750 train_time:85453ms step_avg:95.59ms
step:895/1750 train_time:85551ms step_avg:95.59ms
step:896/1750 train_time:85649ms step_avg:95.59ms
step:897/1750 train_time:85747ms step_avg:95.59ms
step:898/1750 train_time:85846ms step_avg:95.60ms
step:899/1750 train_time:85944ms step_avg:95.60ms
step:900/1750 train_time:86042ms step_avg:95.60ms
step:901/1750 train_time:86141ms step_avg:95.61ms
step:902/1750 train_time:86238ms step_avg:95.61ms
step:903/1750 train_time:86336ms step_avg:95.61ms
step:904/1750 train_time:86434ms step_avg:95.61ms
step:905/1750 train_time:86531ms step_avg:95.61ms
step:906/1750 train_time:86628ms step_avg:95.62ms
step:907/1750 train_time:86726ms step_avg:95.62ms
step:908/1750 train_time:86824ms step_avg:95.62ms
step:909/1750 train_time:86923ms step_avg:95.63ms
step:910/1750 train_time:87023ms step_avg:95.63ms
step:911/1750 train_time:87122ms step_avg:95.63ms
step:912/1750 train_time:87222ms step_avg:95.64ms
step:913/1750 train_time:87320ms step_avg:95.64ms
step:914/1750 train_time:87421ms step_avg:95.65ms
step:915/1750 train_time:87521ms step_avg:95.65ms
step:916/1750 train_time:87621ms step_avg:95.66ms
step:917/1750 train_time:87721ms step_avg:95.66ms
step:918/1750 train_time:87820ms step_avg:95.66ms
step:919/1750 train_time:87920ms step_avg:95.67ms
step:920/1750 train_time:88021ms step_avg:95.68ms
step:921/1750 train_time:88121ms step_avg:95.68ms
step:922/1750 train_time:88220ms step_avg:95.68ms
step:923/1750 train_time:88320ms step_avg:95.69ms
step:924/1750 train_time:88420ms step_avg:95.69ms
step:925/1750 train_time:88519ms step_avg:95.70ms
step:926/1750 train_time:88619ms step_avg:95.70ms
step:927/1750 train_time:88718ms step_avg:95.70ms
step:928/1750 train_time:88817ms step_avg:95.71ms
step:929/1750 train_time:88917ms step_avg:95.71ms
step:930/1750 train_time:89016ms step_avg:95.72ms
step:931/1750 train_time:89116ms step_avg:95.72ms
step:932/1750 train_time:89214ms step_avg:95.72ms
step:933/1750 train_time:89314ms step_avg:95.73ms
step:934/1750 train_time:89413ms step_avg:95.73ms
step:935/1750 train_time:89513ms step_avg:95.74ms
step:936/1750 train_time:89613ms step_avg:95.74ms
step:937/1750 train_time:89712ms step_avg:95.74ms
step:938/1750 train_time:89812ms step_avg:95.75ms
step:939/1750 train_time:89912ms step_avg:95.75ms
step:940/1750 train_time:90012ms step_avg:95.76ms
step:941/1750 train_time:90112ms step_avg:95.76ms
step:942/1750 train_time:90212ms step_avg:95.77ms
step:943/1750 train_time:90313ms step_avg:95.77ms
step:944/1750 train_time:90412ms step_avg:95.78ms
step:945/1750 train_time:90512ms step_avg:95.78ms
step:946/1750 train_time:90611ms step_avg:95.78ms
step:947/1750 train_time:90711ms step_avg:95.79ms
step:948/1750 train_time:90810ms step_avg:95.79ms
step:949/1750 train_time:90910ms step_avg:95.80ms
step:950/1750 train_time:91009ms step_avg:95.80ms
step:951/1750 train_time:91109ms step_avg:95.80ms
step:952/1750 train_time:91209ms step_avg:95.81ms
step:953/1750 train_time:91309ms step_avg:95.81ms
step:954/1750 train_time:91408ms step_avg:95.82ms
step:955/1750 train_time:91508ms step_avg:95.82ms
step:956/1750 train_time:91606ms step_avg:95.82ms
step:957/1750 train_time:91706ms step_avg:95.83ms
step:958/1750 train_time:91806ms step_avg:95.83ms
step:959/1750 train_time:91906ms step_avg:95.83ms
step:960/1750 train_time:92005ms step_avg:95.84ms
step:961/1750 train_time:92106ms step_avg:95.84ms
step:962/1750 train_time:92205ms step_avg:95.85ms
step:963/1750 train_time:92304ms step_avg:95.85ms
step:964/1750 train_time:92405ms step_avg:95.86ms
step:965/1750 train_time:92504ms step_avg:95.86ms
step:966/1750 train_time:92604ms step_avg:95.86ms
step:967/1750 train_time:92704ms step_avg:95.87ms
step:968/1750 train_time:92804ms step_avg:95.87ms
step:969/1750 train_time:92904ms step_avg:95.88ms
step:970/1750 train_time:93004ms step_avg:95.88ms
step:971/1750 train_time:93105ms step_avg:95.89ms
step:972/1750 train_time:93204ms step_avg:95.89ms
step:973/1750 train_time:93304ms step_avg:95.89ms
step:974/1750 train_time:93403ms step_avg:95.90ms
step:975/1750 train_time:93503ms step_avg:95.90ms
step:976/1750 train_time:93602ms step_avg:95.90ms
step:977/1750 train_time:93702ms step_avg:95.91ms
step:978/1750 train_time:93801ms step_avg:95.91ms
step:979/1750 train_time:93901ms step_avg:95.91ms
step:980/1750 train_time:94000ms step_avg:95.92ms
step:981/1750 train_time:94099ms step_avg:95.92ms
step:982/1750 train_time:94199ms step_avg:95.93ms
step:983/1750 train_time:94299ms step_avg:95.93ms
step:984/1750 train_time:94398ms step_avg:95.93ms
step:985/1750 train_time:94497ms step_avg:95.94ms
step:986/1750 train_time:94596ms step_avg:95.94ms
step:987/1750 train_time:94696ms step_avg:95.94ms
step:988/1750 train_time:94796ms step_avg:95.95ms
step:989/1750 train_time:94895ms step_avg:95.95ms
step:990/1750 train_time:94994ms step_avg:95.95ms
step:991/1750 train_time:95095ms step_avg:95.96ms
step:992/1750 train_time:95194ms step_avg:95.96ms
step:993/1750 train_time:95294ms step_avg:95.97ms
step:994/1750 train_time:95394ms step_avg:95.97ms
step:995/1750 train_time:95493ms step_avg:95.97ms
step:996/1750 train_time:95592ms step_avg:95.98ms
step:997/1750 train_time:95692ms step_avg:95.98ms
step:998/1750 train_time:95791ms step_avg:95.98ms
step:999/1750 train_time:95891ms step_avg:95.99ms
step:1000/1750 train_time:95990ms step_avg:95.99ms
step:1000/1750 val_loss:3.5067 train_time:96078ms step_avg:96.08ms
step:1001/1750 train_time:96098ms step_avg:96.00ms
step:1002/1750 train_time:96195ms step_avg:96.00ms
step:1003/1750 train_time:96295ms step_avg:96.01ms
step:1004/1750 train_time:96395ms step_avg:96.01ms
step:1005/1750 train_time:96494ms step_avg:96.01ms
step:1006/1750 train_time:96593ms step_avg:96.02ms
step:1007/1750 train_time:96692ms step_avg:96.02ms
step:1008/1750 train_time:96790ms step_avg:96.02ms
step:1009/1750 train_time:96888ms step_avg:96.02ms
step:1010/1750 train_time:96987ms step_avg:96.03ms
step:1011/1750 train_time:97088ms step_avg:96.03ms
step:1012/1750 train_time:97190ms step_avg:96.04ms
step:1013/1750 train_time:97291ms step_avg:96.04ms
step:1014/1750 train_time:97391ms step_avg:96.05ms
step:1015/1750 train_time:97490ms step_avg:96.05ms
step:1016/1750 train_time:97590ms step_avg:96.05ms
step:1017/1750 train_time:97689ms step_avg:96.06ms
step:1018/1750 train_time:97788ms step_avg:96.06ms
step:1019/1750 train_time:97887ms step_avg:96.06ms
step:1020/1750 train_time:97986ms step_avg:96.06ms
step:1021/1750 train_time:98087ms step_avg:96.07ms
step:1022/1750 train_time:98187ms step_avg:96.07ms
step:1023/1750 train_time:98287ms step_avg:96.08ms
step:1024/1750 train_time:98389ms step_avg:96.08ms
step:1025/1750 train_time:98489ms step_avg:96.09ms
step:1026/1750 train_time:98590ms step_avg:96.09ms
step:1027/1750 train_time:98689ms step_avg:96.09ms
step:1028/1750 train_time:98788ms step_avg:96.10ms
step:1029/1750 train_time:98889ms step_avg:96.10ms
step:1030/1750 train_time:98988ms step_avg:96.11ms
step:1031/1750 train_time:99089ms step_avg:96.11ms
step:1032/1750 train_time:99189ms step_avg:96.11ms
step:1033/1750 train_time:99290ms step_avg:96.12ms
step:1034/1750 train_time:99389ms step_avg:96.12ms
step:1035/1750 train_time:99488ms step_avg:96.12ms
step:1036/1750 train_time:99589ms step_avg:96.13ms
step:1037/1750 train_time:99689ms step_avg:96.13ms
step:1038/1750 train_time:99787ms step_avg:96.13ms
step:1039/1750 train_time:99887ms step_avg:96.14ms
step:1040/1750 train_time:99986ms step_avg:96.14ms
step:1041/1750 train_time:100085ms step_avg:96.14ms
step:1042/1750 train_time:100439ms step_avg:96.39ms
step:1043/1750 train_time:100537ms step_avg:96.39ms
step:1044/1750 train_time:100636ms step_avg:96.39ms
step:1045/1750 train_time:100734ms step_avg:96.40ms
step:1046/1750 train_time:100833ms step_avg:96.40ms
step:1047/1750 train_time:100930ms step_avg:96.40ms
step:1048/1750 train_time:101029ms step_avg:96.40ms
step:1049/1750 train_time:101127ms step_avg:96.40ms
step:1050/1750 train_time:101225ms step_avg:96.40ms
step:1051/1750 train_time:101327ms step_avg:96.41ms
step:1052/1750 train_time:101432ms step_avg:96.42ms
step:1053/1750 train_time:101533ms step_avg:96.42ms
step:1054/1750 train_time:101632ms step_avg:96.42ms
step:1055/1750 train_time:101730ms step_avg:96.43ms
step:1056/1750 train_time:101830ms step_avg:96.43ms
step:1057/1750 train_time:101929ms step_avg:96.43ms
step:1058/1750 train_time:102028ms step_avg:96.43ms
step:1059/1750 train_time:102127ms step_avg:96.44ms
step:1060/1750 train_time:102225ms step_avg:96.44ms
step:1061/1750 train_time:102326ms step_avg:96.44ms
step:1062/1750 train_time:102426ms step_avg:96.45ms
step:1063/1750 train_time:102528ms step_avg:96.45ms
step:1064/1750 train_time:102629ms step_avg:96.46ms
step:1065/1750 train_time:102729ms step_avg:96.46ms
step:1066/1750 train_time:102829ms step_avg:96.46ms
step:1067/1750 train_time:102929ms step_avg:96.47ms
step:1068/1750 train_time:103028ms step_avg:96.47ms
step:1069/1750 train_time:103127ms step_avg:96.47ms
step:1070/1750 train_time:103227ms step_avg:96.47ms
step:1071/1750 train_time:103327ms step_avg:96.48ms
step:1072/1750 train_time:103429ms step_avg:96.48ms
step:1073/1750 train_time:103529ms step_avg:96.49ms
step:1074/1750 train_time:103630ms step_avg:96.49ms
step:1075/1750 train_time:103731ms step_avg:96.49ms
step:1076/1750 train_time:103830ms step_avg:96.50ms
step:1077/1750 train_time:103929ms step_avg:96.50ms
step:1078/1750 train_time:104315ms step_avg:96.77ms
step:1079/1750 train_time:104413ms step_avg:96.77ms
step:1080/1750 train_time:104815ms step_avg:97.05ms
step:1081/1750 train_time:104913ms step_avg:97.05ms
step:1082/1750 train_time:105012ms step_avg:97.05ms
step:1083/1750 train_time:105110ms step_avg:97.05ms
step:1084/1750 train_time:105210ms step_avg:97.06ms
step:1085/1750 train_time:105308ms step_avg:97.06ms
step:1086/1750 train_time:105406ms step_avg:97.06ms
step:1087/1750 train_time:105505ms step_avg:97.06ms
step:1088/1750 train_time:105603ms step_avg:97.06ms
step:1089/1750 train_time:105703ms step_avg:97.06ms
step:1090/1750 train_time:105809ms step_avg:97.07ms
step:1091/1750 train_time:105910ms step_avg:97.08ms
step:1092/1750 train_time:106010ms step_avg:97.08ms
step:1093/1750 train_time:106109ms step_avg:97.08ms
step:1094/1750 train_time:106208ms step_avg:97.08ms
step:1095/1750 train_time:106306ms step_avg:97.08ms
step:1096/1750 train_time:106405ms step_avg:97.09ms
step:1097/1750 train_time:106504ms step_avg:97.09ms
step:1098/1750 train_time:106603ms step_avg:97.09ms
step:1099/1750 train_time:106704ms step_avg:97.09ms
step:1100/1750 train_time:106805ms step_avg:97.10ms
step:1101/1750 train_time:106906ms step_avg:97.10ms
step:1102/1750 train_time:107006ms step_avg:97.10ms
step:1103/1750 train_time:107106ms step_avg:97.10ms
step:1104/1750 train_time:107205ms step_avg:97.11ms
step:1105/1750 train_time:107304ms step_avg:97.11ms
step:1106/1750 train_time:107403ms step_avg:97.11ms
step:1107/1750 train_time:107502ms step_avg:97.11ms
step:1108/1750 train_time:107602ms step_avg:97.11ms
step:1109/1750 train_time:107701ms step_avg:97.12ms
step:1110/1750 train_time:107803ms step_avg:97.12ms
step:1111/1750 train_time:107903ms step_avg:97.12ms
step:1112/1750 train_time:108003ms step_avg:97.12ms
step:1113/1750 train_time:108102ms step_avg:97.13ms
step:1114/1750 train_time:108202ms step_avg:97.13ms
step:1115/1750 train_time:108302ms step_avg:97.13ms
step:1116/1750 train_time:108401ms step_avg:97.13ms
step:1117/1750 train_time:108499ms step_avg:97.13ms
step:1118/1750 train_time:108598ms step_avg:97.14ms
step:1119/1750 train_time:108697ms step_avg:97.14ms
step:1120/1750 train_time:108797ms step_avg:97.14ms
step:1121/1750 train_time:108897ms step_avg:97.14ms
step:1122/1750 train_time:108997ms step_avg:97.15ms
step:1123/1750 train_time:109098ms step_avg:97.15ms
step:1124/1750 train_time:109198ms step_avg:97.15ms
step:1125/1750 train_time:109300ms step_avg:97.16ms
step:1125/1750 val_loss:3.4566 train_time:109388ms step_avg:97.23ms
step:1126/1750 train_time:109408ms step_avg:97.16ms
step:1127/1750 train_time:109509ms step_avg:97.17ms
step:1128/1750 train_time:109608ms step_avg:97.17ms
step:1129/1750 train_time:109708ms step_avg:97.17ms
step:1130/1750 train_time:109808ms step_avg:97.18ms
step:1131/1750 train_time:109907ms step_avg:97.18ms
step:1132/1750 train_time:110005ms step_avg:97.18ms
step:1133/1750 train_time:110104ms step_avg:97.18ms
step:1134/1750 train_time:110202ms step_avg:97.18ms
step:1135/1750 train_time:110301ms step_avg:97.18ms
step:1136/1750 train_time:110403ms step_avg:97.19ms
step:1137/1750 train_time:110506ms step_avg:97.19ms
step:1138/1750 train_time:110606ms step_avg:97.19ms
step:1139/1750 train_time:110706ms step_avg:97.20ms
step:1140/1750 train_time:110805ms step_avg:97.20ms
step:1141/1750 train_time:110904ms step_avg:97.20ms
step:1142/1750 train_time:111003ms step_avg:97.20ms
step:1143/1750 train_time:111102ms step_avg:97.20ms
step:1144/1750 train_time:111201ms step_avg:97.20ms
step:1145/1750 train_time:111300ms step_avg:97.21ms
step:1146/1750 train_time:111400ms step_avg:97.21ms
step:1147/1750 train_time:111501ms step_avg:97.21ms
step:1148/1750 train_time:111601ms step_avg:97.21ms
step:1149/1750 train_time:111949ms step_avg:97.43ms
step:1150/1750 train_time:112047ms step_avg:97.43ms
step:1151/1750 train_time:112145ms step_avg:97.43ms
step:1152/1750 train_time:112243ms step_avg:97.43ms
step:1153/1750 train_time:112342ms step_avg:97.43ms
step:1154/1750 train_time:112441ms step_avg:97.44ms
step:1155/1750 train_time:112540ms step_avg:97.44ms
step:1156/1750 train_time:112638ms step_avg:97.44ms
step:1157/1750 train_time:112737ms step_avg:97.44ms
step:1158/1750 train_time:112844ms step_avg:97.45ms
step:1159/1750 train_time:112945ms step_avg:97.45ms
step:1160/1750 train_time:113046ms step_avg:97.45ms
step:1161/1750 train_time:113145ms step_avg:97.45ms
step:1162/1750 train_time:113244ms step_avg:97.46ms
step:1163/1750 train_time:113344ms step_avg:97.46ms
step:1164/1750 train_time:113442ms step_avg:97.46ms
step:1165/1750 train_time:113541ms step_avg:97.46ms
step:1166/1750 train_time:113641ms step_avg:97.46ms
step:1167/1750 train_time:114066ms step_avg:97.74ms
step:1168/1750 train_time:114164ms step_avg:97.74ms
step:1169/1750 train_time:114524ms step_avg:97.97ms
step:1170/1750 train_time:114623ms step_avg:97.97ms
step:1171/1750 train_time:114721ms step_avg:97.97ms
step:1172/1750 train_time:114821ms step_avg:97.97ms
step:1173/1750 train_time:114921ms step_avg:97.97ms
step:1174/1750 train_time:115021ms step_avg:97.97ms
step:1175/1750 train_time:115120ms step_avg:97.97ms
step:1176/1750 train_time:115219ms step_avg:97.98ms
step:1177/1750 train_time:115318ms step_avg:97.98ms
step:1178/1750 train_time:115425ms step_avg:97.98ms
step:1179/1750 train_time:115532ms step_avg:97.99ms
step:1180/1750 train_time:115633ms step_avg:97.99ms
step:1181/1750 train_time:115732ms step_avg:97.99ms
step:1182/1750 train_time:115833ms step_avg:98.00ms
step:1183/1750 train_time:115933ms step_avg:98.00ms
step:1184/1750 train_time:116034ms step_avg:98.00ms
step:1185/1750 train_time:116134ms step_avg:98.00ms
step:1186/1750 train_time:116234ms step_avg:98.00ms
step:1187/1750 train_time:116334ms step_avg:98.01ms
step:1188/1750 train_time:116436ms step_avg:98.01ms
step:1189/1750 train_time:116537ms step_avg:98.01ms
step:1190/1750 train_time:116637ms step_avg:98.01ms
step:1191/1750 train_time:116738ms step_avg:98.02ms
step:1192/1750 train_time:116840ms step_avg:98.02ms
step:1193/1750 train_time:116941ms step_avg:98.02ms
step:1194/1750 train_time:117043ms step_avg:98.03ms
step:1195/1750 train_time:117143ms step_avg:98.03ms
step:1196/1750 train_time:117243ms step_avg:98.03ms
step:1197/1750 train_time:117344ms step_avg:98.03ms
step:1198/1750 train_time:117444ms step_avg:98.03ms
step:1199/1750 train_time:117545ms step_avg:98.04ms
step:1200/1750 train_time:117645ms step_avg:98.04ms
step:1201/1750 train_time:117746ms step_avg:98.04ms
step:1202/1750 train_time:118178ms step_avg:98.32ms
step:1203/1750 train_time:118241ms step_avg:98.29ms
step:1204/1750 train_time:118341ms step_avg:98.29ms
step:1205/1750 train_time:118440ms step_avg:98.29ms
step:1206/1750 train_time:118539ms step_avg:98.29ms
step:1207/1750 train_time:118639ms step_avg:98.29ms
step:1208/1750 train_time:118738ms step_avg:98.29ms
step:1209/1750 train_time:118838ms step_avg:98.29ms
step:1210/1750 train_time:118937ms step_avg:98.30ms
step:1211/1750 train_time:119037ms step_avg:98.30ms
step:1212/1750 train_time:119142ms step_avg:98.30ms
step:1213/1750 train_time:119249ms step_avg:98.31ms
step:1214/1750 train_time:119349ms step_avg:98.31ms
step:1215/1750 train_time:119451ms step_avg:98.31ms
step:1216/1750 train_time:119552ms step_avg:98.32ms
step:1217/1750 train_time:119652ms step_avg:98.32ms
step:1218/1750 train_time:119752ms step_avg:98.32ms
step:1219/1750 train_time:119853ms step_avg:98.32ms
step:1220/1750 train_time:120310ms step_avg:98.62ms
step:1221/1750 train_time:120408ms step_avg:98.61ms
step:1222/1750 train_time:120508ms step_avg:98.62ms
step:1223/1750 train_time:120608ms step_avg:98.62ms
step:1224/1750 train_time:120707ms step_avg:98.62ms
step:1225/1750 train_time:120807ms step_avg:98.62ms
step:1226/1750 train_time:120906ms step_avg:98.62ms
step:1227/1750 train_time:121005ms step_avg:98.62ms
step:1228/1750 train_time:121104ms step_avg:98.62ms
step:1229/1750 train_time:121208ms step_avg:98.62ms
step:1230/1750 train_time:121313ms step_avg:98.63ms
step:1231/1750 train_time:121414ms step_avg:98.63ms
step:1232/1750 train_time:121514ms step_avg:98.63ms
step:1233/1750 train_time:121614ms step_avg:98.63ms
step:1234/1750 train_time:121716ms step_avg:98.64ms
step:1235/1750 train_time:121815ms step_avg:98.64ms
step:1236/1750 train_time:121916ms step_avg:98.64ms
step:1237/1750 train_time:122016ms step_avg:98.64ms
step:1238/1750 train_time:122116ms step_avg:98.64ms
step:1239/1750 train_time:122218ms step_avg:98.64ms
step:1240/1750 train_time:122321ms step_avg:98.65ms
step:1241/1750 train_time:122424ms step_avg:98.65ms
step:1242/1750 train_time:122524ms step_avg:98.65ms
step:1243/1750 train_time:122625ms step_avg:98.65ms
step:1244/1750 train_time:122725ms step_avg:98.65ms
step:1245/1750 train_time:122825ms step_avg:98.65ms
step:1246/1750 train_time:122926ms step_avg:98.66ms
step:1247/1750 train_time:123026ms step_avg:98.66ms
step:1248/1750 train_time:123127ms step_avg:98.66ms
step:1249/1750 train_time:123228ms step_avg:98.66ms
step:1250/1750 train_time:123330ms step_avg:98.66ms
step:1250/1750 val_loss:3.4115 train_time:123421ms step_avg:98.74ms
step:1251/1750 train_time:123441ms step_avg:98.67ms
step:1252/1750 train_time:123542ms step_avg:98.68ms
step:1253/1750 train_time:123643ms step_avg:98.68ms
step:1254/1750 train_time:123744ms step_avg:98.68ms
step:1255/1750 train_time:123843ms step_avg:98.68ms
step:1256/1750 train_time:123943ms step_avg:98.68ms
step:1257/1750 train_time:124043ms step_avg:98.68ms
step:1258/1750 train_time:124142ms step_avg:98.68ms
step:1259/1750 train_time:124242ms step_avg:98.68ms
step:1260/1750 train_time:124344ms step_avg:98.69ms
step:1261/1750 train_time:124448ms step_avg:98.69ms
step:1262/1750 train_time:124550ms step_avg:98.69ms
step:1263/1750 train_time:124650ms step_avg:98.69ms
step:1264/1750 train_time:124750ms step_avg:98.69ms
step:1265/1750 train_time:124851ms step_avg:98.70ms
step:1266/1750 train_time:124951ms step_avg:98.70ms
step:1267/1750 train_time:125052ms step_avg:98.70ms
step:1268/1750 train_time:125153ms step_avg:98.70ms
step:1269/1750 train_time:125254ms step_avg:98.70ms
step:1270/1750 train_time:125354ms step_avg:98.70ms
step:1271/1750 train_time:125456ms step_avg:98.71ms
step:1272/1750 train_time:125556ms step_avg:98.71ms
step:1273/1750 train_time:125656ms step_avg:98.71ms
step:1274/1750 train_time:125757ms step_avg:98.71ms
step:1275/1750 train_time:125857ms step_avg:98.71ms
step:1276/1750 train_time:125959ms step_avg:98.71ms
step:1277/1750 train_time:126060ms step_avg:98.72ms
step:1278/1750 train_time:126160ms step_avg:98.72ms
step:1279/1750 train_time:126261ms step_avg:98.72ms
step:1280/1750 train_time:126361ms step_avg:98.72ms
step:1281/1750 train_time:126464ms step_avg:98.72ms
step:1282/1750 train_time:126565ms step_avg:98.72ms
step:1283/1750 train_time:126666ms step_avg:98.73ms
step:1284/1750 train_time:126767ms step_avg:98.73ms
step:1285/1750 train_time:126867ms step_avg:98.73ms
step:1286/1750 train_time:126968ms step_avg:98.73ms
step:1287/1750 train_time:127068ms step_avg:98.73ms
step:1288/1750 train_time:127169ms step_avg:98.73ms
step:1289/1750 train_time:127270ms step_avg:98.74ms
step:1290/1750 train_time:127371ms step_avg:98.74ms
step:1291/1750 train_time:127471ms step_avg:98.74ms
step:1292/1750 train_time:127572ms step_avg:98.74ms
step:1293/1750 train_time:127674ms step_avg:98.74ms
step:1294/1750 train_time:127776ms step_avg:98.74ms
step:1295/1750 train_time:127876ms step_avg:98.75ms
step:1296/1750 train_time:127976ms step_avg:98.75ms
step:1297/1750 train_time:128076ms step_avg:98.75ms
step:1298/1750 train_time:128177ms step_avg:98.75ms
step:1299/1750 train_time:128278ms step_avg:98.75ms
step:1300/1750 train_time:128378ms step_avg:98.75ms
step:1301/1750 train_time:128480ms step_avg:98.75ms
step:1302/1750 train_time:128580ms step_avg:98.76ms
step:1303/1750 train_time:128681ms step_avg:98.76ms
step:1304/1750 train_time:128781ms step_avg:98.76ms
step:1305/1750 train_time:128881ms step_avg:98.76ms
step:1306/1750 train_time:128983ms step_avg:98.76ms
step:1307/1750 train_time:129085ms step_avg:98.76ms
step:1308/1750 train_time:129187ms step_avg:98.77ms
step:1309/1750 train_time:129287ms step_avg:98.77ms
step:1310/1750 train_time:129388ms step_avg:98.77ms
step:1311/1750 train_time:129490ms step_avg:98.77ms
step:1312/1750 train_time:129590ms step_avg:98.77ms
step:1313/1750 train_time:129692ms step_avg:98.78ms
step:1314/1750 train_time:129793ms step_avg:98.78ms
step:1315/1750 train_time:129894ms step_avg:98.78ms
step:1316/1750 train_time:129995ms step_avg:98.78ms
step:1317/1750 train_time:130096ms step_avg:98.78ms
step:1318/1750 train_time:130196ms step_avg:98.78ms
step:1319/1750 train_time:130296ms step_avg:98.78ms
step:1320/1750 train_time:130399ms step_avg:98.79ms
step:1321/1750 train_time:130500ms step_avg:98.79ms
step:1322/1750 train_time:130601ms step_avg:98.79ms
step:1323/1750 train_time:130701ms step_avg:98.79ms
step:1324/1750 train_time:130802ms step_avg:98.79ms
step:1325/1750 train_time:130905ms step_avg:98.80ms
step:1326/1750 train_time:131008ms step_avg:98.80ms
step:1327/1750 train_time:131109ms step_avg:98.80ms
step:1328/1750 train_time:131209ms step_avg:98.80ms
step:1329/1750 train_time:131309ms step_avg:98.80ms
step:1330/1750 train_time:131410ms step_avg:98.80ms
step:1331/1750 train_time:131511ms step_avg:98.81ms
step:1332/1750 train_time:131612ms step_avg:98.81ms
step:1333/1750 train_time:131714ms step_avg:98.81ms
step:1334/1750 train_time:131814ms step_avg:98.81ms
step:1335/1750 train_time:131916ms step_avg:98.81ms
step:1336/1750 train_time:132017ms step_avg:98.82ms
step:1337/1750 train_time:132118ms step_avg:98.82ms
step:1338/1750 train_time:132218ms step_avg:98.82ms
step:1339/1750 train_time:132319ms step_avg:98.82ms
step:1340/1750 train_time:132419ms step_avg:98.82ms
step:1341/1750 train_time:132519ms step_avg:98.82ms
step:1342/1750 train_time:132621ms step_avg:98.82ms
step:1343/1750 train_time:132722ms step_avg:98.82ms
step:1344/1750 train_time:132823ms step_avg:98.83ms
step:1345/1750 train_time:132925ms step_avg:98.83ms
step:1346/1750 train_time:133028ms step_avg:98.83ms
step:1347/1750 train_time:133130ms step_avg:98.83ms
step:1348/1750 train_time:133229ms step_avg:98.83ms
step:1349/1750 train_time:133330ms step_avg:98.84ms
step:1350/1750 train_time:133431ms step_avg:98.84ms
step:1351/1750 train_time:133533ms step_avg:98.84ms
step:1352/1750 train_time:133633ms step_avg:98.84ms
step:1353/1750 train_time:133735ms step_avg:98.84ms
step:1354/1750 train_time:133836ms step_avg:98.84ms
step:1355/1750 train_time:133936ms step_avg:98.85ms
step:1356/1750 train_time:134036ms step_avg:98.85ms
step:1357/1750 train_time:134138ms step_avg:98.85ms
step:1358/1750 train_time:134238ms step_avg:98.85ms
step:1359/1750 train_time:134339ms step_avg:98.85ms
step:1360/1750 train_time:134440ms step_avg:98.85ms
step:1361/1750 train_time:134540ms step_avg:98.85ms
step:1362/1750 train_time:134641ms step_avg:98.86ms
step:1363/1750 train_time:134743ms step_avg:98.86ms
step:1364/1750 train_time:134844ms step_avg:98.86ms
step:1365/1750 train_time:134946ms step_avg:98.86ms
step:1366/1750 train_time:135047ms step_avg:98.86ms
step:1367/1750 train_time:135148ms step_avg:98.86ms
step:1368/1750 train_time:135249ms step_avg:98.87ms
step:1369/1750 train_time:135349ms step_avg:98.87ms
step:1370/1750 train_time:135449ms step_avg:98.87ms
step:1371/1750 train_time:135549ms step_avg:98.87ms
step:1372/1750 train_time:135649ms step_avg:98.87ms
step:1373/1750 train_time:135750ms step_avg:98.87ms
step:1374/1750 train_time:135851ms step_avg:98.87ms
step:1375/1750 train_time:135953ms step_avg:98.87ms
step:1375/1750 val_loss:3.3718 train_time:136043ms step_avg:98.94ms
step:1376/1750 train_time:136063ms step_avg:98.88ms
step:1377/1750 train_time:136171ms step_avg:98.89ms
step:1378/1750 train_time:136272ms step_avg:98.89ms
step:1379/1750 train_time:136372ms step_avg:98.89ms
step:1380/1750 train_time:136473ms step_avg:98.89ms
step:1381/1750 train_time:136573ms step_avg:98.89ms
step:1382/1750 train_time:136672ms step_avg:98.89ms
step:1383/1750 train_time:136772ms step_avg:98.90ms
step:1384/1750 train_time:136871ms step_avg:98.90ms
step:1385/1750 train_time:136971ms step_avg:98.90ms
step:1386/1750 train_time:137073ms step_avg:98.90ms
step:1387/1750 train_time:137176ms step_avg:98.90ms
step:1388/1750 train_time:137277ms step_avg:98.90ms
step:1389/1750 train_time:137378ms step_avg:98.90ms
step:1390/1750 train_time:137479ms step_avg:98.91ms
step:1391/1750 train_time:137580ms step_avg:98.91ms
step:1392/1750 train_time:137681ms step_avg:98.91ms
step:1393/1750 train_time:137782ms step_avg:98.91ms
step:1394/1750 train_time:137883ms step_avg:98.91ms
step:1395/1750 train_time:137984ms step_avg:98.91ms
step:1396/1750 train_time:138085ms step_avg:98.91ms
step:1397/1750 train_time:138187ms step_avg:98.92ms
step:1398/1750 train_time:138288ms step_avg:98.92ms
step:1399/1750 train_time:138390ms step_avg:98.92ms
step:1400/1750 train_time:138491ms step_avg:98.92ms
step:1401/1750 train_time:138593ms step_avg:98.92ms
step:1402/1750 train_time:138693ms step_avg:98.93ms
step:1403/1750 train_time:138794ms step_avg:98.93ms
step:1404/1750 train_time:138896ms step_avg:98.93ms
step:1405/1750 train_time:138997ms step_avg:98.93ms
step:1406/1750 train_time:139097ms step_avg:98.93ms
step:1407/1750 train_time:139199ms step_avg:98.93ms
step:1408/1750 train_time:139301ms step_avg:98.94ms
step:1409/1750 train_time:139403ms step_avg:98.94ms
step:1410/1750 train_time:139504ms step_avg:98.94ms
step:1411/1750 train_time:139605ms step_avg:98.94ms
step:1412/1750 train_time:139706ms step_avg:98.94ms
step:1413/1750 train_time:139806ms step_avg:98.94ms
step:1414/1750 train_time:139909ms step_avg:98.95ms
step:1415/1750 train_time:140010ms step_avg:98.95ms
step:1416/1750 train_time:140111ms step_avg:98.95ms
step:1417/1750 train_time:140212ms step_avg:98.95ms
step:1418/1750 train_time:140312ms step_avg:98.95ms
step:1419/1750 train_time:140413ms step_avg:98.95ms
step:1420/1750 train_time:140515ms step_avg:98.95ms
step:1421/1750 train_time:140615ms step_avg:98.96ms
step:1422/1750 train_time:140716ms step_avg:98.96ms
step:1423/1750 train_time:140816ms step_avg:98.96ms
step:1424/1750 train_time:140918ms step_avg:98.96ms
step:1425/1750 train_time:141018ms step_avg:98.96ms
step:1426/1750 train_time:141121ms step_avg:98.96ms
step:1427/1750 train_time:141223ms step_avg:98.96ms
step:1428/1750 train_time:141325ms step_avg:98.97ms
step:1429/1750 train_time:141427ms step_avg:98.97ms
step:1430/1750 train_time:141529ms step_avg:98.97ms
step:1431/1750 train_time:141631ms step_avg:98.97ms
step:1432/1750 train_time:141732ms step_avg:98.97ms
step:1433/1750 train_time:141833ms step_avg:98.98ms
step:1434/1750 train_time:141933ms step_avg:98.98ms
step:1435/1750 train_time:142036ms step_avg:98.98ms
step:1436/1750 train_time:142138ms step_avg:98.98ms
step:1437/1750 train_time:142241ms step_avg:98.98ms
step:1438/1750 train_time:142343ms step_avg:98.99ms
step:1439/1750 train_time:142447ms step_avg:98.99ms
step:1440/1750 train_time:142550ms step_avg:98.99ms
step:1441/1750 train_time:142652ms step_avg:99.00ms
step:1442/1750 train_time:142752ms step_avg:99.00ms
step:1443/1750 train_time:142852ms step_avg:99.00ms
step:1444/1750 train_time:142954ms step_avg:99.00ms
step:1445/1750 train_time:143056ms step_avg:99.00ms
step:1446/1750 train_time:143157ms step_avg:99.00ms
step:1447/1750 train_time:143258ms step_avg:99.00ms
step:1448/1750 train_time:143363ms step_avg:99.01ms
step:1449/1750 train_time:143464ms step_avg:99.01ms
step:1450/1750 train_time:143568ms step_avg:99.01ms
step:1451/1750 train_time:143668ms step_avg:99.01ms
step:1452/1750 train_time:143769ms step_avg:99.01ms
step:1453/1750 train_time:143872ms step_avg:99.02ms
step:1454/1750 train_time:143975ms step_avg:99.02ms
step:1455/1750 train_time:144076ms step_avg:99.02ms
step:1456/1750 train_time:144177ms step_avg:99.02ms
step:1457/1750 train_time:144279ms step_avg:99.02ms
step:1458/1750 train_time:144382ms step_avg:99.03ms
step:1459/1750 train_time:144485ms step_avg:99.03ms
step:1460/1750 train_time:144587ms step_avg:99.03ms
step:1461/1750 train_time:144688ms step_avg:99.03ms
step:1462/1750 train_time:144790ms step_avg:99.04ms
step:1463/1750 train_time:144892ms step_avg:99.04ms
step:1464/1750 train_time:144993ms step_avg:99.04ms
step:1465/1750 train_time:145095ms step_avg:99.04ms
step:1466/1750 train_time:145197ms step_avg:99.04ms
step:1467/1750 train_time:145298ms step_avg:99.04ms
step:1468/1750 train_time:145401ms step_avg:99.05ms
step:1469/1750 train_time:145503ms step_avg:99.05ms
step:1470/1750 train_time:145605ms step_avg:99.05ms
step:1471/1750 train_time:145708ms step_avg:99.05ms
step:1472/1750 train_time:145810ms step_avg:99.06ms
step:1473/1750 train_time:145911ms step_avg:99.06ms
step:1474/1750 train_time:146011ms step_avg:99.06ms
step:1475/1750 train_time:146113ms step_avg:99.06ms
step:1476/1750 train_time:146215ms step_avg:99.06ms
step:1477/1750 train_time:146317ms step_avg:99.06ms
step:1478/1750 train_time:146419ms step_avg:99.07ms
step:1479/1750 train_time:146520ms step_avg:99.07ms
step:1480/1750 train_time:146623ms step_avg:99.07ms
step:1481/1750 train_time:146725ms step_avg:99.07ms
step:1482/1750 train_time:146828ms step_avg:99.07ms
step:1483/1750 train_time:146930ms step_avg:99.08ms
step:1484/1750 train_time:147032ms step_avg:99.08ms
step:1485/1750 train_time:147134ms step_avg:99.08ms
step:1486/1750 train_time:147236ms step_avg:99.08ms
step:1487/1750 train_time:147338ms step_avg:99.08ms
step:1488/1750 train_time:147440ms step_avg:99.09ms
step:1489/1750 train_time:147542ms step_avg:99.09ms
step:1490/1750 train_time:147645ms step_avg:99.09ms
step:1491/1750 train_time:147747ms step_avg:99.09ms
step:1492/1750 train_time:147848ms step_avg:99.09ms
step:1493/1750 train_time:147950ms step_avg:99.10ms
step:1494/1750 train_time:148051ms step_avg:99.10ms
step:1495/1750 train_time:148152ms step_avg:99.10ms
step:1496/1750 train_time:148254ms step_avg:99.10ms
step:1497/1750 train_time:148355ms step_avg:99.10ms
step:1498/1750 train_time:148457ms step_avg:99.10ms
step:1499/1750 train_time:148558ms step_avg:99.11ms
step:1500/1750 train_time:148660ms step_avg:99.11ms
step:1500/1750 val_loss:3.3361 train_time:148751ms step_avg:99.17ms
step:1501/1750 train_time:148771ms step_avg:99.11ms
step:1502/1750 train_time:148875ms step_avg:99.12ms
step:1503/1750 train_time:148978ms step_avg:99.12ms
step:1504/1750 train_time:149079ms step_avg:99.12ms
step:1505/1750 train_time:149180ms step_avg:99.12ms
step:1506/1750 train_time:149282ms step_avg:99.12ms
step:1507/1750 train_time:149384ms step_avg:99.13ms
step:1508/1750 train_time:149485ms step_avg:99.13ms
step:1509/1750 train_time:149586ms step_avg:99.13ms
step:1510/1750 train_time:149688ms step_avg:99.13ms
step:1511/1750 train_time:149793ms step_avg:99.14ms
step:1512/1750 train_time:149895ms step_avg:99.14ms
step:1513/1750 train_time:149997ms step_avg:99.14ms
step:1514/1750 train_time:150098ms step_avg:99.14ms
step:1515/1750 train_time:150202ms step_avg:99.14ms
step:1516/1750 train_time:150303ms step_avg:99.14ms
step:1517/1750 train_time:150403ms step_avg:99.15ms
step:1518/1750 train_time:150504ms step_avg:99.15ms
step:1519/1750 train_time:150607ms step_avg:99.15ms
step:1520/1750 train_time:150708ms step_avg:99.15ms
step:1521/1750 train_time:150810ms step_avg:99.15ms
step:1522/1750 train_time:150912ms step_avg:99.15ms
step:1523/1750 train_time:151013ms step_avg:99.16ms
step:1524/1750 train_time:151116ms step_avg:99.16ms
step:1525/1750 train_time:151218ms step_avg:99.16ms
step:1526/1750 train_time:151321ms step_avg:99.16ms
step:1527/1750 train_time:151422ms step_avg:99.16ms
step:1528/1750 train_time:151528ms step_avg:99.17ms
step:1529/1750 train_time:151629ms step_avg:99.17ms
step:1530/1750 train_time:151732ms step_avg:99.17ms
step:1531/1750 train_time:151833ms step_avg:99.17ms
step:1532/1750 train_time:151934ms step_avg:99.17ms
step:1533/1750 train_time:152035ms step_avg:99.17ms
step:1534/1750 train_time:152138ms step_avg:99.18ms
step:1535/1750 train_time:152239ms step_avg:99.18ms
step:1536/1750 train_time:152340ms step_avg:99.18ms
step:1537/1750 train_time:152443ms step_avg:99.18ms
step:1538/1750 train_time:152544ms step_avg:99.18ms
step:1539/1750 train_time:152646ms step_avg:99.19ms
step:1540/1750 train_time:152748ms step_avg:99.19ms
step:1541/1750 train_time:152851ms step_avg:99.19ms
step:1542/1750 train_time:152954ms step_avg:99.19ms
step:1543/1750 train_time:153055ms step_avg:99.19ms
step:1544/1750 train_time:153156ms step_avg:99.19ms
step:1545/1750 train_time:153257ms step_avg:99.20ms
step:1546/1750 train_time:153359ms step_avg:99.20ms
step:1547/1750 train_time:153462ms step_avg:99.20ms
step:1548/1750 train_time:153564ms step_avg:99.20ms
step:1549/1750 train_time:153666ms step_avg:99.20ms
step:1550/1750 train_time:153769ms step_avg:99.21ms
step:1551/1750 train_time:153872ms step_avg:99.21ms
step:1552/1750 train_time:153973ms step_avg:99.21ms
step:1553/1750 train_time:154074ms step_avg:99.21ms
step:1554/1750 train_time:154175ms step_avg:99.21ms
step:1555/1750 train_time:154277ms step_avg:99.21ms
step:1556/1750 train_time:154379ms step_avg:99.22ms
step:1557/1750 train_time:154481ms step_avg:99.22ms
step:1558/1750 train_time:154584ms step_avg:99.22ms
step:1559/1750 train_time:154688ms step_avg:99.22ms
step:1560/1750 train_time:154789ms step_avg:99.22ms
step:1561/1750 train_time:154890ms step_avg:99.23ms
step:1562/1750 train_time:154993ms step_avg:99.23ms
step:1563/1750 train_time:155098ms step_avg:99.23ms
step:1564/1750 train_time:155200ms step_avg:99.23ms
step:1565/1750 train_time:155301ms step_avg:99.23ms
step:1566/1750 train_time:155403ms step_avg:99.24ms
step:1567/1750 train_time:155504ms step_avg:99.24ms
step:1568/1750 train_time:155605ms step_avg:99.24ms
step:1569/1750 train_time:155707ms step_avg:99.24ms
step:1570/1750 train_time:155810ms step_avg:99.24ms
step:1571/1750 train_time:155912ms step_avg:99.24ms
step:1572/1750 train_time:156014ms step_avg:99.25ms
step:1573/1750 train_time:156116ms step_avg:99.25ms
step:1574/1750 train_time:156218ms step_avg:99.25ms
step:1575/1750 train_time:156319ms step_avg:99.25ms
step:1576/1750 train_time:156423ms step_avg:99.25ms
step:1577/1750 train_time:156526ms step_avg:99.26ms
step:1578/1750 train_time:156627ms step_avg:99.26ms
step:1579/1750 train_time:156728ms step_avg:99.26ms
step:1580/1750 train_time:156831ms step_avg:99.26ms
step:1581/1750 train_time:156933ms step_avg:99.26ms
step:1582/1750 train_time:157035ms step_avg:99.26ms
step:1583/1750 train_time:157139ms step_avg:99.27ms
step:1584/1750 train_time:157241ms step_avg:99.27ms
step:1585/1750 train_time:157342ms step_avg:99.27ms
step:1586/1750 train_time:157445ms step_avg:99.27ms
step:1587/1750 train_time:157546ms step_avg:99.27ms
step:1588/1750 train_time:157648ms step_avg:99.27ms
step:1589/1750 train_time:157749ms step_avg:99.28ms
step:1590/1750 train_time:157851ms step_avg:99.28ms
step:1591/1750 train_time:157954ms step_avg:99.28ms
step:1592/1750 train_time:158056ms step_avg:99.28ms
step:1593/1750 train_time:158158ms step_avg:99.28ms
step:1594/1750 train_time:158263ms step_avg:99.29ms
step:1595/1750 train_time:158364ms step_avg:99.29ms
step:1596/1750 train_time:158466ms step_avg:99.29ms
step:1597/1750 train_time:158567ms step_avg:99.29ms
step:1598/1750 train_time:158670ms step_avg:99.29ms
step:1599/1750 train_time:158771ms step_avg:99.29ms
step:1600/1750 train_time:158872ms step_avg:99.30ms
step:1601/1750 train_time:158975ms step_avg:99.30ms
step:1602/1750 train_time:159076ms step_avg:99.30ms
step:1603/1750 train_time:159176ms step_avg:99.30ms
step:1604/1750 train_time:159279ms step_avg:99.30ms
step:1605/1750 train_time:159382ms step_avg:99.30ms
step:1606/1750 train_time:159486ms step_avg:99.31ms
step:1607/1750 train_time:159587ms step_avg:99.31ms
step:1608/1750 train_time:159688ms step_avg:99.31ms
step:1609/1750 train_time:159790ms step_avg:99.31ms
step:1610/1750 train_time:159892ms step_avg:99.31ms
step:1611/1750 train_time:159994ms step_avg:99.31ms
step:1612/1750 train_time:160097ms step_avg:99.32ms
step:1613/1750 train_time:160198ms step_avg:99.32ms
step:1614/1750 train_time:160298ms step_avg:99.32ms
step:1615/1750 train_time:160401ms step_avg:99.32ms
step:1616/1750 train_time:160502ms step_avg:99.32ms
step:1617/1750 train_time:160605ms step_avg:99.32ms
step:1618/1750 train_time:160707ms step_avg:99.32ms
step:1619/1750 train_time:160808ms step_avg:99.33ms
step:1620/1750 train_time:160911ms step_avg:99.33ms
step:1621/1750 train_time:161012ms step_avg:99.33ms
step:1622/1750 train_time:161114ms step_avg:99.33ms
step:1623/1750 train_time:161215ms step_avg:99.33ms
step:1624/1750 train_time:161318ms step_avg:99.33ms
step:1625/1750 train_time:161422ms step_avg:99.34ms
step:1625/1750 val_loss:3.3060 train_time:161513ms step_avg:99.39ms
step:1626/1750 train_time:161535ms step_avg:99.35ms
step:1627/1750 train_time:161637ms step_avg:99.35ms
step:1628/1750 train_time:161739ms step_avg:99.35ms
step:1629/1750 train_time:161842ms step_avg:99.35ms
step:1630/1750 train_time:161943ms step_avg:99.35ms
step:1631/1750 train_time:162044ms step_avg:99.35ms
step:1632/1750 train_time:162145ms step_avg:99.35ms
step:1633/1750 train_time:162245ms step_avg:99.35ms
step:1634/1750 train_time:162349ms step_avg:99.36ms
step:1635/1750 train_time:162451ms step_avg:99.36ms
step:1636/1750 train_time:162554ms step_avg:99.36ms
step:1637/1750 train_time:162657ms step_avg:99.36ms
step:1638/1750 train_time:162760ms step_avg:99.36ms
step:1639/1750 train_time:162862ms step_avg:99.37ms
step:1640/1750 train_time:162963ms step_avg:99.37ms
step:1641/1750 train_time:163064ms step_avg:99.37ms
step:1642/1750 train_time:163165ms step_avg:99.37ms
step:1643/1750 train_time:163266ms step_avg:99.37ms
step:1644/1750 train_time:163368ms step_avg:99.37ms
step:1645/1750 train_time:163470ms step_avg:99.37ms
step:1646/1750 train_time:163573ms step_avg:99.38ms
step:1647/1750 train_time:163677ms step_avg:99.38ms
step:1648/1750 train_time:163780ms step_avg:99.38ms
step:1649/1750 train_time:163881ms step_avg:99.38ms
step:1650/1750 train_time:163982ms step_avg:99.38ms
step:1651/1750 train_time:164084ms step_avg:99.38ms
step:1652/1750 train_time:164186ms step_avg:99.39ms
step:1653/1750 train_time:164287ms step_avg:99.39ms
step:1654/1750 train_time:164388ms step_avg:99.39ms
step:1655/1750 train_time:164491ms step_avg:99.39ms
step:1656/1750 train_time:164595ms step_avg:99.39ms
step:1657/1750 train_time:164696ms step_avg:99.39ms
step:1658/1750 train_time:164797ms step_avg:99.40ms
step:1659/1750 train_time:164902ms step_avg:99.40ms
step:1660/1750 train_time:165004ms step_avg:99.40ms
step:1661/1750 train_time:165106ms step_avg:99.40ms
step:1662/1750 train_time:165208ms step_avg:99.40ms
step:1663/1750 train_time:165311ms step_avg:99.41ms
step:1664/1750 train_time:165413ms step_avg:99.41ms
step:1665/1750 train_time:165517ms step_avg:99.41ms
step:1666/1750 train_time:165620ms step_avg:99.41ms
step:1667/1750 train_time:165722ms step_avg:99.41ms
step:1668/1750 train_time:165825ms step_avg:99.42ms
step:1669/1750 train_time:165928ms step_avg:99.42ms
step:1670/1750 train_time:166029ms step_avg:99.42ms
step:1671/1750 train_time:166130ms step_avg:99.42ms
step:1672/1750 train_time:166231ms step_avg:99.42ms
step:1673/1750 train_time:166333ms step_avg:99.42ms
step:1674/1750 train_time:166434ms step_avg:99.42ms
step:1675/1750 train_time:166536ms step_avg:99.42ms
step:1676/1750 train_time:166639ms step_avg:99.43ms
step:1677/1750 train_time:166741ms step_avg:99.43ms
step:1678/1750 train_time:166844ms step_avg:99.43ms
step:1679/1750 train_time:166946ms step_avg:99.43ms
step:1680/1750 train_time:167047ms step_avg:99.43ms
step:1681/1750 train_time:167149ms step_avg:99.43ms
step:1682/1750 train_time:167253ms step_avg:99.44ms
step:1683/1750 train_time:167354ms step_avg:99.44ms
step:1684/1750 train_time:167457ms step_avg:99.44ms
step:1685/1750 train_time:167559ms step_avg:99.44ms
step:1686/1750 train_time:167661ms step_avg:99.44ms
step:1687/1750 train_time:167763ms step_avg:99.44ms
step:1688/1750 train_time:167865ms step_avg:99.45ms
step:1689/1750 train_time:167968ms step_avg:99.45ms
step:1690/1750 train_time:168071ms step_avg:99.45ms
step:1691/1750 train_time:168173ms step_avg:99.45ms
step:1692/1750 train_time:168274ms step_avg:99.45ms
step:1693/1750 train_time:168378ms step_avg:99.46ms
step:1694/1750 train_time:168480ms step_avg:99.46ms
step:1695/1750 train_time:168584ms step_avg:99.46ms
step:1696/1750 train_time:168686ms step_avg:99.46ms
step:1697/1750 train_time:168790ms step_avg:99.46ms
step:1698/1750 train_time:168892ms step_avg:99.47ms
step:1699/1750 train_time:168994ms step_avg:99.47ms
step:1700/1750 train_time:169097ms step_avg:99.47ms
step:1701/1750 train_time:169201ms step_avg:99.47ms
step:1702/1750 train_time:169305ms step_avg:99.47ms
step:1703/1750 train_time:169407ms step_avg:99.48ms
step:1704/1750 train_time:169510ms step_avg:99.48ms
step:1705/1750 train_time:169611ms step_avg:99.48ms
step:1706/1750 train_time:169714ms step_avg:99.48ms
step:1707/1750 train_time:169817ms step_avg:99.48ms
step:1708/1750 train_time:169921ms step_avg:99.49ms
step:1709/1750 train_time:170024ms step_avg:99.49ms
step:1710/1750 train_time:170126ms step_avg:99.49ms
step:1711/1750 train_time:170229ms step_avg:99.49ms
step:1712/1750 train_time:170331ms step_avg:99.49ms
step:1713/1750 train_time:170435ms step_avg:99.49ms
step:1714/1750 train_time:170538ms step_avg:99.50ms
step:1715/1750 train_time:170645ms step_avg:99.50ms
step:1716/1750 train_time:170747ms step_avg:99.50ms
step:1717/1750 train_time:170849ms step_avg:99.50ms
step:1718/1750 train_time:170951ms step_avg:99.51ms
step:1719/1750 train_time:171056ms step_avg:99.51ms
step:1720/1750 train_time:171158ms step_avg:99.51ms
step:1721/1750 train_time:171261ms step_avg:99.51ms
step:1722/1750 train_time:171364ms step_avg:99.51ms
step:1723/1750 train_time:171466ms step_avg:99.52ms
step:1724/1750 train_time:171569ms step_avg:99.52ms
step:1725/1750 train_time:171672ms step_avg:99.52ms
step:1726/1750 train_time:171774ms step_avg:99.52ms
step:1727/1750 train_time:171876ms step_avg:99.52ms
step:1728/1750 train_time:171979ms step_avg:99.53ms
step:1729/1750 train_time:172082ms step_avg:99.53ms
step:1730/1750 train_time:172184ms step_avg:99.53ms
step:1731/1750 train_time:172287ms step_avg:99.53ms
step:1732/1750 train_time:172390ms step_avg:99.53ms
step:1733/1750 train_time:172493ms step_avg:99.53ms
step:1734/1750 train_time:172597ms step_avg:99.54ms
step:1735/1750 train_time:172700ms step_avg:99.54ms
step:1736/1750 train_time:172803ms step_avg:99.54ms
step:1737/1750 train_time:172906ms step_avg:99.54ms
step:1738/1750 train_time:173010ms step_avg:99.55ms
step:1739/1750 train_time:173111ms step_avg:99.55ms
step:1740/1750 train_time:173213ms step_avg:99.55ms
step:1741/1750 train_time:173320ms step_avg:99.55ms
step:1742/1750 train_time:173423ms step_avg:99.55ms
step:1743/1750 train_time:173527ms step_avg:99.56ms
step:1744/1750 train_time:173631ms step_avg:99.56ms
step:1745/1750 train_time:173732ms step_avg:99.56ms
step:1746/1750 train_time:173834ms step_avg:99.56ms
step:1747/1750 train_time:173939ms step_avg:99.56ms
step:1748/1750 train_time:174042ms step_avg:99.57ms
step:1749/1750 train_time:174144ms step_avg:99.57ms
step:1750/1750 train_time:174247ms step_avg:99.57ms
step:1750/1750 val_loss:3.2824 train_time:174339ms step_avg:99.62ms
peak memory allocated: 33278 MiB reserved: 49134 MiB
