import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X
'''
import torch.utils.dlpack as thd
from cupyx.scipy.sparse.linalg import svds as cupyx_svds

def several_sv_svds_approximation(W_torch, k, num_iter=50):
    """SVD approximation using the top k singular values and corresponding vectors."""
    # Store original device and dtype
    original_device = W_torch.device
    original_dtype = W_torch.dtype
    
    W = cp.from_dlpack(thd.to_dlpack(W_torch)).astype(cp.float32)
    U, S, Vt = cupyx_svds(W, k=min([k, W.shape[0] - 1, W.shape[1] - 1]), maxiter=num_iter, which='LM')

    # Convert back to torch tensors and ensure they're on the correct device
    approx_torch_U = thd.from_dlpack(U.toDlpack()).to(device=original_device, dtype=original_dtype)
    approx_torch_S = thd.from_dlpack(S.toDlpack()).to(device=original_device, dtype=original_dtype)
    approx_torch_Vt = thd.from_dlpack(Vt.toDlpack()).to(device=original_device, dtype=original_dtype)
    
    return approx_torch_U, approx_torch_S, approx_torch_Vt

class NormNeon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    # v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    u, s, vt = several_sv_svds_approximation(grad.bfloat16(), 5)
                    p.add_(other=u@vt, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()
'''
class FastNormMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, sgd_coeff=0):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, sgd_coeff=sgd_coeff)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        self.sgd_coeff = sgd_coeff

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            sgd_coeff = group["sgd_coeff"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    # assume: grad is a Tensor, momentum_buffer exists, momentum, sgd_coeff, eps, eff_lr are defined
                    with torch.no_grad():
                        # inplace blend with momentum (keeps original semantic: grad becomes the lerped tensor)
                        grad.lerp_(momentum_buffer, momentum)

                        # compute zeropower part from the (post-lerp) grad, in bfloat16 to match the original code
                        if sgd_coeff != 1:
                            # create update_part in bfloat16 (may allocate once per call; unavoidable if zeropower expects bfloat16)
                            update_part = zeropower_via_newtonschulz5(grad.to(torch.bfloat16), 5)

                            # upcast update_part to grad dtype to do mixed-dtype blending without extra temporaries later
                            if update_part.dtype != grad.dtype:
                                update_part = update_part.to(grad.dtype)

                        # compute inverse norm scalar efficiently (no large temp tensors)
                        # - view(-1) is cheap (no copy)
                        # - dot(view, view) is a single reduction
                        norm_sq = grad.view(-1).dot(grad.view(-1))
                        inv_norm = (norm_sq + eps).rsqrt()  # stable rsqrt on the scalar

                        # normalize grad in-place (so we avoid allocating g_normalized)
                        grad.mul_(inv_norm)   # now grad == g_normalized (in-place)

                        # build final update (reuse update_part buffer if available)
                        if sgd_coeff != 1:
                            # update_part := (1 - sgd_coeff) * update_part + sgd_coeff * grad
                            # do it in-place on update_part to avoid creating another temp
                            update_part.mul_(1 - sgd_coeff)    # in-place scale
                            update_part.lerp_(grad, sgd_coeff) # in-place linear interpolation
                            update = update_part
                        else:
                            # if sgd_coeff == 1 we can use grad directly (already normalized in-place)
                            update = grad

                        # apply update (same as original)
                        p.add_(other=update, alpha=-eff_lr)
                    '''
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    eps = 1e-12
                    g_normalized = grad / (grad.norm() + eps)
                    if sgd_coeff != 1:
                        update_part = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                        update = (1 - sgd_coeff) * update_part + sgd_coeff * g_normalized
                    else:
                        update = sgd_coeff * g_normalized
                    p.add_(other=update, alpha=-eff_lr)
                    '''
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class NormMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, sgd_coeff=0):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, sgd_coeff=sgd_coeff)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        self.sgd_coeff = sgd_coeff

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            sgd_coeff = group["sgd_coeff"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    eps = 1e-12
                    g_normalized = grad / (grad.norm() + eps)
                    if sgd_coeff != 1:
                        update_part = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                        update = (1 - sgd_coeff) * update_part + sgd_coeff * g_normalized
                    else:
                        update = sgd_coeff * g_normalized
                    p.add_(other=update, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()
class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)
                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_reduce_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 27.5
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos : pos + max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()
    starts = []
    for i in range(1, len(boundary_positions)):
        end = boundary_positions[i].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == dist.get_world_size():
                return starts, end - pos
            start = end
    assert False # increase max_batch_span if necessary

def distributed_data_generator(filename_pattern: str, batch_size: int, align_to_bos: bool):
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    max_batch_span = 2 * batch_size if align_to_bos else batch_size # provide buffer to handle samples up to length local_batch_size
    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            batch_span = batch_size
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1750 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
from datetime import datetime
if master_process:
    # run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_id = str(int(time.time() * 1000))
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0)
optimizer2 = NormMuon(hidden_matrix_params, lr=0.05, momentum=0.95, weight_decay=0, sgd_coeff=-0.1)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.11 | packaged by conda-forge | (main, Jun  4 2025, 14:45:31) [GCC 13.3.0]
Running PyTorch 2.9.0.dev20250824+cu126 compiled for CUDA 12.6
Sun Aug 24 18:19:36 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:04:00.0 Off |                    0 |
| N/A   42C    P0            123W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:23:00.0 Off |                    0 |
| N/A   35C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:43:00.0 Off |                    0 |
| N/A   40C    P0            126W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   34C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:84:00.0 Off |                    0 |
| N/A   40C    P0            123W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:A3:00.0 Off |                    0 |
| N/A   34C    P0            123W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:C3:00.0 Off |                    0 |
| N/A   41C    P0            123W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E4:00.0 Off |                    0 |
| N/A   33C    P0            113W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.06ms
step:1/1750 train_time:147ms step_avg:147.44ms
step:2/1750 train_time:167ms step_avg:83.59ms
step:3/1750 train_time:249ms step_avg:83.02ms
step:4/1750 train_time:340ms step_avg:85.00ms
step:5/1750 train_time:432ms step_avg:86.42ms
step:6/1750 train_time:524ms step_avg:87.40ms
step:7/1750 train_time:617ms step_avg:88.11ms
step:8/1750 train_time:709ms step_avg:88.64ms
step:9/1750 train_time:802ms step_avg:89.07ms
step:10/1750 train_time:894ms step_avg:89.40ms
step:11/1750 train_time:986ms step_avg:89.68ms
step:12/1750 train_time:1081ms step_avg:90.12ms
step:13/1750 train_time:1178ms step_avg:90.61ms
step:14/1750 train_time:1272ms step_avg:90.88ms
step:15/1750 train_time:1365ms step_avg:91.01ms
step:16/1750 train_time:1458ms step_avg:91.14ms
step:17/1750 train_time:1552ms step_avg:91.27ms
step:18/1750 train_time:1644ms step_avg:91.34ms
step:19/1750 train_time:1737ms step_avg:91.40ms
step:20/1750 train_time:1830ms step_avg:91.49ms
step:21/1750 train_time:1923ms step_avg:91.56ms
step:22/1750 train_time:2017ms step_avg:91.66ms
step:23/1750 train_time:2111ms step_avg:91.79ms
step:24/1750 train_time:2206ms step_avg:91.90ms
step:25/1750 train_time:2300ms step_avg:92.01ms
step:26/1750 train_time:2394ms step_avg:92.08ms
step:27/1750 train_time:2488ms step_avg:92.15ms
step:28/1750 train_time:2581ms step_avg:92.18ms
step:29/1750 train_time:2674ms step_avg:92.22ms
step:30/1750 train_time:2767ms step_avg:92.23ms
step:31/1750 train_time:2859ms step_avg:92.23ms
step:32/1750 train_time:2952ms step_avg:92.26ms
step:33/1750 train_time:3045ms step_avg:92.28ms
step:34/1750 train_time:3140ms step_avg:92.34ms
step:35/1750 train_time:3233ms step_avg:92.37ms
step:36/1750 train_time:3328ms step_avg:92.44ms
step:37/1750 train_time:3422ms step_avg:92.48ms
step:38/1750 train_time:3515ms step_avg:92.51ms
step:39/1750 train_time:3609ms step_avg:92.54ms
step:40/1750 train_time:3702ms step_avg:92.55ms
step:41/1750 train_time:3795ms step_avg:92.57ms
step:42/1750 train_time:3889ms step_avg:92.59ms
step:43/1750 train_time:3982ms step_avg:92.60ms
step:44/1750 train_time:4075ms step_avg:92.62ms
step:45/1750 train_time:4169ms step_avg:92.65ms
step:46/1750 train_time:4262ms step_avg:92.66ms
step:47/1750 train_time:4357ms step_avg:92.70ms
step:48/1750 train_time:4451ms step_avg:92.73ms
step:49/1750 train_time:4544ms step_avg:92.74ms
step:50/1750 train_time:4637ms step_avg:92.74ms
step:51/1750 train_time:4730ms step_avg:92.75ms
step:52/1750 train_time:4823ms step_avg:92.75ms
step:53/1750 train_time:4916ms step_avg:92.75ms
step:54/1750 train_time:5010ms step_avg:92.77ms
step:55/1750 train_time:5104ms step_avg:92.79ms
step:56/1750 train_time:5197ms step_avg:92.81ms
step:57/1750 train_time:5292ms step_avg:92.84ms
step:58/1750 train_time:5386ms step_avg:92.86ms
step:59/1750 train_time:5480ms step_avg:92.88ms
step:60/1750 train_time:5573ms step_avg:92.89ms
step:61/1750 train_time:5667ms step_avg:92.90ms
step:62/1750 train_time:5760ms step_avg:92.90ms
step:63/1750 train_time:5853ms step_avg:92.91ms
step:64/1750 train_time:5946ms step_avg:92.90ms
step:65/1750 train_time:6039ms step_avg:92.90ms
step:66/1750 train_time:6132ms step_avg:92.92ms
step:67/1750 train_time:6225ms step_avg:92.92ms
step:68/1750 train_time:6320ms step_avg:92.94ms
step:69/1750 train_time:6414ms step_avg:92.96ms
step:70/1750 train_time:6508ms step_avg:92.98ms
step:71/1750 train_time:6602ms step_avg:92.99ms
step:72/1750 train_time:6696ms step_avg:92.99ms
step:73/1750 train_time:6790ms step_avg:93.01ms
step:74/1750 train_time:6884ms step_avg:93.02ms
step:75/1750 train_time:6976ms step_avg:93.02ms
step:76/1750 train_time:7070ms step_avg:93.03ms
step:77/1750 train_time:7163ms step_avg:93.03ms
step:78/1750 train_time:7256ms step_avg:93.03ms
step:79/1750 train_time:7350ms step_avg:93.04ms
step:80/1750 train_time:7444ms step_avg:93.05ms
step:81/1750 train_time:7538ms step_avg:93.06ms
step:82/1750 train_time:7631ms step_avg:93.07ms
step:83/1750 train_time:7725ms step_avg:93.07ms
step:84/1750 train_time:7818ms step_avg:93.07ms
step:85/1750 train_time:7911ms step_avg:93.07ms
step:86/1750 train_time:8005ms step_avg:93.08ms
step:87/1750 train_time:8098ms step_avg:93.08ms
step:88/1750 train_time:8192ms step_avg:93.09ms
step:89/1750 train_time:8286ms step_avg:93.10ms
step:90/1750 train_time:8379ms step_avg:93.10ms
step:91/1750 train_time:8472ms step_avg:93.10ms
step:92/1750 train_time:8566ms step_avg:93.11ms
step:93/1750 train_time:8659ms step_avg:93.11ms
step:94/1750 train_time:8752ms step_avg:93.11ms
step:95/1750 train_time:8846ms step_avg:93.12ms
step:96/1750 train_time:8939ms step_avg:93.11ms
step:97/1750 train_time:9033ms step_avg:93.12ms
step:98/1750 train_time:9127ms step_avg:93.14ms
step:99/1750 train_time:9220ms step_avg:93.13ms
step:100/1750 train_time:9313ms step_avg:93.13ms
step:101/1750 train_time:9406ms step_avg:93.13ms
step:102/1750 train_time:9499ms step_avg:93.13ms
step:103/1750 train_time:9593ms step_avg:93.13ms
step:104/1750 train_time:9687ms step_avg:93.14ms
step:105/1750 train_time:9780ms step_avg:93.14ms
step:106/1750 train_time:9873ms step_avg:93.14ms
step:107/1750 train_time:9966ms step_avg:93.14ms
step:108/1750 train_time:10059ms step_avg:93.14ms
step:109/1750 train_time:10152ms step_avg:93.14ms
step:110/1750 train_time:10246ms step_avg:93.15ms
step:111/1750 train_time:10339ms step_avg:93.14ms
step:112/1750 train_time:10432ms step_avg:93.14ms
step:113/1750 train_time:10525ms step_avg:93.14ms
step:114/1750 train_time:10618ms step_avg:93.14ms
step:115/1750 train_time:10712ms step_avg:93.15ms
step:116/1750 train_time:10805ms step_avg:93.15ms
step:117/1750 train_time:10898ms step_avg:93.15ms
step:118/1750 train_time:10992ms step_avg:93.15ms
step:119/1750 train_time:11086ms step_avg:93.16ms
step:120/1750 train_time:11179ms step_avg:93.16ms
step:121/1750 train_time:11272ms step_avg:93.16ms
step:122/1750 train_time:11365ms step_avg:93.16ms
step:123/1750 train_time:11459ms step_avg:93.16ms
step:124/1750 train_time:11552ms step_avg:93.16ms
step:125/1750 train_time:11645ms step_avg:93.16ms
step:125/1750 val_loss:4.6470 train_time:11728ms step_avg:93.82ms
step:126/1750 train_time:11750ms step_avg:93.25ms
step:127/1750 train_time:11839ms step_avg:93.22ms
step:128/1750 train_time:11942ms step_avg:93.30ms
step:129/1750 train_time:12037ms step_avg:93.31ms
step:130/1750 train_time:12130ms step_avg:93.31ms
step:131/1750 train_time:12222ms step_avg:93.30ms
step:132/1750 train_time:12315ms step_avg:93.30ms
step:133/1750 train_time:12408ms step_avg:93.29ms
step:134/1750 train_time:12501ms step_avg:93.29ms
step:135/1750 train_time:12593ms step_avg:93.28ms
step:136/1750 train_time:12686ms step_avg:93.28ms
step:137/1750 train_time:12780ms step_avg:93.29ms
step:138/1750 train_time:12878ms step_avg:93.32ms
step:139/1750 train_time:12974ms step_avg:93.33ms
step:140/1750 train_time:13068ms step_avg:93.34ms
step:141/1750 train_time:13161ms step_avg:93.34ms
step:142/1750 train_time:13255ms step_avg:93.34ms
step:143/1750 train_time:13348ms step_avg:93.34ms
step:144/1750 train_time:13440ms step_avg:93.34ms
step:145/1750 train_time:13534ms step_avg:93.33ms
step:146/1750 train_time:13627ms step_avg:93.33ms
step:147/1750 train_time:13720ms step_avg:93.34ms
step:148/1750 train_time:13816ms step_avg:93.35ms
step:149/1750 train_time:13910ms step_avg:93.35ms
step:150/1750 train_time:14004ms step_avg:93.36ms
step:151/1750 train_time:14099ms step_avg:93.37ms
step:152/1750 train_time:14194ms step_avg:93.38ms
step:153/1750 train_time:14287ms step_avg:93.38ms
step:154/1750 train_time:14381ms step_avg:93.38ms
step:155/1750 train_time:14474ms step_avg:93.38ms
step:156/1750 train_time:14567ms step_avg:93.38ms
step:157/1750 train_time:14660ms step_avg:93.37ms
step:158/1750 train_time:14754ms step_avg:93.38ms
step:159/1750 train_time:14848ms step_avg:93.38ms
step:160/1750 train_time:14942ms step_avg:93.39ms
step:161/1750 train_time:15037ms step_avg:93.40ms
step:162/1750 train_time:15131ms step_avg:93.40ms
step:163/1750 train_time:15224ms step_avg:93.40ms
step:164/1750 train_time:15318ms step_avg:93.40ms
step:165/1750 train_time:15412ms step_avg:93.40ms
step:166/1750 train_time:15505ms step_avg:93.40ms
step:167/1750 train_time:15598ms step_avg:93.40ms
step:168/1750 train_time:15691ms step_avg:93.40ms
step:169/1750 train_time:15784ms step_avg:93.40ms
step:170/1750 train_time:15879ms step_avg:93.41ms
step:171/1750 train_time:15973ms step_avg:93.41ms
step:172/1750 train_time:16067ms step_avg:93.41ms
step:173/1750 train_time:16161ms step_avg:93.41ms
step:174/1750 train_time:16255ms step_avg:93.42ms
step:175/1750 train_time:16348ms step_avg:93.42ms
step:176/1750 train_time:16441ms step_avg:93.42ms
step:177/1750 train_time:16535ms step_avg:93.42ms
step:178/1750 train_time:16628ms step_avg:93.41ms
step:179/1750 train_time:16721ms step_avg:93.41ms
step:180/1750 train_time:16815ms step_avg:93.42ms
step:181/1750 train_time:16909ms step_avg:93.42ms
step:182/1750 train_time:17003ms step_avg:93.42ms
step:183/1750 train_time:17098ms step_avg:93.43ms
step:184/1750 train_time:17192ms step_avg:93.44ms
step:185/1750 train_time:17286ms step_avg:93.44ms
step:186/1750 train_time:17380ms step_avg:93.44ms
step:187/1750 train_time:17474ms step_avg:93.45ms
step:188/1750 train_time:17567ms step_avg:93.44ms
step:189/1750 train_time:17661ms step_avg:93.44ms
step:190/1750 train_time:17755ms step_avg:93.45ms
step:191/1750 train_time:17848ms step_avg:93.45ms
step:192/1750 train_time:17942ms step_avg:93.45ms
step:193/1750 train_time:18038ms step_avg:93.46ms
step:194/1750 train_time:18132ms step_avg:93.46ms
step:195/1750 train_time:18225ms step_avg:93.46ms
step:196/1750 train_time:18319ms step_avg:93.47ms
step:197/1750 train_time:18414ms step_avg:93.47ms
step:198/1750 train_time:18507ms step_avg:93.47ms
step:199/1750 train_time:18601ms step_avg:93.47ms
step:200/1750 train_time:18695ms step_avg:93.47ms
step:201/1750 train_time:18788ms step_avg:93.47ms
step:202/1750 train_time:18881ms step_avg:93.47ms
step:203/1750 train_time:18975ms step_avg:93.48ms
step:204/1750 train_time:19070ms step_avg:93.48ms
step:205/1750 train_time:19163ms step_avg:93.48ms
step:206/1750 train_time:19257ms step_avg:93.48ms
step:207/1750 train_time:19351ms step_avg:93.48ms
step:208/1750 train_time:19444ms step_avg:93.48ms
step:209/1750 train_time:19538ms step_avg:93.48ms
step:210/1750 train_time:19632ms step_avg:93.48ms
step:211/1750 train_time:19725ms step_avg:93.48ms
step:212/1750 train_time:19818ms step_avg:93.48ms
step:213/1750 train_time:19912ms step_avg:93.48ms
step:214/1750 train_time:20006ms step_avg:93.48ms
step:215/1750 train_time:20100ms step_avg:93.49ms
step:216/1750 train_time:20193ms step_avg:93.49ms
step:217/1750 train_time:20288ms step_avg:93.49ms
step:218/1750 train_time:20381ms step_avg:93.49ms
step:219/1750 train_time:20474ms step_avg:93.49ms
step:220/1750 train_time:20568ms step_avg:93.49ms
step:221/1750 train_time:20662ms step_avg:93.49ms
step:222/1750 train_time:20755ms step_avg:93.49ms
step:223/1750 train_time:20848ms step_avg:93.49ms
step:224/1750 train_time:20942ms step_avg:93.49ms
step:225/1750 train_time:21036ms step_avg:93.49ms
step:226/1750 train_time:21130ms step_avg:93.50ms
step:227/1750 train_time:21224ms step_avg:93.50ms
step:228/1750 train_time:21319ms step_avg:93.50ms
step:229/1750 train_time:21413ms step_avg:93.50ms
step:230/1750 train_time:21506ms step_avg:93.50ms
step:231/1750 train_time:21599ms step_avg:93.50ms
step:232/1750 train_time:21693ms step_avg:93.50ms
step:233/1750 train_time:21786ms step_avg:93.50ms
step:234/1750 train_time:21880ms step_avg:93.50ms
step:235/1750 train_time:21974ms step_avg:93.50ms
step:236/1750 train_time:22068ms step_avg:93.51ms
step:237/1750 train_time:22161ms step_avg:93.51ms
step:238/1750 train_time:22254ms step_avg:93.50ms
step:239/1750 train_time:22348ms step_avg:93.51ms
step:240/1750 train_time:22441ms step_avg:93.51ms
step:241/1750 train_time:22535ms step_avg:93.51ms
step:242/1750 train_time:22628ms step_avg:93.50ms
step:243/1750 train_time:22721ms step_avg:93.50ms
step:244/1750 train_time:22815ms step_avg:93.51ms
step:245/1750 train_time:22908ms step_avg:93.50ms
step:246/1750 train_time:23002ms step_avg:93.50ms
step:247/1750 train_time:23096ms step_avg:93.51ms
step:248/1750 train_time:23189ms step_avg:93.51ms
step:249/1750 train_time:23283ms step_avg:93.51ms
step:250/1750 train_time:23377ms step_avg:93.51ms
step:250/1750 val_loss:4.1051 train_time:23460ms step_avg:93.84ms
step:251/1750 train_time:23480ms step_avg:93.55ms
step:252/1750 train_time:23572ms step_avg:93.54ms
step:253/1750 train_time:23667ms step_avg:93.55ms
step:254/1750 train_time:23761ms step_avg:93.55ms
step:255/1750 train_time:23855ms step_avg:93.55ms
step:256/1750 train_time:23947ms step_avg:93.54ms
step:257/1750 train_time:24040ms step_avg:93.54ms
step:258/1750 train_time:24133ms step_avg:93.54ms
step:259/1750 train_time:24226ms step_avg:93.54ms
step:260/1750 train_time:24319ms step_avg:93.53ms
step:261/1750 train_time:24414ms step_avg:93.54ms
step:262/1750 train_time:24511ms step_avg:93.55ms
step:263/1750 train_time:24606ms step_avg:93.56ms
step:264/1750 train_time:24701ms step_avg:93.56ms
step:265/1750 train_time:24796ms step_avg:93.57ms
step:266/1750 train_time:24890ms step_avg:93.57ms
step:267/1750 train_time:24984ms step_avg:93.57ms
step:268/1750 train_time:25077ms step_avg:93.57ms
step:269/1750 train_time:25170ms step_avg:93.57ms
step:270/1750 train_time:25264ms step_avg:93.57ms
step:271/1750 train_time:25358ms step_avg:93.57ms
step:272/1750 train_time:25454ms step_avg:93.58ms
step:273/1750 train_time:25550ms step_avg:93.59ms
step:274/1750 train_time:25644ms step_avg:93.59ms
step:275/1750 train_time:25739ms step_avg:93.60ms
step:276/1750 train_time:25834ms step_avg:93.60ms
step:277/1750 train_time:25927ms step_avg:93.60ms
step:278/1750 train_time:26021ms step_avg:93.60ms
step:279/1750 train_time:26115ms step_avg:93.60ms
step:280/1750 train_time:26208ms step_avg:93.60ms
step:281/1750 train_time:26302ms step_avg:93.60ms
step:282/1750 train_time:26395ms step_avg:93.60ms
step:283/1750 train_time:26489ms step_avg:93.60ms
step:284/1750 train_time:26584ms step_avg:93.61ms
step:285/1750 train_time:26678ms step_avg:93.61ms
step:286/1750 train_time:26772ms step_avg:93.61ms
step:287/1750 train_time:26866ms step_avg:93.61ms
step:288/1750 train_time:26960ms step_avg:93.61ms
step:289/1750 train_time:27054ms step_avg:93.61ms
step:290/1750 train_time:27147ms step_avg:93.61ms
step:291/1750 train_time:27241ms step_avg:93.61ms
step:292/1750 train_time:27335ms step_avg:93.61ms
step:293/1750 train_time:27428ms step_avg:93.61ms
step:294/1750 train_time:27523ms step_avg:93.61ms
step:295/1750 train_time:27617ms step_avg:93.62ms
step:296/1750 train_time:27712ms step_avg:93.62ms
step:297/1750 train_time:27806ms step_avg:93.62ms
step:298/1750 train_time:27900ms step_avg:93.63ms
step:299/1750 train_time:27994ms step_avg:93.63ms
step:300/1750 train_time:28088ms step_avg:93.63ms
step:301/1750 train_time:28181ms step_avg:93.63ms
step:302/1750 train_time:28276ms step_avg:93.63ms
step:303/1750 train_time:28369ms step_avg:93.63ms
step:304/1750 train_time:28462ms step_avg:93.63ms
step:305/1750 train_time:28557ms step_avg:93.63ms
step:306/1750 train_time:28651ms step_avg:93.63ms
step:307/1750 train_time:28745ms step_avg:93.63ms
step:308/1750 train_time:28839ms step_avg:93.63ms
step:309/1750 train_time:28933ms step_avg:93.63ms
step:310/1750 train_time:29028ms step_avg:93.64ms
step:311/1750 train_time:29123ms step_avg:93.64ms
step:312/1750 train_time:29217ms step_avg:93.65ms
step:313/1750 train_time:29311ms step_avg:93.65ms
step:314/1750 train_time:29405ms step_avg:93.65ms
step:315/1750 train_time:29499ms step_avg:93.65ms
step:316/1750 train_time:29594ms step_avg:93.65ms
step:317/1750 train_time:29688ms step_avg:93.65ms
step:318/1750 train_time:29782ms step_avg:93.65ms
step:319/1750 train_time:29877ms step_avg:93.66ms
step:320/1750 train_time:29971ms step_avg:93.66ms
step:321/1750 train_time:30065ms step_avg:93.66ms
step:322/1750 train_time:30159ms step_avg:93.66ms
step:323/1750 train_time:30254ms step_avg:93.66ms
step:324/1750 train_time:30347ms step_avg:93.66ms
step:325/1750 train_time:30441ms step_avg:93.66ms
step:326/1750 train_time:30535ms step_avg:93.67ms
step:327/1750 train_time:30629ms step_avg:93.67ms
step:328/1750 train_time:31061ms step_avg:94.70ms
step:329/1750 train_time:31119ms step_avg:94.59ms
step:330/1750 train_time:31212ms step_avg:94.58ms
step:331/1750 train_time:31305ms step_avg:94.58ms
step:332/1750 train_time:31397ms step_avg:94.57ms
step:333/1750 train_time:31491ms step_avg:94.57ms
step:334/1750 train_time:31584ms step_avg:94.56ms
step:335/1750 train_time:31677ms step_avg:94.56ms
step:336/1750 train_time:31770ms step_avg:94.55ms
step:337/1750 train_time:31863ms step_avg:94.55ms
step:338/1750 train_time:31959ms step_avg:94.55ms
step:339/1750 train_time:32058ms step_avg:94.57ms
step:340/1750 train_time:32155ms step_avg:94.57ms
step:341/1750 train_time:32250ms step_avg:94.58ms
step:342/1750 train_time:32344ms step_avg:94.57ms
step:343/1750 train_time:32437ms step_avg:94.57ms
step:344/1750 train_time:32531ms step_avg:94.57ms
step:345/1750 train_time:32624ms step_avg:94.56ms
step:346/1750 train_time:32717ms step_avg:94.56ms
step:347/1750 train_time:32811ms step_avg:94.56ms
step:348/1750 train_time:32905ms step_avg:94.55ms
step:349/1750 train_time:33001ms step_avg:94.56ms
step:350/1750 train_time:33096ms step_avg:94.56ms
step:351/1750 train_time:33192ms step_avg:94.56ms
step:352/1750 train_time:33286ms step_avg:94.56ms
step:353/1750 train_time:33380ms step_avg:94.56ms
step:354/1750 train_time:33474ms step_avg:94.56ms
step:355/1750 train_time:33567ms step_avg:94.56ms
step:356/1750 train_time:33661ms step_avg:94.55ms
step:357/1750 train_time:33755ms step_avg:94.55ms
step:358/1750 train_time:33847ms step_avg:94.55ms
step:359/1750 train_time:33941ms step_avg:94.54ms
step:360/1750 train_time:34036ms step_avg:94.54ms
step:361/1750 train_time:34131ms step_avg:94.55ms
step:362/1750 train_time:34225ms step_avg:94.55ms
step:363/1750 train_time:34320ms step_avg:94.55ms
step:364/1750 train_time:34414ms step_avg:94.54ms
step:365/1750 train_time:34508ms step_avg:94.54ms
step:366/1750 train_time:34602ms step_avg:94.54ms
step:367/1750 train_time:34696ms step_avg:94.54ms
step:368/1750 train_time:34790ms step_avg:94.54ms
step:369/1750 train_time:34883ms step_avg:94.53ms
step:370/1750 train_time:34977ms step_avg:94.53ms
step:371/1750 train_time:35071ms step_avg:94.53ms
step:372/1750 train_time:35165ms step_avg:94.53ms
step:373/1750 train_time:35260ms step_avg:94.53ms
step:374/1750 train_time:35356ms step_avg:94.53ms
step:375/1750 train_time:35451ms step_avg:94.53ms
step:375/1750 val_loss:3.9027 train_time:35534ms step_avg:94.76ms
step:376/1750 train_time:35555ms step_avg:94.56ms
step:377/1750 train_time:35646ms step_avg:94.55ms
step:378/1750 train_time:35745ms step_avg:94.56ms
step:379/1750 train_time:35839ms step_avg:94.56ms
step:380/1750 train_time:35932ms step_avg:94.56ms
step:381/1750 train_time:36026ms step_avg:94.56ms
step:382/1750 train_time:36118ms step_avg:94.55ms
step:383/1750 train_time:36211ms step_avg:94.55ms
step:384/1750 train_time:36305ms step_avg:94.54ms
step:385/1750 train_time:36398ms step_avg:94.54ms
step:386/1750 train_time:36492ms step_avg:94.54ms
step:387/1750 train_time:36587ms step_avg:94.54ms
step:388/1750 train_time:36684ms step_avg:94.55ms
step:389/1750 train_time:36779ms step_avg:94.55ms
step:390/1750 train_time:36873ms step_avg:94.55ms
step:391/1750 train_time:36968ms step_avg:94.55ms
step:392/1750 train_time:37064ms step_avg:94.55ms
step:393/1750 train_time:37159ms step_avg:94.55ms
step:394/1750 train_time:37254ms step_avg:94.55ms
step:395/1750 train_time:37350ms step_avg:94.56ms
step:396/1750 train_time:37445ms step_avg:94.56ms
step:397/1750 train_time:37542ms step_avg:94.56ms
step:398/1750 train_time:37639ms step_avg:94.57ms
step:399/1750 train_time:37737ms step_avg:94.58ms
step:400/1750 train_time:37834ms step_avg:94.58ms
step:401/1750 train_time:37930ms step_avg:94.59ms
step:402/1750 train_time:38026ms step_avg:94.59ms
step:403/1750 train_time:38121ms step_avg:94.59ms
step:404/1750 train_time:38216ms step_avg:94.60ms
step:405/1750 train_time:38311ms step_avg:94.60ms
step:406/1750 train_time:38407ms step_avg:94.60ms
step:407/1750 train_time:38502ms step_avg:94.60ms
step:408/1750 train_time:38599ms step_avg:94.60ms
step:409/1750 train_time:38697ms step_avg:94.61ms
step:410/1750 train_time:38794ms step_avg:94.62ms
step:411/1750 train_time:38890ms step_avg:94.62ms
step:412/1750 train_time:38987ms step_avg:94.63ms
step:413/1750 train_time:39083ms step_avg:94.63ms
step:414/1750 train_time:39179ms step_avg:94.63ms
step:415/1750 train_time:39274ms step_avg:94.64ms
step:416/1750 train_time:39369ms step_avg:94.64ms
step:417/1750 train_time:39464ms step_avg:94.64ms
step:418/1750 train_time:39560ms step_avg:94.64ms
step:419/1750 train_time:39656ms step_avg:94.65ms
step:420/1750 train_time:39753ms step_avg:94.65ms
step:421/1750 train_time:39849ms step_avg:94.65ms
step:422/1750 train_time:39945ms step_avg:94.66ms
step:423/1750 train_time:40040ms step_avg:94.66ms
step:424/1750 train_time:40136ms step_avg:94.66ms
step:425/1750 train_time:40232ms step_avg:94.66ms
step:426/1750 train_time:40327ms step_avg:94.67ms
step:427/1750 train_time:40423ms step_avg:94.67ms
step:428/1750 train_time:40519ms step_avg:94.67ms
step:429/1750 train_time:40614ms step_avg:94.67ms
step:430/1750 train_time:40710ms step_avg:94.67ms
step:431/1750 train_time:40806ms step_avg:94.68ms
step:432/1750 train_time:40902ms step_avg:94.68ms
step:433/1750 train_time:40998ms step_avg:94.68ms
step:434/1750 train_time:41094ms step_avg:94.69ms
step:435/1750 train_time:41190ms step_avg:94.69ms
step:436/1750 train_time:41287ms step_avg:94.69ms
step:437/1750 train_time:41382ms step_avg:94.70ms
step:438/1750 train_time:41478ms step_avg:94.70ms
step:439/1750 train_time:41574ms step_avg:94.70ms
step:440/1750 train_time:41670ms step_avg:94.70ms
step:441/1750 train_time:41766ms step_avg:94.71ms
step:442/1750 train_time:41861ms step_avg:94.71ms
step:443/1750 train_time:41958ms step_avg:94.71ms
step:444/1750 train_time:42054ms step_avg:94.72ms
step:445/1750 train_time:42150ms step_avg:94.72ms
step:446/1750 train_time:42246ms step_avg:94.72ms
step:447/1750 train_time:42341ms step_avg:94.72ms
step:448/1750 train_time:42437ms step_avg:94.73ms
step:449/1750 train_time:42533ms step_avg:94.73ms
step:450/1750 train_time:42629ms step_avg:94.73ms
step:451/1750 train_time:42725ms step_avg:94.73ms
step:452/1750 train_time:42821ms step_avg:94.74ms
step:453/1750 train_time:42917ms step_avg:94.74ms
step:454/1750 train_time:43012ms step_avg:94.74ms
step:455/1750 train_time:43108ms step_avg:94.74ms
step:456/1750 train_time:43204ms step_avg:94.75ms
step:457/1750 train_time:43300ms step_avg:94.75ms
step:458/1750 train_time:43395ms step_avg:94.75ms
step:459/1750 train_time:43491ms step_avg:94.75ms
step:460/1750 train_time:43587ms step_avg:94.75ms
step:461/1750 train_time:43683ms step_avg:94.76ms
step:462/1750 train_time:43779ms step_avg:94.76ms
step:463/1750 train_time:43875ms step_avg:94.76ms
step:464/1750 train_time:43971ms step_avg:94.76ms
step:465/1750 train_time:44067ms step_avg:94.77ms
step:466/1750 train_time:44163ms step_avg:94.77ms
step:467/1750 train_time:44259ms step_avg:94.77ms
step:468/1750 train_time:44354ms step_avg:94.77ms
step:469/1750 train_time:44450ms step_avg:94.78ms
step:470/1750 train_time:44546ms step_avg:94.78ms
step:471/1750 train_time:44642ms step_avg:94.78ms
step:472/1750 train_time:44738ms step_avg:94.78ms
step:473/1750 train_time:44834ms step_avg:94.79ms
step:474/1750 train_time:44930ms step_avg:94.79ms
step:475/1750 train_time:45026ms step_avg:94.79ms
step:476/1750 train_time:45121ms step_avg:94.79ms
step:477/1750 train_time:45217ms step_avg:94.80ms
step:478/1750 train_time:45314ms step_avg:94.80ms
step:479/1750 train_time:45410ms step_avg:94.80ms
step:480/1750 train_time:45506ms step_avg:94.80ms
step:481/1750 train_time:45602ms step_avg:94.81ms
step:482/1750 train_time:45697ms step_avg:94.81ms
step:483/1750 train_time:45793ms step_avg:94.81ms
step:484/1750 train_time:45889ms step_avg:94.81ms
step:485/1750 train_time:45986ms step_avg:94.82ms
step:486/1750 train_time:46081ms step_avg:94.82ms
step:487/1750 train_time:46177ms step_avg:94.82ms
step:488/1750 train_time:46274ms step_avg:94.82ms
step:489/1750 train_time:46370ms step_avg:94.83ms
step:490/1750 train_time:46465ms step_avg:94.83ms
step:491/1750 train_time:46561ms step_avg:94.83ms
step:492/1750 train_time:46657ms step_avg:94.83ms
step:493/1750 train_time:46753ms step_avg:94.83ms
step:494/1750 train_time:46849ms step_avg:94.84ms
step:495/1750 train_time:46945ms step_avg:94.84ms
step:496/1750 train_time:47041ms step_avg:94.84ms
step:497/1750 train_time:47137ms step_avg:94.84ms
step:498/1750 train_time:47233ms step_avg:94.85ms
step:499/1750 train_time:47329ms step_avg:94.85ms
step:500/1750 train_time:47425ms step_avg:94.85ms
step:500/1750 val_loss:3.7526 train_time:47510ms step_avg:95.02ms
step:501/1750 train_time:47531ms step_avg:94.87ms
step:502/1750 train_time:47623ms step_avg:94.87ms
step:503/1750 train_time:47722ms step_avg:94.88ms
step:504/1750 train_time:47819ms step_avg:94.88ms
step:505/1750 train_time:47915ms step_avg:94.88ms
step:506/1750 train_time:48010ms step_avg:94.88ms
step:507/1750 train_time:48105ms step_avg:94.88ms
step:508/1750 train_time:48199ms step_avg:94.88ms
step:509/1750 train_time:48295ms step_avg:94.88ms
step:510/1750 train_time:48391ms step_avg:94.88ms
step:511/1750 train_time:48489ms step_avg:94.89ms
step:512/1750 train_time:48586ms step_avg:94.90ms
step:513/1750 train_time:48683ms step_avg:94.90ms
step:514/1750 train_time:48780ms step_avg:94.90ms
step:515/1750 train_time:48876ms step_avg:94.90ms
step:516/1750 train_time:48972ms step_avg:94.91ms
step:517/1750 train_time:49067ms step_avg:94.91ms
step:518/1750 train_time:49162ms step_avg:94.91ms
step:519/1750 train_time:49258ms step_avg:94.91ms
step:520/1750 train_time:49353ms step_avg:94.91ms
step:521/1750 train_time:49450ms step_avg:94.91ms
step:522/1750 train_time:49546ms step_avg:94.92ms
step:523/1750 train_time:49643ms step_avg:94.92ms
step:524/1750 train_time:49740ms step_avg:94.92ms
step:525/1750 train_time:49836ms step_avg:94.93ms
step:526/1750 train_time:49932ms step_avg:94.93ms
step:527/1750 train_time:50028ms step_avg:94.93ms
step:528/1750 train_time:50124ms step_avg:94.93ms
step:529/1750 train_time:50219ms step_avg:94.93ms
step:530/1750 train_time:50315ms step_avg:94.93ms
step:531/1750 train_time:50411ms step_avg:94.94ms
step:532/1750 train_time:50507ms step_avg:94.94ms
step:533/1750 train_time:50604ms step_avg:94.94ms
step:534/1750 train_time:50701ms step_avg:94.94ms
step:535/1750 train_time:50798ms step_avg:94.95ms
step:536/1750 train_time:50895ms step_avg:94.95ms
step:537/1750 train_time:50991ms step_avg:94.96ms
step:538/1750 train_time:51087ms step_avg:94.96ms
step:539/1750 train_time:51183ms step_avg:94.96ms
step:540/1750 train_time:51279ms step_avg:94.96ms
step:541/1750 train_time:51375ms step_avg:94.96ms
step:542/1750 train_time:51471ms step_avg:94.97ms
step:543/1750 train_time:51568ms step_avg:94.97ms
step:544/1750 train_time:51664ms step_avg:94.97ms
step:545/1750 train_time:51760ms step_avg:94.97ms
step:546/1750 train_time:51858ms step_avg:94.98ms
step:547/1750 train_time:51955ms step_avg:94.98ms
step:548/1750 train_time:52052ms step_avg:94.99ms
step:549/1750 train_time:52148ms step_avg:94.99ms
step:550/1750 train_time:52244ms step_avg:94.99ms
step:551/1750 train_time:52341ms step_avg:94.99ms
step:552/1750 train_time:52437ms step_avg:94.99ms
step:553/1750 train_time:52533ms step_avg:95.00ms
step:554/1750 train_time:52629ms step_avg:95.00ms
step:555/1750 train_time:52725ms step_avg:95.00ms
step:556/1750 train_time:52823ms step_avg:95.01ms
step:557/1750 train_time:52920ms step_avg:95.01ms
step:558/1750 train_time:53016ms step_avg:95.01ms
step:559/1750 train_time:53113ms step_avg:95.01ms
step:560/1750 train_time:53209ms step_avg:95.02ms
step:561/1750 train_time:53305ms step_avg:95.02ms
step:562/1750 train_time:53401ms step_avg:95.02ms
step:563/1750 train_time:53496ms step_avg:95.02ms
step:564/1750 train_time:53592ms step_avg:95.02ms
step:565/1750 train_time:53689ms step_avg:95.02ms
step:566/1750 train_time:53786ms step_avg:95.03ms
step:567/1750 train_time:53883ms step_avg:95.03ms
step:568/1750 train_time:53979ms step_avg:95.03ms
step:569/1750 train_time:54078ms step_avg:95.04ms
step:570/1750 train_time:54174ms step_avg:95.04ms
step:571/1750 train_time:54271ms step_avg:95.05ms
step:572/1750 train_time:54367ms step_avg:95.05ms
step:573/1750 train_time:54464ms step_avg:95.05ms
step:574/1750 train_time:54560ms step_avg:95.05ms
step:575/1750 train_time:54657ms step_avg:95.06ms
step:576/1750 train_time:54754ms step_avg:95.06ms
step:577/1750 train_time:54850ms step_avg:95.06ms
step:578/1750 train_time:54948ms step_avg:95.06ms
step:579/1750 train_time:55044ms step_avg:95.07ms
step:580/1750 train_time:55141ms step_avg:95.07ms
step:581/1750 train_time:55237ms step_avg:95.07ms
step:582/1750 train_time:55333ms step_avg:95.07ms
step:583/1750 train_time:55429ms step_avg:95.08ms
step:584/1750 train_time:55525ms step_avg:95.08ms
step:585/1750 train_time:55621ms step_avg:95.08ms
step:586/1750 train_time:55718ms step_avg:95.08ms
step:587/1750 train_time:55815ms step_avg:95.09ms
step:588/1750 train_time:55911ms step_avg:95.09ms
step:589/1750 train_time:56007ms step_avg:95.09ms
step:590/1750 train_time:56104ms step_avg:95.09ms
step:591/1750 train_time:56201ms step_avg:95.09ms
step:592/1750 train_time:56298ms step_avg:95.10ms
step:593/1750 train_time:56394ms step_avg:95.10ms
step:594/1750 train_time:56489ms step_avg:95.10ms
step:595/1750 train_time:56586ms step_avg:95.10ms
step:596/1750 train_time:56683ms step_avg:95.11ms
step:597/1750 train_time:56780ms step_avg:95.11ms
step:598/1750 train_time:56876ms step_avg:95.11ms
step:599/1750 train_time:56973ms step_avg:95.11ms
step:600/1750 train_time:57070ms step_avg:95.12ms
step:601/1750 train_time:57166ms step_avg:95.12ms
step:602/1750 train_time:57263ms step_avg:95.12ms
step:603/1750 train_time:57359ms step_avg:95.12ms
step:604/1750 train_time:57456ms step_avg:95.13ms
step:605/1750 train_time:57552ms step_avg:95.13ms
step:606/1750 train_time:57648ms step_avg:95.13ms
step:607/1750 train_time:57745ms step_avg:95.13ms
step:608/1750 train_time:57842ms step_avg:95.13ms
step:609/1750 train_time:57939ms step_avg:95.14ms
step:610/1750 train_time:58035ms step_avg:95.14ms
step:611/1750 train_time:58132ms step_avg:95.14ms
step:612/1750 train_time:58229ms step_avg:95.15ms
step:613/1750 train_time:58326ms step_avg:95.15ms
step:614/1750 train_time:58423ms step_avg:95.15ms
step:615/1750 train_time:58520ms step_avg:95.15ms
step:616/1750 train_time:58616ms step_avg:95.16ms
step:617/1750 train_time:58712ms step_avg:95.16ms
step:618/1750 train_time:58808ms step_avg:95.16ms
step:619/1750 train_time:58904ms step_avg:95.16ms
step:620/1750 train_time:59001ms step_avg:95.16ms
step:621/1750 train_time:59098ms step_avg:95.17ms
step:622/1750 train_time:59194ms step_avg:95.17ms
step:623/1750 train_time:59291ms step_avg:95.17ms
step:624/1750 train_time:59387ms step_avg:95.17ms
step:625/1750 train_time:59483ms step_avg:95.17ms
step:625/1750 val_loss:3.6683 train_time:59569ms step_avg:95.31ms
step:626/1750 train_time:59589ms step_avg:95.19ms
step:627/1750 train_time:59684ms step_avg:95.19ms
step:628/1750 train_time:59782ms step_avg:95.19ms
step:629/1750 train_time:59878ms step_avg:95.20ms
step:630/1750 train_time:59973ms step_avg:95.20ms
step:631/1750 train_time:60068ms step_avg:95.20ms
step:632/1750 train_time:60164ms step_avg:95.20ms
step:633/1750 train_time:60259ms step_avg:95.20ms
step:634/1750 train_time:60355ms step_avg:95.20ms
step:635/1750 train_time:60450ms step_avg:95.20ms
step:636/1750 train_time:60548ms step_avg:95.20ms
step:637/1750 train_time:60646ms step_avg:95.21ms
step:638/1750 train_time:60744ms step_avg:95.21ms
step:639/1750 train_time:60841ms step_avg:95.21ms
step:640/1750 train_time:60937ms step_avg:95.21ms
step:641/1750 train_time:61033ms step_avg:95.22ms
step:642/1750 train_time:61128ms step_avg:95.21ms
step:643/1750 train_time:61223ms step_avg:95.22ms
step:644/1750 train_time:61319ms step_avg:95.22ms
step:645/1750 train_time:61414ms step_avg:95.22ms
step:646/1750 train_time:61510ms step_avg:95.22ms
step:647/1750 train_time:61608ms step_avg:95.22ms
step:648/1750 train_time:61706ms step_avg:95.22ms
step:649/1750 train_time:61804ms step_avg:95.23ms
step:650/1750 train_time:61901ms step_avg:95.23ms
step:651/1750 train_time:61998ms step_avg:95.23ms
step:652/1750 train_time:62095ms step_avg:95.24ms
step:653/1750 train_time:62192ms step_avg:95.24ms
step:654/1750 train_time:62289ms step_avg:95.24ms
step:655/1750 train_time:62386ms step_avg:95.25ms
step:656/1750 train_time:62484ms step_avg:95.25ms
step:657/1750 train_time:62581ms step_avg:95.25ms
step:658/1750 train_time:62680ms step_avg:95.26ms
step:659/1750 train_time:62779ms step_avg:95.26ms
step:660/1750 train_time:62877ms step_avg:95.27ms
step:661/1750 train_time:62975ms step_avg:95.27ms
step:662/1750 train_time:63073ms step_avg:95.28ms
step:663/1750 train_time:63170ms step_avg:95.28ms
step:664/1750 train_time:63268ms step_avg:95.28ms
step:665/1750 train_time:63365ms step_avg:95.29ms
step:666/1750 train_time:63463ms step_avg:95.29ms
step:667/1750 train_time:63560ms step_avg:95.29ms
step:668/1750 train_time:63658ms step_avg:95.30ms
step:669/1750 train_time:63756ms step_avg:95.30ms
step:670/1750 train_time:63855ms step_avg:95.31ms
step:671/1750 train_time:63953ms step_avg:95.31ms
step:672/1750 train_time:64051ms step_avg:95.31ms
step:673/1750 train_time:64149ms step_avg:95.32ms
step:674/1750 train_time:64246ms step_avg:95.32ms
step:675/1750 train_time:64343ms step_avg:95.32ms
step:676/1750 train_time:64441ms step_avg:95.33ms
step:677/1750 train_time:64538ms step_avg:95.33ms
step:678/1750 train_time:64636ms step_avg:95.33ms
step:679/1750 train_time:64735ms step_avg:95.34ms
step:680/1750 train_time:64833ms step_avg:95.34ms
step:681/1750 train_time:64930ms step_avg:95.35ms
step:682/1750 train_time:65029ms step_avg:95.35ms
step:683/1750 train_time:65127ms step_avg:95.35ms
step:684/1750 train_time:65224ms step_avg:95.36ms
step:685/1750 train_time:65321ms step_avg:95.36ms
step:686/1750 train_time:65418ms step_avg:95.36ms
step:687/1750 train_time:65516ms step_avg:95.37ms
step:688/1750 train_time:65614ms step_avg:95.37ms
step:689/1750 train_time:65712ms step_avg:95.37ms
step:690/1750 train_time:65810ms step_avg:95.38ms
step:691/1750 train_time:65909ms step_avg:95.38ms
step:692/1750 train_time:66007ms step_avg:95.39ms
step:693/1750 train_time:66105ms step_avg:95.39ms
step:694/1750 train_time:66202ms step_avg:95.39ms
step:695/1750 train_time:66300ms step_avg:95.39ms
step:696/1750 train_time:66397ms step_avg:95.40ms
step:697/1750 train_time:66495ms step_avg:95.40ms
step:698/1750 train_time:66593ms step_avg:95.41ms
step:699/1750 train_time:66690ms step_avg:95.41ms
step:700/1750 train_time:66788ms step_avg:95.41ms
step:701/1750 train_time:66886ms step_avg:95.42ms
step:702/1750 train_time:66985ms step_avg:95.42ms
step:703/1750 train_time:67084ms step_avg:95.42ms
step:704/1750 train_time:67181ms step_avg:95.43ms
step:705/1750 train_time:67278ms step_avg:95.43ms
step:706/1750 train_time:67376ms step_avg:95.43ms
step:707/1750 train_time:67473ms step_avg:95.44ms
step:708/1750 train_time:67571ms step_avg:95.44ms
step:709/1750 train_time:67669ms step_avg:95.44ms
step:710/1750 train_time:67768ms step_avg:95.45ms
step:711/1750 train_time:67865ms step_avg:95.45ms
step:712/1750 train_time:67963ms step_avg:95.45ms
step:713/1750 train_time:68061ms step_avg:95.46ms
step:714/1750 train_time:68159ms step_avg:95.46ms
step:715/1750 train_time:68257ms step_avg:95.46ms
step:716/1750 train_time:68355ms step_avg:95.47ms
step:717/1750 train_time:68453ms step_avg:95.47ms
step:718/1750 train_time:68550ms step_avg:95.47ms
step:719/1750 train_time:68649ms step_avg:95.48ms
step:720/1750 train_time:68747ms step_avg:95.48ms
step:721/1750 train_time:68845ms step_avg:95.48ms
step:722/1750 train_time:68942ms step_avg:95.49ms
step:723/1750 train_time:69039ms step_avg:95.49ms
step:724/1750 train_time:69137ms step_avg:95.49ms
step:725/1750 train_time:69235ms step_avg:95.50ms
step:726/1750 train_time:69333ms step_avg:95.50ms
step:727/1750 train_time:69430ms step_avg:95.50ms
step:728/1750 train_time:69528ms step_avg:95.51ms
step:729/1750 train_time:69626ms step_avg:95.51ms
step:730/1750 train_time:69724ms step_avg:95.51ms
step:731/1750 train_time:69822ms step_avg:95.52ms
step:732/1750 train_time:69919ms step_avg:95.52ms
step:733/1750 train_time:70017ms step_avg:95.52ms
step:734/1750 train_time:70115ms step_avg:95.52ms
step:735/1750 train_time:70214ms step_avg:95.53ms
step:736/1750 train_time:70312ms step_avg:95.53ms
step:737/1750 train_time:70409ms step_avg:95.54ms
step:738/1750 train_time:70508ms step_avg:95.54ms
step:739/1750 train_time:70605ms step_avg:95.54ms
step:740/1750 train_time:70703ms step_avg:95.54ms
step:741/1750 train_time:70800ms step_avg:95.55ms
step:742/1750 train_time:70898ms step_avg:95.55ms
step:743/1750 train_time:70995ms step_avg:95.55ms
step:744/1750 train_time:71092ms step_avg:95.55ms
step:745/1750 train_time:71190ms step_avg:95.56ms
step:746/1750 train_time:71288ms step_avg:95.56ms
step:747/1750 train_time:71386ms step_avg:95.56ms
step:748/1750 train_time:71484ms step_avg:95.57ms
step:749/1750 train_time:71582ms step_avg:95.57ms
step:750/1750 train_time:71679ms step_avg:95.57ms
step:750/1750 val_loss:3.6018 train_time:71765ms step_avg:95.69ms
step:751/1750 train_time:71786ms step_avg:95.59ms
step:752/1750 train_time:71882ms step_avg:95.59ms
step:753/1750 train_time:71984ms step_avg:95.60ms
step:754/1750 train_time:72082ms step_avg:95.60ms
step:755/1750 train_time:72180ms step_avg:95.60ms
step:756/1750 train_time:72277ms step_avg:95.60ms
step:757/1750 train_time:72374ms step_avg:95.61ms
step:758/1750 train_time:72471ms step_avg:95.61ms
step:759/1750 train_time:72567ms step_avg:95.61ms
step:760/1750 train_time:72665ms step_avg:95.61ms
step:761/1750 train_time:72763ms step_avg:95.61ms
step:762/1750 train_time:72862ms step_avg:95.62ms
step:763/1750 train_time:72963ms step_avg:95.63ms
step:764/1750 train_time:73061ms step_avg:95.63ms
step:765/1750 train_time:73159ms step_avg:95.63ms
step:766/1750 train_time:73257ms step_avg:95.64ms
step:767/1750 train_time:73355ms step_avg:95.64ms
step:768/1750 train_time:73453ms step_avg:95.64ms
step:769/1750 train_time:73549ms step_avg:95.64ms
step:770/1750 train_time:73646ms step_avg:95.64ms
step:771/1750 train_time:73743ms step_avg:95.65ms
step:772/1750 train_time:73841ms step_avg:95.65ms
step:773/1750 train_time:73943ms step_avg:95.66ms
step:774/1750 train_time:74041ms step_avg:95.66ms
step:775/1750 train_time:74139ms step_avg:95.66ms
step:776/1750 train_time:74236ms step_avg:95.67ms
step:777/1750 train_time:74334ms step_avg:95.67ms
step:778/1750 train_time:74431ms step_avg:95.67ms
step:779/1750 train_time:74529ms step_avg:95.67ms
step:780/1750 train_time:74627ms step_avg:95.68ms
step:781/1750 train_time:74725ms step_avg:95.68ms
step:782/1750 train_time:74824ms step_avg:95.68ms
step:783/1750 train_time:74922ms step_avg:95.69ms
step:784/1750 train_time:75021ms step_avg:95.69ms
step:785/1750 train_time:75120ms step_avg:95.69ms
step:786/1750 train_time:75218ms step_avg:95.70ms
step:787/1750 train_time:75316ms step_avg:95.70ms
step:788/1750 train_time:75414ms step_avg:95.70ms
step:789/1750 train_time:75512ms step_avg:95.71ms
step:790/1750 train_time:75610ms step_avg:95.71ms
step:791/1750 train_time:75707ms step_avg:95.71ms
step:792/1750 train_time:75805ms step_avg:95.71ms
step:793/1750 train_time:75903ms step_avg:95.72ms
step:794/1750 train_time:76001ms step_avg:95.72ms
step:795/1750 train_time:76099ms step_avg:95.72ms
step:796/1750 train_time:76197ms step_avg:95.72ms
step:797/1750 train_time:76295ms step_avg:95.73ms
step:798/1750 train_time:76393ms step_avg:95.73ms
step:799/1750 train_time:76491ms step_avg:95.73ms
step:800/1750 train_time:76588ms step_avg:95.74ms
step:801/1750 train_time:76685ms step_avg:95.74ms
step:802/1750 train_time:76783ms step_avg:95.74ms
step:803/1750 train_time:76881ms step_avg:95.74ms
step:804/1750 train_time:76979ms step_avg:95.75ms
step:805/1750 train_time:77077ms step_avg:95.75ms
step:806/1750 train_time:77176ms step_avg:95.75ms
step:807/1750 train_time:77274ms step_avg:95.76ms
step:808/1750 train_time:77373ms step_avg:95.76ms
step:809/1750 train_time:77471ms step_avg:95.76ms
step:810/1750 train_time:77568ms step_avg:95.76ms
step:811/1750 train_time:77666ms step_avg:95.77ms
step:812/1750 train_time:77763ms step_avg:95.77ms
step:813/1750 train_time:77861ms step_avg:95.77ms
step:814/1750 train_time:77959ms step_avg:95.77ms
step:815/1750 train_time:78057ms step_avg:95.78ms
step:816/1750 train_time:78156ms step_avg:95.78ms
step:817/1750 train_time:78253ms step_avg:95.78ms
step:818/1750 train_time:78351ms step_avg:95.78ms
step:819/1750 train_time:78449ms step_avg:95.79ms
step:820/1750 train_time:78546ms step_avg:95.79ms
step:821/1750 train_time:78644ms step_avg:95.79ms
step:822/1750 train_time:78742ms step_avg:95.79ms
step:823/1750 train_time:78840ms step_avg:95.80ms
step:824/1750 train_time:78938ms step_avg:95.80ms
step:825/1750 train_time:79035ms step_avg:95.80ms
step:826/1750 train_time:79134ms step_avg:95.80ms
step:827/1750 train_time:79232ms step_avg:95.81ms
step:828/1750 train_time:79330ms step_avg:95.81ms
step:829/1750 train_time:79428ms step_avg:95.81ms
step:830/1750 train_time:79525ms step_avg:95.81ms
step:831/1750 train_time:79624ms step_avg:95.82ms
step:832/1750 train_time:79721ms step_avg:95.82ms
step:833/1750 train_time:79819ms step_avg:95.82ms
step:834/1750 train_time:79917ms step_avg:95.82ms
step:835/1750 train_time:80014ms step_avg:95.83ms
step:836/1750 train_time:80113ms step_avg:95.83ms
step:837/1750 train_time:80211ms step_avg:95.83ms
step:838/1750 train_time:80309ms step_avg:95.83ms
step:839/1750 train_time:80407ms step_avg:95.84ms
step:840/1750 train_time:80505ms step_avg:95.84ms
step:841/1750 train_time:80602ms step_avg:95.84ms
step:842/1750 train_time:80702ms step_avg:95.85ms
step:843/1750 train_time:80800ms step_avg:95.85ms
step:844/1750 train_time:80898ms step_avg:95.85ms
step:845/1750 train_time:80995ms step_avg:95.85ms
step:846/1750 train_time:81093ms step_avg:95.85ms
step:847/1750 train_time:81191ms step_avg:95.86ms
step:848/1750 train_time:81288ms step_avg:95.86ms
step:849/1750 train_time:81386ms step_avg:95.86ms
step:850/1750 train_time:81484ms step_avg:95.86ms
step:851/1750 train_time:81582ms step_avg:95.87ms
step:852/1750 train_time:81680ms step_avg:95.87ms
step:853/1750 train_time:81777ms step_avg:95.87ms
step:854/1750 train_time:81876ms step_avg:95.87ms
step:855/1750 train_time:81974ms step_avg:95.88ms
step:856/1750 train_time:82073ms step_avg:95.88ms
step:857/1750 train_time:82170ms step_avg:95.88ms
step:858/1750 train_time:82268ms step_avg:95.88ms
step:859/1750 train_time:82366ms step_avg:95.89ms
step:860/1750 train_time:82464ms step_avg:95.89ms
step:861/1750 train_time:82561ms step_avg:95.89ms
step:862/1750 train_time:82659ms step_avg:95.89ms
step:863/1750 train_time:82757ms step_avg:95.89ms
step:864/1750 train_time:82855ms step_avg:95.90ms
step:865/1750 train_time:82953ms step_avg:95.90ms
step:866/1750 train_time:83051ms step_avg:95.90ms
step:867/1750 train_time:83149ms step_avg:95.90ms
step:868/1750 train_time:83247ms step_avg:95.91ms
step:869/1750 train_time:83345ms step_avg:95.91ms
step:870/1750 train_time:83443ms step_avg:95.91ms
step:871/1750 train_time:83541ms step_avg:95.91ms
step:872/1750 train_time:83639ms step_avg:95.92ms
step:873/1750 train_time:83736ms step_avg:95.92ms
step:874/1750 train_time:83834ms step_avg:95.92ms
step:875/1750 train_time:83932ms step_avg:95.92ms
step:875/1750 val_loss:3.5529 train_time:84018ms step_avg:96.02ms
step:876/1750 train_time:84040ms step_avg:95.94ms
step:877/1750 train_time:84133ms step_avg:95.93ms
step:878/1750 train_time:84235ms step_avg:95.94ms
step:879/1750 train_time:84332ms step_avg:95.94ms
step:880/1750 train_time:84430ms step_avg:95.94ms
step:881/1750 train_time:84527ms step_avg:95.94ms
step:882/1750 train_time:84624ms step_avg:95.95ms
step:883/1750 train_time:84721ms step_avg:95.95ms
step:884/1750 train_time:84818ms step_avg:95.95ms
step:885/1750 train_time:84915ms step_avg:95.95ms
step:886/1750 train_time:85014ms step_avg:95.95ms
step:887/1750 train_time:85113ms step_avg:95.96ms
step:888/1750 train_time:85212ms step_avg:95.96ms
step:889/1750 train_time:85310ms step_avg:95.96ms
step:890/1750 train_time:85407ms step_avg:95.96ms
step:891/1750 train_time:85505ms step_avg:95.97ms
step:892/1750 train_time:85603ms step_avg:95.97ms
step:893/1750 train_time:85700ms step_avg:95.97ms
step:894/1750 train_time:85797ms step_avg:95.97ms
step:895/1750 train_time:85895ms step_avg:95.97ms
step:896/1750 train_time:85992ms step_avg:95.97ms
step:897/1750 train_time:86090ms step_avg:95.98ms
step:898/1750 train_time:86189ms step_avg:95.98ms
step:899/1750 train_time:86288ms step_avg:95.98ms
step:900/1750 train_time:86386ms step_avg:95.98ms
step:901/1750 train_time:86484ms step_avg:95.99ms
step:902/1750 train_time:86582ms step_avg:95.99ms
step:903/1750 train_time:86680ms step_avg:95.99ms
step:904/1750 train_time:86777ms step_avg:95.99ms
step:905/1750 train_time:86874ms step_avg:95.99ms
step:906/1750 train_time:86972ms step_avg:96.00ms
step:907/1750 train_time:87070ms step_avg:96.00ms
step:908/1750 train_time:87169ms step_avg:96.00ms
step:909/1750 train_time:87267ms step_avg:96.00ms
step:910/1750 train_time:87367ms step_avg:96.01ms
step:911/1750 train_time:87466ms step_avg:96.01ms
step:912/1750 train_time:87565ms step_avg:96.01ms
step:913/1750 train_time:87664ms step_avg:96.02ms
step:914/1750 train_time:87763ms step_avg:96.02ms
step:915/1750 train_time:87864ms step_avg:96.03ms
step:916/1750 train_time:87964ms step_avg:96.03ms
step:917/1750 train_time:88064ms step_avg:96.04ms
step:918/1750 train_time:88166ms step_avg:96.04ms
step:919/1750 train_time:88266ms step_avg:96.05ms
step:920/1750 train_time:88366ms step_avg:96.05ms
step:921/1750 train_time:88465ms step_avg:96.05ms
step:922/1750 train_time:88565ms step_avg:96.06ms
step:923/1750 train_time:88665ms step_avg:96.06ms
step:924/1750 train_time:88764ms step_avg:96.06ms
step:925/1750 train_time:88864ms step_avg:96.07ms
step:926/1750 train_time:88963ms step_avg:96.07ms
step:927/1750 train_time:89063ms step_avg:96.08ms
step:928/1750 train_time:89162ms step_avg:96.08ms
step:929/1750 train_time:89263ms step_avg:96.08ms
step:930/1750 train_time:89362ms step_avg:96.09ms
step:931/1750 train_time:89462ms step_avg:96.09ms
step:932/1750 train_time:89561ms step_avg:96.10ms
step:933/1750 train_time:89660ms step_avg:96.10ms
step:934/1750 train_time:89759ms step_avg:96.10ms
step:935/1750 train_time:89859ms step_avg:96.11ms
step:936/1750 train_time:89959ms step_avg:96.11ms
step:937/1750 train_time:90059ms step_avg:96.11ms
step:938/1750 train_time:90158ms step_avg:96.12ms
step:939/1750 train_time:90258ms step_avg:96.12ms
step:940/1750 train_time:90358ms step_avg:96.13ms
step:941/1750 train_time:90458ms step_avg:96.13ms
step:942/1750 train_time:90558ms step_avg:96.13ms
step:943/1750 train_time:90659ms step_avg:96.14ms
step:944/1750 train_time:90757ms step_avg:96.14ms
step:945/1750 train_time:90856ms step_avg:96.14ms
step:946/1750 train_time:90955ms step_avg:96.15ms
step:947/1750 train_time:91054ms step_avg:96.15ms
step:948/1750 train_time:91153ms step_avg:96.15ms
step:949/1750 train_time:91252ms step_avg:96.16ms
step:950/1750 train_time:91352ms step_avg:96.16ms
step:951/1750 train_time:91451ms step_avg:96.16ms
step:952/1750 train_time:91551ms step_avg:96.17ms
step:953/1750 train_time:91650ms step_avg:96.17ms
step:954/1750 train_time:91749ms step_avg:96.17ms
step:955/1750 train_time:91847ms step_avg:96.18ms
step:956/1750 train_time:91947ms step_avg:96.18ms
step:957/1750 train_time:92047ms step_avg:96.18ms
step:958/1750 train_time:92147ms step_avg:96.19ms
step:959/1750 train_time:92246ms step_avg:96.19ms
step:960/1750 train_time:92347ms step_avg:96.20ms
step:961/1750 train_time:92447ms step_avg:96.20ms
step:962/1750 train_time:92548ms step_avg:96.20ms
step:963/1750 train_time:92647ms step_avg:96.21ms
step:964/1750 train_time:92746ms step_avg:96.21ms
step:965/1750 train_time:92846ms step_avg:96.21ms
step:966/1750 train_time:92945ms step_avg:96.22ms
step:967/1750 train_time:93045ms step_avg:96.22ms
step:968/1750 train_time:93146ms step_avg:96.22ms
step:969/1750 train_time:93247ms step_avg:96.23ms
step:970/1750 train_time:93347ms step_avg:96.23ms
step:971/1750 train_time:93446ms step_avg:96.24ms
step:972/1750 train_time:93546ms step_avg:96.24ms
step:973/1750 train_time:93645ms step_avg:96.24ms
step:974/1750 train_time:93744ms step_avg:96.25ms
step:975/1750 train_time:93843ms step_avg:96.25ms
step:976/1750 train_time:93942ms step_avg:96.25ms
step:977/1750 train_time:94042ms step_avg:96.26ms
step:978/1750 train_time:94142ms step_avg:96.26ms
step:979/1750 train_time:94243ms step_avg:96.26ms
step:980/1750 train_time:94342ms step_avg:96.27ms
step:981/1750 train_time:94443ms step_avg:96.27ms
step:982/1750 train_time:94543ms step_avg:96.28ms
step:983/1750 train_time:94642ms step_avg:96.28ms
step:984/1750 train_time:94741ms step_avg:96.28ms
step:985/1750 train_time:94840ms step_avg:96.28ms
step:986/1750 train_time:94940ms step_avg:96.29ms
step:987/1750 train_time:95040ms step_avg:96.29ms
step:988/1750 train_time:95138ms step_avg:96.29ms
step:989/1750 train_time:95238ms step_avg:96.30ms
step:990/1750 train_time:95340ms step_avg:96.30ms
step:991/1750 train_time:95440ms step_avg:96.31ms
step:992/1750 train_time:95539ms step_avg:96.31ms
step:993/1750 train_time:95639ms step_avg:96.31ms
step:994/1750 train_time:95738ms step_avg:96.32ms
step:995/1750 train_time:95838ms step_avg:96.32ms
step:996/1750 train_time:95938ms step_avg:96.32ms
step:997/1750 train_time:96038ms step_avg:96.33ms
step:998/1750 train_time:96137ms step_avg:96.33ms
step:999/1750 train_time:96236ms step_avg:96.33ms
step:1000/1750 train_time:96335ms step_avg:96.34ms
step:1000/1750 val_loss:3.5117 train_time:96424ms step_avg:96.42ms
step:1001/1750 train_time:96444ms step_avg:96.35ms
step:1002/1750 train_time:96543ms step_avg:96.35ms
step:1003/1750 train_time:96648ms step_avg:96.36ms
step:1004/1750 train_time:96748ms step_avg:96.36ms
step:1005/1750 train_time:96847ms step_avg:96.37ms
step:1006/1750 train_time:96946ms step_avg:96.37ms
step:1007/1750 train_time:97044ms step_avg:96.37ms
step:1008/1750 train_time:97142ms step_avg:96.37ms
step:1009/1750 train_time:97242ms step_avg:96.37ms
step:1010/1750 train_time:97340ms step_avg:96.38ms
step:1011/1750 train_time:97442ms step_avg:96.38ms
step:1012/1750 train_time:97544ms step_avg:96.39ms
step:1013/1750 train_time:97646ms step_avg:96.39ms
step:1014/1750 train_time:97746ms step_avg:96.40ms
step:1015/1750 train_time:97845ms step_avg:96.40ms
step:1016/1750 train_time:97944ms step_avg:96.40ms
step:1017/1750 train_time:98043ms step_avg:96.40ms
step:1018/1750 train_time:98142ms step_avg:96.41ms
step:1019/1750 train_time:98241ms step_avg:96.41ms
step:1020/1750 train_time:98340ms step_avg:96.41ms
step:1021/1750 train_time:98441ms step_avg:96.42ms
step:1022/1750 train_time:98543ms step_avg:96.42ms
step:1023/1750 train_time:98644ms step_avg:96.43ms
step:1024/1750 train_time:98746ms step_avg:96.43ms
step:1025/1750 train_time:98846ms step_avg:96.44ms
step:1026/1750 train_time:98945ms step_avg:96.44ms
step:1027/1750 train_time:99044ms step_avg:96.44ms
step:1028/1750 train_time:99143ms step_avg:96.44ms
step:1029/1750 train_time:99242ms step_avg:96.45ms
step:1030/1750 train_time:99342ms step_avg:96.45ms
step:1031/1750 train_time:99443ms step_avg:96.45ms
step:1032/1750 train_time:99543ms step_avg:96.46ms
step:1033/1750 train_time:99644ms step_avg:96.46ms
step:1034/1750 train_time:99745ms step_avg:96.46ms
step:1035/1750 train_time:99845ms step_avg:96.47ms
step:1036/1750 train_time:99944ms step_avg:96.47ms
step:1037/1750 train_time:100044ms step_avg:96.47ms
step:1038/1750 train_time:100142ms step_avg:96.48ms
step:1039/1750 train_time:100241ms step_avg:96.48ms
step:1040/1750 train_time:100340ms step_avg:96.48ms
step:1041/1750 train_time:100441ms step_avg:96.48ms
step:1042/1750 train_time:100541ms step_avg:96.49ms
step:1043/1750 train_time:100642ms step_avg:96.49ms
step:1044/1750 train_time:100743ms step_avg:96.50ms
step:1045/1750 train_time:100843ms step_avg:96.50ms
step:1046/1750 train_time:100944ms step_avg:96.51ms
step:1047/1750 train_time:101044ms step_avg:96.51ms
step:1048/1750 train_time:101144ms step_avg:96.51ms
step:1049/1750 train_time:101243ms step_avg:96.51ms
step:1050/1750 train_time:101342ms step_avg:96.52ms
step:1051/1750 train_time:101694ms step_avg:96.76ms
step:1052/1750 train_time:101792ms step_avg:96.76ms
step:1053/1750 train_time:101890ms step_avg:96.76ms
step:1054/1750 train_time:101987ms step_avg:96.76ms
step:1055/1750 train_time:102086ms step_avg:96.76ms
step:1056/1750 train_time:102185ms step_avg:96.77ms
step:1057/1750 train_time:102283ms step_avg:96.77ms
step:1058/1750 train_time:102382ms step_avg:96.77ms
step:1059/1750 train_time:102480ms step_avg:96.77ms
step:1060/1750 train_time:102582ms step_avg:96.78ms
step:1061/1750 train_time:102688ms step_avg:96.78ms
step:1062/1750 train_time:102787ms step_avg:96.79ms
step:1063/1750 train_time:102888ms step_avg:96.79ms
step:1064/1750 train_time:102987ms step_avg:96.79ms
step:1065/1750 train_time:103086ms step_avg:96.79ms
step:1066/1750 train_time:103185ms step_avg:96.80ms
step:1067/1750 train_time:103283ms step_avg:96.80ms
step:1068/1750 train_time:103382ms step_avg:96.80ms
step:1069/1750 train_time:103481ms step_avg:96.80ms
step:1070/1750 train_time:103582ms step_avg:96.81ms
step:1071/1750 train_time:103684ms step_avg:96.81ms
step:1072/1750 train_time:103784ms step_avg:96.81ms
step:1073/1750 train_time:103884ms step_avg:96.82ms
step:1074/1750 train_time:103984ms step_avg:96.82ms
step:1075/1750 train_time:104083ms step_avg:96.82ms
step:1076/1750 train_time:104182ms step_avg:96.82ms
step:1077/1750 train_time:104282ms step_avg:96.83ms
step:1078/1750 train_time:104381ms step_avg:96.83ms
step:1079/1750 train_time:104480ms step_avg:96.83ms
step:1080/1750 train_time:104580ms step_avg:96.83ms
step:1081/1750 train_time:104680ms step_avg:96.84ms
step:1082/1750 train_time:104781ms step_avg:96.84ms
step:1083/1750 train_time:104881ms step_avg:96.84ms
step:1084/1750 train_time:104982ms step_avg:96.85ms
step:1085/1750 train_time:105081ms step_avg:96.85ms
step:1086/1750 train_time:105181ms step_avg:96.85ms
step:1087/1750 train_time:105280ms step_avg:96.85ms
step:1088/1750 train_time:105380ms step_avg:96.86ms
step:1089/1750 train_time:105480ms step_avg:96.86ms
step:1090/1750 train_time:105580ms step_avg:96.86ms
step:1091/1750 train_time:105681ms step_avg:96.87ms
step:1092/1750 train_time:105782ms step_avg:96.87ms
step:1093/1750 train_time:105882ms step_avg:96.87ms
step:1094/1750 train_time:105982ms step_avg:96.88ms
step:1095/1750 train_time:106082ms step_avg:96.88ms
step:1096/1750 train_time:106183ms step_avg:96.88ms
step:1097/1750 train_time:106282ms step_avg:96.88ms
step:1098/1750 train_time:106383ms step_avg:96.89ms
step:1099/1750 train_time:106482ms step_avg:96.89ms
step:1100/1750 train_time:106582ms step_avg:96.89ms
step:1101/1750 train_time:106683ms step_avg:96.90ms
step:1102/1750 train_time:106783ms step_avg:96.90ms
step:1103/1750 train_time:106882ms step_avg:96.90ms
step:1104/1750 train_time:106982ms step_avg:96.90ms
step:1105/1750 train_time:107082ms step_avg:96.91ms
step:1106/1750 train_time:107182ms step_avg:96.91ms
step:1107/1750 train_time:107281ms step_avg:96.91ms
step:1108/1750 train_time:107381ms step_avg:96.91ms
step:1109/1750 train_time:107480ms step_avg:96.92ms
step:1110/1750 train_time:107581ms step_avg:96.92ms
step:1111/1750 train_time:107682ms step_avg:96.92ms
step:1112/1750 train_time:107782ms step_avg:96.93ms
step:1113/1750 train_time:107882ms step_avg:96.93ms
step:1114/1750 train_time:107982ms step_avg:96.93ms
step:1115/1750 train_time:108082ms step_avg:96.93ms
step:1116/1750 train_time:108181ms step_avg:96.94ms
step:1117/1750 train_time:108281ms step_avg:96.94ms
step:1118/1750 train_time:108380ms step_avg:96.94ms
step:1119/1750 train_time:108479ms step_avg:96.94ms
step:1120/1750 train_time:108580ms step_avg:96.95ms
step:1121/1750 train_time:108681ms step_avg:96.95ms
step:1122/1750 train_time:108781ms step_avg:96.95ms
step:1123/1750 train_time:108881ms step_avg:96.96ms
step:1124/1750 train_time:108982ms step_avg:96.96ms
step:1125/1750 train_time:109082ms step_avg:96.96ms
step:1125/1750 val_loss:3.4581 train_time:109171ms step_avg:97.04ms
step:1126/1750 train_time:109191ms step_avg:96.97ms
step:1127/1750 train_time:109291ms step_avg:96.98ms
step:1128/1750 train_time:109392ms step_avg:96.98ms
step:1129/1750 train_time:109492ms step_avg:96.98ms
step:1130/1750 train_time:109590ms step_avg:96.98ms
step:1131/1750 train_time:109688ms step_avg:96.98ms
step:1132/1750 train_time:109786ms step_avg:96.98ms
step:1133/1750 train_time:109885ms step_avg:96.99ms
step:1134/1750 train_time:109983ms step_avg:96.99ms
step:1135/1750 train_time:110082ms step_avg:96.99ms
step:1136/1750 train_time:110184ms step_avg:96.99ms
step:1137/1750 train_time:110285ms step_avg:97.00ms
step:1138/1750 train_time:110388ms step_avg:97.00ms
step:1139/1750 train_time:110488ms step_avg:97.00ms
step:1140/1750 train_time:110841ms step_avg:97.23ms
step:1141/1750 train_time:110938ms step_avg:97.23ms
step:1142/1750 train_time:111036ms step_avg:97.23ms
step:1143/1750 train_time:111135ms step_avg:97.23ms
step:1144/1750 train_time:111233ms step_avg:97.23ms
step:1145/1750 train_time:111331ms step_avg:97.23ms
step:1146/1750 train_time:111429ms step_avg:97.23ms
step:1147/1750 train_time:111527ms step_avg:97.23ms
step:1148/1750 train_time:111625ms step_avg:97.23ms
step:1149/1750 train_time:111977ms step_avg:97.46ms
step:1150/1750 train_time:112076ms step_avg:97.46ms
step:1151/1750 train_time:112175ms step_avg:97.46ms
step:1152/1750 train_time:112274ms step_avg:97.46ms
step:1153/1750 train_time:112371ms step_avg:97.46ms
step:1154/1750 train_time:112470ms step_avg:97.46ms
step:1155/1750 train_time:112568ms step_avg:97.46ms
step:1156/1750 train_time:112936ms step_avg:97.70ms
step:1157/1750 train_time:113033ms step_avg:97.70ms
step:1158/1750 train_time:113132ms step_avg:97.70ms
step:1159/1750 train_time:113230ms step_avg:97.70ms
step:1160/1750 train_time:113328ms step_avg:97.70ms
step:1161/1750 train_time:113426ms step_avg:97.70ms
step:1162/1750 train_time:113525ms step_avg:97.70ms
step:1163/1750 train_time:113623ms step_avg:97.70ms
step:1164/1750 train_time:113722ms step_avg:97.70ms
step:1165/1750 train_time:113823ms step_avg:97.70ms
step:1166/1750 train_time:113927ms step_avg:97.71ms
step:1167/1750 train_time:114027ms step_avg:97.71ms
step:1168/1750 train_time:114127ms step_avg:97.71ms
step:1169/1750 train_time:114227ms step_avg:97.71ms
step:1170/1750 train_time:114327ms step_avg:97.72ms
step:1171/1750 train_time:114426ms step_avg:97.72ms
step:1172/1750 train_time:114526ms step_avg:97.72ms
step:1173/1750 train_time:114625ms step_avg:97.72ms
step:1174/1750 train_time:114725ms step_avg:97.72ms
step:1175/1750 train_time:114826ms step_avg:97.72ms
step:1176/1750 train_time:114928ms step_avg:97.73ms
step:1177/1750 train_time:115029ms step_avg:97.73ms
step:1178/1750 train_time:115129ms step_avg:97.73ms
step:1179/1750 train_time:115232ms step_avg:97.74ms
step:1180/1750 train_time:115334ms step_avg:97.74ms
step:1181/1750 train_time:115433ms step_avg:97.74ms
step:1182/1750 train_time:115533ms step_avg:97.74ms
step:1183/1750 train_time:115634ms step_avg:97.75ms
step:1184/1750 train_time:115734ms step_avg:97.75ms
step:1185/1750 train_time:115834ms step_avg:97.75ms
step:1186/1750 train_time:115936ms step_avg:97.75ms
step:1187/1750 train_time:116323ms step_avg:98.00ms
step:1188/1750 train_time:116422ms step_avg:98.00ms
step:1189/1750 train_time:116521ms step_avg:98.00ms
step:1190/1750 train_time:116621ms step_avg:98.00ms
step:1191/1750 train_time:116720ms step_avg:98.00ms
step:1192/1750 train_time:116820ms step_avg:98.00ms
step:1193/1750 train_time:116919ms step_avg:98.00ms
step:1194/1750 train_time:117329ms step_avg:98.27ms
step:1195/1750 train_time:117427ms step_avg:98.27ms
step:1196/1750 train_time:117526ms step_avg:98.27ms
step:1197/1750 train_time:117627ms step_avg:98.27ms
step:1198/1750 train_time:117726ms step_avg:98.27ms
step:1199/1750 train_time:117826ms step_avg:98.27ms
step:1200/1750 train_time:117924ms step_avg:98.27ms
step:1201/1750 train_time:118024ms step_avg:98.27ms
step:1202/1750 train_time:118124ms step_avg:98.27ms
step:1203/1750 train_time:118228ms step_avg:98.28ms
step:1204/1750 train_time:118331ms step_avg:98.28ms
step:1205/1750 train_time:118431ms step_avg:98.28ms
step:1206/1750 train_time:118532ms step_avg:98.29ms
step:1207/1750 train_time:118633ms step_avg:98.29ms
step:1208/1750 train_time:118732ms step_avg:98.29ms
step:1209/1750 train_time:118833ms step_avg:98.29ms
step:1210/1750 train_time:118934ms step_avg:98.29ms
step:1211/1750 train_time:119034ms step_avg:98.29ms
step:1212/1750 train_time:119136ms step_avg:98.30ms
step:1213/1750 train_time:119237ms step_avg:98.30ms
step:1214/1750 train_time:119339ms step_avg:98.30ms
step:1215/1750 train_time:119441ms step_avg:98.31ms
step:1216/1750 train_time:119543ms step_avg:98.31ms
step:1217/1750 train_time:119644ms step_avg:98.31ms
step:1218/1750 train_time:120041ms step_avg:98.56ms
step:1219/1750 train_time:120140ms step_avg:98.56ms
step:1220/1750 train_time:120240ms step_avg:98.56ms
step:1221/1750 train_time:120340ms step_avg:98.56ms
step:1222/1750 train_time:120440ms step_avg:98.56ms
step:1223/1750 train_time:120539ms step_avg:98.56ms
step:1224/1750 train_time:120639ms step_avg:98.56ms
step:1225/1750 train_time:120739ms step_avg:98.56ms
step:1226/1750 train_time:120838ms step_avg:98.56ms
step:1227/1750 train_time:120945ms step_avg:98.57ms
step:1228/1750 train_time:121049ms step_avg:98.57ms
step:1229/1750 train_time:121149ms step_avg:98.58ms
step:1230/1750 train_time:121249ms step_avg:98.58ms
step:1231/1750 train_time:121350ms step_avg:98.58ms
step:1232/1750 train_time:121449ms step_avg:98.58ms
step:1233/1750 train_time:121549ms step_avg:98.58ms
step:1234/1750 train_time:121650ms step_avg:98.58ms
step:1235/1750 train_time:121750ms step_avg:98.58ms
step:1236/1750 train_time:121852ms step_avg:98.59ms
step:1237/1750 train_time:121954ms step_avg:98.59ms
step:1238/1750 train_time:122056ms step_avg:98.59ms
step:1239/1750 train_time:122158ms step_avg:98.59ms
step:1240/1750 train_time:122258ms step_avg:98.60ms
step:1241/1750 train_time:122361ms step_avg:98.60ms
step:1242/1750 train_time:122462ms step_avg:98.60ms
step:1243/1750 train_time:122562ms step_avg:98.60ms
step:1244/1750 train_time:122663ms step_avg:98.60ms
step:1245/1750 train_time:122763ms step_avg:98.61ms
step:1246/1750 train_time:122864ms step_avg:98.61ms
step:1247/1750 train_time:122966ms step_avg:98.61ms
step:1248/1750 train_time:123069ms step_avg:98.61ms
step:1249/1750 train_time:123169ms step_avg:98.61ms
step:1250/1750 train_time:123269ms step_avg:98.62ms
step:1250/1750 val_loss:3.4122 train_time:123359ms step_avg:98.69ms
step:1251/1750 train_time:123379ms step_avg:98.62ms
step:1252/1750 train_time:123477ms step_avg:98.62ms
step:1253/1750 train_time:123578ms step_avg:98.63ms
step:1254/1750 train_time:123680ms step_avg:98.63ms
step:1255/1750 train_time:123780ms step_avg:98.63ms
step:1256/1750 train_time:123880ms step_avg:98.63ms
step:1257/1750 train_time:123980ms step_avg:98.63ms
step:1258/1750 train_time:124079ms step_avg:98.63ms
step:1259/1750 train_time:124179ms step_avg:98.63ms
step:1260/1750 train_time:124279ms step_avg:98.63ms
step:1261/1750 train_time:124380ms step_avg:98.64ms
step:1262/1750 train_time:124482ms step_avg:98.64ms
step:1263/1750 train_time:124584ms step_avg:98.64ms
step:1264/1750 train_time:124685ms step_avg:98.64ms
step:1265/1750 train_time:124786ms step_avg:98.64ms
step:1266/1750 train_time:124885ms step_avg:98.65ms
step:1267/1750 train_time:124986ms step_avg:98.65ms
step:1268/1750 train_time:125086ms step_avg:98.65ms
step:1269/1750 train_time:125185ms step_avg:98.65ms
step:1270/1750 train_time:125286ms step_avg:98.65ms
step:1271/1750 train_time:125387ms step_avg:98.65ms
step:1272/1750 train_time:125488ms step_avg:98.65ms
step:1273/1750 train_time:125589ms step_avg:98.66ms
step:1274/1750 train_time:125689ms step_avg:98.66ms
step:1275/1750 train_time:125790ms step_avg:98.66ms
step:1276/1750 train_time:125891ms step_avg:98.66ms
step:1277/1750 train_time:125992ms step_avg:98.66ms
step:1278/1750 train_time:126092ms step_avg:98.66ms
step:1279/1750 train_time:126193ms step_avg:98.67ms
step:1280/1750 train_time:126293ms step_avg:98.67ms
step:1281/1750 train_time:126393ms step_avg:98.67ms
step:1282/1750 train_time:126495ms step_avg:98.67ms
step:1283/1750 train_time:126595ms step_avg:98.67ms
step:1284/1750 train_time:126696ms step_avg:98.67ms
step:1285/1750 train_time:126796ms step_avg:98.67ms
step:1286/1750 train_time:126895ms step_avg:98.67ms
step:1287/1750 train_time:126995ms step_avg:98.68ms
step:1288/1750 train_time:127096ms step_avg:98.68ms
step:1289/1750 train_time:127197ms step_avg:98.68ms
step:1290/1750 train_time:127297ms step_avg:98.68ms
step:1291/1750 train_time:127397ms step_avg:98.68ms
step:1292/1750 train_time:127498ms step_avg:98.68ms
step:1293/1750 train_time:127599ms step_avg:98.68ms
step:1294/1750 train_time:127702ms step_avg:98.69ms
step:1295/1750 train_time:127803ms step_avg:98.69ms
step:1296/1750 train_time:127904ms step_avg:98.69ms
step:1297/1750 train_time:128005ms step_avg:98.69ms
step:1298/1750 train_time:128106ms step_avg:98.69ms
step:1299/1750 train_time:128207ms step_avg:98.70ms
step:1300/1750 train_time:128308ms step_avg:98.70ms
step:1301/1750 train_time:128409ms step_avg:98.70ms
step:1302/1750 train_time:128511ms step_avg:98.70ms
step:1303/1750 train_time:128614ms step_avg:98.71ms
step:1304/1750 train_time:128715ms step_avg:98.71ms
step:1305/1750 train_time:128816ms step_avg:98.71ms
step:1306/1750 train_time:128916ms step_avg:98.71ms
step:1307/1750 train_time:129017ms step_avg:98.71ms
step:1308/1750 train_time:129117ms step_avg:98.71ms
step:1309/1750 train_time:129217ms step_avg:98.71ms
step:1310/1750 train_time:129317ms step_avg:98.72ms
step:1311/1750 train_time:129418ms step_avg:98.72ms
step:1312/1750 train_time:129519ms step_avg:98.72ms
step:1313/1750 train_time:129621ms step_avg:98.72ms
step:1314/1750 train_time:129721ms step_avg:98.72ms
step:1315/1750 train_time:129821ms step_avg:98.72ms
step:1316/1750 train_time:129924ms step_avg:98.73ms
step:1317/1750 train_time:130024ms step_avg:98.73ms
step:1318/1750 train_time:130125ms step_avg:98.73ms
step:1319/1750 train_time:130225ms step_avg:98.73ms
step:1320/1750 train_time:130327ms step_avg:98.73ms
step:1321/1750 train_time:130428ms step_avg:98.73ms
step:1322/1750 train_time:130529ms step_avg:98.74ms
step:1323/1750 train_time:130631ms step_avg:98.74ms
step:1324/1750 train_time:130733ms step_avg:98.74ms
step:1325/1750 train_time:130835ms step_avg:98.74ms
step:1326/1750 train_time:130936ms step_avg:98.75ms
step:1327/1750 train_time:131037ms step_avg:98.75ms
step:1328/1750 train_time:131136ms step_avg:98.75ms
step:1329/1750 train_time:131235ms step_avg:98.75ms
step:1330/1750 train_time:131336ms step_avg:98.75ms
step:1331/1750 train_time:131436ms step_avg:98.75ms
step:1332/1750 train_time:131538ms step_avg:98.75ms
step:1333/1750 train_time:131638ms step_avg:98.75ms
step:1334/1750 train_time:131739ms step_avg:98.75ms
step:1335/1750 train_time:131840ms step_avg:98.76ms
step:1336/1750 train_time:131940ms step_avg:98.76ms
step:1337/1750 train_time:132041ms step_avg:98.76ms
step:1338/1750 train_time:132141ms step_avg:98.76ms
step:1339/1750 train_time:132242ms step_avg:98.76ms
step:1340/1750 train_time:132343ms step_avg:98.76ms
step:1341/1750 train_time:132446ms step_avg:98.77ms
step:1342/1750 train_time:132548ms step_avg:98.77ms
step:1343/1750 train_time:132649ms step_avg:98.77ms
step:1344/1750 train_time:132750ms step_avg:98.77ms
step:1345/1750 train_time:132851ms step_avg:98.77ms
step:1346/1750 train_time:132953ms step_avg:98.78ms
step:1347/1750 train_time:133054ms step_avg:98.78ms
step:1348/1750 train_time:133156ms step_avg:98.78ms
step:1349/1750 train_time:133255ms step_avg:98.78ms
step:1350/1750 train_time:133356ms step_avg:98.78ms
step:1351/1750 train_time:133457ms step_avg:98.78ms
step:1352/1750 train_time:133557ms step_avg:98.78ms
step:1353/1750 train_time:133657ms step_avg:98.79ms
step:1354/1750 train_time:133758ms step_avg:98.79ms
step:1355/1750 train_time:133858ms step_avg:98.79ms
step:1356/1750 train_time:133959ms step_avg:98.79ms
step:1357/1750 train_time:134059ms step_avg:98.79ms
step:1358/1750 train_time:134159ms step_avg:98.79ms
step:1359/1750 train_time:134260ms step_avg:98.79ms
step:1360/1750 train_time:134360ms step_avg:98.79ms
step:1361/1750 train_time:134461ms step_avg:98.80ms
step:1362/1750 train_time:134561ms step_avg:98.80ms
step:1363/1750 train_time:134664ms step_avg:98.80ms
step:1364/1750 train_time:134765ms step_avg:98.80ms
step:1365/1750 train_time:134867ms step_avg:98.80ms
step:1366/1750 train_time:134967ms step_avg:98.80ms
step:1367/1750 train_time:135068ms step_avg:98.81ms
step:1368/1750 train_time:135170ms step_avg:98.81ms
step:1369/1750 train_time:135271ms step_avg:98.81ms
step:1370/1750 train_time:135372ms step_avg:98.81ms
step:1371/1750 train_time:135473ms step_avg:98.81ms
step:1372/1750 train_time:135574ms step_avg:98.82ms
step:1373/1750 train_time:135675ms step_avg:98.82ms
step:1374/1750 train_time:135776ms step_avg:98.82ms
step:1375/1750 train_time:135877ms step_avg:98.82ms
step:1375/1750 val_loss:3.3720 train_time:135966ms step_avg:98.88ms
step:1376/1750 train_time:135987ms step_avg:98.83ms
step:1377/1750 train_time:136085ms step_avg:98.83ms
step:1378/1750 train_time:136188ms step_avg:98.83ms
step:1379/1750 train_time:136288ms step_avg:98.83ms
step:1380/1750 train_time:136392ms step_avg:98.83ms
step:1381/1750 train_time:136492ms step_avg:98.84ms
step:1382/1750 train_time:136591ms step_avg:98.84ms
step:1383/1750 train_time:136690ms step_avg:98.84ms
step:1384/1750 train_time:136790ms step_avg:98.84ms
step:1385/1750 train_time:136891ms step_avg:98.84ms
step:1386/1750 train_time:136995ms step_avg:98.84ms
step:1387/1750 train_time:137098ms step_avg:98.84ms
step:1388/1750 train_time:137200ms step_avg:98.85ms
step:1389/1750 train_time:137301ms step_avg:98.85ms
step:1390/1750 train_time:137400ms step_avg:98.85ms
step:1391/1750 train_time:137501ms step_avg:98.85ms
step:1392/1750 train_time:137601ms step_avg:98.85ms
step:1393/1750 train_time:137701ms step_avg:98.85ms
step:1394/1750 train_time:137801ms step_avg:98.85ms
step:1395/1750 train_time:137900ms step_avg:98.85ms
step:1396/1750 train_time:138000ms step_avg:98.85ms
step:1397/1750 train_time:138103ms step_avg:98.86ms
step:1398/1750 train_time:138204ms step_avg:98.86ms
step:1399/1750 train_time:138305ms step_avg:98.86ms
step:1400/1750 train_time:138406ms step_avg:98.86ms
step:1401/1750 train_time:138507ms step_avg:98.86ms
step:1402/1750 train_time:138609ms step_avg:98.87ms
step:1403/1750 train_time:138710ms step_avg:98.87ms
step:1404/1750 train_time:138811ms step_avg:98.87ms
step:1405/1750 train_time:138912ms step_avg:98.87ms
step:1406/1750 train_time:139014ms step_avg:98.87ms
step:1407/1750 train_time:139116ms step_avg:98.87ms
step:1408/1750 train_time:139217ms step_avg:98.88ms
step:1409/1750 train_time:139320ms step_avg:98.88ms
step:1410/1750 train_time:139421ms step_avg:98.88ms
step:1411/1750 train_time:139521ms step_avg:98.88ms
step:1412/1750 train_time:139622ms step_avg:98.88ms
step:1413/1750 train_time:139721ms step_avg:98.88ms
step:1414/1750 train_time:139821ms step_avg:98.88ms
step:1415/1750 train_time:139922ms step_avg:98.89ms
step:1416/1750 train_time:140023ms step_avg:98.89ms
step:1417/1750 train_time:140123ms step_avg:98.89ms
step:1418/1750 train_time:140223ms step_avg:98.89ms
step:1419/1750 train_time:140324ms step_avg:98.89ms
step:1420/1750 train_time:140425ms step_avg:98.89ms
step:1421/1750 train_time:140527ms step_avg:98.89ms
step:1422/1750 train_time:140628ms step_avg:98.89ms
step:1423/1750 train_time:140728ms step_avg:98.90ms
step:1424/1750 train_time:140829ms step_avg:98.90ms
step:1425/1750 train_time:140930ms step_avg:98.90ms
step:1426/1750 train_time:141032ms step_avg:98.90ms
step:1427/1750 train_time:141134ms step_avg:98.90ms
step:1428/1750 train_time:141235ms step_avg:98.90ms
step:1429/1750 train_time:141337ms step_avg:98.91ms
step:1430/1750 train_time:141441ms step_avg:98.91ms
step:1431/1750 train_time:141543ms step_avg:98.91ms
step:1432/1750 train_time:141644ms step_avg:98.91ms
step:1433/1750 train_time:141744ms step_avg:98.91ms
step:1434/1750 train_time:141843ms step_avg:98.91ms
step:1435/1750 train_time:141945ms step_avg:98.92ms
step:1436/1750 train_time:142047ms step_avg:98.92ms
step:1437/1750 train_time:142150ms step_avg:98.92ms
step:1438/1750 train_time:142252ms step_avg:98.92ms
step:1439/1750 train_time:142357ms step_avg:98.93ms
step:1440/1750 train_time:142461ms step_avg:98.93ms
step:1441/1750 train_time:142563ms step_avg:98.93ms
step:1442/1750 train_time:142663ms step_avg:98.93ms
step:1443/1750 train_time:142764ms step_avg:98.94ms
step:1444/1750 train_time:142864ms step_avg:98.94ms
step:1445/1750 train_time:142965ms step_avg:98.94ms
step:1446/1750 train_time:143067ms step_avg:98.94ms
step:1447/1750 train_time:143168ms step_avg:98.94ms
step:1448/1750 train_time:143274ms step_avg:98.95ms
step:1449/1750 train_time:143375ms step_avg:98.95ms
step:1450/1750 train_time:143477ms step_avg:98.95ms
step:1451/1750 train_time:143579ms step_avg:98.95ms
step:1452/1750 train_time:143681ms step_avg:98.95ms
step:1453/1750 train_time:143783ms step_avg:98.96ms
step:1454/1750 train_time:143885ms step_avg:98.96ms
step:1455/1750 train_time:143986ms step_avg:98.96ms
step:1456/1750 train_time:144087ms step_avg:98.96ms
step:1457/1750 train_time:144190ms step_avg:98.96ms
step:1458/1750 train_time:144293ms step_avg:98.97ms
step:1459/1750 train_time:144396ms step_avg:98.97ms
step:1460/1750 train_time:144497ms step_avg:98.97ms
step:1461/1750 train_time:144598ms step_avg:98.97ms
step:1462/1750 train_time:144700ms step_avg:98.97ms
step:1463/1750 train_time:144802ms step_avg:98.98ms
step:1464/1750 train_time:144903ms step_avg:98.98ms
step:1465/1750 train_time:145004ms step_avg:98.98ms
step:1466/1750 train_time:145105ms step_avg:98.98ms
step:1467/1750 train_time:145206ms step_avg:98.98ms
step:1468/1750 train_time:145309ms step_avg:98.98ms
step:1469/1750 train_time:145412ms step_avg:98.99ms
step:1470/1750 train_time:145515ms step_avg:98.99ms
step:1471/1750 train_time:145617ms step_avg:98.99ms
step:1472/1750 train_time:145720ms step_avg:98.99ms
step:1473/1750 train_time:145821ms step_avg:99.00ms
step:1474/1750 train_time:145922ms step_avg:99.00ms
step:1475/1750 train_time:146024ms step_avg:99.00ms
step:1476/1750 train_time:146125ms step_avg:99.00ms
step:1477/1750 train_time:146227ms step_avg:99.00ms
step:1478/1750 train_time:146329ms step_avg:99.00ms
step:1479/1750 train_time:146431ms step_avg:99.01ms
step:1480/1750 train_time:146534ms step_avg:99.01ms
step:1481/1750 train_time:146635ms step_avg:99.01ms
step:1482/1750 train_time:146738ms step_avg:99.01ms
step:1483/1750 train_time:146840ms step_avg:99.02ms
step:1484/1750 train_time:146942ms step_avg:99.02ms
step:1485/1750 train_time:147045ms step_avg:99.02ms
step:1486/1750 train_time:147146ms step_avg:99.02ms
step:1487/1750 train_time:147248ms step_avg:99.02ms
step:1488/1750 train_time:147351ms step_avg:99.03ms
step:1489/1750 train_time:147453ms step_avg:99.03ms
step:1490/1750 train_time:147555ms step_avg:99.03ms
step:1491/1750 train_time:147658ms step_avg:99.03ms
step:1492/1750 train_time:147759ms step_avg:99.03ms
step:1493/1750 train_time:147860ms step_avg:99.04ms
step:1494/1750 train_time:147962ms step_avg:99.04ms
step:1495/1750 train_time:148063ms step_avg:99.04ms
step:1496/1750 train_time:148164ms step_avg:99.04ms
step:1497/1750 train_time:148265ms step_avg:99.04ms
step:1498/1750 train_time:148367ms step_avg:99.04ms
step:1499/1750 train_time:148469ms step_avg:99.05ms
step:1500/1750 train_time:148571ms step_avg:99.05ms
step:1500/1750 val_loss:3.3363 train_time:148662ms step_avg:99.11ms
step:1501/1750 train_time:148683ms step_avg:99.06ms
step:1502/1750 train_time:148785ms step_avg:99.06ms
step:1503/1750 train_time:148887ms step_avg:99.06ms
step:1504/1750 train_time:148988ms step_avg:99.06ms
step:1505/1750 train_time:149088ms step_avg:99.06ms
step:1506/1750 train_time:149189ms step_avg:99.06ms
step:1507/1750 train_time:149291ms step_avg:99.06ms
step:1508/1750 train_time:149391ms step_avg:99.07ms
step:1509/1750 train_time:149493ms step_avg:99.07ms
step:1510/1750 train_time:149595ms step_avg:99.07ms
step:1511/1750 train_time:149700ms step_avg:99.07ms
step:1512/1750 train_time:149803ms step_avg:99.08ms
step:1513/1750 train_time:149904ms step_avg:99.08ms
step:1514/1750 train_time:150005ms step_avg:99.08ms
step:1515/1750 train_time:150109ms step_avg:99.08ms
step:1516/1750 train_time:150209ms step_avg:99.08ms
step:1517/1750 train_time:150310ms step_avg:99.08ms
step:1518/1750 train_time:150410ms step_avg:99.08ms
step:1519/1750 train_time:150512ms step_avg:99.09ms
step:1520/1750 train_time:150615ms step_avg:99.09ms
step:1521/1750 train_time:150719ms step_avg:99.09ms
step:1522/1750 train_time:150821ms step_avg:99.09ms
step:1523/1750 train_time:150922ms step_avg:99.10ms
step:1524/1750 train_time:151025ms step_avg:99.10ms
step:1525/1750 train_time:151127ms step_avg:99.10ms
step:1526/1750 train_time:151230ms step_avg:99.10ms
step:1527/1750 train_time:151331ms step_avg:99.10ms
step:1528/1750 train_time:151437ms step_avg:99.11ms
step:1529/1750 train_time:151538ms step_avg:99.11ms
step:1530/1750 train_time:151643ms step_avg:99.11ms
step:1531/1750 train_time:151744ms step_avg:99.11ms
step:1532/1750 train_time:151845ms step_avg:99.12ms
step:1533/1750 train_time:151947ms step_avg:99.12ms
step:1534/1750 train_time:152049ms step_avg:99.12ms
step:1535/1750 train_time:152151ms step_avg:99.12ms
step:1536/1750 train_time:152252ms step_avg:99.12ms
step:1537/1750 train_time:152354ms step_avg:99.12ms
step:1538/1750 train_time:152455ms step_avg:99.13ms
step:1539/1750 train_time:152557ms step_avg:99.13ms
step:1540/1750 train_time:152660ms step_avg:99.13ms
step:1541/1750 train_time:152764ms step_avg:99.13ms
step:1542/1750 train_time:152867ms step_avg:99.14ms
step:1543/1750 train_time:152970ms step_avg:99.14ms
step:1544/1750 train_time:153072ms step_avg:99.14ms
step:1545/1750 train_time:153174ms step_avg:99.14ms
step:1546/1750 train_time:153276ms step_avg:99.14ms
step:1547/1750 train_time:153378ms step_avg:99.15ms
step:1548/1750 train_time:153480ms step_avg:99.15ms
step:1549/1750 train_time:153582ms step_avg:99.15ms
step:1550/1750 train_time:153683ms step_avg:99.15ms
step:1551/1750 train_time:153786ms step_avg:99.15ms
step:1552/1750 train_time:153889ms step_avg:99.16ms
step:1553/1750 train_time:153991ms step_avg:99.16ms
step:1554/1750 train_time:154091ms step_avg:99.16ms
step:1555/1750 train_time:154193ms step_avg:99.16ms
step:1556/1750 train_time:154296ms step_avg:99.16ms
step:1557/1750 train_time:154398ms step_avg:99.16ms
step:1558/1750 train_time:154500ms step_avg:99.17ms
step:1559/1750 train_time:154602ms step_avg:99.17ms
step:1560/1750 train_time:154703ms step_avg:99.17ms
step:1561/1750 train_time:154805ms step_avg:99.17ms
step:1562/1750 train_time:154908ms step_avg:99.17ms
step:1563/1750 train_time:155013ms step_avg:99.18ms
step:1564/1750 train_time:155114ms step_avg:99.18ms
step:1565/1750 train_time:155216ms step_avg:99.18ms
step:1566/1750 train_time:155317ms step_avg:99.18ms
step:1567/1750 train_time:155419ms step_avg:99.18ms
step:1568/1750 train_time:155519ms step_avg:99.18ms
step:1569/1750 train_time:155622ms step_avg:99.19ms
step:1570/1750 train_time:155725ms step_avg:99.19ms
step:1571/1750 train_time:155826ms step_avg:99.19ms
step:1572/1750 train_time:155928ms step_avg:99.19ms
step:1573/1750 train_time:156030ms step_avg:99.19ms
step:1574/1750 train_time:156133ms step_avg:99.19ms
step:1575/1750 train_time:156235ms step_avg:99.20ms
step:1576/1750 train_time:156338ms step_avg:99.20ms
step:1577/1750 train_time:156441ms step_avg:99.20ms
step:1578/1750 train_time:156542ms step_avg:99.20ms
step:1579/1750 train_time:156643ms step_avg:99.20ms
step:1580/1750 train_time:156745ms step_avg:99.21ms
step:1581/1750 train_time:156846ms step_avg:99.21ms
step:1582/1750 train_time:156947ms step_avg:99.21ms
step:1583/1750 train_time:157050ms step_avg:99.21ms
step:1584/1750 train_time:157153ms step_avg:99.21ms
step:1585/1750 train_time:157254ms step_avg:99.21ms
step:1586/1750 train_time:157358ms step_avg:99.22ms
step:1587/1750 train_time:157459ms step_avg:99.22ms
step:1588/1750 train_time:157562ms step_avg:99.22ms
step:1589/1750 train_time:157663ms step_avg:99.22ms
step:1590/1750 train_time:157765ms step_avg:99.22ms
step:1591/1750 train_time:157867ms step_avg:99.22ms
step:1592/1750 train_time:157969ms step_avg:99.23ms
step:1593/1750 train_time:158070ms step_avg:99.23ms
step:1594/1750 train_time:158176ms step_avg:99.23ms
step:1595/1750 train_time:158278ms step_avg:99.23ms
step:1596/1750 train_time:158380ms step_avg:99.24ms
step:1597/1750 train_time:158481ms step_avg:99.24ms
step:1598/1750 train_time:158584ms step_avg:99.24ms
step:1599/1750 train_time:158685ms step_avg:99.24ms
step:1600/1750 train_time:158786ms step_avg:99.24ms
step:1601/1750 train_time:158889ms step_avg:99.24ms
step:1602/1750 train_time:158991ms step_avg:99.25ms
step:1603/1750 train_time:159092ms step_avg:99.25ms
step:1604/1750 train_time:159194ms step_avg:99.25ms
step:1605/1750 train_time:159297ms step_avg:99.25ms
step:1606/1750 train_time:159398ms step_avg:99.25ms
step:1607/1750 train_time:159500ms step_avg:99.25ms
step:1608/1750 train_time:159601ms step_avg:99.25ms
step:1609/1750 train_time:159704ms step_avg:99.26ms
step:1610/1750 train_time:159806ms step_avg:99.26ms
step:1611/1750 train_time:159909ms step_avg:99.26ms
step:1612/1750 train_time:160011ms step_avg:99.26ms
step:1613/1750 train_time:160113ms step_avg:99.26ms
step:1614/1750 train_time:160215ms step_avg:99.27ms
step:1615/1750 train_time:160315ms step_avg:99.27ms
step:1616/1750 train_time:160418ms step_avg:99.27ms
step:1617/1750 train_time:160520ms step_avg:99.27ms
step:1618/1750 train_time:160622ms step_avg:99.27ms
step:1619/1750 train_time:160723ms step_avg:99.27ms
step:1620/1750 train_time:160826ms step_avg:99.28ms
step:1621/1750 train_time:160927ms step_avg:99.28ms
step:1622/1750 train_time:161028ms step_avg:99.28ms
step:1623/1750 train_time:161132ms step_avg:99.28ms
step:1624/1750 train_time:161235ms step_avg:99.28ms
step:1625/1750 train_time:161338ms step_avg:99.28ms
step:1625/1750 val_loss:3.3057 train_time:161429ms step_avg:99.34ms
step:1626/1750 train_time:161449ms step_avg:99.29ms
step:1627/1750 train_time:161550ms step_avg:99.29ms
step:1628/1750 train_time:161651ms step_avg:99.29ms
step:1629/1750 train_time:161754ms step_avg:99.30ms
step:1630/1750 train_time:161854ms step_avg:99.30ms
step:1631/1750 train_time:161957ms step_avg:99.30ms
step:1632/1750 train_time:162058ms step_avg:99.30ms
step:1633/1750 train_time:162159ms step_avg:99.30ms
step:1634/1750 train_time:162262ms step_avg:99.30ms
step:1635/1750 train_time:162365ms step_avg:99.31ms
step:1636/1750 train_time:162468ms step_avg:99.31ms
step:1637/1750 train_time:162571ms step_avg:99.31ms
step:1638/1750 train_time:162672ms step_avg:99.31ms
step:1639/1750 train_time:162773ms step_avg:99.31ms
step:1640/1750 train_time:162875ms step_avg:99.31ms
step:1641/1750 train_time:162977ms step_avg:99.32ms
step:1642/1750 train_time:163078ms step_avg:99.32ms
step:1643/1750 train_time:163178ms step_avg:99.32ms
step:1644/1750 train_time:163280ms step_avg:99.32ms
step:1645/1750 train_time:163383ms step_avg:99.32ms
step:1646/1750 train_time:163486ms step_avg:99.32ms
step:1647/1750 train_time:163590ms step_avg:99.33ms
step:1648/1750 train_time:163693ms step_avg:99.33ms
step:1649/1750 train_time:163794ms step_avg:99.33ms
step:1650/1750 train_time:163895ms step_avg:99.33ms
step:1651/1750 train_time:163997ms step_avg:99.33ms
step:1652/1750 train_time:164100ms step_avg:99.33ms
step:1653/1750 train_time:164201ms step_avg:99.34ms
step:1654/1750 train_time:164302ms step_avg:99.34ms
step:1655/1750 train_time:164404ms step_avg:99.34ms
step:1656/1750 train_time:164507ms step_avg:99.34ms
step:1657/1750 train_time:164609ms step_avg:99.34ms
step:1658/1750 train_time:164712ms step_avg:99.34ms
step:1659/1750 train_time:164815ms step_avg:99.35ms
step:1660/1750 train_time:164916ms step_avg:99.35ms
step:1661/1750 train_time:165019ms step_avg:99.35ms
step:1662/1750 train_time:165123ms step_avg:99.35ms
step:1663/1750 train_time:165225ms step_avg:99.35ms
step:1664/1750 train_time:165327ms step_avg:99.36ms
step:1665/1750 train_time:165431ms step_avg:99.36ms
step:1666/1750 train_time:165533ms step_avg:99.36ms
step:1667/1750 train_time:165635ms step_avg:99.36ms
step:1668/1750 train_time:165739ms step_avg:99.36ms
step:1669/1750 train_time:165842ms step_avg:99.37ms
step:1670/1750 train_time:165944ms step_avg:99.37ms
step:1671/1750 train_time:166046ms step_avg:99.37ms
step:1672/1750 train_time:166146ms step_avg:99.37ms
step:1673/1750 train_time:166247ms step_avg:99.37ms
step:1674/1750 train_time:166349ms step_avg:99.37ms
step:1675/1750 train_time:166451ms step_avg:99.37ms
step:1676/1750 train_time:166555ms step_avg:99.38ms
step:1677/1750 train_time:166657ms step_avg:99.38ms
step:1678/1750 train_time:166760ms step_avg:99.38ms
step:1679/1750 train_time:166864ms step_avg:99.38ms
step:1680/1750 train_time:166965ms step_avg:99.38ms
step:1681/1750 train_time:167067ms step_avg:99.39ms
step:1682/1750 train_time:167170ms step_avg:99.39ms
step:1683/1750 train_time:167271ms step_avg:99.39ms
step:1684/1750 train_time:167374ms step_avg:99.39ms
step:1685/1750 train_time:167477ms step_avg:99.39ms
step:1686/1750 train_time:167578ms step_avg:99.39ms
step:1687/1750 train_time:167681ms step_avg:99.40ms
step:1688/1750 train_time:167784ms step_avg:99.40ms
step:1689/1750 train_time:167886ms step_avg:99.40ms
step:1690/1750 train_time:167988ms step_avg:99.40ms
step:1691/1750 train_time:168091ms step_avg:99.40ms
step:1692/1750 train_time:168193ms step_avg:99.40ms
step:1693/1750 train_time:168297ms step_avg:99.41ms
step:1694/1750 train_time:168402ms step_avg:99.41ms
step:1695/1750 train_time:168506ms step_avg:99.41ms
step:1696/1750 train_time:168607ms step_avg:99.41ms
step:1697/1750 train_time:168711ms step_avg:99.42ms
step:1698/1750 train_time:168815ms step_avg:99.42ms
step:1699/1750 train_time:168917ms step_avg:99.42ms
step:1700/1750 train_time:169022ms step_avg:99.42ms
step:1701/1750 train_time:169125ms step_avg:99.43ms
step:1702/1750 train_time:169229ms step_avg:99.43ms
step:1703/1750 train_time:169331ms step_avg:99.43ms
step:1704/1750 train_time:169434ms step_avg:99.43ms
step:1705/1750 train_time:169537ms step_avg:99.44ms
step:1706/1750 train_time:169639ms step_avg:99.44ms
step:1707/1750 train_time:169744ms step_avg:99.44ms
step:1708/1750 train_time:169847ms step_avg:99.44ms
step:1709/1750 train_time:169949ms step_avg:99.44ms
step:1710/1750 train_time:170051ms step_avg:99.44ms
step:1711/1750 train_time:170155ms step_avg:99.45ms
step:1712/1750 train_time:170258ms step_avg:99.45ms
step:1713/1750 train_time:170362ms step_avg:99.45ms
step:1714/1750 train_time:170465ms step_avg:99.45ms
step:1715/1750 train_time:170569ms step_avg:99.46ms
step:1716/1750 train_time:170671ms step_avg:99.46ms
step:1717/1750 train_time:170774ms step_avg:99.46ms
step:1718/1750 train_time:170876ms step_avg:99.46ms
step:1719/1750 train_time:170982ms step_avg:99.47ms
step:1720/1750 train_time:171084ms step_avg:99.47ms
step:1721/1750 train_time:171187ms step_avg:99.47ms
step:1722/1750 train_time:171290ms step_avg:99.47ms
step:1723/1750 train_time:171392ms step_avg:99.47ms
step:1724/1750 train_time:171498ms step_avg:99.48ms
step:1725/1750 train_time:171602ms step_avg:99.48ms
step:1726/1750 train_time:171705ms step_avg:99.48ms
step:1727/1750 train_time:171808ms step_avg:99.48ms
step:1728/1750 train_time:171910ms step_avg:99.48ms
step:1729/1750 train_time:172013ms step_avg:99.49ms
step:1730/1750 train_time:172115ms step_avg:99.49ms
step:1731/1750 train_time:172219ms step_avg:99.49ms
step:1732/1750 train_time:172322ms step_avg:99.49ms
step:1733/1750 train_time:172425ms step_avg:99.50ms
step:1734/1750 train_time:172529ms step_avg:99.50ms
step:1735/1750 train_time:172631ms step_avg:99.50ms
step:1736/1750 train_time:172733ms step_avg:99.50ms
step:1737/1750 train_time:172836ms step_avg:99.50ms
step:1738/1750 train_time:172939ms step_avg:99.50ms
step:1739/1750 train_time:173042ms step_avg:99.51ms
step:1740/1750 train_time:173144ms step_avg:99.51ms
step:1741/1750 train_time:173249ms step_avg:99.51ms
step:1742/1750 train_time:173352ms step_avg:99.51ms
step:1743/1750 train_time:173456ms step_avg:99.52ms
step:1744/1750 train_time:173559ms step_avg:99.52ms
step:1745/1750 train_time:173660ms step_avg:99.52ms
step:1746/1750 train_time:173764ms step_avg:99.52ms
step:1747/1750 train_time:173868ms step_avg:99.52ms
step:1748/1750 train_time:173969ms step_avg:99.52ms
step:1749/1750 train_time:174071ms step_avg:99.53ms
step:1750/1750 train_time:174176ms step_avg:99.53ms
step:1750/1750 val_loss:3.2818 train_time:174266ms step_avg:99.58ms
peak memory allocated: 33278 MiB reserved: 48134 MiB
