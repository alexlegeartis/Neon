import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X
'''
import torch.utils.dlpack as thd
from cupyx.scipy.sparse.linalg import svds as cupyx_svds

def several_sv_svds_approximation(W_torch, k, num_iter=50):
    """SVD approximation using the top k singular values and corresponding vectors."""
    # Store original device and dtype
    original_device = W_torch.device
    original_dtype = W_torch.dtype
    
    W = cp.from_dlpack(thd.to_dlpack(W_torch)).astype(cp.float32)
    U, S, Vt = cupyx_svds(W, k=min([k, W.shape[0] - 1, W.shape[1] - 1]), maxiter=num_iter, which='LM')

    # Convert back to torch tensors and ensure they're on the correct device
    approx_torch_U = thd.from_dlpack(U.toDlpack()).to(device=original_device, dtype=original_dtype)
    approx_torch_S = thd.from_dlpack(S.toDlpack()).to(device=original_device, dtype=original_dtype)
    approx_torch_Vt = thd.from_dlpack(Vt.toDlpack()).to(device=original_device, dtype=original_dtype)
    
    return approx_torch_U, approx_torch_S, approx_torch_Vt

class NormNeon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    # v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    u, s, vt = several_sv_svds_approximation(grad.bfloat16(), 5)
                    p.add_(other=u@vt, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()
'''
class FastNormMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, sgd_coeff=0):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, sgd_coeff=sgd_coeff)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        self.sgd_coeff = sgd_coeff

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            sgd_coeff = group["sgd_coeff"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    # assume: grad is a Tensor, momentum_buffer exists, momentum, sgd_coeff, eps, eff_lr are defined
                    with torch.no_grad():
                        # inplace blend with momentum (keeps original semantic: grad becomes the lerped tensor)
                        grad.lerp_(momentum_buffer, momentum)

                        # compute zeropower part from the (post-lerp) grad, in bfloat16 to match the original code
                        if sgd_coeff != 1:
                            # create update_part in bfloat16 (may allocate once per call; unavoidable if zeropower expects bfloat16)
                            update_part = zeropower_via_newtonschulz5(grad.to(torch.bfloat16), 5)

                            # upcast update_part to grad dtype to do mixed-dtype blending without extra temporaries later
                            if update_part.dtype != grad.dtype:
                                update_part = update_part.to(grad.dtype)

                        # compute inverse norm scalar efficiently (no large temp tensors)
                        # - view(-1) is cheap (no copy)
                        # - dot(view, view) is a single reduction
                        norm_sq = grad.view(-1).dot(grad.view(-1))
                        inv_norm = (norm_sq + eps).rsqrt()  # stable rsqrt on the scalar

                        # normalize grad in-place (so we avoid allocating g_normalized)
                        grad.mul_(inv_norm)   # now grad == g_normalized (in-place)

                        # build final update (reuse update_part buffer if available)
                        if sgd_coeff != 1:
                            # update_part := (1 - sgd_coeff) * update_part + sgd_coeff * grad
                            # do it in-place on update_part to avoid creating another temp
                            update_part.mul_(1 - sgd_coeff)    # in-place scale
                            update_part.lerp_(grad, sgd_coeff) # in-place linear interpolation
                            update = update_part
                        else:
                            # if sgd_coeff == 1 we can use grad directly (already normalized in-place)
                            update = grad

                        # apply update (same as original)
                        p.add_(other=update, alpha=-eff_lr)
                    '''
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    eps = 1e-12
                    g_normalized = grad / (grad.norm() + eps)
                    if sgd_coeff != 1:
                        update_part = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                        update = (1 - sgd_coeff) * update_part + sgd_coeff * g_normalized
                    else:
                        update = sgd_coeff * g_normalized
                    p.add_(other=update, alpha=-eff_lr)
                    '''
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class NormMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, sgd_coeff=0):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, sgd_coeff=sgd_coeff)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        self.sgd_coeff = sgd_coeff

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            sgd_coeff = group["sgd_coeff"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    eps = 1e-12
                    g_normalized = grad / (grad.norm() + eps)
                    if sgd_coeff != 1:
                        update_part = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                        update = (1 - sgd_coeff) * update_part + sgd_coeff * g_normalized
                    else:
                        update = sgd_coeff * g_normalized
                    p.add_(other=update, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()
class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)
                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_reduce_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 27.5
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos : pos + max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()
    starts = []
    for i in range(1, len(boundary_positions)):
        end = boundary_positions[i].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == dist.get_world_size():
                return starts, end - pos
            start = end
    assert False # increase max_batch_span if necessary

def distributed_data_generator(filename_pattern: str, batch_size: int, align_to_bos: bool):
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    max_batch_span = 2 * batch_size if align_to_bos else batch_size # provide buffer to handle samples up to length local_batch_size
    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            batch_span = batch_size
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1750 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
from datetime import datetime
if master_process:
    # run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_id = str(int(time.time() * 1000))
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0)
optimizer2 = NormMuon(hidden_matrix_params, lr=0.07, momentum=0.95, weight_decay=0.0, sgd_coeff=0.5)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.11 | packaged by conda-forge | (main, Jun  4 2025, 14:45:31) [GCC 13.3.0]
Running PyTorch 2.9.0.dev20250824+cu126 compiled for CUDA 12.6
Sun Aug 24 18:02:31 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:04:00.0 Off |                    0 |
| N/A   40C    P0            123W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:23:00.0 Off |                    0 |
| N/A   34C    P0            115W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:43:00.0 Off |                    0 |
| N/A   39C    P0            125W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   33C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:84:00.0 Off |                    0 |
| N/A   39C    P0            123W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:A3:00.0 Off |                    0 |
| N/A   34C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:C3:00.0 Off |                    0 |
| N/A   40C    P0            121W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E4:00.0 Off |                    0 |
| N/A   32C    P0            113W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.29ms
step:1/1750 train_time:145ms step_avg:145.41ms
step:2/1750 train_time:168ms step_avg:83.93ms
step:3/1750 train_time:247ms step_avg:82.39ms
step:4/1750 train_time:339ms step_avg:84.74ms
step:5/1750 train_time:431ms step_avg:86.19ms
step:6/1750 train_time:523ms step_avg:87.18ms
step:7/1750 train_time:615ms step_avg:87.91ms
step:8/1750 train_time:708ms step_avg:88.44ms
step:9/1750 train_time:800ms step_avg:88.88ms
step:10/1750 train_time:892ms step_avg:89.23ms
step:11/1750 train_time:986ms step_avg:89.60ms
step:12/1750 train_time:1082ms step_avg:90.18ms
step:13/1750 train_time:1177ms step_avg:90.57ms
step:14/1750 train_time:1272ms step_avg:90.88ms
step:15/1750 train_time:1366ms step_avg:91.10ms
step:16/1750 train_time:1460ms step_avg:91.28ms
step:17/1750 train_time:1553ms step_avg:91.35ms
step:18/1750 train_time:1646ms step_avg:91.44ms
step:19/1750 train_time:1739ms step_avg:91.50ms
step:20/1750 train_time:1831ms step_avg:91.55ms
step:21/1750 train_time:1924ms step_avg:91.64ms
step:22/1750 train_time:2019ms step_avg:91.79ms
step:23/1750 train_time:2114ms step_avg:91.90ms
step:24/1750 train_time:2208ms step_avg:91.99ms
step:25/1750 train_time:2302ms step_avg:92.08ms
step:26/1750 train_time:2396ms step_avg:92.17ms
step:27/1750 train_time:2489ms step_avg:92.20ms
step:28/1750 train_time:2584ms step_avg:92.27ms
step:29/1750 train_time:2676ms step_avg:92.28ms
step:30/1750 train_time:2769ms step_avg:92.29ms
step:31/1750 train_time:2861ms step_avg:92.30ms
step:32/1750 train_time:2955ms step_avg:92.33ms
step:33/1750 train_time:3049ms step_avg:92.38ms
step:34/1750 train_time:3143ms step_avg:92.43ms
step:35/1750 train_time:3236ms step_avg:92.47ms
step:36/1750 train_time:3330ms step_avg:92.50ms
step:37/1750 train_time:3424ms step_avg:92.54ms
step:38/1750 train_time:3517ms step_avg:92.55ms
step:39/1750 train_time:3610ms step_avg:92.56ms
step:40/1750 train_time:3703ms step_avg:92.58ms
step:41/1750 train_time:3796ms step_avg:92.58ms
step:42/1750 train_time:3889ms step_avg:92.60ms
step:43/1750 train_time:3983ms step_avg:92.63ms
step:44/1750 train_time:4076ms step_avg:92.64ms
step:45/1750 train_time:4170ms step_avg:92.66ms
step:46/1750 train_time:4263ms step_avg:92.68ms
step:47/1750 train_time:4357ms step_avg:92.71ms
step:48/1750 train_time:4451ms step_avg:92.74ms
step:49/1750 train_time:4545ms step_avg:92.75ms
step:50/1750 train_time:4638ms step_avg:92.76ms
step:51/1750 train_time:4731ms step_avg:92.76ms
step:52/1750 train_time:4824ms step_avg:92.77ms
step:53/1750 train_time:4917ms step_avg:92.77ms
step:54/1750 train_time:5009ms step_avg:92.77ms
step:55/1750 train_time:5103ms step_avg:92.79ms
step:56/1750 train_time:5196ms step_avg:92.79ms
step:57/1750 train_time:5290ms step_avg:92.80ms
step:58/1750 train_time:5384ms step_avg:92.83ms
step:59/1750 train_time:5478ms step_avg:92.84ms
step:60/1750 train_time:5571ms step_avg:92.85ms
step:61/1750 train_time:5665ms step_avg:92.86ms
step:62/1750 train_time:5758ms step_avg:92.86ms
step:63/1750 train_time:5851ms step_avg:92.87ms
step:64/1750 train_time:5944ms step_avg:92.87ms
step:65/1750 train_time:6037ms step_avg:92.87ms
step:66/1750 train_time:6129ms step_avg:92.87ms
step:67/1750 train_time:6223ms step_avg:92.88ms
step:68/1750 train_time:6316ms step_avg:92.88ms
step:69/1750 train_time:6410ms step_avg:92.89ms
step:70/1750 train_time:6503ms step_avg:92.90ms
step:71/1750 train_time:6597ms step_avg:92.91ms
step:72/1750 train_time:6691ms step_avg:92.92ms
step:73/1750 train_time:6784ms step_avg:92.93ms
step:74/1750 train_time:6878ms step_avg:92.94ms
step:75/1750 train_time:6970ms step_avg:92.94ms
step:76/1750 train_time:7064ms step_avg:92.94ms
step:77/1750 train_time:7156ms step_avg:92.94ms
step:78/1750 train_time:7249ms step_avg:92.93ms
step:79/1750 train_time:7342ms step_avg:92.94ms
step:80/1750 train_time:7436ms step_avg:92.95ms
step:81/1750 train_time:7529ms step_avg:92.95ms
step:82/1750 train_time:7624ms step_avg:92.97ms
step:83/1750 train_time:7716ms step_avg:92.97ms
step:84/1750 train_time:7809ms step_avg:92.96ms
step:85/1750 train_time:7903ms step_avg:92.98ms
step:86/1750 train_time:7996ms step_avg:92.97ms
step:87/1750 train_time:8089ms step_avg:92.97ms
step:88/1750 train_time:8182ms step_avg:92.98ms
step:89/1750 train_time:8275ms step_avg:92.97ms
step:90/1750 train_time:8368ms step_avg:92.98ms
step:91/1750 train_time:8461ms step_avg:92.98ms
step:92/1750 train_time:8554ms step_avg:92.98ms
step:93/1750 train_time:8648ms step_avg:92.99ms
step:94/1750 train_time:8741ms step_avg:92.99ms
step:95/1750 train_time:8835ms step_avg:93.00ms
step:96/1750 train_time:8927ms step_avg:92.99ms
step:97/1750 train_time:9022ms step_avg:93.01ms
step:98/1750 train_time:9115ms step_avg:93.01ms
step:99/1750 train_time:9208ms step_avg:93.01ms
step:100/1750 train_time:9301ms step_avg:93.01ms
step:101/1750 train_time:9394ms step_avg:93.01ms
step:102/1750 train_time:9488ms step_avg:93.01ms
step:103/1750 train_time:9582ms step_avg:93.03ms
step:104/1750 train_time:9674ms step_avg:93.02ms
step:105/1750 train_time:9767ms step_avg:93.02ms
step:106/1750 train_time:9861ms step_avg:93.03ms
step:107/1750 train_time:9955ms step_avg:93.03ms
step:108/1750 train_time:10048ms step_avg:93.04ms
step:109/1750 train_time:10142ms step_avg:93.05ms
step:110/1750 train_time:10235ms step_avg:93.04ms
step:111/1750 train_time:10327ms step_avg:93.04ms
step:112/1750 train_time:10422ms step_avg:93.05ms
step:113/1750 train_time:10515ms step_avg:93.05ms
step:114/1750 train_time:10607ms step_avg:93.05ms
step:115/1750 train_time:10700ms step_avg:93.04ms
step:116/1750 train_time:10793ms step_avg:93.04ms
step:117/1750 train_time:10886ms step_avg:93.04ms
step:118/1750 train_time:10979ms step_avg:93.05ms
step:119/1750 train_time:11073ms step_avg:93.05ms
step:120/1750 train_time:11166ms step_avg:93.05ms
step:121/1750 train_time:11259ms step_avg:93.05ms
step:122/1750 train_time:11352ms step_avg:93.05ms
step:123/1750 train_time:11446ms step_avg:93.06ms
step:124/1750 train_time:11539ms step_avg:93.05ms
step:125/1750 train_time:11631ms step_avg:93.05ms
step:125/1750 val_loss:4.6708 train_time:11714ms step_avg:93.72ms
step:126/1750 train_time:11746ms step_avg:93.22ms
step:127/1750 train_time:11826ms step_avg:93.12ms
step:128/1750 train_time:11927ms step_avg:93.18ms
step:129/1750 train_time:12021ms step_avg:93.18ms
step:130/1750 train_time:12114ms step_avg:93.19ms
step:131/1750 train_time:12207ms step_avg:93.18ms
step:132/1750 train_time:12299ms step_avg:93.18ms
step:133/1750 train_time:12392ms step_avg:93.17ms
step:134/1750 train_time:12484ms step_avg:93.16ms
step:135/1750 train_time:12577ms step_avg:93.16ms
step:136/1750 train_time:12670ms step_avg:93.16ms
step:137/1750 train_time:12764ms step_avg:93.17ms
step:138/1750 train_time:12860ms step_avg:93.19ms
step:139/1750 train_time:12957ms step_avg:93.21ms
step:140/1750 train_time:13052ms step_avg:93.23ms
step:141/1750 train_time:13145ms step_avg:93.23ms
step:142/1750 train_time:13239ms step_avg:93.23ms
step:143/1750 train_time:13333ms step_avg:93.24ms
step:144/1750 train_time:13426ms step_avg:93.24ms
step:145/1750 train_time:13519ms step_avg:93.23ms
step:146/1750 train_time:13613ms step_avg:93.24ms
step:147/1750 train_time:13706ms step_avg:93.24ms
step:148/1750 train_time:13801ms step_avg:93.25ms
step:149/1750 train_time:13896ms step_avg:93.26ms
step:150/1750 train_time:13992ms step_avg:93.28ms
step:151/1750 train_time:14086ms step_avg:93.28ms
step:152/1750 train_time:14180ms step_avg:93.29ms
step:153/1750 train_time:14274ms step_avg:93.29ms
step:154/1750 train_time:14366ms step_avg:93.29ms
step:155/1750 train_time:14460ms step_avg:93.29ms
step:156/1750 train_time:14554ms step_avg:93.29ms
step:157/1750 train_time:14647ms step_avg:93.29ms
step:158/1750 train_time:14741ms step_avg:93.30ms
step:159/1750 train_time:14834ms step_avg:93.30ms
step:160/1750 train_time:14928ms step_avg:93.30ms
step:161/1750 train_time:15022ms step_avg:93.30ms
step:162/1750 train_time:15116ms step_avg:93.31ms
step:163/1750 train_time:15210ms step_avg:93.31ms
step:164/1750 train_time:15303ms step_avg:93.31ms
step:165/1750 train_time:15397ms step_avg:93.31ms
step:166/1750 train_time:15490ms step_avg:93.31ms
step:167/1750 train_time:15583ms step_avg:93.31ms
step:168/1750 train_time:15677ms step_avg:93.32ms
step:169/1750 train_time:15771ms step_avg:93.32ms
step:170/1750 train_time:15865ms step_avg:93.32ms
step:171/1750 train_time:15959ms step_avg:93.33ms
step:172/1750 train_time:16053ms step_avg:93.33ms
step:173/1750 train_time:16147ms step_avg:93.34ms
step:174/1750 train_time:16241ms step_avg:93.34ms
step:175/1750 train_time:16335ms step_avg:93.34ms
step:176/1750 train_time:16429ms step_avg:93.35ms
step:177/1750 train_time:16522ms step_avg:93.35ms
step:178/1750 train_time:16616ms step_avg:93.35ms
step:179/1750 train_time:16710ms step_avg:93.35ms
step:180/1750 train_time:16803ms step_avg:93.35ms
step:181/1750 train_time:16897ms step_avg:93.36ms
step:182/1750 train_time:16992ms step_avg:93.36ms
step:183/1750 train_time:17086ms step_avg:93.37ms
step:184/1750 train_time:17180ms step_avg:93.37ms
step:185/1750 train_time:17273ms step_avg:93.37ms
step:186/1750 train_time:17367ms step_avg:93.37ms
step:187/1750 train_time:17461ms step_avg:93.37ms
step:188/1750 train_time:17555ms step_avg:93.38ms
step:189/1750 train_time:17648ms step_avg:93.38ms
step:190/1750 train_time:17742ms step_avg:93.38ms
step:191/1750 train_time:17836ms step_avg:93.38ms
step:192/1750 train_time:17929ms step_avg:93.38ms
step:193/1750 train_time:18022ms step_avg:93.38ms
step:194/1750 train_time:18116ms step_avg:93.38ms
step:195/1750 train_time:18210ms step_avg:93.38ms
step:196/1750 train_time:18303ms step_avg:93.38ms
step:197/1750 train_time:18397ms step_avg:93.39ms
step:198/1750 train_time:18491ms step_avg:93.39ms
step:199/1750 train_time:18584ms step_avg:93.39ms
step:200/1750 train_time:18679ms step_avg:93.39ms
step:201/1750 train_time:18773ms step_avg:93.40ms
step:202/1750 train_time:18866ms step_avg:93.40ms
step:203/1750 train_time:18961ms step_avg:93.40ms
step:204/1750 train_time:19054ms step_avg:93.40ms
step:205/1750 train_time:19148ms step_avg:93.40ms
step:206/1750 train_time:19242ms step_avg:93.41ms
step:207/1750 train_time:19336ms step_avg:93.41ms
step:208/1750 train_time:19429ms step_avg:93.41ms
step:209/1750 train_time:19523ms step_avg:93.41ms
step:210/1750 train_time:19618ms step_avg:93.42ms
step:211/1750 train_time:19713ms step_avg:93.43ms
step:212/1750 train_time:19807ms step_avg:93.43ms
step:213/1750 train_time:19901ms step_avg:93.43ms
step:214/1750 train_time:19995ms step_avg:93.43ms
step:215/1750 train_time:20088ms step_avg:93.43ms
step:216/1750 train_time:20181ms step_avg:93.43ms
step:217/1750 train_time:20276ms step_avg:93.44ms
step:218/1750 train_time:20369ms step_avg:93.43ms
step:219/1750 train_time:20462ms step_avg:93.43ms
step:220/1750 train_time:20556ms step_avg:93.44ms
step:221/1750 train_time:20650ms step_avg:93.44ms
step:222/1750 train_time:20744ms step_avg:93.44ms
step:223/1750 train_time:20837ms step_avg:93.44ms
step:224/1750 train_time:20930ms step_avg:93.44ms
step:225/1750 train_time:21024ms step_avg:93.44ms
step:226/1750 train_time:21119ms step_avg:93.44ms
step:227/1750 train_time:21212ms step_avg:93.45ms
step:228/1750 train_time:21306ms step_avg:93.45ms
step:229/1750 train_time:21399ms step_avg:93.45ms
step:230/1750 train_time:21493ms step_avg:93.45ms
step:231/1750 train_time:21586ms step_avg:93.45ms
step:232/1750 train_time:21680ms step_avg:93.45ms
step:233/1750 train_time:21775ms step_avg:93.45ms
step:234/1750 train_time:21868ms step_avg:93.45ms
step:235/1750 train_time:21962ms step_avg:93.45ms
step:236/1750 train_time:22056ms step_avg:93.46ms
step:237/1750 train_time:22149ms step_avg:93.46ms
step:238/1750 train_time:22243ms step_avg:93.46ms
step:239/1750 train_time:22337ms step_avg:93.46ms
step:240/1750 train_time:22430ms step_avg:93.46ms
step:241/1750 train_time:22524ms step_avg:93.46ms
step:242/1750 train_time:22617ms step_avg:93.46ms
step:243/1750 train_time:22711ms step_avg:93.46ms
step:244/1750 train_time:22805ms step_avg:93.46ms
step:245/1750 train_time:22899ms step_avg:93.46ms
step:246/1750 train_time:22993ms step_avg:93.47ms
step:247/1750 train_time:23087ms step_avg:93.47ms
step:248/1750 train_time:23180ms step_avg:93.47ms
step:249/1750 train_time:23274ms step_avg:93.47ms
step:250/1750 train_time:23368ms step_avg:93.47ms
step:250/1750 val_loss:4.1069 train_time:23451ms step_avg:93.80ms
step:251/1750 train_time:23475ms step_avg:93.53ms
step:252/1750 train_time:23565ms step_avg:93.51ms
step:253/1750 train_time:23661ms step_avg:93.52ms
step:254/1750 train_time:23756ms step_avg:93.53ms
step:255/1750 train_time:23848ms step_avg:93.52ms
step:256/1750 train_time:23941ms step_avg:93.52ms
step:257/1750 train_time:24034ms step_avg:93.52ms
step:258/1750 train_time:24127ms step_avg:93.52ms
step:259/1750 train_time:24219ms step_avg:93.51ms
step:260/1750 train_time:24312ms step_avg:93.51ms
step:261/1750 train_time:24405ms step_avg:93.51ms
step:262/1750 train_time:24501ms step_avg:93.51ms
step:263/1750 train_time:24597ms step_avg:93.52ms
step:264/1750 train_time:24693ms step_avg:93.53ms
step:265/1750 train_time:24788ms step_avg:93.54ms
step:266/1750 train_time:24881ms step_avg:93.54ms
step:267/1750 train_time:24974ms step_avg:93.54ms
step:268/1750 train_time:25068ms step_avg:93.54ms
step:269/1750 train_time:25161ms step_avg:93.54ms
step:270/1750 train_time:25255ms step_avg:93.54ms
step:271/1750 train_time:25348ms step_avg:93.54ms
step:272/1750 train_time:25443ms step_avg:93.54ms
step:273/1750 train_time:25538ms step_avg:93.54ms
step:274/1750 train_time:25633ms step_avg:93.55ms
step:275/1750 train_time:25729ms step_avg:93.56ms
step:276/1750 train_time:25823ms step_avg:93.56ms
step:277/1750 train_time:25917ms step_avg:93.56ms
step:278/1750 train_time:26011ms step_avg:93.56ms
step:279/1750 train_time:26104ms step_avg:93.56ms
step:280/1750 train_time:26199ms step_avg:93.57ms
step:281/1750 train_time:26292ms step_avg:93.57ms
step:282/1750 train_time:26386ms step_avg:93.57ms
step:283/1750 train_time:26480ms step_avg:93.57ms
step:284/1750 train_time:26574ms step_avg:93.57ms
step:285/1750 train_time:26669ms step_avg:93.58ms
step:286/1750 train_time:26763ms step_avg:93.58ms
step:287/1750 train_time:26858ms step_avg:93.58ms
step:288/1750 train_time:26954ms step_avg:93.59ms
step:289/1750 train_time:27047ms step_avg:93.59ms
step:290/1750 train_time:27140ms step_avg:93.59ms
step:291/1750 train_time:27234ms step_avg:93.59ms
step:292/1750 train_time:27328ms step_avg:93.59ms
step:293/1750 train_time:27422ms step_avg:93.59ms
step:294/1750 train_time:27516ms step_avg:93.59ms
step:295/1750 train_time:27611ms step_avg:93.60ms
step:296/1750 train_time:27704ms step_avg:93.60ms
step:297/1750 train_time:27799ms step_avg:93.60ms
step:298/1750 train_time:27893ms step_avg:93.60ms
step:299/1750 train_time:27987ms step_avg:93.60ms
step:300/1750 train_time:28080ms step_avg:93.60ms
step:301/1750 train_time:28174ms step_avg:93.60ms
step:302/1750 train_time:28267ms step_avg:93.60ms
step:303/1750 train_time:28361ms step_avg:93.60ms
step:304/1750 train_time:28456ms step_avg:93.61ms
step:305/1750 train_time:28550ms step_avg:93.61ms
step:306/1750 train_time:28644ms step_avg:93.61ms
step:307/1750 train_time:28738ms step_avg:93.61ms
step:308/1750 train_time:28833ms step_avg:93.61ms
step:309/1750 train_time:28926ms step_avg:93.61ms
step:310/1750 train_time:29021ms step_avg:93.62ms
step:311/1750 train_time:29116ms step_avg:93.62ms
step:312/1750 train_time:29210ms step_avg:93.62ms
step:313/1750 train_time:29305ms step_avg:93.62ms
step:314/1750 train_time:29399ms step_avg:93.63ms
step:315/1750 train_time:29493ms step_avg:93.63ms
step:316/1750 train_time:29587ms step_avg:93.63ms
step:317/1750 train_time:29681ms step_avg:93.63ms
step:318/1750 train_time:29776ms step_avg:93.64ms
step:319/1750 train_time:29870ms step_avg:93.64ms
step:320/1750 train_time:29964ms step_avg:93.64ms
step:321/1750 train_time:30058ms step_avg:93.64ms
step:322/1750 train_time:30152ms step_avg:93.64ms
step:323/1750 train_time:30246ms step_avg:93.64ms
step:324/1750 train_time:30340ms step_avg:93.64ms
step:325/1750 train_time:30435ms step_avg:93.65ms
step:326/1750 train_time:30529ms step_avg:93.65ms
step:327/1750 train_time:30623ms step_avg:93.65ms
step:328/1750 train_time:30718ms step_avg:93.65ms
step:329/1750 train_time:30812ms step_avg:93.65ms
step:330/1750 train_time:30905ms step_avg:93.65ms
step:331/1750 train_time:30999ms step_avg:93.65ms
step:332/1750 train_time:31093ms step_avg:93.65ms
step:333/1750 train_time:31187ms step_avg:93.66ms
step:334/1750 train_time:31281ms step_avg:93.66ms
step:335/1750 train_time:31375ms step_avg:93.66ms
step:336/1750 train_time:31470ms step_avg:93.66ms
step:337/1750 train_time:31565ms step_avg:93.66ms
step:338/1750 train_time:31658ms step_avg:93.66ms
step:339/1750 train_time:31753ms step_avg:93.67ms
step:340/1750 train_time:31846ms step_avg:93.67ms
step:341/1750 train_time:31940ms step_avg:93.67ms
step:342/1750 train_time:32035ms step_avg:93.67ms
step:343/1750 train_time:32128ms step_avg:93.67ms
step:344/1750 train_time:32222ms step_avg:93.67ms
step:345/1750 train_time:32317ms step_avg:93.67ms
step:346/1750 train_time:32411ms step_avg:93.67ms
step:347/1750 train_time:32505ms step_avg:93.67ms
step:348/1750 train_time:32599ms step_avg:93.68ms
step:349/1750 train_time:32693ms step_avg:93.68ms
step:350/1750 train_time:32787ms step_avg:93.68ms
step:351/1750 train_time:32881ms step_avg:93.68ms
step:352/1750 train_time:32975ms step_avg:93.68ms
step:353/1750 train_time:33069ms step_avg:93.68ms
step:354/1750 train_time:33163ms step_avg:93.68ms
step:355/1750 train_time:33257ms step_avg:93.68ms
step:356/1750 train_time:33351ms step_avg:93.68ms
step:357/1750 train_time:33444ms step_avg:93.68ms
step:358/1750 train_time:33539ms step_avg:93.68ms
step:359/1750 train_time:33633ms step_avg:93.69ms
step:360/1750 train_time:33727ms step_avg:93.69ms
step:361/1750 train_time:33821ms step_avg:93.69ms
step:362/1750 train_time:33915ms step_avg:93.69ms
step:363/1750 train_time:34009ms step_avg:93.69ms
step:364/1750 train_time:34104ms step_avg:93.69ms
step:365/1750 train_time:34198ms step_avg:93.69ms
step:366/1750 train_time:34291ms step_avg:93.69ms
step:367/1750 train_time:34385ms step_avg:93.69ms
step:368/1750 train_time:34480ms step_avg:93.69ms
step:369/1750 train_time:34574ms step_avg:93.70ms
step:370/1750 train_time:34668ms step_avg:93.70ms
step:371/1750 train_time:34762ms step_avg:93.70ms
step:372/1750 train_time:34856ms step_avg:93.70ms
step:373/1750 train_time:34950ms step_avg:93.70ms
step:374/1750 train_time:35044ms step_avg:93.70ms
step:375/1750 train_time:35138ms step_avg:93.70ms
step:375/1750 val_loss:3.8933 train_time:35222ms step_avg:93.93ms
step:376/1750 train_time:35245ms step_avg:93.74ms
step:377/1750 train_time:35334ms step_avg:93.72ms
step:378/1750 train_time:35433ms step_avg:93.74ms
step:379/1750 train_time:35527ms step_avg:93.74ms
step:380/1750 train_time:35621ms step_avg:93.74ms
step:381/1750 train_time:35714ms step_avg:93.74ms
step:382/1750 train_time:35807ms step_avg:93.74ms
step:383/1750 train_time:35901ms step_avg:93.74ms
step:384/1750 train_time:35994ms step_avg:93.73ms
step:385/1750 train_time:36087ms step_avg:93.73ms
step:386/1750 train_time:36183ms step_avg:93.74ms
step:387/1750 train_time:36279ms step_avg:93.74ms
step:388/1750 train_time:36375ms step_avg:93.75ms
step:389/1750 train_time:36470ms step_avg:93.75ms
step:390/1750 train_time:36564ms step_avg:93.75ms
step:391/1750 train_time:36660ms step_avg:93.76ms
step:392/1750 train_time:36755ms step_avg:93.76ms
step:393/1750 train_time:36850ms step_avg:93.77ms
step:394/1750 train_time:36947ms step_avg:93.77ms
step:395/1750 train_time:37042ms step_avg:93.78ms
step:396/1750 train_time:37138ms step_avg:93.78ms
step:397/1750 train_time:37235ms step_avg:93.79ms
step:398/1750 train_time:37332ms step_avg:93.80ms
step:399/1750 train_time:37430ms step_avg:93.81ms
step:400/1750 train_time:37527ms step_avg:93.82ms
step:401/1750 train_time:37623ms step_avg:93.82ms
step:402/1750 train_time:37718ms step_avg:93.83ms
step:403/1750 train_time:37813ms step_avg:93.83ms
step:404/1750 train_time:37909ms step_avg:93.83ms
step:405/1750 train_time:38004ms step_avg:93.84ms
step:406/1750 train_time:38099ms step_avg:93.84ms
step:407/1750 train_time:38195ms step_avg:93.85ms
step:408/1750 train_time:38291ms step_avg:93.85ms
step:409/1750 train_time:38388ms step_avg:93.86ms
step:410/1750 train_time:38485ms step_avg:93.87ms
step:411/1750 train_time:38580ms step_avg:93.87ms
step:412/1750 train_time:38676ms step_avg:93.87ms
step:413/1750 train_time:38772ms step_avg:93.88ms
step:414/1750 train_time:38867ms step_avg:93.88ms
step:415/1750 train_time:38963ms step_avg:93.89ms
step:416/1750 train_time:39058ms step_avg:93.89ms
step:417/1750 train_time:39154ms step_avg:93.89ms
step:418/1750 train_time:39249ms step_avg:93.90ms
step:419/1750 train_time:39346ms step_avg:93.90ms
step:420/1750 train_time:39442ms step_avg:93.91ms
step:421/1750 train_time:39538ms step_avg:93.91ms
step:422/1750 train_time:39634ms step_avg:93.92ms
step:423/1750 train_time:39729ms step_avg:93.92ms
step:424/1750 train_time:39825ms step_avg:93.93ms
step:425/1750 train_time:39920ms step_avg:93.93ms
step:426/1750 train_time:40016ms step_avg:93.93ms
step:427/1750 train_time:40111ms step_avg:93.94ms
step:428/1750 train_time:40208ms step_avg:93.94ms
step:429/1750 train_time:40303ms step_avg:93.95ms
step:430/1750 train_time:40399ms step_avg:93.95ms
step:431/1750 train_time:40496ms step_avg:93.96ms
step:432/1750 train_time:40592ms step_avg:93.96ms
step:433/1750 train_time:40688ms step_avg:93.97ms
step:434/1750 train_time:40783ms step_avg:93.97ms
step:435/1750 train_time:40879ms step_avg:93.97ms
step:436/1750 train_time:40974ms step_avg:93.98ms
step:437/1750 train_time:41070ms step_avg:93.98ms
step:438/1750 train_time:41166ms step_avg:93.99ms
step:439/1750 train_time:41261ms step_avg:93.99ms
step:440/1750 train_time:41357ms step_avg:93.99ms
step:441/1750 train_time:41453ms step_avg:94.00ms
step:442/1750 train_time:41549ms step_avg:94.00ms
step:443/1750 train_time:41645ms step_avg:94.01ms
step:444/1750 train_time:41741ms step_avg:94.01ms
step:445/1750 train_time:41836ms step_avg:94.01ms
step:446/1750 train_time:41932ms step_avg:94.02ms
step:447/1750 train_time:42028ms step_avg:94.02ms
step:448/1750 train_time:42124ms step_avg:94.03ms
step:449/1750 train_time:42219ms step_avg:94.03ms
step:450/1750 train_time:42316ms step_avg:94.03ms
step:451/1750 train_time:42412ms step_avg:94.04ms
step:452/1750 train_time:42508ms step_avg:94.04ms
step:453/1750 train_time:42604ms step_avg:94.05ms
step:454/1750 train_time:42700ms step_avg:94.05ms
step:455/1750 train_time:42796ms step_avg:94.06ms
step:456/1750 train_time:42892ms step_avg:94.06ms
step:457/1750 train_time:42989ms step_avg:94.07ms
step:458/1750 train_time:43085ms step_avg:94.07ms
step:459/1750 train_time:43181ms step_avg:94.08ms
step:460/1750 train_time:43276ms step_avg:94.08ms
step:461/1750 train_time:43372ms step_avg:94.08ms
step:462/1750 train_time:43469ms step_avg:94.09ms
step:463/1750 train_time:43564ms step_avg:94.09ms
step:464/1750 train_time:43660ms step_avg:94.10ms
step:465/1750 train_time:43756ms step_avg:94.10ms
step:466/1750 train_time:43852ms step_avg:94.10ms
step:467/1750 train_time:43949ms step_avg:94.11ms
step:468/1750 train_time:44045ms step_avg:94.11ms
step:469/1750 train_time:44141ms step_avg:94.12ms
step:470/1750 train_time:44236ms step_avg:94.12ms
step:471/1750 train_time:44332ms step_avg:94.12ms
step:472/1750 train_time:44428ms step_avg:94.13ms
step:473/1750 train_time:44524ms step_avg:94.13ms
step:474/1750 train_time:44620ms step_avg:94.13ms
step:475/1750 train_time:44716ms step_avg:94.14ms
step:476/1750 train_time:44812ms step_avg:94.14ms
step:477/1750 train_time:44908ms step_avg:94.15ms
step:478/1750 train_time:45003ms step_avg:94.15ms
step:479/1750 train_time:45099ms step_avg:94.15ms
step:480/1750 train_time:45195ms step_avg:94.16ms
step:481/1750 train_time:45291ms step_avg:94.16ms
step:482/1750 train_time:45387ms step_avg:94.16ms
step:483/1750 train_time:45483ms step_avg:94.17ms
step:484/1750 train_time:45579ms step_avg:94.17ms
step:485/1750 train_time:45675ms step_avg:94.18ms
step:486/1750 train_time:45771ms step_avg:94.18ms
step:487/1750 train_time:45867ms step_avg:94.18ms
step:488/1750 train_time:45964ms step_avg:94.19ms
step:489/1750 train_time:46059ms step_avg:94.19ms
step:490/1750 train_time:46155ms step_avg:94.19ms
step:491/1750 train_time:46250ms step_avg:94.20ms
step:492/1750 train_time:46347ms step_avg:94.20ms
step:493/1750 train_time:46443ms step_avg:94.20ms
step:494/1750 train_time:46539ms step_avg:94.21ms
step:495/1750 train_time:46635ms step_avg:94.21ms
step:496/1750 train_time:46730ms step_avg:94.21ms
step:497/1750 train_time:46827ms step_avg:94.22ms
step:498/1750 train_time:46922ms step_avg:94.22ms
step:499/1750 train_time:47018ms step_avg:94.22ms
step:500/1750 train_time:47114ms step_avg:94.23ms
step:500/1750 val_loss:3.7462 train_time:47199ms step_avg:94.40ms
step:501/1750 train_time:47222ms step_avg:94.26ms
step:502/1750 train_time:47314ms step_avg:94.25ms
step:503/1750 train_time:47413ms step_avg:94.26ms
step:504/1750 train_time:47510ms step_avg:94.27ms
step:505/1750 train_time:47606ms step_avg:94.27ms
step:506/1750 train_time:47701ms step_avg:94.27ms
step:507/1750 train_time:47795ms step_avg:94.27ms
step:508/1750 train_time:47890ms step_avg:94.27ms
step:509/1750 train_time:47985ms step_avg:94.27ms
step:510/1750 train_time:48080ms step_avg:94.28ms
step:511/1750 train_time:48176ms step_avg:94.28ms
step:512/1750 train_time:48274ms step_avg:94.29ms
step:513/1750 train_time:48372ms step_avg:94.29ms
step:514/1750 train_time:48469ms step_avg:94.30ms
step:515/1750 train_time:48565ms step_avg:94.30ms
step:516/1750 train_time:48661ms step_avg:94.30ms
step:517/1750 train_time:48756ms step_avg:94.30ms
step:518/1750 train_time:48851ms step_avg:94.31ms
step:519/1750 train_time:48946ms step_avg:94.31ms
step:520/1750 train_time:49042ms step_avg:94.31ms
step:521/1750 train_time:49138ms step_avg:94.31ms
step:522/1750 train_time:49235ms step_avg:94.32ms
step:523/1750 train_time:49332ms step_avg:94.33ms
step:524/1750 train_time:49429ms step_avg:94.33ms
step:525/1750 train_time:49526ms step_avg:94.34ms
step:526/1750 train_time:49623ms step_avg:94.34ms
step:527/1750 train_time:49719ms step_avg:94.34ms
step:528/1750 train_time:49814ms step_avg:94.34ms
step:529/1750 train_time:49910ms step_avg:94.35ms
step:530/1750 train_time:50006ms step_avg:94.35ms
step:531/1750 train_time:50102ms step_avg:94.35ms
step:532/1750 train_time:50199ms step_avg:94.36ms
step:533/1750 train_time:50296ms step_avg:94.36ms
step:534/1750 train_time:50393ms step_avg:94.37ms
step:535/1750 train_time:50490ms step_avg:94.37ms
step:536/1750 train_time:50586ms step_avg:94.38ms
step:537/1750 train_time:50682ms step_avg:94.38ms
step:538/1750 train_time:50777ms step_avg:94.38ms
step:539/1750 train_time:50873ms step_avg:94.38ms
step:540/1750 train_time:50970ms step_avg:94.39ms
step:541/1750 train_time:51066ms step_avg:94.39ms
step:542/1750 train_time:51163ms step_avg:94.40ms
step:543/1750 train_time:51260ms step_avg:94.40ms
step:544/1750 train_time:51356ms step_avg:94.40ms
step:545/1750 train_time:51453ms step_avg:94.41ms
step:546/1750 train_time:51549ms step_avg:94.41ms
step:547/1750 train_time:51647ms step_avg:94.42ms
step:548/1750 train_time:51745ms step_avg:94.42ms
step:549/1750 train_time:51841ms step_avg:94.43ms
step:550/1750 train_time:51936ms step_avg:94.43ms
step:551/1750 train_time:52033ms step_avg:94.43ms
step:552/1750 train_time:52129ms step_avg:94.44ms
step:553/1750 train_time:52225ms step_avg:94.44ms
step:554/1750 train_time:52321ms step_avg:94.44ms
step:555/1750 train_time:52417ms step_avg:94.44ms
step:556/1750 train_time:52513ms step_avg:94.45ms
step:557/1750 train_time:52611ms step_avg:94.45ms
step:558/1750 train_time:52708ms step_avg:94.46ms
step:559/1750 train_time:52805ms step_avg:94.46ms
step:560/1750 train_time:52903ms step_avg:94.47ms
step:561/1750 train_time:52999ms step_avg:94.47ms
step:562/1750 train_time:53095ms step_avg:94.47ms
step:563/1750 train_time:53192ms step_avg:94.48ms
step:564/1750 train_time:53288ms step_avg:94.48ms
step:565/1750 train_time:53385ms step_avg:94.49ms
step:566/1750 train_time:53482ms step_avg:94.49ms
step:567/1750 train_time:53578ms step_avg:94.49ms
step:568/1750 train_time:53675ms step_avg:94.50ms
step:569/1750 train_time:53773ms step_avg:94.50ms
step:570/1750 train_time:53870ms step_avg:94.51ms
step:571/1750 train_time:53967ms step_avg:94.51ms
step:572/1750 train_time:54064ms step_avg:94.52ms
step:573/1750 train_time:54160ms step_avg:94.52ms
step:574/1750 train_time:54256ms step_avg:94.52ms
step:575/1750 train_time:54354ms step_avg:94.53ms
step:576/1750 train_time:54450ms step_avg:94.53ms
step:577/1750 train_time:54546ms step_avg:94.53ms
step:578/1750 train_time:54642ms step_avg:94.54ms
step:579/1750 train_time:54738ms step_avg:94.54ms
step:580/1750 train_time:54835ms step_avg:94.54ms
step:581/1750 train_time:54932ms step_avg:94.55ms
step:582/1750 train_time:55028ms step_avg:94.55ms
step:583/1750 train_time:55125ms step_avg:94.55ms
step:584/1750 train_time:55222ms step_avg:94.56ms
step:585/1750 train_time:55318ms step_avg:94.56ms
step:586/1750 train_time:55414ms step_avg:94.56ms
step:587/1750 train_time:55510ms step_avg:94.57ms
step:588/1750 train_time:55607ms step_avg:94.57ms
step:589/1750 train_time:55704ms step_avg:94.57ms
step:590/1750 train_time:55800ms step_avg:94.58ms
step:591/1750 train_time:55896ms step_avg:94.58ms
step:592/1750 train_time:55993ms step_avg:94.58ms
step:593/1750 train_time:56089ms step_avg:94.59ms
step:594/1750 train_time:56186ms step_avg:94.59ms
step:595/1750 train_time:56282ms step_avg:94.59ms
step:596/1750 train_time:56378ms step_avg:94.59ms
step:597/1750 train_time:56474ms step_avg:94.60ms
step:598/1750 train_time:56570ms step_avg:94.60ms
step:599/1750 train_time:56667ms step_avg:94.60ms
step:600/1750 train_time:56763ms step_avg:94.61ms
step:601/1750 train_time:56859ms step_avg:94.61ms
step:602/1750 train_time:56955ms step_avg:94.61ms
step:603/1750 train_time:57052ms step_avg:94.61ms
step:604/1750 train_time:57148ms step_avg:94.62ms
step:605/1750 train_time:57244ms step_avg:94.62ms
step:606/1750 train_time:57340ms step_avg:94.62ms
step:607/1750 train_time:57436ms step_avg:94.62ms
step:608/1750 train_time:57532ms step_avg:94.63ms
step:609/1750 train_time:57628ms step_avg:94.63ms
step:610/1750 train_time:57725ms step_avg:94.63ms
step:611/1750 train_time:57822ms step_avg:94.64ms
step:612/1750 train_time:57919ms step_avg:94.64ms
step:613/1750 train_time:58015ms step_avg:94.64ms
step:614/1750 train_time:58112ms step_avg:94.64ms
step:615/1750 train_time:58208ms step_avg:94.65ms
step:616/1750 train_time:58304ms step_avg:94.65ms
step:617/1750 train_time:58401ms step_avg:94.65ms
step:618/1750 train_time:58497ms step_avg:94.66ms
step:619/1750 train_time:58593ms step_avg:94.66ms
step:620/1750 train_time:58690ms step_avg:94.66ms
step:621/1750 train_time:58786ms step_avg:94.66ms
step:622/1750 train_time:58882ms step_avg:94.67ms
step:623/1750 train_time:58978ms step_avg:94.67ms
step:624/1750 train_time:59075ms step_avg:94.67ms
step:625/1750 train_time:59171ms step_avg:94.67ms
step:625/1750 val_loss:3.6609 train_time:59256ms step_avg:94.81ms
step:626/1750 train_time:59278ms step_avg:94.69ms
step:627/1750 train_time:59374ms step_avg:94.70ms
step:628/1750 train_time:59476ms step_avg:94.71ms
step:629/1750 train_time:59573ms step_avg:94.71ms
step:630/1750 train_time:59669ms step_avg:94.71ms
step:631/1750 train_time:59764ms step_avg:94.71ms
step:632/1750 train_time:59859ms step_avg:94.71ms
step:633/1750 train_time:59955ms step_avg:94.72ms
step:634/1750 train_time:60050ms step_avg:94.72ms
step:635/1750 train_time:60145ms step_avg:94.72ms
step:636/1750 train_time:60242ms step_avg:94.72ms
step:637/1750 train_time:60339ms step_avg:94.72ms
step:638/1750 train_time:60437ms step_avg:94.73ms
step:639/1750 train_time:60534ms step_avg:94.73ms
step:640/1750 train_time:60631ms step_avg:94.74ms
step:641/1750 train_time:60728ms step_avg:94.74ms
step:642/1750 train_time:60824ms step_avg:94.74ms
step:643/1750 train_time:60920ms step_avg:94.74ms
step:644/1750 train_time:61015ms step_avg:94.74ms
step:645/1750 train_time:61111ms step_avg:94.75ms
step:646/1750 train_time:61207ms step_avg:94.75ms
step:647/1750 train_time:61304ms step_avg:94.75ms
step:648/1750 train_time:61400ms step_avg:94.75ms
step:649/1750 train_time:61497ms step_avg:94.76ms
step:650/1750 train_time:61595ms step_avg:94.76ms
step:651/1750 train_time:61694ms step_avg:94.77ms
step:652/1750 train_time:61792ms step_avg:94.77ms
step:653/1750 train_time:61889ms step_avg:94.78ms
step:654/1750 train_time:61987ms step_avg:94.78ms
step:655/1750 train_time:62084ms step_avg:94.78ms
step:656/1750 train_time:62180ms step_avg:94.79ms
step:657/1750 train_time:62277ms step_avg:94.79ms
step:658/1750 train_time:62375ms step_avg:94.80ms
step:659/1750 train_time:62474ms step_avg:94.80ms
step:660/1750 train_time:62571ms step_avg:94.81ms
step:661/1750 train_time:62670ms step_avg:94.81ms
step:662/1750 train_time:62768ms step_avg:94.82ms
step:663/1750 train_time:62867ms step_avg:94.82ms
step:664/1750 train_time:62964ms step_avg:94.83ms
step:665/1750 train_time:63062ms step_avg:94.83ms
step:666/1750 train_time:63159ms step_avg:94.83ms
step:667/1750 train_time:63256ms step_avg:94.84ms
step:668/1750 train_time:63354ms step_avg:94.84ms
step:669/1750 train_time:63452ms step_avg:94.85ms
step:670/1750 train_time:63550ms step_avg:94.85ms
step:671/1750 train_time:63648ms step_avg:94.85ms
step:672/1750 train_time:63745ms step_avg:94.86ms
step:673/1750 train_time:63843ms step_avg:94.86ms
step:674/1750 train_time:63941ms step_avg:94.87ms
step:675/1750 train_time:64038ms step_avg:94.87ms
step:676/1750 train_time:64136ms step_avg:94.88ms
step:677/1750 train_time:64233ms step_avg:94.88ms
step:678/1750 train_time:64330ms step_avg:94.88ms
step:679/1750 train_time:64429ms step_avg:94.89ms
step:680/1750 train_time:64528ms step_avg:94.89ms
step:681/1750 train_time:64625ms step_avg:94.90ms
step:682/1750 train_time:64723ms step_avg:94.90ms
step:683/1750 train_time:64820ms step_avg:94.91ms
step:684/1750 train_time:64918ms step_avg:94.91ms
step:685/1750 train_time:65016ms step_avg:94.91ms
step:686/1750 train_time:65114ms step_avg:94.92ms
step:687/1750 train_time:65212ms step_avg:94.92ms
step:688/1750 train_time:65310ms step_avg:94.93ms
step:689/1750 train_time:65408ms step_avg:94.93ms
step:690/1750 train_time:65506ms step_avg:94.94ms
step:691/1750 train_time:65604ms step_avg:94.94ms
step:692/1750 train_time:65701ms step_avg:94.94ms
step:693/1750 train_time:65799ms step_avg:94.95ms
step:694/1750 train_time:65897ms step_avg:94.95ms
step:695/1750 train_time:65995ms step_avg:94.96ms
step:696/1750 train_time:66093ms step_avg:94.96ms
step:697/1750 train_time:66191ms step_avg:94.97ms
step:698/1750 train_time:66288ms step_avg:94.97ms
step:699/1750 train_time:66387ms step_avg:94.97ms
step:700/1750 train_time:66484ms step_avg:94.98ms
step:701/1750 train_time:66581ms step_avg:94.98ms
step:702/1750 train_time:66679ms step_avg:94.98ms
step:703/1750 train_time:66777ms step_avg:94.99ms
step:704/1750 train_time:66876ms step_avg:94.99ms
step:705/1750 train_time:66974ms step_avg:95.00ms
step:706/1750 train_time:67071ms step_avg:95.00ms
step:707/1750 train_time:67169ms step_avg:95.01ms
step:708/1750 train_time:67267ms step_avg:95.01ms
step:709/1750 train_time:67365ms step_avg:95.01ms
step:710/1750 train_time:67463ms step_avg:95.02ms
step:711/1750 train_time:67561ms step_avg:95.02ms
step:712/1750 train_time:67658ms step_avg:95.03ms
step:713/1750 train_time:67756ms step_avg:95.03ms
step:714/1750 train_time:67855ms step_avg:95.03ms
step:715/1750 train_time:67953ms step_avg:95.04ms
step:716/1750 train_time:68051ms step_avg:95.04ms
step:717/1750 train_time:68148ms step_avg:95.05ms
step:718/1750 train_time:68245ms step_avg:95.05ms
step:719/1750 train_time:68343ms step_avg:95.05ms
step:720/1750 train_time:68440ms step_avg:95.06ms
step:721/1750 train_time:68537ms step_avg:95.06ms
step:722/1750 train_time:68636ms step_avg:95.06ms
step:723/1750 train_time:68734ms step_avg:95.07ms
step:724/1750 train_time:68832ms step_avg:95.07ms
step:725/1750 train_time:68930ms step_avg:95.08ms
step:726/1750 train_time:69028ms step_avg:95.08ms
step:727/1750 train_time:69126ms step_avg:95.08ms
step:728/1750 train_time:69225ms step_avg:95.09ms
step:729/1750 train_time:69322ms step_avg:95.09ms
step:730/1750 train_time:69420ms step_avg:95.10ms
step:731/1750 train_time:69517ms step_avg:95.10ms
step:732/1750 train_time:69615ms step_avg:95.10ms
step:733/1750 train_time:69713ms step_avg:95.11ms
step:734/1750 train_time:69810ms step_avg:95.11ms
step:735/1750 train_time:69908ms step_avg:95.11ms
step:736/1750 train_time:70006ms step_avg:95.12ms
step:737/1750 train_time:70103ms step_avg:95.12ms
step:738/1750 train_time:70201ms step_avg:95.12ms
step:739/1750 train_time:70298ms step_avg:95.13ms
step:740/1750 train_time:70397ms step_avg:95.13ms
step:741/1750 train_time:70494ms step_avg:95.13ms
step:742/1750 train_time:70592ms step_avg:95.14ms
step:743/1750 train_time:70690ms step_avg:95.14ms
step:744/1750 train_time:70788ms step_avg:95.14ms
step:745/1750 train_time:70886ms step_avg:95.15ms
step:746/1750 train_time:70983ms step_avg:95.15ms
step:747/1750 train_time:71081ms step_avg:95.16ms
step:748/1750 train_time:71179ms step_avg:95.16ms
step:749/1750 train_time:71277ms step_avg:95.16ms
step:750/1750 train_time:71375ms step_avg:95.17ms
step:750/1750 val_loss:3.5959 train_time:71463ms step_avg:95.28ms
step:751/1750 train_time:71484ms step_avg:95.19ms
step:752/1750 train_time:71580ms step_avg:95.19ms
step:753/1750 train_time:71678ms step_avg:95.19ms
step:754/1750 train_time:71776ms step_avg:95.19ms
step:755/1750 train_time:71873ms step_avg:95.20ms
step:756/1750 train_time:71970ms step_avg:95.20ms
step:757/1750 train_time:72067ms step_avg:95.20ms
step:758/1750 train_time:72164ms step_avg:95.20ms
step:759/1750 train_time:72260ms step_avg:95.20ms
step:760/1750 train_time:72357ms step_avg:95.21ms
step:761/1750 train_time:72460ms step_avg:95.22ms
step:762/1750 train_time:72560ms step_avg:95.22ms
step:763/1750 train_time:72658ms step_avg:95.23ms
step:764/1750 train_time:72758ms step_avg:95.23ms
step:765/1750 train_time:72855ms step_avg:95.24ms
step:766/1750 train_time:72953ms step_avg:95.24ms
step:767/1750 train_time:73050ms step_avg:95.24ms
step:768/1750 train_time:73148ms step_avg:95.24ms
step:769/1750 train_time:73244ms step_avg:95.25ms
step:770/1750 train_time:73342ms step_avg:95.25ms
step:771/1750 train_time:73441ms step_avg:95.25ms
step:772/1750 train_time:73540ms step_avg:95.26ms
step:773/1750 train_time:73639ms step_avg:95.26ms
step:774/1750 train_time:73739ms step_avg:95.27ms
step:775/1750 train_time:73838ms step_avg:95.27ms
step:776/1750 train_time:73936ms step_avg:95.28ms
step:777/1750 train_time:74033ms step_avg:95.28ms
step:778/1750 train_time:74131ms step_avg:95.28ms
step:779/1750 train_time:74228ms step_avg:95.29ms
step:780/1750 train_time:74326ms step_avg:95.29ms
step:781/1750 train_time:74424ms step_avg:95.29ms
step:782/1750 train_time:74522ms step_avg:95.30ms
step:783/1750 train_time:74619ms step_avg:95.30ms
step:784/1750 train_time:74718ms step_avg:95.30ms
step:785/1750 train_time:74817ms step_avg:95.31ms
step:786/1750 train_time:74916ms step_avg:95.31ms
step:787/1750 train_time:75013ms step_avg:95.32ms
step:788/1750 train_time:75111ms step_avg:95.32ms
step:789/1750 train_time:75209ms step_avg:95.32ms
step:790/1750 train_time:75307ms step_avg:95.33ms
step:791/1750 train_time:75406ms step_avg:95.33ms
step:792/1750 train_time:75504ms step_avg:95.33ms
step:793/1750 train_time:75602ms step_avg:95.34ms
step:794/1750 train_time:75700ms step_avg:95.34ms
step:795/1750 train_time:75799ms step_avg:95.34ms
step:796/1750 train_time:75897ms step_avg:95.35ms
step:797/1750 train_time:75996ms step_avg:95.35ms
step:798/1750 train_time:76094ms step_avg:95.36ms
step:799/1750 train_time:76192ms step_avg:95.36ms
step:800/1750 train_time:76290ms step_avg:95.36ms
step:801/1750 train_time:76387ms step_avg:95.37ms
step:802/1750 train_time:76486ms step_avg:95.37ms
step:803/1750 train_time:76584ms step_avg:95.37ms
step:804/1750 train_time:76683ms step_avg:95.38ms
step:805/1750 train_time:76781ms step_avg:95.38ms
step:806/1750 train_time:76879ms step_avg:95.38ms
step:807/1750 train_time:76977ms step_avg:95.39ms
step:808/1750 train_time:77077ms step_avg:95.39ms
step:809/1750 train_time:77175ms step_avg:95.40ms
step:810/1750 train_time:77273ms step_avg:95.40ms
step:811/1750 train_time:77372ms step_avg:95.40ms
step:812/1750 train_time:77471ms step_avg:95.41ms
step:813/1750 train_time:77570ms step_avg:95.41ms
step:814/1750 train_time:77669ms step_avg:95.42ms
step:815/1750 train_time:77767ms step_avg:95.42ms
step:816/1750 train_time:77866ms step_avg:95.42ms
step:817/1750 train_time:77964ms step_avg:95.43ms
step:818/1750 train_time:78062ms step_avg:95.43ms
step:819/1750 train_time:78161ms step_avg:95.43ms
step:820/1750 train_time:78258ms step_avg:95.44ms
step:821/1750 train_time:78357ms step_avg:95.44ms
step:822/1750 train_time:78457ms step_avg:95.45ms
step:823/1750 train_time:78555ms step_avg:95.45ms
step:824/1750 train_time:78654ms step_avg:95.45ms
step:825/1750 train_time:78753ms step_avg:95.46ms
step:826/1750 train_time:78852ms step_avg:95.46ms
step:827/1750 train_time:78951ms step_avg:95.47ms
step:828/1750 train_time:79049ms step_avg:95.47ms
step:829/1750 train_time:79148ms step_avg:95.47ms
step:830/1750 train_time:79247ms step_avg:95.48ms
step:831/1750 train_time:79346ms step_avg:95.48ms
step:832/1750 train_time:79445ms step_avg:95.49ms
step:833/1750 train_time:79543ms step_avg:95.49ms
step:834/1750 train_time:79641ms step_avg:95.49ms
step:835/1750 train_time:79739ms step_avg:95.50ms
step:836/1750 train_time:79837ms step_avg:95.50ms
step:837/1750 train_time:79935ms step_avg:95.50ms
step:838/1750 train_time:80033ms step_avg:95.51ms
step:839/1750 train_time:80132ms step_avg:95.51ms
step:840/1750 train_time:80231ms step_avg:95.51ms
step:841/1750 train_time:80329ms step_avg:95.52ms
step:842/1750 train_time:80429ms step_avg:95.52ms
step:843/1750 train_time:80528ms step_avg:95.52ms
step:844/1750 train_time:80626ms step_avg:95.53ms
step:845/1750 train_time:80724ms step_avg:95.53ms
step:846/1750 train_time:80822ms step_avg:95.53ms
step:847/1750 train_time:80920ms step_avg:95.54ms
step:848/1750 train_time:81018ms step_avg:95.54ms
step:849/1750 train_time:81116ms step_avg:95.54ms
step:850/1750 train_time:81214ms step_avg:95.55ms
step:851/1750 train_time:81313ms step_avg:95.55ms
step:852/1750 train_time:81411ms step_avg:95.55ms
step:853/1750 train_time:81509ms step_avg:95.56ms
step:854/1750 train_time:81608ms step_avg:95.56ms
step:855/1750 train_time:81706ms step_avg:95.56ms
step:856/1750 train_time:81805ms step_avg:95.57ms
step:857/1750 train_time:81903ms step_avg:95.57ms
step:858/1750 train_time:82001ms step_avg:95.57ms
step:859/1750 train_time:82099ms step_avg:95.58ms
step:860/1750 train_time:82197ms step_avg:95.58ms
step:861/1750 train_time:82295ms step_avg:95.58ms
step:862/1750 train_time:82394ms step_avg:95.58ms
step:863/1750 train_time:82492ms step_avg:95.59ms
step:864/1750 train_time:82590ms step_avg:95.59ms
step:865/1750 train_time:82688ms step_avg:95.59ms
step:866/1750 train_time:82786ms step_avg:95.60ms
step:867/1750 train_time:82883ms step_avg:95.60ms
step:868/1750 train_time:82982ms step_avg:95.60ms
step:869/1750 train_time:83080ms step_avg:95.60ms
step:870/1750 train_time:83178ms step_avg:95.61ms
step:871/1750 train_time:83276ms step_avg:95.61ms
step:872/1750 train_time:83374ms step_avg:95.61ms
step:873/1750 train_time:83472ms step_avg:95.61ms
step:874/1750 train_time:83570ms step_avg:95.62ms
step:875/1750 train_time:83668ms step_avg:95.62ms
step:875/1750 val_loss:3.5472 train_time:83755ms step_avg:95.72ms
step:876/1750 train_time:83775ms step_avg:95.63ms
step:877/1750 train_time:83874ms step_avg:95.64ms
step:878/1750 train_time:83974ms step_avg:95.64ms
step:879/1750 train_time:84072ms step_avg:95.65ms
step:880/1750 train_time:84170ms step_avg:95.65ms
step:881/1750 train_time:84267ms step_avg:95.65ms
step:882/1750 train_time:84364ms step_avg:95.65ms
step:883/1750 train_time:84461ms step_avg:95.65ms
step:884/1750 train_time:84559ms step_avg:95.65ms
step:885/1750 train_time:84656ms step_avg:95.66ms
step:886/1750 train_time:84756ms step_avg:95.66ms
step:887/1750 train_time:84856ms step_avg:95.67ms
step:888/1750 train_time:84955ms step_avg:95.67ms
step:889/1750 train_time:85053ms step_avg:95.67ms
step:890/1750 train_time:85151ms step_avg:95.68ms
step:891/1750 train_time:85248ms step_avg:95.68ms
step:892/1750 train_time:85346ms step_avg:95.68ms
step:893/1750 train_time:85443ms step_avg:95.68ms
step:894/1750 train_time:85539ms step_avg:95.68ms
step:895/1750 train_time:85637ms step_avg:95.68ms
step:896/1750 train_time:85736ms step_avg:95.69ms
step:897/1750 train_time:85836ms step_avg:95.69ms
step:898/1750 train_time:85936ms step_avg:95.70ms
step:899/1750 train_time:86034ms step_avg:95.70ms
step:900/1750 train_time:86132ms step_avg:95.70ms
step:901/1750 train_time:86231ms step_avg:95.71ms
step:902/1750 train_time:86329ms step_avg:95.71ms
step:903/1750 train_time:86426ms step_avg:95.71ms
step:904/1750 train_time:86524ms step_avg:95.71ms
step:905/1750 train_time:86621ms step_avg:95.71ms
step:906/1750 train_time:86719ms step_avg:95.72ms
step:907/1750 train_time:86818ms step_avg:95.72ms
step:908/1750 train_time:86916ms step_avg:95.72ms
step:909/1750 train_time:87015ms step_avg:95.73ms
step:910/1750 train_time:87115ms step_avg:95.73ms
step:911/1750 train_time:87215ms step_avg:95.74ms
step:912/1750 train_time:87314ms step_avg:95.74ms
step:913/1750 train_time:87414ms step_avg:95.74ms
step:914/1750 train_time:87513ms step_avg:95.75ms
step:915/1750 train_time:87613ms step_avg:95.75ms
step:916/1750 train_time:87713ms step_avg:95.76ms
step:917/1750 train_time:87813ms step_avg:95.76ms
step:918/1750 train_time:87913ms step_avg:95.77ms
step:919/1750 train_time:88012ms step_avg:95.77ms
step:920/1750 train_time:88112ms step_avg:95.77ms
step:921/1750 train_time:88211ms step_avg:95.78ms
step:922/1750 train_time:88310ms step_avg:95.78ms
step:923/1750 train_time:88409ms step_avg:95.78ms
step:924/1750 train_time:88507ms step_avg:95.79ms
step:925/1750 train_time:88606ms step_avg:95.79ms
step:926/1750 train_time:88707ms step_avg:95.80ms
step:927/1750 train_time:88805ms step_avg:95.80ms
step:928/1750 train_time:88905ms step_avg:95.80ms
step:929/1750 train_time:89004ms step_avg:95.81ms
step:930/1750 train_time:89104ms step_avg:95.81ms
step:931/1750 train_time:89205ms step_avg:95.82ms
step:932/1750 train_time:89305ms step_avg:95.82ms
step:933/1750 train_time:89405ms step_avg:95.83ms
step:934/1750 train_time:89503ms step_avg:95.83ms
step:935/1750 train_time:89604ms step_avg:95.83ms
step:936/1750 train_time:89704ms step_avg:95.84ms
step:937/1750 train_time:89804ms step_avg:95.84ms
step:938/1750 train_time:89904ms step_avg:95.85ms
step:939/1750 train_time:90005ms step_avg:95.85ms
step:940/1750 train_time:90105ms step_avg:95.86ms
step:941/1750 train_time:90205ms step_avg:95.86ms
step:942/1750 train_time:90304ms step_avg:95.86ms
step:943/1750 train_time:90404ms step_avg:95.87ms
step:944/1750 train_time:90503ms step_avg:95.87ms
step:945/1750 train_time:90603ms step_avg:95.88ms
step:946/1750 train_time:90703ms step_avg:95.88ms
step:947/1750 train_time:90802ms step_avg:95.88ms
step:948/1750 train_time:90902ms step_avg:95.89ms
step:949/1750 train_time:91002ms step_avg:95.89ms
step:950/1750 train_time:91102ms step_avg:95.90ms
step:951/1750 train_time:91204ms step_avg:95.90ms
step:952/1750 train_time:91304ms step_avg:95.91ms
step:953/1750 train_time:91404ms step_avg:95.91ms
step:954/1750 train_time:91504ms step_avg:95.92ms
step:955/1750 train_time:91604ms step_avg:95.92ms
step:956/1750 train_time:91703ms step_avg:95.92ms
step:957/1750 train_time:91802ms step_avg:95.93ms
step:958/1750 train_time:91902ms step_avg:95.93ms
step:959/1750 train_time:92002ms step_avg:95.93ms
step:960/1750 train_time:92102ms step_avg:95.94ms
step:961/1750 train_time:92201ms step_avg:95.94ms
step:962/1750 train_time:92300ms step_avg:95.95ms
step:963/1750 train_time:92401ms step_avg:95.95ms
step:964/1750 train_time:92500ms step_avg:95.95ms
step:965/1750 train_time:92600ms step_avg:95.96ms
step:966/1750 train_time:92699ms step_avg:95.96ms
step:967/1750 train_time:92799ms step_avg:95.97ms
step:968/1750 train_time:92899ms step_avg:95.97ms
step:969/1750 train_time:92999ms step_avg:95.97ms
step:970/1750 train_time:93098ms step_avg:95.98ms
step:971/1750 train_time:93198ms step_avg:95.98ms
step:972/1750 train_time:93297ms step_avg:95.98ms
step:973/1750 train_time:93397ms step_avg:95.99ms
step:974/1750 train_time:93497ms step_avg:95.99ms
step:975/1750 train_time:93596ms step_avg:96.00ms
step:976/1750 train_time:93696ms step_avg:96.00ms
step:977/1750 train_time:93795ms step_avg:96.00ms
step:978/1750 train_time:93895ms step_avg:96.01ms
step:979/1750 train_time:93994ms step_avg:96.01ms
step:980/1750 train_time:94092ms step_avg:96.01ms
step:981/1750 train_time:94191ms step_avg:96.02ms
step:982/1750 train_time:94290ms step_avg:96.02ms
step:983/1750 train_time:94390ms step_avg:96.02ms
step:984/1750 train_time:94490ms step_avg:96.03ms
step:985/1750 train_time:94589ms step_avg:96.03ms
step:986/1750 train_time:94689ms step_avg:96.03ms
step:987/1750 train_time:94790ms step_avg:96.04ms
step:988/1750 train_time:94888ms step_avg:96.04ms
step:989/1750 train_time:94988ms step_avg:96.04ms
step:990/1750 train_time:95087ms step_avg:96.05ms
step:991/1750 train_time:95186ms step_avg:96.05ms
step:992/1750 train_time:95286ms step_avg:96.05ms
step:993/1750 train_time:95386ms step_avg:96.06ms
step:994/1750 train_time:95486ms step_avg:96.06ms
step:995/1750 train_time:95586ms step_avg:96.07ms
step:996/1750 train_time:95686ms step_avg:96.07ms
step:997/1750 train_time:95786ms step_avg:96.07ms
step:998/1750 train_time:95885ms step_avg:96.08ms
step:999/1750 train_time:95984ms step_avg:96.08ms
step:1000/1750 train_time:96084ms step_avg:96.08ms
step:1000/1750 val_loss:3.5067 train_time:96173ms step_avg:96.17ms
step:1001/1750 train_time:96193ms step_avg:96.10ms
step:1002/1750 train_time:96290ms step_avg:96.10ms
step:1003/1750 train_time:96391ms step_avg:96.10ms
step:1004/1750 train_time:96491ms step_avg:96.11ms
step:1005/1750 train_time:96590ms step_avg:96.11ms
step:1006/1750 train_time:96689ms step_avg:96.11ms
step:1007/1750 train_time:96787ms step_avg:96.11ms
step:1008/1750 train_time:96886ms step_avg:96.12ms
step:1009/1750 train_time:96984ms step_avg:96.12ms
step:1010/1750 train_time:97082ms step_avg:96.12ms
step:1011/1750 train_time:97183ms step_avg:96.13ms
step:1012/1750 train_time:97284ms step_avg:96.13ms
step:1013/1750 train_time:97385ms step_avg:96.13ms
step:1014/1750 train_time:97484ms step_avg:96.14ms
step:1015/1750 train_time:97583ms step_avg:96.14ms
step:1016/1750 train_time:97681ms step_avg:96.14ms
step:1017/1750 train_time:97780ms step_avg:96.15ms
step:1018/1750 train_time:97879ms step_avg:96.15ms
step:1019/1750 train_time:97977ms step_avg:96.15ms
step:1020/1750 train_time:98078ms step_avg:96.16ms
step:1021/1750 train_time:98179ms step_avg:96.16ms
step:1022/1750 train_time:98280ms step_avg:96.16ms
step:1023/1750 train_time:98380ms step_avg:96.17ms
step:1024/1750 train_time:98480ms step_avg:96.17ms
step:1025/1750 train_time:98579ms step_avg:96.17ms
step:1026/1750 train_time:98678ms step_avg:96.18ms
step:1027/1750 train_time:98778ms step_avg:96.18ms
step:1028/1750 train_time:98877ms step_avg:96.18ms
step:1029/1750 train_time:98976ms step_avg:96.19ms
step:1030/1750 train_time:99076ms step_avg:96.19ms
step:1031/1750 train_time:99176ms step_avg:96.19ms
step:1032/1750 train_time:99277ms step_avg:96.20ms
step:1033/1750 train_time:99378ms step_avg:96.20ms
step:1034/1750 train_time:99478ms step_avg:96.21ms
step:1035/1750 train_time:99578ms step_avg:96.21ms
step:1036/1750 train_time:99678ms step_avg:96.21ms
step:1037/1750 train_time:99777ms step_avg:96.22ms
step:1038/1750 train_time:99876ms step_avg:96.22ms
step:1039/1750 train_time:99975ms step_avg:96.22ms
step:1040/1750 train_time:100075ms step_avg:96.23ms
step:1041/1750 train_time:100176ms step_avg:96.23ms
step:1042/1750 train_time:100278ms step_avg:96.24ms
step:1043/1750 train_time:100379ms step_avg:96.24ms
step:1044/1750 train_time:100479ms step_avg:96.24ms
step:1045/1750 train_time:100578ms step_avg:96.25ms
step:1046/1750 train_time:100678ms step_avg:96.25ms
step:1047/1750 train_time:100778ms step_avg:96.25ms
step:1048/1750 train_time:100878ms step_avg:96.26ms
step:1049/1750 train_time:100976ms step_avg:96.26ms
step:1050/1750 train_time:101076ms step_avg:96.26ms
step:1051/1750 train_time:101176ms step_avg:96.27ms
step:1052/1750 train_time:101277ms step_avg:96.27ms
step:1053/1750 train_time:101377ms step_avg:96.27ms
step:1054/1750 train_time:101478ms step_avg:96.28ms
step:1055/1750 train_time:101578ms step_avg:96.28ms
step:1056/1750 train_time:101678ms step_avg:96.29ms
step:1057/1750 train_time:101777ms step_avg:96.29ms
step:1058/1750 train_time:101876ms step_avg:96.29ms
step:1059/1750 train_time:101976ms step_avg:96.29ms
step:1060/1750 train_time:102339ms step_avg:96.55ms
step:1061/1750 train_time:102505ms step_avg:96.61ms
step:1062/1750 train_time:102602ms step_avg:96.61ms
step:1063/1750 train_time:102700ms step_avg:96.61ms
step:1064/1750 train_time:102799ms step_avg:96.62ms
step:1065/1750 train_time:102897ms step_avg:96.62ms
step:1066/1750 train_time:102995ms step_avg:96.62ms
step:1067/1750 train_time:103094ms step_avg:96.62ms
step:1068/1750 train_time:103193ms step_avg:96.62ms
step:1069/1750 train_time:103292ms step_avg:96.62ms
step:1070/1750 train_time:103680ms step_avg:96.90ms
step:1071/1750 train_time:103777ms step_avg:96.90ms
step:1072/1750 train_time:103876ms step_avg:96.90ms
step:1073/1750 train_time:103975ms step_avg:96.90ms
step:1074/1750 train_time:104073ms step_avg:96.90ms
step:1075/1750 train_time:104171ms step_avg:96.90ms
step:1076/1750 train_time:104269ms step_avg:96.90ms
step:1077/1750 train_time:104368ms step_avg:96.91ms
step:1078/1750 train_time:104466ms step_avg:96.91ms
step:1079/1750 train_time:104567ms step_avg:96.91ms
step:1080/1750 train_time:104672ms step_avg:96.92ms
step:1081/1750 train_time:104772ms step_avg:96.92ms
step:1082/1750 train_time:104873ms step_avg:96.92ms
step:1083/1750 train_time:104972ms step_avg:96.93ms
step:1084/1750 train_time:105070ms step_avg:96.93ms
step:1085/1750 train_time:105169ms step_avg:96.93ms
step:1086/1750 train_time:105268ms step_avg:96.93ms
step:1087/1750 train_time:105367ms step_avg:96.93ms
step:1088/1750 train_time:105466ms step_avg:96.94ms
step:1089/1750 train_time:105566ms step_avg:96.94ms
step:1090/1750 train_time:105667ms step_avg:96.94ms
step:1091/1750 train_time:105767ms step_avg:96.95ms
step:1092/1750 train_time:105869ms step_avg:96.95ms
step:1093/1750 train_time:105969ms step_avg:96.95ms
step:1094/1750 train_time:106069ms step_avg:96.96ms
step:1095/1750 train_time:106168ms step_avg:96.96ms
step:1096/1750 train_time:106268ms step_avg:96.96ms
step:1097/1750 train_time:106366ms step_avg:96.96ms
step:1098/1750 train_time:106466ms step_avg:96.96ms
step:1099/1750 train_time:106565ms step_avg:96.97ms
step:1100/1750 train_time:106664ms step_avg:96.97ms
step:1101/1750 train_time:106764ms step_avg:96.97ms
step:1102/1750 train_time:106863ms step_avg:96.97ms
step:1103/1750 train_time:106963ms step_avg:96.97ms
step:1104/1750 train_time:107063ms step_avg:96.98ms
step:1105/1750 train_time:107162ms step_avg:96.98ms
step:1106/1750 train_time:107687ms step_avg:97.37ms
step:1107/1750 train_time:107749ms step_avg:97.33ms
step:1108/1750 train_time:107848ms step_avg:97.34ms
step:1109/1750 train_time:107946ms step_avg:97.34ms
step:1110/1750 train_time:108045ms step_avg:97.34ms
step:1111/1750 train_time:108144ms step_avg:97.34ms
step:1112/1750 train_time:108242ms step_avg:97.34ms
step:1113/1750 train_time:108340ms step_avg:97.34ms
step:1114/1750 train_time:108438ms step_avg:97.34ms
step:1115/1750 train_time:108536ms step_avg:97.34ms
step:1116/1750 train_time:108638ms step_avg:97.35ms
step:1117/1750 train_time:108741ms step_avg:97.35ms
step:1118/1750 train_time:108840ms step_avg:97.35ms
step:1119/1750 train_time:108939ms step_avg:97.35ms
step:1120/1750 train_time:109040ms step_avg:97.36ms
step:1121/1750 train_time:109139ms step_avg:97.36ms
step:1122/1750 train_time:109239ms step_avg:97.36ms
step:1123/1750 train_time:109338ms step_avg:97.36ms
step:1124/1750 train_time:109437ms step_avg:97.36ms
step:1125/1750 train_time:109537ms step_avg:97.37ms
step:1125/1750 val_loss:3.4539 train_time:109625ms step_avg:97.44ms
step:1126/1750 train_time:109646ms step_avg:97.38ms
step:1127/1750 train_time:109744ms step_avg:97.38ms
step:1128/1750 train_time:109848ms step_avg:97.38ms
step:1129/1750 train_time:109948ms step_avg:97.39ms
step:1130/1750 train_time:110047ms step_avg:97.39ms
step:1131/1750 train_time:110146ms step_avg:97.39ms
step:1132/1750 train_time:110245ms step_avg:97.39ms
step:1133/1750 train_time:110344ms step_avg:97.39ms
step:1134/1750 train_time:110443ms step_avg:97.39ms
step:1135/1750 train_time:110543ms step_avg:97.39ms
step:1136/1750 train_time:110644ms step_avg:97.40ms
step:1137/1750 train_time:110745ms step_avg:97.40ms
step:1138/1750 train_time:110846ms step_avg:97.40ms
step:1139/1750 train_time:110947ms step_avg:97.41ms
step:1140/1750 train_time:111047ms step_avg:97.41ms
step:1141/1750 train_time:111146ms step_avg:97.41ms
step:1142/1750 train_time:111245ms step_avg:97.41ms
step:1143/1750 train_time:111344ms step_avg:97.41ms
step:1144/1750 train_time:111444ms step_avg:97.42ms
step:1145/1750 train_time:111544ms step_avg:97.42ms
step:1146/1750 train_time:111644ms step_avg:97.42ms
step:1147/1750 train_time:111745ms step_avg:97.42ms
step:1148/1750 train_time:111846ms step_avg:97.43ms
step:1149/1750 train_time:111946ms step_avg:97.43ms
step:1150/1750 train_time:112047ms step_avg:97.43ms
step:1151/1750 train_time:112146ms step_avg:97.43ms
step:1152/1750 train_time:112246ms step_avg:97.44ms
step:1153/1750 train_time:112344ms step_avg:97.44ms
step:1154/1750 train_time:112444ms step_avg:97.44ms
step:1155/1750 train_time:112544ms step_avg:97.44ms
step:1156/1750 train_time:112645ms step_avg:97.44ms
step:1157/1750 train_time:112745ms step_avg:97.45ms
step:1158/1750 train_time:112846ms step_avg:97.45ms
step:1159/1750 train_time:112945ms step_avg:97.45ms
step:1160/1750 train_time:113044ms step_avg:97.45ms
step:1161/1750 train_time:113144ms step_avg:97.45ms
step:1162/1750 train_time:113243ms step_avg:97.46ms
step:1163/1750 train_time:113344ms step_avg:97.46ms
step:1164/1750 train_time:113445ms step_avg:97.46ms
step:1165/1750 train_time:113799ms step_avg:97.68ms
step:1166/1750 train_time:113899ms step_avg:97.68ms
step:1167/1750 train_time:114254ms step_avg:97.90ms
step:1168/1750 train_time:114352ms step_avg:97.90ms
step:1169/1750 train_time:114451ms step_avg:97.91ms
step:1170/1750 train_time:114551ms step_avg:97.91ms
step:1171/1750 train_time:114650ms step_avg:97.91ms
step:1172/1750 train_time:114751ms step_avg:97.91ms
step:1173/1750 train_time:114850ms step_avg:97.91ms
step:1174/1750 train_time:114950ms step_avg:97.91ms
step:1175/1750 train_time:115049ms step_avg:97.91ms
step:1176/1750 train_time:115153ms step_avg:97.92ms
step:1177/1750 train_time:115259ms step_avg:97.93ms
step:1178/1750 train_time:115359ms step_avg:97.93ms
step:1179/1750 train_time:115461ms step_avg:97.93ms
step:1180/1750 train_time:115561ms step_avg:97.93ms
step:1181/1750 train_time:115661ms step_avg:97.93ms
step:1182/1750 train_time:115761ms step_avg:97.94ms
step:1183/1750 train_time:115863ms step_avg:97.94ms
step:1184/1750 train_time:115964ms step_avg:97.94ms
step:1185/1750 train_time:116064ms step_avg:97.94ms
step:1186/1750 train_time:116167ms step_avg:97.95ms
step:1187/1750 train_time:116269ms step_avg:97.95ms
step:1188/1750 train_time:116371ms step_avg:97.96ms
step:1189/1750 train_time:116472ms step_avg:97.96ms
step:1190/1750 train_time:116573ms step_avg:97.96ms
step:1191/1750 train_time:116674ms step_avg:97.96ms
step:1192/1750 train_time:116774ms step_avg:97.96ms
step:1193/1750 train_time:116874ms step_avg:97.97ms
step:1194/1750 train_time:116974ms step_avg:97.97ms
step:1195/1750 train_time:117073ms step_avg:97.97ms
step:1196/1750 train_time:117173ms step_avg:97.97ms
step:1197/1750 train_time:117275ms step_avg:97.97ms
step:1198/1750 train_time:117376ms step_avg:97.98ms
step:1199/1750 train_time:117477ms step_avg:97.98ms
step:1200/1750 train_time:117577ms step_avg:97.98ms
step:1201/1750 train_time:117678ms step_avg:97.98ms
step:1202/1750 train_time:117779ms step_avg:97.99ms
step:1203/1750 train_time:117879ms step_avg:97.99ms
step:1204/1750 train_time:117979ms step_avg:97.99ms
step:1205/1750 train_time:118079ms step_avg:97.99ms
step:1206/1750 train_time:118180ms step_avg:97.99ms
step:1207/1750 train_time:118282ms step_avg:98.00ms
step:1208/1750 train_time:118382ms step_avg:98.00ms
step:1209/1750 train_time:118482ms step_avg:98.00ms
step:1210/1750 train_time:118583ms step_avg:98.00ms
step:1211/1750 train_time:118684ms step_avg:98.00ms
step:1212/1750 train_time:118785ms step_avg:98.01ms
step:1213/1750 train_time:118886ms step_avg:98.01ms
step:1214/1750 train_time:118988ms step_avg:98.01ms
step:1215/1750 train_time:119089ms step_avg:98.02ms
step:1216/1750 train_time:119191ms step_avg:98.02ms
step:1217/1750 train_time:119293ms step_avg:98.02ms
step:1218/1750 train_time:119395ms step_avg:98.03ms
step:1219/1750 train_time:119496ms step_avg:98.03ms
step:1220/1750 train_time:119596ms step_avg:98.03ms
step:1221/1750 train_time:119696ms step_avg:98.03ms
step:1222/1750 train_time:119796ms step_avg:98.03ms
step:1223/1750 train_time:119898ms step_avg:98.04ms
step:1224/1750 train_time:119998ms step_avg:98.04ms
step:1225/1750 train_time:120098ms step_avg:98.04ms
step:1226/1750 train_time:120199ms step_avg:98.04ms
step:1227/1750 train_time:120602ms step_avg:98.29ms
step:1228/1750 train_time:120700ms step_avg:98.29ms
step:1229/1750 train_time:120799ms step_avg:98.29ms
step:1230/1750 train_time:120899ms step_avg:98.29ms
step:1231/1750 train_time:120998ms step_avg:98.29ms
step:1232/1750 train_time:121098ms step_avg:98.29ms
step:1233/1750 train_time:121198ms step_avg:98.30ms
step:1234/1750 train_time:121299ms step_avg:98.30ms
step:1235/1750 train_time:121398ms step_avg:98.30ms
step:1236/1750 train_time:121502ms step_avg:98.30ms
step:1237/1750 train_time:121609ms step_avg:98.31ms
step:1238/1750 train_time:121709ms step_avg:98.31ms
step:1239/1750 train_time:121810ms step_avg:98.31ms
step:1240/1750 train_time:121911ms step_avg:98.32ms
step:1241/1750 train_time:122013ms step_avg:98.32ms
step:1242/1750 train_time:122114ms step_avg:98.32ms
step:1243/1750 train_time:122214ms step_avg:98.32ms
step:1244/1750 train_time:122314ms step_avg:98.32ms
step:1245/1750 train_time:122415ms step_avg:98.33ms
step:1246/1750 train_time:122517ms step_avg:98.33ms
step:1247/1750 train_time:122617ms step_avg:98.33ms
step:1248/1750 train_time:122717ms step_avg:98.33ms
step:1249/1750 train_time:122818ms step_avg:98.33ms
step:1250/1750 train_time:122919ms step_avg:98.33ms
step:1250/1750 val_loss:3.4096 train_time:123008ms step_avg:98.41ms
step:1251/1750 train_time:123032ms step_avg:98.35ms
step:1252/1750 train_time:123127ms step_avg:98.34ms
step:1253/1750 train_time:123228ms step_avg:98.35ms
step:1254/1750 train_time:123328ms step_avg:98.35ms
step:1255/1750 train_time:123428ms step_avg:98.35ms
step:1256/1750 train_time:123527ms step_avg:98.35ms
step:1257/1750 train_time:123627ms step_avg:98.35ms
step:1258/1750 train_time:123726ms step_avg:98.35ms
step:1259/1750 train_time:123826ms step_avg:98.35ms
step:1260/1750 train_time:123926ms step_avg:98.35ms
step:1261/1750 train_time:124028ms step_avg:98.36ms
step:1262/1750 train_time:124129ms step_avg:98.36ms
step:1263/1750 train_time:124231ms step_avg:98.36ms
step:1264/1750 train_time:124332ms step_avg:98.36ms
step:1265/1750 train_time:124432ms step_avg:98.37ms
step:1266/1750 train_time:124532ms step_avg:98.37ms
step:1267/1750 train_time:124633ms step_avg:98.37ms
step:1268/1750 train_time:124732ms step_avg:98.37ms
step:1269/1750 train_time:124833ms step_avg:98.37ms
step:1270/1750 train_time:124934ms step_avg:98.37ms
step:1271/1750 train_time:125037ms step_avg:98.38ms
step:1272/1750 train_time:125138ms step_avg:98.38ms
step:1273/1750 train_time:125240ms step_avg:98.38ms
step:1274/1750 train_time:125340ms step_avg:98.38ms
step:1275/1750 train_time:125441ms step_avg:98.38ms
step:1276/1750 train_time:125541ms step_avg:98.39ms
step:1277/1750 train_time:125642ms step_avg:98.39ms
step:1278/1750 train_time:125742ms step_avg:98.39ms
step:1279/1750 train_time:125842ms step_avg:98.39ms
step:1280/1750 train_time:125943ms step_avg:98.39ms
step:1281/1750 train_time:126043ms step_avg:98.39ms
step:1282/1750 train_time:126143ms step_avg:98.40ms
step:1283/1750 train_time:126244ms step_avg:98.40ms
step:1284/1750 train_time:126344ms step_avg:98.40ms
step:1285/1750 train_time:126444ms step_avg:98.40ms
step:1286/1750 train_time:126544ms step_avg:98.40ms
step:1287/1750 train_time:126644ms step_avg:98.40ms
step:1288/1750 train_time:126744ms step_avg:98.40ms
step:1289/1750 train_time:126843ms step_avg:98.40ms
step:1290/1750 train_time:126943ms step_avg:98.41ms
step:1291/1750 train_time:127044ms step_avg:98.41ms
step:1292/1750 train_time:127144ms step_avg:98.41ms
step:1293/1750 train_time:127244ms step_avg:98.41ms
step:1294/1750 train_time:127346ms step_avg:98.41ms
step:1295/1750 train_time:127447ms step_avg:98.41ms
step:1296/1750 train_time:127547ms step_avg:98.42ms
step:1297/1750 train_time:127647ms step_avg:98.42ms
step:1298/1750 train_time:127747ms step_avg:98.42ms
step:1299/1750 train_time:127848ms step_avg:98.42ms
step:1300/1750 train_time:127948ms step_avg:98.42ms
step:1301/1750 train_time:128049ms step_avg:98.42ms
step:1302/1750 train_time:128149ms step_avg:98.43ms
step:1303/1750 train_time:128251ms step_avg:98.43ms
step:1304/1750 train_time:128352ms step_avg:98.43ms
step:1305/1750 train_time:128452ms step_avg:98.43ms
step:1306/1750 train_time:128553ms step_avg:98.43ms
step:1307/1750 train_time:128655ms step_avg:98.44ms
step:1308/1750 train_time:128756ms step_avg:98.44ms
step:1309/1750 train_time:128857ms step_avg:98.44ms
step:1310/1750 train_time:128957ms step_avg:98.44ms
step:1311/1750 train_time:129058ms step_avg:98.44ms
step:1312/1750 train_time:129159ms step_avg:98.44ms
step:1313/1750 train_time:129261ms step_avg:98.45ms
step:1314/1750 train_time:129362ms step_avg:98.45ms
step:1315/1750 train_time:129463ms step_avg:98.45ms
step:1316/1750 train_time:129563ms step_avg:98.45ms
step:1317/1750 train_time:129663ms step_avg:98.45ms
step:1318/1750 train_time:129764ms step_avg:98.46ms
step:1319/1750 train_time:129865ms step_avg:98.46ms
step:1320/1750 train_time:129966ms step_avg:98.46ms
step:1321/1750 train_time:130067ms step_avg:98.46ms
step:1322/1750 train_time:130168ms step_avg:98.46ms
step:1323/1750 train_time:130267ms step_avg:98.46ms
step:1324/1750 train_time:130368ms step_avg:98.47ms
step:1325/1750 train_time:130469ms step_avg:98.47ms
step:1326/1750 train_time:130571ms step_avg:98.47ms
step:1327/1750 train_time:130673ms step_avg:98.47ms
step:1328/1750 train_time:130774ms step_avg:98.47ms
step:1329/1750 train_time:130874ms step_avg:98.48ms
step:1330/1750 train_time:130976ms step_avg:98.48ms
step:1331/1750 train_time:131078ms step_avg:98.48ms
step:1332/1750 train_time:131179ms step_avg:98.48ms
step:1333/1750 train_time:131281ms step_avg:98.49ms
step:1334/1750 train_time:131382ms step_avg:98.49ms
step:1335/1750 train_time:131483ms step_avg:98.49ms
step:1336/1750 train_time:131584ms step_avg:98.49ms
step:1337/1750 train_time:131685ms step_avg:98.49ms
step:1338/1750 train_time:131785ms step_avg:98.49ms
step:1339/1750 train_time:131885ms step_avg:98.50ms
step:1340/1750 train_time:131986ms step_avg:98.50ms
step:1341/1750 train_time:132086ms step_avg:98.50ms
step:1342/1750 train_time:132187ms step_avg:98.50ms
step:1343/1750 train_time:132288ms step_avg:98.50ms
step:1344/1750 train_time:132389ms step_avg:98.50ms
step:1345/1750 train_time:132489ms step_avg:98.50ms
step:1346/1750 train_time:132591ms step_avg:98.51ms
step:1347/1750 train_time:132693ms step_avg:98.51ms
step:1348/1750 train_time:132794ms step_avg:98.51ms
step:1349/1750 train_time:132894ms step_avg:98.51ms
step:1350/1750 train_time:132996ms step_avg:98.52ms
step:1351/1750 train_time:133097ms step_avg:98.52ms
step:1352/1750 train_time:133199ms step_avg:98.52ms
step:1353/1750 train_time:133300ms step_avg:98.52ms
step:1354/1750 train_time:133401ms step_avg:98.52ms
step:1355/1750 train_time:133501ms step_avg:98.52ms
step:1356/1750 train_time:133603ms step_avg:98.53ms
step:1357/1750 train_time:133702ms step_avg:98.53ms
step:1358/1750 train_time:133803ms step_avg:98.53ms
step:1359/1750 train_time:133903ms step_avg:98.53ms
step:1360/1750 train_time:134003ms step_avg:98.53ms
step:1361/1750 train_time:134103ms step_avg:98.53ms
step:1362/1750 train_time:134204ms step_avg:98.53ms
step:1363/1750 train_time:134305ms step_avg:98.54ms
step:1364/1750 train_time:134405ms step_avg:98.54ms
step:1365/1750 train_time:134506ms step_avg:98.54ms
step:1366/1750 train_time:134606ms step_avg:98.54ms
step:1367/1750 train_time:134706ms step_avg:98.54ms
step:1368/1750 train_time:134807ms step_avg:98.54ms
step:1369/1750 train_time:134908ms step_avg:98.54ms
step:1370/1750 train_time:135008ms step_avg:98.55ms
step:1371/1750 train_time:135110ms step_avg:98.55ms
step:1372/1750 train_time:135211ms step_avg:98.55ms
step:1373/1750 train_time:135313ms step_avg:98.55ms
step:1374/1750 train_time:135414ms step_avg:98.55ms
step:1375/1750 train_time:135515ms step_avg:98.56ms
step:1375/1750 val_loss:3.3694 train_time:135606ms step_avg:98.62ms
step:1376/1750 train_time:135629ms step_avg:98.57ms
step:1377/1750 train_time:135728ms step_avg:98.57ms
step:1378/1750 train_time:135832ms step_avg:98.57ms
step:1379/1750 train_time:135931ms step_avg:98.57ms
step:1380/1750 train_time:136033ms step_avg:98.57ms
step:1381/1750 train_time:136132ms step_avg:98.58ms
step:1382/1750 train_time:136231ms step_avg:98.58ms
step:1383/1750 train_time:136331ms step_avg:98.58ms
step:1384/1750 train_time:136430ms step_avg:98.58ms
step:1385/1750 train_time:136530ms step_avg:98.58ms
step:1386/1750 train_time:136633ms step_avg:98.58ms
step:1387/1750 train_time:136736ms step_avg:98.58ms
step:1388/1750 train_time:136836ms step_avg:98.59ms
step:1389/1750 train_time:136937ms step_avg:98.59ms
step:1390/1750 train_time:137037ms step_avg:98.59ms
step:1391/1750 train_time:137138ms step_avg:98.59ms
step:1392/1750 train_time:137239ms step_avg:98.59ms
step:1393/1750 train_time:137339ms step_avg:98.59ms
step:1394/1750 train_time:137439ms step_avg:98.59ms
step:1395/1750 train_time:137540ms step_avg:98.60ms
step:1396/1750 train_time:137643ms step_avg:98.60ms
step:1397/1750 train_time:137746ms step_avg:98.60ms
step:1398/1750 train_time:137847ms step_avg:98.60ms
step:1399/1750 train_time:137949ms step_avg:98.61ms
step:1400/1750 train_time:138051ms step_avg:98.61ms
step:1401/1750 train_time:138152ms step_avg:98.61ms
step:1402/1750 train_time:138252ms step_avg:98.61ms
step:1403/1750 train_time:138352ms step_avg:98.61ms
step:1404/1750 train_time:138452ms step_avg:98.61ms
step:1405/1750 train_time:138552ms step_avg:98.61ms
step:1406/1750 train_time:138654ms step_avg:98.62ms
step:1407/1750 train_time:138755ms step_avg:98.62ms
step:1408/1750 train_time:138856ms step_avg:98.62ms
step:1409/1750 train_time:138958ms step_avg:98.62ms
step:1410/1750 train_time:139061ms step_avg:98.62ms
step:1411/1750 train_time:139162ms step_avg:98.63ms
step:1412/1750 train_time:139265ms step_avg:98.63ms
step:1413/1750 train_time:139365ms step_avg:98.63ms
step:1414/1750 train_time:139465ms step_avg:98.63ms
step:1415/1750 train_time:139567ms step_avg:98.63ms
step:1416/1750 train_time:139669ms step_avg:98.64ms
step:1417/1750 train_time:139770ms step_avg:98.64ms
step:1418/1750 train_time:139871ms step_avg:98.64ms
step:1419/1750 train_time:139973ms step_avg:98.64ms
step:1420/1750 train_time:140073ms step_avg:98.64ms
step:1421/1750 train_time:140174ms step_avg:98.64ms
step:1422/1750 train_time:140273ms step_avg:98.64ms
step:1423/1750 train_time:140372ms step_avg:98.65ms
step:1424/1750 train_time:140473ms step_avg:98.65ms
step:1425/1750 train_time:140573ms step_avg:98.65ms
step:1426/1750 train_time:140674ms step_avg:98.65ms
step:1427/1750 train_time:140776ms step_avg:98.65ms
step:1428/1750 train_time:140879ms step_avg:98.65ms
step:1429/1750 train_time:140981ms step_avg:98.66ms
step:1430/1750 train_time:141083ms step_avg:98.66ms
step:1431/1750 train_time:141185ms step_avg:98.66ms
step:1432/1750 train_time:141287ms step_avg:98.66ms
step:1433/1750 train_time:141389ms step_avg:98.67ms
step:1434/1750 train_time:141489ms step_avg:98.67ms
step:1435/1750 train_time:141593ms step_avg:98.67ms
step:1436/1750 train_time:141696ms step_avg:98.67ms
step:1437/1750 train_time:141798ms step_avg:98.68ms
step:1438/1750 train_time:141898ms step_avg:98.68ms
step:1439/1750 train_time:142002ms step_avg:98.68ms
step:1440/1750 train_time:142104ms step_avg:98.68ms
step:1441/1750 train_time:142206ms step_avg:98.69ms
step:1442/1750 train_time:142307ms step_avg:98.69ms
step:1443/1750 train_time:142409ms step_avg:98.69ms
step:1444/1750 train_time:142512ms step_avg:98.69ms
step:1445/1750 train_time:142614ms step_avg:98.69ms
step:1446/1750 train_time:142715ms step_avg:98.70ms
step:1447/1750 train_time:142816ms step_avg:98.70ms
step:1448/1750 train_time:142919ms step_avg:98.70ms
step:1449/1750 train_time:143019ms step_avg:98.70ms
step:1450/1750 train_time:143122ms step_avg:98.70ms
step:1451/1750 train_time:143224ms step_avg:98.71ms
step:1452/1750 train_time:143325ms step_avg:98.71ms
step:1453/1750 train_time:143427ms step_avg:98.71ms
step:1454/1750 train_time:143530ms step_avg:98.71ms
step:1455/1750 train_time:143633ms step_avg:98.72ms
step:1456/1750 train_time:143734ms step_avg:98.72ms
step:1457/1750 train_time:143836ms step_avg:98.72ms
step:1458/1750 train_time:143938ms step_avg:98.72ms
step:1459/1750 train_time:144039ms step_avg:98.72ms
step:1460/1750 train_time:144141ms step_avg:98.73ms
step:1461/1750 train_time:144245ms step_avg:98.73ms
step:1462/1750 train_time:144346ms step_avg:98.73ms
step:1463/1750 train_time:144448ms step_avg:98.73ms
step:1464/1750 train_time:144550ms step_avg:98.74ms
step:1465/1750 train_time:144652ms step_avg:98.74ms
step:1466/1750 train_time:144754ms step_avg:98.74ms
step:1467/1750 train_time:144855ms step_avg:98.74ms
step:1468/1750 train_time:144958ms step_avg:98.74ms
step:1469/1750 train_time:145060ms step_avg:98.75ms
step:1470/1750 train_time:145161ms step_avg:98.75ms
step:1471/1750 train_time:145262ms step_avg:98.75ms
step:1472/1750 train_time:145364ms step_avg:98.75ms
step:1473/1750 train_time:145465ms step_avg:98.75ms
step:1474/1750 train_time:145567ms step_avg:98.76ms
step:1475/1750 train_time:145669ms step_avg:98.76ms
step:1476/1750 train_time:145772ms step_avg:98.76ms
step:1477/1750 train_time:145874ms step_avg:98.76ms
step:1478/1750 train_time:145975ms step_avg:98.77ms
step:1479/1750 train_time:146076ms step_avg:98.77ms
step:1480/1750 train_time:146177ms step_avg:98.77ms
step:1481/1750 train_time:146277ms step_avg:98.77ms
step:1482/1750 train_time:146380ms step_avg:98.77ms
step:1483/1750 train_time:146482ms step_avg:98.77ms
step:1484/1750 train_time:146584ms step_avg:98.78ms
step:1485/1750 train_time:146685ms step_avg:98.78ms
step:1486/1750 train_time:146788ms step_avg:98.78ms
step:1487/1750 train_time:146891ms step_avg:98.78ms
step:1488/1750 train_time:146995ms step_avg:98.79ms
step:1489/1750 train_time:147096ms step_avg:98.79ms
step:1490/1750 train_time:147197ms step_avg:98.79ms
step:1491/1750 train_time:147298ms step_avg:98.79ms
step:1492/1750 train_time:147399ms step_avg:98.79ms
step:1493/1750 train_time:147501ms step_avg:98.79ms
step:1494/1750 train_time:147603ms step_avg:98.80ms
step:1495/1750 train_time:147706ms step_avg:98.80ms
step:1496/1750 train_time:147808ms step_avg:98.80ms
step:1497/1750 train_time:147911ms step_avg:98.80ms
step:1498/1750 train_time:148014ms step_avg:98.81ms
step:1499/1750 train_time:148115ms step_avg:98.81ms
step:1500/1750 train_time:148217ms step_avg:98.81ms
step:1500/1750 val_loss:3.3341 train_time:148307ms step_avg:98.87ms
step:1501/1750 train_time:148328ms step_avg:98.82ms
step:1502/1750 train_time:148427ms step_avg:98.82ms
step:1503/1750 train_time:148528ms step_avg:98.82ms
step:1504/1750 train_time:148628ms step_avg:98.82ms
step:1505/1750 train_time:148729ms step_avg:98.82ms
step:1506/1750 train_time:148829ms step_avg:98.82ms
step:1507/1750 train_time:148929ms step_avg:98.82ms
step:1508/1750 train_time:149030ms step_avg:98.83ms
step:1509/1750 train_time:149132ms step_avg:98.83ms
step:1510/1750 train_time:149234ms step_avg:98.83ms
step:1511/1750 train_time:149341ms step_avg:98.84ms
step:1512/1750 train_time:149443ms step_avg:98.84ms
step:1513/1750 train_time:149545ms step_avg:98.84ms
step:1514/1750 train_time:149646ms step_avg:98.84ms
step:1515/1750 train_time:149751ms step_avg:98.85ms
step:1516/1750 train_time:149851ms step_avg:98.85ms
step:1517/1750 train_time:149951ms step_avg:98.85ms
step:1518/1750 train_time:150052ms step_avg:98.85ms
step:1519/1750 train_time:150154ms step_avg:98.85ms
step:1520/1750 train_time:150255ms step_avg:98.85ms
step:1521/1750 train_time:150358ms step_avg:98.85ms
step:1522/1750 train_time:150461ms step_avg:98.86ms
step:1523/1750 train_time:150564ms step_avg:98.86ms
step:1524/1750 train_time:150668ms step_avg:98.86ms
step:1525/1750 train_time:150770ms step_avg:98.87ms
step:1526/1750 train_time:150870ms step_avg:98.87ms
step:1527/1750 train_time:150972ms step_avg:98.87ms
step:1528/1750 train_time:151075ms step_avg:98.87ms
step:1529/1750 train_time:151176ms step_avg:98.87ms
step:1530/1750 train_time:151281ms step_avg:98.88ms
step:1531/1750 train_time:151382ms step_avg:98.88ms
step:1532/1750 train_time:151485ms step_avg:98.88ms
step:1533/1750 train_time:151587ms step_avg:98.88ms
step:1534/1750 train_time:151689ms step_avg:98.88ms
step:1535/1750 train_time:151791ms step_avg:98.89ms
step:1536/1750 train_time:151892ms step_avg:98.89ms
step:1537/1750 train_time:151993ms step_avg:98.89ms
step:1538/1750 train_time:152094ms step_avg:98.89ms
step:1539/1750 train_time:152196ms step_avg:98.89ms
step:1540/1750 train_time:152299ms step_avg:98.90ms
step:1541/1750 train_time:152404ms step_avg:98.90ms
step:1542/1750 train_time:152508ms step_avg:98.90ms
step:1543/1750 train_time:152610ms step_avg:98.90ms
step:1544/1750 train_time:152711ms step_avg:98.91ms
step:1545/1750 train_time:152812ms step_avg:98.91ms
step:1546/1750 train_time:152912ms step_avg:98.91ms
step:1547/1750 train_time:153015ms step_avg:98.91ms
step:1548/1750 train_time:153116ms step_avg:98.91ms
step:1549/1750 train_time:153219ms step_avg:98.91ms
step:1550/1750 train_time:153321ms step_avg:98.92ms
step:1551/1750 train_time:153425ms step_avg:98.92ms
step:1552/1750 train_time:153528ms step_avg:98.92ms
step:1553/1750 train_time:153631ms step_avg:98.93ms
step:1554/1750 train_time:153732ms step_avg:98.93ms
step:1555/1750 train_time:153833ms step_avg:98.93ms
step:1556/1750 train_time:153934ms step_avg:98.93ms
step:1557/1750 train_time:154036ms step_avg:98.93ms
step:1558/1750 train_time:154138ms step_avg:98.93ms
step:1559/1750 train_time:154241ms step_avg:98.94ms
step:1560/1750 train_time:154342ms step_avg:98.94ms
step:1561/1750 train_time:154444ms step_avg:98.94ms
step:1562/1750 train_time:154547ms step_avg:98.94ms
step:1563/1750 train_time:154652ms step_avg:98.95ms
step:1564/1750 train_time:154753ms step_avg:98.95ms
step:1565/1750 train_time:154854ms step_avg:98.95ms
step:1566/1750 train_time:154955ms step_avg:98.95ms
step:1567/1750 train_time:155057ms step_avg:98.95ms
step:1568/1750 train_time:155158ms step_avg:98.95ms
step:1569/1750 train_time:155261ms step_avg:98.96ms
step:1570/1750 train_time:155364ms step_avg:98.96ms
step:1571/1750 train_time:155466ms step_avg:98.96ms
step:1572/1750 train_time:155568ms step_avg:98.96ms
step:1573/1750 train_time:155670ms step_avg:98.96ms
step:1574/1750 train_time:155772ms step_avg:98.97ms
step:1575/1750 train_time:155873ms step_avg:98.97ms
step:1576/1750 train_time:155976ms step_avg:98.97ms
step:1577/1750 train_time:156079ms step_avg:98.97ms
step:1578/1750 train_time:156180ms step_avg:98.97ms
step:1579/1750 train_time:156282ms step_avg:98.98ms
step:1580/1750 train_time:156385ms step_avg:98.98ms
step:1581/1750 train_time:156488ms step_avg:98.98ms
step:1582/1750 train_time:156589ms step_avg:98.98ms
step:1583/1750 train_time:156693ms step_avg:98.98ms
step:1584/1750 train_time:156796ms step_avg:98.99ms
step:1585/1750 train_time:156897ms step_avg:98.99ms
step:1586/1750 train_time:157000ms step_avg:98.99ms
step:1587/1750 train_time:157101ms step_avg:98.99ms
step:1588/1750 train_time:157203ms step_avg:98.99ms
step:1589/1750 train_time:157305ms step_avg:99.00ms
step:1590/1750 train_time:157407ms step_avg:99.00ms
step:1591/1750 train_time:157509ms step_avg:99.00ms
step:1592/1750 train_time:157610ms step_avg:99.00ms
step:1593/1750 train_time:157712ms step_avg:99.00ms
step:1594/1750 train_time:157816ms step_avg:99.01ms
step:1595/1750 train_time:157918ms step_avg:99.01ms
step:1596/1750 train_time:158019ms step_avg:99.01ms
step:1597/1750 train_time:158121ms step_avg:99.01ms
step:1598/1750 train_time:158224ms step_avg:99.01ms
step:1599/1750 train_time:158325ms step_avg:99.01ms
step:1600/1750 train_time:158426ms step_avg:99.02ms
step:1601/1750 train_time:158528ms step_avg:99.02ms
step:1602/1750 train_time:158629ms step_avg:99.02ms
step:1603/1750 train_time:158730ms step_avg:99.02ms
step:1604/1750 train_time:158831ms step_avg:99.02ms
step:1605/1750 train_time:158935ms step_avg:99.02ms
step:1606/1750 train_time:159038ms step_avg:99.03ms
step:1607/1750 train_time:159139ms step_avg:99.03ms
step:1608/1750 train_time:159241ms step_avg:99.03ms
step:1609/1750 train_time:159343ms step_avg:99.03ms
step:1610/1750 train_time:159445ms step_avg:99.03ms
step:1611/1750 train_time:159547ms step_avg:99.04ms
step:1612/1750 train_time:159649ms step_avg:99.04ms
step:1613/1750 train_time:159751ms step_avg:99.04ms
step:1614/1750 train_time:159852ms step_avg:99.04ms
step:1615/1750 train_time:159953ms step_avg:99.04ms
step:1616/1750 train_time:160055ms step_avg:99.04ms
step:1617/1750 train_time:160158ms step_avg:99.05ms
step:1618/1750 train_time:160260ms step_avg:99.05ms
step:1619/1750 train_time:160361ms step_avg:99.05ms
step:1620/1750 train_time:160464ms step_avg:99.05ms
step:1621/1750 train_time:160566ms step_avg:99.05ms
step:1622/1750 train_time:160668ms step_avg:99.06ms
step:1623/1750 train_time:160770ms step_avg:99.06ms
step:1624/1750 train_time:160873ms step_avg:99.06ms
step:1625/1750 train_time:160976ms step_avg:99.06ms
step:1625/1750 val_loss:3.3043 train_time:161066ms step_avg:99.12ms
step:1626/1750 train_time:161089ms step_avg:99.07ms
step:1627/1750 train_time:161194ms step_avg:99.07ms
step:1628/1750 train_time:161295ms step_avg:99.08ms
step:1629/1750 train_time:161396ms step_avg:99.08ms
step:1630/1750 train_time:161497ms step_avg:99.08ms
step:1631/1750 train_time:161599ms step_avg:99.08ms
step:1632/1750 train_time:161701ms step_avg:99.08ms
step:1633/1750 train_time:161801ms step_avg:99.08ms
step:1634/1750 train_time:161903ms step_avg:99.08ms
step:1635/1750 train_time:162005ms step_avg:99.09ms
step:1636/1750 train_time:162110ms step_avg:99.09ms
step:1637/1750 train_time:162213ms step_avg:99.09ms
step:1638/1750 train_time:162315ms step_avg:99.09ms
step:1639/1750 train_time:162416ms step_avg:99.09ms
step:1640/1750 train_time:162517ms step_avg:99.10ms
step:1641/1750 train_time:162618ms step_avg:99.10ms
step:1642/1750 train_time:162720ms step_avg:99.10ms
step:1643/1750 train_time:162820ms step_avg:99.10ms
step:1644/1750 train_time:162922ms step_avg:99.10ms
step:1645/1750 train_time:163026ms step_avg:99.10ms
step:1646/1750 train_time:163129ms step_avg:99.11ms
step:1647/1750 train_time:163233ms step_avg:99.11ms
step:1648/1750 train_time:163335ms step_avg:99.11ms
step:1649/1750 train_time:163437ms step_avg:99.11ms
step:1650/1750 train_time:163538ms step_avg:99.11ms
step:1651/1750 train_time:163640ms step_avg:99.12ms
step:1652/1750 train_time:163742ms step_avg:99.12ms
step:1653/1750 train_time:163843ms step_avg:99.12ms
step:1654/1750 train_time:163943ms step_avg:99.12ms
step:1655/1750 train_time:164047ms step_avg:99.12ms
step:1656/1750 train_time:164149ms step_avg:99.12ms
step:1657/1750 train_time:164251ms step_avg:99.13ms
step:1658/1750 train_time:164354ms step_avg:99.13ms
step:1659/1750 train_time:164458ms step_avg:99.13ms
step:1660/1750 train_time:164559ms step_avg:99.13ms
step:1661/1750 train_time:164662ms step_avg:99.13ms
step:1662/1750 train_time:164765ms step_avg:99.14ms
step:1663/1750 train_time:164867ms step_avg:99.14ms
step:1664/1750 train_time:164969ms step_avg:99.14ms
step:1665/1750 train_time:165074ms step_avg:99.14ms
step:1666/1750 train_time:165176ms step_avg:99.15ms
step:1667/1750 train_time:165278ms step_avg:99.15ms
step:1668/1750 train_time:165383ms step_avg:99.15ms
step:1669/1750 train_time:165485ms step_avg:99.15ms
step:1670/1750 train_time:165587ms step_avg:99.15ms
step:1671/1750 train_time:165689ms step_avg:99.16ms
step:1672/1750 train_time:165790ms step_avg:99.16ms
step:1673/1750 train_time:165891ms step_avg:99.16ms
step:1674/1750 train_time:165992ms step_avg:99.16ms
step:1675/1750 train_time:166094ms step_avg:99.16ms
step:1676/1750 train_time:166198ms step_avg:99.16ms
step:1677/1750 train_time:166299ms step_avg:99.16ms
step:1678/1750 train_time:166403ms step_avg:99.17ms
step:1679/1750 train_time:166505ms step_avg:99.17ms
step:1680/1750 train_time:166606ms step_avg:99.17ms
step:1681/1750 train_time:166709ms step_avg:99.17ms
step:1682/1750 train_time:166812ms step_avg:99.18ms
step:1683/1750 train_time:166913ms step_avg:99.18ms
step:1684/1750 train_time:167015ms step_avg:99.18ms
step:1685/1750 train_time:167116ms step_avg:99.18ms
step:1686/1750 train_time:167217ms step_avg:99.18ms
step:1687/1750 train_time:167319ms step_avg:99.18ms
step:1688/1750 train_time:167422ms step_avg:99.18ms
step:1689/1750 train_time:167526ms step_avg:99.19ms
step:1690/1750 train_time:167629ms step_avg:99.19ms
step:1691/1750 train_time:167731ms step_avg:99.19ms
step:1692/1750 train_time:167833ms step_avg:99.19ms
step:1693/1750 train_time:167936ms step_avg:99.19ms
step:1694/1750 train_time:168039ms step_avg:99.20ms
step:1695/1750 train_time:168142ms step_avg:99.20ms
step:1696/1750 train_time:168245ms step_avg:99.20ms
step:1697/1750 train_time:168349ms step_avg:99.20ms
step:1698/1750 train_time:168451ms step_avg:99.21ms
step:1699/1750 train_time:168552ms step_avg:99.21ms
step:1700/1750 train_time:168655ms step_avg:99.21ms
step:1701/1750 train_time:168758ms step_avg:99.21ms
step:1702/1750 train_time:168864ms step_avg:99.22ms
step:1703/1750 train_time:168966ms step_avg:99.22ms
step:1704/1750 train_time:169069ms step_avg:99.22ms
step:1705/1750 train_time:169171ms step_avg:99.22ms
step:1706/1750 train_time:169273ms step_avg:99.22ms
step:1707/1750 train_time:169375ms step_avg:99.22ms
step:1708/1750 train_time:169477ms step_avg:99.23ms
step:1709/1750 train_time:169580ms step_avg:99.23ms
step:1710/1750 train_time:169683ms step_avg:99.23ms
step:1711/1750 train_time:169787ms step_avg:99.23ms
step:1712/1750 train_time:169889ms step_avg:99.23ms
step:1713/1750 train_time:169993ms step_avg:99.24ms
step:1714/1750 train_time:170095ms step_avg:99.24ms
step:1715/1750 train_time:170198ms step_avg:99.24ms
step:1716/1750 train_time:170302ms step_avg:99.24ms
step:1717/1750 train_time:170405ms step_avg:99.25ms
step:1718/1750 train_time:170506ms step_avg:99.25ms
step:1719/1750 train_time:170612ms step_avg:99.25ms
step:1720/1750 train_time:170713ms step_avg:99.25ms
step:1721/1750 train_time:170816ms step_avg:99.25ms
step:1722/1750 train_time:170919ms step_avg:99.26ms
step:1723/1750 train_time:171023ms step_avg:99.26ms
step:1724/1750 train_time:171127ms step_avg:99.26ms
step:1725/1750 train_time:171231ms step_avg:99.26ms
step:1726/1750 train_time:171333ms step_avg:99.27ms
step:1727/1750 train_time:171436ms step_avg:99.27ms
step:1728/1750 train_time:171539ms step_avg:99.27ms
step:1729/1750 train_time:171643ms step_avg:99.27ms
step:1730/1750 train_time:171745ms step_avg:99.27ms
step:1731/1750 train_time:171849ms step_avg:99.28ms
step:1732/1750 train_time:171952ms step_avg:99.28ms
step:1733/1750 train_time:172054ms step_avg:99.28ms
step:1734/1750 train_time:172159ms step_avg:99.28ms
step:1735/1750 train_time:172261ms step_avg:99.29ms
step:1736/1750 train_time:172363ms step_avg:99.29ms
step:1737/1750 train_time:172467ms step_avg:99.29ms
step:1738/1750 train_time:172570ms step_avg:99.29ms
step:1739/1750 train_time:172672ms step_avg:99.29ms
step:1740/1750 train_time:172773ms step_avg:99.29ms
step:1741/1750 train_time:172879ms step_avg:99.30ms
step:1742/1750 train_time:172983ms step_avg:99.30ms
step:1743/1750 train_time:173086ms step_avg:99.30ms
step:1744/1750 train_time:173190ms step_avg:99.31ms
step:1745/1750 train_time:173292ms step_avg:99.31ms
step:1746/1750 train_time:173394ms step_avg:99.31ms
step:1747/1750 train_time:173496ms step_avg:99.31ms
step:1748/1750 train_time:173600ms step_avg:99.31ms
step:1749/1750 train_time:173703ms step_avg:99.32ms
step:1750/1750 train_time:173807ms step_avg:99.32ms
step:1750/1750 val_loss:3.2811 train_time:173897ms step_avg:99.37ms
peak memory allocated: 33278 MiB reserved: 48954 MiB
