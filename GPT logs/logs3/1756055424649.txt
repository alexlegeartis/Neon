import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class FastNormMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, sgd_coeff=0):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, sgd_coeff=sgd_coeff)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        self.sgd_coeff = sgd_coeff

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            sgd_coeff = group["sgd_coeff"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    # assume: grad is a Tensor, momentum_buffer exists, momentum, sgd_coeff, eps, eff_lr are defined
                    with torch.no_grad():
                        # inplace blend with momentum (keeps original semantic: grad becomes the lerped tensor)
                        grad.lerp_(momentum_buffer, momentum)

                        # compute zeropower part from the (post-lerp) grad, in bfloat16 to match the original code
                        if sgd_coeff != 1:
                            # create update_part in bfloat16 (may allocate once per call; unavoidable if zeropower expects bfloat16)
                            update_part = zeropower_via_newtonschulz5(grad.to(torch.bfloat16), 5)

                            # upcast update_part to grad dtype to do mixed-dtype blending without extra temporaries later
                            if update_part.dtype != grad.dtype:
                                update_part = update_part.to(grad.dtype)

                        # compute inverse norm scalar efficiently (no large temp tensors)
                        # - view(-1) is cheap (no copy)
                        # - dot(view, view) is a single reduction
                        norm_sq = grad.view(-1).dot(grad.view(-1))
                        inv_norm = (norm_sq + eps).rsqrt()  # stable rsqrt on the scalar

                        # normalize grad in-place (so we avoid allocating g_normalized)
                        grad.mul_(inv_norm)   # now grad == g_normalized (in-place)

                        # build final update (reuse update_part buffer if available)
                        if sgd_coeff != 1:
                            # update_part := (1 - sgd_coeff) * update_part + sgd_coeff * grad
                            # do it in-place on update_part to avoid creating another temp
                            update_part.mul_(1 - sgd_coeff)    # in-place scale
                            update_part.lerp_(grad, sgd_coeff) # in-place linear interpolation
                            update = update_part
                        else:
                            # if sgd_coeff == 1 we can use grad directly (already normalized in-place)
                            update = grad

                        # apply update (same as original)
                        p.add_(other=update, alpha=-eff_lr)
                    '''
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    eps = 1e-12
                    g_normalized = grad / (grad.norm() + eps)
                    if sgd_coeff != 1:
                        update_part = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                        update = (1 - sgd_coeff) * update_part + sgd_coeff * g_normalized
                    else:
                        update = sgd_coeff * g_normalized
                    p.add_(other=update, alpha=-eff_lr)
                    '''
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class NormMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, sgd_coeff=0):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, sgd_coeff=sgd_coeff)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        self.sgd_coeff = sgd_coeff

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            sgd_coeff = group["sgd_coeff"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    eps = 1e-12
                    g_normalized = grad / (grad.norm() + eps)
                    if sgd_coeff != 1:
                        update_part = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                        update = (1 - sgd_coeff) * update_part + sgd_coeff * g_normalized
                    else:
                        update = sgd_coeff * g_normalized
                    p.add_(other=update, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()
class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)
                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_reduce_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 27.5
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos : pos + max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()
    starts = []
    for i in range(1, len(boundary_positions)):
        end = boundary_positions[i].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == dist.get_world_size():
                return starts, end - pos
            start = end
    assert False # increase max_batch_span if necessary

def distributed_data_generator(filename_pattern: str, batch_size: int, align_to_bos: bool):
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    max_batch_span = 2 * batch_size if align_to_bos else batch_size # provide buffer to handle samples up to length local_batch_size
    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            batch_span = batch_size
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1750 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
from datetime import datetime
if master_process:
    # run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_id = str(int(time.time() * 1000))
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0)
optimizer2 = NormMuon(hidden_matrix_params, lr=0.07, momentum=0.96, weight_decay=0.0, sgd_coeff=1)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.11 | packaged by conda-forge | (main, Jun  4 2025, 14:45:31) [GCC 13.3.0]
Running PyTorch 2.9.0.dev20250824+cu126 compiled for CUDA 12.6
Sun Aug 24 17:10:24 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:04:00.0 Off |                    0 |
| N/A   39C    P0            120W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:23:00.0 Off |                    0 |
| N/A   33C    P0            115W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:43:00.0 Off |                    0 |
| N/A   37C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   32C    P0            115W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:84:00.0 Off |                    0 |
| N/A   38C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:A3:00.0 Off |                    0 |
| N/A   33C    P0            121W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:C3:00.0 Off |                    0 |
| N/A   39C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E4:00.0 Off |                    0 |
| N/A   31C    P0            113W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.06ms
step:1/1750 train_time:143ms step_avg:143.34ms
step:2/1750 train_time:161ms step_avg:80.33ms
step:3/1750 train_time:226ms step_avg:75.40ms
step:4/1750 train_time:316ms step_avg:78.96ms
step:5/1750 train_time:407ms step_avg:81.30ms
step:6/1750 train_time:498ms step_avg:83.02ms
step:7/1750 train_time:590ms step_avg:84.35ms
step:8/1750 train_time:681ms step_avg:85.18ms
step:9/1750 train_time:773ms step_avg:85.85ms
step:10/1750 train_time:864ms step_avg:86.41ms
step:11/1750 train_time:956ms step_avg:86.89ms
step:12/1750 train_time:1050ms step_avg:87.48ms
step:13/1750 train_time:1143ms step_avg:87.90ms
step:14/1750 train_time:1238ms step_avg:88.45ms
step:15/1750 train_time:1332ms step_avg:88.82ms
step:16/1750 train_time:1424ms step_avg:89.02ms
step:17/1750 train_time:1516ms step_avg:89.18ms
step:18/1750 train_time:1608ms step_avg:89.34ms
step:19/1750 train_time:1700ms step_avg:89.48ms
step:20/1750 train_time:1793ms step_avg:89.65ms
step:21/1750 train_time:1884ms step_avg:89.71ms
step:22/1750 train_time:1975ms step_avg:89.78ms
step:23/1750 train_time:2068ms step_avg:89.92ms
step:24/1750 train_time:2161ms step_avg:90.03ms
step:25/1750 train_time:2256ms step_avg:90.23ms
step:26/1750 train_time:2349ms step_avg:90.34ms
step:27/1750 train_time:2440ms step_avg:90.38ms
step:28/1750 train_time:2534ms step_avg:90.49ms
step:29/1750 train_time:2625ms step_avg:90.53ms
step:30/1750 train_time:2717ms step_avg:90.56ms
step:31/1750 train_time:2809ms step_avg:90.61ms
step:32/1750 train_time:2900ms step_avg:90.62ms
step:33/1750 train_time:2992ms step_avg:90.67ms
step:34/1750 train_time:3084ms step_avg:90.71ms
step:35/1750 train_time:3177ms step_avg:90.78ms
step:36/1750 train_time:3270ms step_avg:90.83ms
step:37/1750 train_time:3363ms step_avg:90.89ms
step:38/1750 train_time:3457ms step_avg:90.97ms
step:39/1750 train_time:3550ms step_avg:91.01ms
step:40/1750 train_time:3642ms step_avg:91.05ms
step:41/1750 train_time:3736ms step_avg:91.11ms
step:42/1750 train_time:3828ms step_avg:91.14ms
step:43/1750 train_time:3920ms step_avg:91.16ms
step:44/1750 train_time:4014ms step_avg:91.23ms
step:45/1750 train_time:4105ms step_avg:91.22ms
step:46/1750 train_time:4197ms step_avg:91.25ms
step:47/1750 train_time:4290ms step_avg:91.29ms
step:48/1750 train_time:4382ms step_avg:91.29ms
step:49/1750 train_time:4476ms step_avg:91.34ms
step:50/1750 train_time:4567ms step_avg:91.34ms
step:51/1750 train_time:4659ms step_avg:91.36ms
step:52/1750 train_time:4753ms step_avg:91.41ms
step:53/1750 train_time:4845ms step_avg:91.41ms
step:54/1750 train_time:4938ms step_avg:91.44ms
step:55/1750 train_time:5030ms step_avg:91.45ms
step:56/1750 train_time:5122ms step_avg:91.47ms
step:57/1750 train_time:5216ms step_avg:91.50ms
step:58/1750 train_time:5308ms step_avg:91.52ms
step:59/1750 train_time:5401ms step_avg:91.54ms
step:60/1750 train_time:5494ms step_avg:91.57ms
step:61/1750 train_time:5586ms step_avg:91.58ms
step:62/1750 train_time:5679ms step_avg:91.60ms
step:63/1750 train_time:5773ms step_avg:91.64ms
step:64/1750 train_time:5865ms step_avg:91.64ms
step:65/1750 train_time:5958ms step_avg:91.66ms
step:66/1750 train_time:6052ms step_avg:91.70ms
step:67/1750 train_time:6144ms step_avg:91.70ms
step:68/1750 train_time:6237ms step_avg:91.72ms
step:69/1750 train_time:6329ms step_avg:91.73ms
step:70/1750 train_time:6421ms step_avg:91.73ms
step:71/1750 train_time:6515ms step_avg:91.76ms
step:72/1750 train_time:6607ms step_avg:91.77ms
step:73/1750 train_time:6699ms step_avg:91.77ms
step:74/1750 train_time:6793ms step_avg:91.80ms
step:75/1750 train_time:6885ms step_avg:91.80ms
step:76/1750 train_time:6977ms step_avg:91.81ms
step:77/1750 train_time:7070ms step_avg:91.81ms
step:78/1750 train_time:7162ms step_avg:91.82ms
step:79/1750 train_time:7255ms step_avg:91.83ms
step:80/1750 train_time:7347ms step_avg:91.84ms
step:81/1750 train_time:7440ms step_avg:91.85ms
step:82/1750 train_time:7533ms step_avg:91.87ms
step:83/1750 train_time:7625ms step_avg:91.87ms
step:84/1750 train_time:7718ms step_avg:91.88ms
step:85/1750 train_time:7811ms step_avg:91.89ms
step:86/1750 train_time:7903ms step_avg:91.89ms
step:87/1750 train_time:7995ms step_avg:91.90ms
step:88/1750 train_time:8088ms step_avg:91.91ms
step:89/1750 train_time:8180ms step_avg:91.91ms
step:90/1750 train_time:8274ms step_avg:91.93ms
step:91/1750 train_time:8366ms step_avg:91.93ms
step:92/1750 train_time:8458ms step_avg:91.94ms
step:93/1750 train_time:8551ms step_avg:91.95ms
step:94/1750 train_time:8643ms step_avg:91.95ms
step:95/1750 train_time:8736ms step_avg:91.96ms
step:96/1750 train_time:8830ms step_avg:91.98ms
step:97/1750 train_time:8921ms step_avg:91.97ms
step:98/1750 train_time:9014ms step_avg:91.98ms
step:99/1750 train_time:9105ms step_avg:91.97ms
step:100/1750 train_time:9198ms step_avg:91.98ms
step:101/1750 train_time:9291ms step_avg:91.99ms
step:102/1750 train_time:9383ms step_avg:91.99ms
step:103/1750 train_time:9477ms step_avg:92.01ms
step:104/1750 train_time:9569ms step_avg:92.01ms
step:105/1750 train_time:9661ms step_avg:92.01ms
step:106/1750 train_time:9756ms step_avg:92.04ms
step:107/1750 train_time:9849ms step_avg:92.04ms
step:108/1750 train_time:9941ms step_avg:92.04ms
step:109/1750 train_time:10034ms step_avg:92.06ms
step:110/1750 train_time:10126ms step_avg:92.05ms
step:111/1750 train_time:10218ms step_avg:92.05ms
step:112/1750 train_time:10311ms step_avg:92.06ms
step:113/1750 train_time:10403ms step_avg:92.07ms
step:114/1750 train_time:10497ms step_avg:92.08ms
step:115/1750 train_time:10590ms step_avg:92.09ms
step:116/1750 train_time:10681ms step_avg:92.08ms
step:117/1750 train_time:10775ms step_avg:92.10ms
step:118/1750 train_time:10867ms step_avg:92.10ms
step:119/1750 train_time:10959ms step_avg:92.09ms
step:120/1750 train_time:11052ms step_avg:92.10ms
step:121/1750 train_time:11144ms step_avg:92.10ms
step:122/1750 train_time:11237ms step_avg:92.10ms
step:123/1750 train_time:11329ms step_avg:92.11ms
step:124/1750 train_time:11421ms step_avg:92.10ms
step:125/1750 train_time:11514ms step_avg:92.12ms
step:125/1750 val_loss:5.1617 train_time:11617ms step_avg:92.94ms
step:126/1750 train_time:11635ms step_avg:92.34ms
step:127/1750 train_time:11704ms step_avg:92.16ms
step:128/1750 train_time:11799ms step_avg:92.18ms
step:129/1750 train_time:11902ms step_avg:92.26ms
step:130/1750 train_time:11996ms step_avg:92.28ms
step:131/1750 train_time:12089ms step_avg:92.28ms
step:132/1750 train_time:12181ms step_avg:92.28ms
step:133/1750 train_time:12273ms step_avg:92.28ms
step:134/1750 train_time:12365ms step_avg:92.27ms
step:135/1750 train_time:12456ms step_avg:92.27ms
step:136/1750 train_time:12550ms step_avg:92.28ms
step:137/1750 train_time:12642ms step_avg:92.28ms
step:138/1750 train_time:12735ms step_avg:92.29ms
step:139/1750 train_time:12831ms step_avg:92.31ms
step:140/1750 train_time:12925ms step_avg:92.32ms
step:141/1750 train_time:13018ms step_avg:92.33ms
step:142/1750 train_time:13113ms step_avg:92.34ms
step:143/1750 train_time:13204ms step_avg:92.34ms
step:144/1750 train_time:13297ms step_avg:92.34ms
step:145/1750 train_time:13390ms step_avg:92.35ms
step:146/1750 train_time:13482ms step_avg:92.34ms
step:147/1750 train_time:13575ms step_avg:92.35ms
step:148/1750 train_time:13668ms step_avg:92.35ms
step:149/1750 train_time:13760ms step_avg:92.35ms
step:150/1750 train_time:13855ms step_avg:92.37ms
step:151/1750 train_time:13949ms step_avg:92.38ms
step:152/1750 train_time:14042ms step_avg:92.38ms
step:153/1750 train_time:14135ms step_avg:92.39ms
step:154/1750 train_time:14227ms step_avg:92.38ms
step:155/1750 train_time:14320ms step_avg:92.39ms
step:156/1750 train_time:14413ms step_avg:92.39ms
step:157/1750 train_time:14506ms step_avg:92.39ms
step:158/1750 train_time:14598ms step_avg:92.39ms
step:159/1750 train_time:14692ms step_avg:92.40ms
step:160/1750 train_time:14786ms step_avg:92.41ms
step:161/1750 train_time:14879ms step_avg:92.42ms
step:162/1750 train_time:14974ms step_avg:92.43ms
step:163/1750 train_time:15068ms step_avg:92.44ms
step:164/1750 train_time:15159ms step_avg:92.43ms
step:165/1750 train_time:15253ms step_avg:92.44ms
step:166/1750 train_time:15345ms step_avg:92.44ms
step:167/1750 train_time:15437ms step_avg:92.44ms
step:168/1750 train_time:15531ms step_avg:92.44ms
step:169/1750 train_time:15622ms step_avg:92.44ms
step:170/1750 train_time:15715ms step_avg:92.44ms
step:171/1750 train_time:15809ms step_avg:92.45ms
step:172/1750 train_time:15901ms step_avg:92.45ms
step:173/1750 train_time:15995ms step_avg:92.46ms
step:174/1750 train_time:16088ms step_avg:92.46ms
step:175/1750 train_time:16181ms step_avg:92.46ms
step:176/1750 train_time:16274ms step_avg:92.47ms
step:177/1750 train_time:16367ms step_avg:92.47ms
step:178/1750 train_time:16459ms step_avg:92.46ms
step:179/1750 train_time:16553ms step_avg:92.47ms
step:180/1750 train_time:16646ms step_avg:92.48ms
step:181/1750 train_time:16737ms step_avg:92.47ms
step:182/1750 train_time:16831ms step_avg:92.48ms
step:183/1750 train_time:16924ms step_avg:92.48ms
step:184/1750 train_time:17018ms step_avg:92.49ms
step:185/1750 train_time:17112ms step_avg:92.50ms
step:186/1750 train_time:17206ms step_avg:92.50ms
step:187/1750 train_time:17298ms step_avg:92.50ms
step:188/1750 train_time:17391ms step_avg:92.51ms
step:189/1750 train_time:17483ms step_avg:92.50ms
step:190/1750 train_time:17577ms step_avg:92.51ms
step:191/1750 train_time:17670ms step_avg:92.51ms
step:192/1750 train_time:17762ms step_avg:92.51ms
step:193/1750 train_time:17856ms step_avg:92.52ms
step:194/1750 train_time:17948ms step_avg:92.52ms
step:195/1750 train_time:18041ms step_avg:92.52ms
step:196/1750 train_time:18134ms step_avg:92.52ms
step:197/1750 train_time:18226ms step_avg:92.52ms
step:198/1750 train_time:18319ms step_avg:92.52ms
step:199/1750 train_time:18413ms step_avg:92.53ms
step:200/1750 train_time:18506ms step_avg:92.53ms
step:201/1750 train_time:18597ms step_avg:92.52ms
step:202/1750 train_time:18690ms step_avg:92.53ms
step:203/1750 train_time:18782ms step_avg:92.52ms
step:204/1750 train_time:18875ms step_avg:92.53ms
step:205/1750 train_time:18969ms step_avg:92.53ms
step:206/1750 train_time:19061ms step_avg:92.53ms
step:207/1750 train_time:19155ms step_avg:92.54ms
step:208/1750 train_time:19249ms step_avg:92.54ms
step:209/1750 train_time:19341ms step_avg:92.54ms
step:210/1750 train_time:19434ms step_avg:92.54ms
step:211/1750 train_time:19527ms step_avg:92.55ms
step:212/1750 train_time:19620ms step_avg:92.55ms
step:213/1750 train_time:19714ms step_avg:92.55ms
step:214/1750 train_time:19806ms step_avg:92.55ms
step:215/1750 train_time:19898ms step_avg:92.55ms
step:216/1750 train_time:19992ms step_avg:92.55ms
step:217/1750 train_time:20085ms step_avg:92.56ms
step:218/1750 train_time:20177ms step_avg:92.56ms
step:219/1750 train_time:20272ms step_avg:92.57ms
step:220/1750 train_time:20364ms step_avg:92.56ms
step:221/1750 train_time:20458ms step_avg:92.57ms
step:222/1750 train_time:20552ms step_avg:92.58ms
step:223/1750 train_time:20645ms step_avg:92.58ms
step:224/1750 train_time:20738ms step_avg:92.58ms
step:225/1750 train_time:20832ms step_avg:92.59ms
step:226/1750 train_time:20925ms step_avg:92.59ms
step:227/1750 train_time:21017ms step_avg:92.59ms
step:228/1750 train_time:21111ms step_avg:92.59ms
step:229/1750 train_time:21203ms step_avg:92.59ms
step:230/1750 train_time:21296ms step_avg:92.59ms
step:231/1750 train_time:21389ms step_avg:92.59ms
step:232/1750 train_time:21482ms step_avg:92.59ms
step:233/1750 train_time:21576ms step_avg:92.60ms
step:234/1750 train_time:21668ms step_avg:92.60ms
step:235/1750 train_time:21760ms step_avg:92.60ms
step:236/1750 train_time:21854ms step_avg:92.60ms
step:237/1750 train_time:21947ms step_avg:92.60ms
step:238/1750 train_time:22040ms step_avg:92.60ms
step:239/1750 train_time:22134ms step_avg:92.61ms
step:240/1750 train_time:22227ms step_avg:92.61ms
step:241/1750 train_time:22319ms step_avg:92.61ms
step:242/1750 train_time:22413ms step_avg:92.62ms
step:243/1750 train_time:22507ms step_avg:92.62ms
step:244/1750 train_time:22599ms step_avg:92.62ms
step:245/1750 train_time:22693ms step_avg:92.62ms
step:246/1750 train_time:22786ms step_avg:92.63ms
step:247/1750 train_time:22879ms step_avg:92.63ms
step:248/1750 train_time:22972ms step_avg:92.63ms
step:249/1750 train_time:23064ms step_avg:92.63ms
step:250/1750 train_time:23157ms step_avg:92.63ms
step:250/1750 val_loss:4.6472 train_time:23261ms step_avg:93.04ms
step:251/1750 train_time:23279ms step_avg:92.74ms
step:252/1750 train_time:23349ms step_avg:92.65ms
step:253/1750 train_time:23452ms step_avg:92.69ms
step:254/1750 train_time:23543ms step_avg:92.69ms
step:255/1750 train_time:23636ms step_avg:92.69ms
step:256/1750 train_time:23729ms step_avg:92.69ms
step:257/1750 train_time:23821ms step_avg:92.69ms
step:258/1750 train_time:23914ms step_avg:92.69ms
step:259/1750 train_time:24005ms step_avg:92.68ms
step:260/1750 train_time:24098ms step_avg:92.68ms
step:261/1750 train_time:24190ms step_avg:92.68ms
step:262/1750 train_time:24283ms step_avg:92.68ms
step:263/1750 train_time:24380ms step_avg:92.70ms
step:264/1750 train_time:24475ms step_avg:92.71ms
step:265/1750 train_time:24570ms step_avg:92.72ms
step:266/1750 train_time:24663ms step_avg:92.72ms
step:267/1750 train_time:24757ms step_avg:92.72ms
step:268/1750 train_time:24850ms step_avg:92.72ms
step:269/1750 train_time:24941ms step_avg:92.72ms
step:270/1750 train_time:25034ms step_avg:92.72ms
step:271/1750 train_time:25127ms step_avg:92.72ms
step:272/1750 train_time:25221ms step_avg:92.72ms
step:273/1750 train_time:25315ms step_avg:92.73ms
step:274/1750 train_time:25408ms step_avg:92.73ms
step:275/1750 train_time:25502ms step_avg:92.74ms
step:276/1750 train_time:25598ms step_avg:92.75ms
step:277/1750 train_time:25690ms step_avg:92.74ms
step:278/1750 train_time:25783ms step_avg:92.75ms
step:279/1750 train_time:25876ms step_avg:92.75ms
step:280/1750 train_time:25970ms step_avg:92.75ms
step:281/1750 train_time:26063ms step_avg:92.75ms
step:282/1750 train_time:26157ms step_avg:92.75ms
step:283/1750 train_time:26250ms step_avg:92.76ms
step:284/1750 train_time:26342ms step_avg:92.75ms
step:285/1750 train_time:26436ms step_avg:92.76ms
step:286/1750 train_time:26531ms step_avg:92.77ms
step:287/1750 train_time:26625ms step_avg:92.77ms
step:288/1750 train_time:26718ms step_avg:92.77ms
step:289/1750 train_time:26812ms step_avg:92.77ms
step:290/1750 train_time:26904ms step_avg:92.77ms
step:291/1750 train_time:26997ms step_avg:92.77ms
step:292/1750 train_time:27091ms step_avg:92.78ms
step:293/1750 train_time:27183ms step_avg:92.78ms
step:294/1750 train_time:27277ms step_avg:92.78ms
step:295/1750 train_time:27370ms step_avg:92.78ms
step:296/1750 train_time:27463ms step_avg:92.78ms
step:297/1750 train_time:27558ms step_avg:92.79ms
step:298/1750 train_time:27652ms step_avg:92.79ms
step:299/1750 train_time:27745ms step_avg:92.79ms
step:300/1750 train_time:27838ms step_avg:92.79ms
step:301/1750 train_time:27932ms step_avg:92.80ms
step:302/1750 train_time:28024ms step_avg:92.80ms
step:303/1750 train_time:28118ms step_avg:92.80ms
step:304/1750 train_time:28211ms step_avg:92.80ms
step:305/1750 train_time:28303ms step_avg:92.80ms
step:306/1750 train_time:28397ms step_avg:92.80ms
step:307/1750 train_time:28491ms step_avg:92.80ms
step:308/1750 train_time:28583ms step_avg:92.80ms
step:309/1750 train_time:28678ms step_avg:92.81ms
step:310/1750 train_time:28773ms step_avg:92.82ms
step:311/1750 train_time:28866ms step_avg:92.82ms
step:312/1750 train_time:28959ms step_avg:92.82ms
step:313/1750 train_time:29052ms step_avg:92.82ms
step:314/1750 train_time:29144ms step_avg:92.82ms
step:315/1750 train_time:29238ms step_avg:92.82ms
step:316/1750 train_time:29332ms step_avg:92.82ms
step:317/1750 train_time:29425ms step_avg:92.82ms
step:318/1750 train_time:29519ms step_avg:92.83ms
step:319/1750 train_time:29614ms step_avg:92.83ms
step:320/1750 train_time:29706ms step_avg:92.83ms
step:321/1750 train_time:29800ms step_avg:92.84ms
step:322/1750 train_time:29894ms step_avg:92.84ms
step:323/1750 train_time:29988ms step_avg:92.84ms
step:324/1750 train_time:30081ms step_avg:92.84ms
step:325/1750 train_time:30175ms step_avg:92.85ms
step:326/1750 train_time:30268ms step_avg:92.85ms
step:327/1750 train_time:30361ms step_avg:92.85ms
step:328/1750 train_time:30455ms step_avg:92.85ms
step:329/1750 train_time:30548ms step_avg:92.85ms
step:330/1750 train_time:30641ms step_avg:92.85ms
step:331/1750 train_time:30735ms step_avg:92.86ms
step:332/1750 train_time:30828ms step_avg:92.86ms
step:333/1750 train_time:30922ms step_avg:92.86ms
step:334/1750 train_time:31016ms step_avg:92.86ms
step:335/1750 train_time:31108ms step_avg:92.86ms
step:336/1750 train_time:31201ms step_avg:92.86ms
step:337/1750 train_time:31296ms step_avg:92.87ms
step:338/1750 train_time:31389ms step_avg:92.87ms
step:339/1750 train_time:31483ms step_avg:92.87ms
step:340/1750 train_time:31577ms step_avg:92.87ms
step:341/1750 train_time:31670ms step_avg:92.87ms
step:342/1750 train_time:31764ms step_avg:92.88ms
step:343/1750 train_time:31858ms step_avg:92.88ms
step:344/1750 train_time:31951ms step_avg:92.88ms
step:345/1750 train_time:32044ms step_avg:92.88ms
step:346/1750 train_time:32138ms step_avg:92.88ms
step:347/1750 train_time:32231ms step_avg:92.88ms
step:348/1750 train_time:32324ms step_avg:92.89ms
step:349/1750 train_time:32418ms step_avg:92.89ms
step:350/1750 train_time:32511ms step_avg:92.89ms
step:351/1750 train_time:32605ms step_avg:92.89ms
step:352/1750 train_time:32699ms step_avg:92.90ms
step:353/1750 train_time:32793ms step_avg:92.90ms
step:354/1750 train_time:32886ms step_avg:92.90ms
step:355/1750 train_time:32980ms step_avg:92.90ms
step:356/1750 train_time:33073ms step_avg:92.90ms
step:357/1750 train_time:33166ms step_avg:92.90ms
step:358/1750 train_time:33260ms step_avg:92.90ms
step:359/1750 train_time:33354ms step_avg:92.91ms
step:360/1750 train_time:33446ms step_avg:92.91ms
step:361/1750 train_time:33539ms step_avg:92.91ms
step:362/1750 train_time:33634ms step_avg:92.91ms
step:363/1750 train_time:33728ms step_avg:92.91ms
step:364/1750 train_time:33820ms step_avg:92.91ms
step:365/1750 train_time:33915ms step_avg:92.92ms
step:366/1750 train_time:34007ms step_avg:92.91ms
step:367/1750 train_time:34101ms step_avg:92.92ms
step:368/1750 train_time:34193ms step_avg:92.92ms
step:369/1750 train_time:34286ms step_avg:92.92ms
step:370/1750 train_time:34380ms step_avg:92.92ms
step:371/1750 train_time:34473ms step_avg:92.92ms
step:372/1750 train_time:34567ms step_avg:92.92ms
step:373/1750 train_time:34660ms step_avg:92.92ms
step:374/1750 train_time:34755ms step_avg:92.93ms
step:375/1750 train_time:34848ms step_avg:92.93ms
step:375/1750 val_loss:4.2760 train_time:34951ms step_avg:93.20ms
step:376/1750 train_time:34967ms step_avg:93.00ms
step:377/1750 train_time:35042ms step_avg:92.95ms
step:378/1750 train_time:35135ms step_avg:92.95ms
step:379/1750 train_time:35232ms step_avg:92.96ms
step:380/1750 train_time:35326ms step_avg:92.96ms
step:381/1750 train_time:35419ms step_avg:92.96ms
step:382/1750 train_time:35512ms step_avg:92.96ms
step:383/1750 train_time:35604ms step_avg:92.96ms
step:384/1750 train_time:35696ms step_avg:92.96ms
step:385/1750 train_time:35789ms step_avg:92.96ms
step:386/1750 train_time:35883ms step_avg:92.96ms
step:387/1750 train_time:35977ms step_avg:92.96ms
step:388/1750 train_time:36072ms step_avg:92.97ms
step:389/1750 train_time:36166ms step_avg:92.97ms
step:390/1750 train_time:36260ms step_avg:92.97ms
step:391/1750 train_time:36356ms step_avg:92.98ms
step:392/1750 train_time:36451ms step_avg:92.99ms
step:393/1750 train_time:36546ms step_avg:92.99ms
step:394/1750 train_time:36640ms step_avg:93.00ms
step:395/1750 train_time:36734ms step_avg:93.00ms
step:396/1750 train_time:36830ms step_avg:93.00ms
step:397/1750 train_time:36925ms step_avg:93.01ms
step:398/1750 train_time:37021ms step_avg:93.02ms
step:399/1750 train_time:37117ms step_avg:93.02ms
step:400/1750 train_time:37214ms step_avg:93.03ms
step:401/1750 train_time:37310ms step_avg:93.04ms
step:402/1750 train_time:37404ms step_avg:93.05ms
step:403/1750 train_time:37497ms step_avg:93.05ms
step:404/1750 train_time:37594ms step_avg:93.05ms
step:405/1750 train_time:37691ms step_avg:93.06ms
step:406/1750 train_time:37785ms step_avg:93.07ms
step:407/1750 train_time:37879ms step_avg:93.07ms
step:408/1750 train_time:37974ms step_avg:93.07ms
step:409/1750 train_time:38071ms step_avg:93.08ms
step:410/1750 train_time:38167ms step_avg:93.09ms
step:411/1750 train_time:38263ms step_avg:93.10ms
step:412/1750 train_time:38358ms step_avg:93.10ms
step:413/1750 train_time:38453ms step_avg:93.11ms
step:414/1750 train_time:38548ms step_avg:93.11ms
step:415/1750 train_time:38642ms step_avg:93.11ms
step:416/1750 train_time:38738ms step_avg:93.12ms
step:417/1750 train_time:38834ms step_avg:93.13ms
step:418/1750 train_time:38929ms step_avg:93.13ms
step:419/1750 train_time:39025ms step_avg:93.14ms
step:420/1750 train_time:39119ms step_avg:93.14ms
step:421/1750 train_time:39216ms step_avg:93.15ms
step:422/1750 train_time:39312ms step_avg:93.16ms
step:423/1750 train_time:39406ms step_avg:93.16ms
step:424/1750 train_time:39501ms step_avg:93.16ms
step:425/1750 train_time:39596ms step_avg:93.17ms
step:426/1750 train_time:39691ms step_avg:93.17ms
step:427/1750 train_time:39787ms step_avg:93.18ms
step:428/1750 train_time:39881ms step_avg:93.18ms
step:429/1750 train_time:39977ms step_avg:93.19ms
step:430/1750 train_time:40074ms step_avg:93.19ms
step:431/1750 train_time:40170ms step_avg:93.20ms
step:432/1750 train_time:40265ms step_avg:93.21ms
step:433/1750 train_time:40360ms step_avg:93.21ms
step:434/1750 train_time:40455ms step_avg:93.21ms
step:435/1750 train_time:40551ms step_avg:93.22ms
step:436/1750 train_time:40647ms step_avg:93.23ms
step:437/1750 train_time:40741ms step_avg:93.23ms
step:438/1750 train_time:40836ms step_avg:93.23ms
step:439/1750 train_time:40932ms step_avg:93.24ms
step:440/1750 train_time:41027ms step_avg:93.24ms
step:441/1750 train_time:41122ms step_avg:93.25ms
step:442/1750 train_time:41216ms step_avg:93.25ms
step:443/1750 train_time:41312ms step_avg:93.26ms
step:444/1750 train_time:41408ms step_avg:93.26ms
step:445/1750 train_time:41503ms step_avg:93.26ms
step:446/1750 train_time:41597ms step_avg:93.27ms
step:447/1750 train_time:41694ms step_avg:93.27ms
step:448/1750 train_time:41788ms step_avg:93.28ms
step:449/1750 train_time:41883ms step_avg:93.28ms
step:450/1750 train_time:41978ms step_avg:93.28ms
step:451/1750 train_time:42073ms step_avg:93.29ms
step:452/1750 train_time:42169ms step_avg:93.29ms
step:453/1750 train_time:42264ms step_avg:93.30ms
step:454/1750 train_time:42359ms step_avg:93.30ms
step:455/1750 train_time:42454ms step_avg:93.31ms
step:456/1750 train_time:42551ms step_avg:93.31ms
step:457/1750 train_time:42647ms step_avg:93.32ms
step:458/1750 train_time:42742ms step_avg:93.32ms
step:459/1750 train_time:42837ms step_avg:93.33ms
step:460/1750 train_time:42933ms step_avg:93.33ms
step:461/1750 train_time:43029ms step_avg:93.34ms
step:462/1750 train_time:43124ms step_avg:93.34ms
step:463/1750 train_time:43220ms step_avg:93.35ms
step:464/1750 train_time:43316ms step_avg:93.35ms
step:465/1750 train_time:43413ms step_avg:93.36ms
step:466/1750 train_time:43507ms step_avg:93.36ms
step:467/1750 train_time:43601ms step_avg:93.36ms
step:468/1750 train_time:43697ms step_avg:93.37ms
step:469/1750 train_time:43793ms step_avg:93.38ms
step:470/1750 train_time:43888ms step_avg:93.38ms
step:471/1750 train_time:43983ms step_avg:93.38ms
step:472/1750 train_time:44078ms step_avg:93.39ms
step:473/1750 train_time:44174ms step_avg:93.39ms
step:474/1750 train_time:44269ms step_avg:93.39ms
step:475/1750 train_time:44364ms step_avg:93.40ms
step:476/1750 train_time:44459ms step_avg:93.40ms
step:477/1750 train_time:44555ms step_avg:93.41ms
step:478/1750 train_time:44652ms step_avg:93.41ms
step:479/1750 train_time:44746ms step_avg:93.42ms
step:480/1750 train_time:44841ms step_avg:93.42ms
step:481/1750 train_time:44936ms step_avg:93.42ms
step:482/1750 train_time:45032ms step_avg:93.43ms
step:483/1750 train_time:45128ms step_avg:93.43ms
step:484/1750 train_time:45222ms step_avg:93.43ms
step:485/1750 train_time:45318ms step_avg:93.44ms
step:486/1750 train_time:45414ms step_avg:93.44ms
step:487/1750 train_time:45509ms step_avg:93.45ms
step:488/1750 train_time:45604ms step_avg:93.45ms
step:489/1750 train_time:45700ms step_avg:93.46ms
step:490/1750 train_time:45795ms step_avg:93.46ms
step:491/1750 train_time:45890ms step_avg:93.46ms
step:492/1750 train_time:45985ms step_avg:93.46ms
step:493/1750 train_time:46080ms step_avg:93.47ms
step:494/1750 train_time:46175ms step_avg:93.47ms
step:495/1750 train_time:46271ms step_avg:93.48ms
step:496/1750 train_time:46365ms step_avg:93.48ms
step:497/1750 train_time:46461ms step_avg:93.48ms
step:498/1750 train_time:46558ms step_avg:93.49ms
step:499/1750 train_time:46653ms step_avg:93.49ms
step:500/1750 train_time:46749ms step_avg:93.50ms
step:500/1750 val_loss:4.0591 train_time:46852ms step_avg:93.70ms
step:501/1750 train_time:46869ms step_avg:93.55ms
step:502/1750 train_time:46945ms step_avg:93.52ms
step:503/1750 train_time:47043ms step_avg:93.52ms
step:504/1750 train_time:47140ms step_avg:93.53ms
step:505/1750 train_time:47236ms step_avg:93.54ms
step:506/1750 train_time:47331ms step_avg:93.54ms
step:507/1750 train_time:47424ms step_avg:93.54ms
step:508/1750 train_time:47519ms step_avg:93.54ms
step:509/1750 train_time:47614ms step_avg:93.54ms
step:510/1750 train_time:47708ms step_avg:93.54ms
step:511/1750 train_time:47801ms step_avg:93.54ms
step:512/1750 train_time:47899ms step_avg:93.55ms
step:513/1750 train_time:47996ms step_avg:93.56ms
step:514/1750 train_time:48092ms step_avg:93.56ms
step:515/1750 train_time:48187ms step_avg:93.57ms
step:516/1750 train_time:48282ms step_avg:93.57ms
step:517/1750 train_time:48378ms step_avg:93.57ms
step:518/1750 train_time:48472ms step_avg:93.58ms
step:519/1750 train_time:48567ms step_avg:93.58ms
step:520/1750 train_time:48661ms step_avg:93.58ms
step:521/1750 train_time:48757ms step_avg:93.58ms
step:522/1750 train_time:48854ms step_avg:93.59ms
step:523/1750 train_time:48951ms step_avg:93.60ms
step:524/1750 train_time:49046ms step_avg:93.60ms
step:525/1750 train_time:49143ms step_avg:93.61ms
step:526/1750 train_time:49240ms step_avg:93.61ms
step:527/1750 train_time:49336ms step_avg:93.62ms
step:528/1750 train_time:49431ms step_avg:93.62ms
step:529/1750 train_time:49525ms step_avg:93.62ms
step:530/1750 train_time:49620ms step_avg:93.62ms
step:531/1750 train_time:49716ms step_avg:93.63ms
step:532/1750 train_time:49811ms step_avg:93.63ms
step:533/1750 train_time:49907ms step_avg:93.63ms
step:534/1750 train_time:50003ms step_avg:93.64ms
step:535/1750 train_time:50100ms step_avg:93.64ms
step:536/1750 train_time:50196ms step_avg:93.65ms
step:537/1750 train_time:50292ms step_avg:93.65ms
step:538/1750 train_time:50388ms step_avg:93.66ms
step:539/1750 train_time:50483ms step_avg:93.66ms
step:540/1750 train_time:50579ms step_avg:93.66ms
step:541/1750 train_time:50674ms step_avg:93.67ms
step:542/1750 train_time:50769ms step_avg:93.67ms
step:543/1750 train_time:50864ms step_avg:93.67ms
step:544/1750 train_time:50960ms step_avg:93.68ms
step:545/1750 train_time:51057ms step_avg:93.68ms
step:546/1750 train_time:51153ms step_avg:93.69ms
step:547/1750 train_time:51248ms step_avg:93.69ms
step:548/1750 train_time:51343ms step_avg:93.69ms
step:549/1750 train_time:51439ms step_avg:93.70ms
step:550/1750 train_time:51535ms step_avg:93.70ms
step:551/1750 train_time:51630ms step_avg:93.70ms
step:552/1750 train_time:51725ms step_avg:93.71ms
step:553/1750 train_time:51821ms step_avg:93.71ms
step:554/1750 train_time:51916ms step_avg:93.71ms
step:555/1750 train_time:52013ms step_avg:93.72ms
step:556/1750 train_time:52108ms step_avg:93.72ms
step:557/1750 train_time:52204ms step_avg:93.72ms
step:558/1750 train_time:52299ms step_avg:93.73ms
step:559/1750 train_time:52395ms step_avg:93.73ms
step:560/1750 train_time:52491ms step_avg:93.73ms
step:561/1750 train_time:52587ms step_avg:93.74ms
step:562/1750 train_time:52681ms step_avg:93.74ms
step:563/1750 train_time:52778ms step_avg:93.74ms
step:564/1750 train_time:52873ms step_avg:93.75ms
step:565/1750 train_time:52968ms step_avg:93.75ms
step:566/1750 train_time:53063ms step_avg:93.75ms
step:567/1750 train_time:53160ms step_avg:93.76ms
step:568/1750 train_time:53257ms step_avg:93.76ms
step:569/1750 train_time:53352ms step_avg:93.76ms
step:570/1750 train_time:53449ms step_avg:93.77ms
step:571/1750 train_time:53545ms step_avg:93.77ms
step:572/1750 train_time:53640ms step_avg:93.78ms
step:573/1750 train_time:53735ms step_avg:93.78ms
step:574/1750 train_time:53830ms step_avg:93.78ms
step:575/1750 train_time:53925ms step_avg:93.78ms
step:576/1750 train_time:54020ms step_avg:93.79ms
step:577/1750 train_time:54116ms step_avg:93.79ms
step:578/1750 train_time:54211ms step_avg:93.79ms
step:579/1750 train_time:54306ms step_avg:93.79ms
step:580/1750 train_time:54402ms step_avg:93.80ms
step:581/1750 train_time:54499ms step_avg:93.80ms
step:582/1750 train_time:54594ms step_avg:93.80ms
step:583/1750 train_time:54691ms step_avg:93.81ms
step:584/1750 train_time:54786ms step_avg:93.81ms
step:585/1750 train_time:54882ms step_avg:93.82ms
step:586/1750 train_time:54978ms step_avg:93.82ms
step:587/1750 train_time:55073ms step_avg:93.82ms
step:588/1750 train_time:55168ms step_avg:93.82ms
step:589/1750 train_time:55263ms step_avg:93.83ms
step:590/1750 train_time:55360ms step_avg:93.83ms
step:591/1750 train_time:55456ms step_avg:93.83ms
step:592/1750 train_time:55551ms step_avg:93.84ms
step:593/1750 train_time:55646ms step_avg:93.84ms
step:594/1750 train_time:55742ms step_avg:93.84ms
step:595/1750 train_time:55838ms step_avg:93.85ms
step:596/1750 train_time:55933ms step_avg:93.85ms
step:597/1750 train_time:56029ms step_avg:93.85ms
step:598/1750 train_time:56124ms step_avg:93.85ms
step:599/1750 train_time:56219ms step_avg:93.85ms
step:600/1750 train_time:56317ms step_avg:93.86ms
step:601/1750 train_time:56412ms step_avg:93.86ms
step:602/1750 train_time:56506ms step_avg:93.86ms
step:603/1750 train_time:56601ms step_avg:93.87ms
step:604/1750 train_time:56697ms step_avg:93.87ms
step:605/1750 train_time:56793ms step_avg:93.87ms
step:606/1750 train_time:56888ms step_avg:93.87ms
step:607/1750 train_time:56983ms step_avg:93.88ms
step:608/1750 train_time:57079ms step_avg:93.88ms
step:609/1750 train_time:57174ms step_avg:93.88ms
step:610/1750 train_time:57269ms step_avg:93.88ms
step:611/1750 train_time:57364ms step_avg:93.89ms
step:612/1750 train_time:57460ms step_avg:93.89ms
step:613/1750 train_time:57557ms step_avg:93.89ms
step:614/1750 train_time:57653ms step_avg:93.90ms
step:615/1750 train_time:57748ms step_avg:93.90ms
step:616/1750 train_time:57844ms step_avg:93.90ms
step:617/1750 train_time:57940ms step_avg:93.91ms
step:618/1750 train_time:58036ms step_avg:93.91ms
step:619/1750 train_time:58130ms step_avg:93.91ms
step:620/1750 train_time:58225ms step_avg:93.91ms
step:621/1750 train_time:58321ms step_avg:93.92ms
step:622/1750 train_time:58417ms step_avg:93.92ms
step:623/1750 train_time:58513ms step_avg:93.92ms
step:624/1750 train_time:58608ms step_avg:93.92ms
step:625/1750 train_time:58704ms step_avg:93.93ms
step:625/1750 val_loss:3.9444 train_time:58809ms step_avg:94.10ms
step:626/1750 train_time:58826ms step_avg:93.97ms
step:627/1750 train_time:58901ms step_avg:93.94ms
step:628/1750 train_time:59001ms step_avg:93.95ms
step:629/1750 train_time:59098ms step_avg:93.96ms
step:630/1750 train_time:59193ms step_avg:93.96ms
step:631/1750 train_time:59287ms step_avg:93.96ms
step:632/1750 train_time:59382ms step_avg:93.96ms
step:633/1750 train_time:59476ms step_avg:93.96ms
step:634/1750 train_time:59571ms step_avg:93.96ms
step:635/1750 train_time:59666ms step_avg:93.96ms
step:636/1750 train_time:59761ms step_avg:93.96ms
step:637/1750 train_time:59862ms step_avg:93.97ms
step:638/1750 train_time:59959ms step_avg:93.98ms
step:639/1750 train_time:60055ms step_avg:93.98ms
step:640/1750 train_time:60150ms step_avg:93.98ms
step:641/1750 train_time:60244ms step_avg:93.98ms
step:642/1750 train_time:60340ms step_avg:93.99ms
step:643/1750 train_time:60435ms step_avg:93.99ms
step:644/1750 train_time:60529ms step_avg:93.99ms
step:645/1750 train_time:60624ms step_avg:93.99ms
step:646/1750 train_time:60719ms step_avg:93.99ms
step:647/1750 train_time:60818ms step_avg:94.00ms
step:648/1750 train_time:60915ms step_avg:94.00ms
step:649/1750 train_time:61011ms step_avg:94.01ms
step:650/1750 train_time:61107ms step_avg:94.01ms
step:651/1750 train_time:61204ms step_avg:94.02ms
step:652/1750 train_time:61302ms step_avg:94.02ms
step:653/1750 train_time:61397ms step_avg:94.02ms
step:654/1750 train_time:61492ms step_avg:94.02ms
step:655/1750 train_time:61590ms step_avg:94.03ms
step:656/1750 train_time:61688ms step_avg:94.04ms
step:657/1750 train_time:61784ms step_avg:94.04ms
step:658/1750 train_time:61883ms step_avg:94.05ms
step:659/1750 train_time:61982ms step_avg:94.05ms
step:660/1750 train_time:62079ms step_avg:94.06ms
step:661/1750 train_time:62176ms step_avg:94.06ms
step:662/1750 train_time:62272ms step_avg:94.07ms
step:663/1750 train_time:62369ms step_avg:94.07ms
step:664/1750 train_time:62467ms step_avg:94.08ms
step:665/1750 train_time:62564ms step_avg:94.08ms
step:666/1750 train_time:62663ms step_avg:94.09ms
step:667/1750 train_time:62759ms step_avg:94.09ms
step:668/1750 train_time:62856ms step_avg:94.10ms
step:669/1750 train_time:62954ms step_avg:94.10ms
step:670/1750 train_time:63050ms step_avg:94.10ms
step:671/1750 train_time:63148ms step_avg:94.11ms
step:672/1750 train_time:63245ms step_avg:94.11ms
step:673/1750 train_time:63342ms step_avg:94.12ms
step:674/1750 train_time:63439ms step_avg:94.12ms
step:675/1750 train_time:63537ms step_avg:94.13ms
step:676/1750 train_time:63634ms step_avg:94.13ms
step:677/1750 train_time:63730ms step_avg:94.14ms
step:678/1750 train_time:63828ms step_avg:94.14ms
step:679/1750 train_time:63925ms step_avg:94.15ms
step:680/1750 train_time:64023ms step_avg:94.15ms
step:681/1750 train_time:64120ms step_avg:94.16ms
step:682/1750 train_time:64217ms step_avg:94.16ms
step:683/1750 train_time:64312ms step_avg:94.16ms
step:684/1750 train_time:64409ms step_avg:94.17ms
step:685/1750 train_time:64507ms step_avg:94.17ms
step:686/1750 train_time:64604ms step_avg:94.17ms
step:687/1750 train_time:64701ms step_avg:94.18ms
step:688/1750 train_time:64799ms step_avg:94.18ms
step:689/1750 train_time:64896ms step_avg:94.19ms
step:690/1750 train_time:64993ms step_avg:94.19ms
step:691/1750 train_time:65090ms step_avg:94.20ms
step:692/1750 train_time:65188ms step_avg:94.20ms
step:693/1750 train_time:65285ms step_avg:94.21ms
step:694/1750 train_time:65382ms step_avg:94.21ms
step:695/1750 train_time:65479ms step_avg:94.21ms
step:696/1750 train_time:65576ms step_avg:94.22ms
step:697/1750 train_time:65673ms step_avg:94.22ms
step:698/1750 train_time:65769ms step_avg:94.22ms
step:699/1750 train_time:65867ms step_avg:94.23ms
step:700/1750 train_time:65964ms step_avg:94.23ms
step:701/1750 train_time:66062ms step_avg:94.24ms
step:702/1750 train_time:66160ms step_avg:94.25ms
step:703/1750 train_time:66258ms step_avg:94.25ms
step:704/1750 train_time:66355ms step_avg:94.25ms
step:705/1750 train_time:66452ms step_avg:94.26ms
step:706/1750 train_time:66549ms step_avg:94.26ms
step:707/1750 train_time:66646ms step_avg:94.27ms
step:708/1750 train_time:66744ms step_avg:94.27ms
step:709/1750 train_time:66842ms step_avg:94.28ms
step:710/1750 train_time:66939ms step_avg:94.28ms
step:711/1750 train_time:67037ms step_avg:94.29ms
step:712/1750 train_time:67133ms step_avg:94.29ms
step:713/1750 train_time:67230ms step_avg:94.29ms
step:714/1750 train_time:67327ms step_avg:94.30ms
step:715/1750 train_time:67425ms step_avg:94.30ms
step:716/1750 train_time:67523ms step_avg:94.31ms
step:717/1750 train_time:67619ms step_avg:94.31ms
step:718/1750 train_time:67716ms step_avg:94.31ms
step:719/1750 train_time:67813ms step_avg:94.32ms
step:720/1750 train_time:67910ms step_avg:94.32ms
step:721/1750 train_time:68007ms step_avg:94.32ms
step:722/1750 train_time:68105ms step_avg:94.33ms
step:723/1750 train_time:68204ms step_avg:94.33ms
step:724/1750 train_time:68301ms step_avg:94.34ms
step:725/1750 train_time:68398ms step_avg:94.34ms
step:726/1750 train_time:68496ms step_avg:94.35ms
step:727/1750 train_time:68593ms step_avg:94.35ms
step:728/1750 train_time:68689ms step_avg:94.35ms
step:729/1750 train_time:68786ms step_avg:94.36ms
step:730/1750 train_time:68883ms step_avg:94.36ms
step:731/1750 train_time:68980ms step_avg:94.36ms
step:732/1750 train_time:69077ms step_avg:94.37ms
step:733/1750 train_time:69174ms step_avg:94.37ms
step:734/1750 train_time:69270ms step_avg:94.37ms
step:735/1750 train_time:69368ms step_avg:94.38ms
step:736/1750 train_time:69466ms step_avg:94.38ms
step:737/1750 train_time:69565ms step_avg:94.39ms
step:738/1750 train_time:69662ms step_avg:94.39ms
step:739/1750 train_time:69759ms step_avg:94.40ms
step:740/1750 train_time:69856ms step_avg:94.40ms
step:741/1750 train_time:69954ms step_avg:94.41ms
step:742/1750 train_time:70050ms step_avg:94.41ms
step:743/1750 train_time:70147ms step_avg:94.41ms
step:744/1750 train_time:70246ms step_avg:94.42ms
step:745/1750 train_time:70344ms step_avg:94.42ms
step:746/1750 train_time:70440ms step_avg:94.42ms
step:747/1750 train_time:70538ms step_avg:94.43ms
step:748/1750 train_time:70635ms step_avg:94.43ms
step:749/1750 train_time:70732ms step_avg:94.44ms
step:750/1750 train_time:70829ms step_avg:94.44ms
step:750/1750 val_loss:3.8544 train_time:70936ms step_avg:94.58ms
step:751/1750 train_time:70952ms step_avg:94.48ms
step:752/1750 train_time:71034ms step_avg:94.46ms
step:753/1750 train_time:71133ms step_avg:94.47ms
step:754/1750 train_time:71231ms step_avg:94.47ms
step:755/1750 train_time:71326ms step_avg:94.47ms
step:756/1750 train_time:71422ms step_avg:94.47ms
step:757/1750 train_time:71518ms step_avg:94.48ms
step:758/1750 train_time:71614ms step_avg:94.48ms
step:759/1750 train_time:71710ms step_avg:94.48ms
step:760/1750 train_time:71806ms step_avg:94.48ms
step:761/1750 train_time:71904ms step_avg:94.49ms
step:762/1750 train_time:72004ms step_avg:94.49ms
step:763/1750 train_time:72101ms step_avg:94.50ms
step:764/1750 train_time:72199ms step_avg:94.50ms
step:765/1750 train_time:72297ms step_avg:94.51ms
step:766/1750 train_time:72395ms step_avg:94.51ms
step:767/1750 train_time:72495ms step_avg:94.52ms
step:768/1750 train_time:72592ms step_avg:94.52ms
step:769/1750 train_time:72688ms step_avg:94.52ms
step:770/1750 train_time:72783ms step_avg:94.52ms
step:771/1750 train_time:72880ms step_avg:94.53ms
step:772/1750 train_time:72978ms step_avg:94.53ms
step:773/1750 train_time:73077ms step_avg:94.54ms
step:774/1750 train_time:73175ms step_avg:94.54ms
step:775/1750 train_time:73273ms step_avg:94.55ms
step:776/1750 train_time:73370ms step_avg:94.55ms
step:777/1750 train_time:73466ms step_avg:94.55ms
step:778/1750 train_time:73563ms step_avg:94.55ms
step:779/1750 train_time:73659ms step_avg:94.56ms
step:780/1750 train_time:73757ms step_avg:94.56ms
step:781/1750 train_time:73855ms step_avg:94.56ms
step:782/1750 train_time:73953ms step_avg:94.57ms
step:783/1750 train_time:74052ms step_avg:94.57ms
step:784/1750 train_time:74150ms step_avg:94.58ms
step:785/1750 train_time:74247ms step_avg:94.58ms
step:786/1750 train_time:74345ms step_avg:94.59ms
step:787/1750 train_time:74443ms step_avg:94.59ms
step:788/1750 train_time:74540ms step_avg:94.59ms
step:789/1750 train_time:74636ms step_avg:94.60ms
step:790/1750 train_time:74734ms step_avg:94.60ms
step:791/1750 train_time:74831ms step_avg:94.60ms
step:792/1750 train_time:74927ms step_avg:94.61ms
step:793/1750 train_time:75023ms step_avg:94.61ms
step:794/1750 train_time:75121ms step_avg:94.61ms
step:795/1750 train_time:75219ms step_avg:94.61ms
step:796/1750 train_time:75318ms step_avg:94.62ms
step:797/1750 train_time:75418ms step_avg:94.63ms
step:798/1750 train_time:75514ms step_avg:94.63ms
step:799/1750 train_time:75612ms step_avg:94.63ms
step:800/1750 train_time:75709ms step_avg:94.64ms
step:801/1750 train_time:75806ms step_avg:94.64ms
step:802/1750 train_time:75904ms step_avg:94.64ms
step:803/1750 train_time:76002ms step_avg:94.65ms
step:804/1750 train_time:76098ms step_avg:94.65ms
step:805/1750 train_time:76198ms step_avg:94.66ms
step:806/1750 train_time:76295ms step_avg:94.66ms
step:807/1750 train_time:76394ms step_avg:94.66ms
step:808/1750 train_time:76491ms step_avg:94.67ms
step:809/1750 train_time:76587ms step_avg:94.67ms
step:810/1750 train_time:76683ms step_avg:94.67ms
step:811/1750 train_time:76781ms step_avg:94.67ms
step:812/1750 train_time:76879ms step_avg:94.68ms
step:813/1750 train_time:76977ms step_avg:94.68ms
step:814/1750 train_time:77074ms step_avg:94.69ms
step:815/1750 train_time:77173ms step_avg:94.69ms
step:816/1750 train_time:77270ms step_avg:94.69ms
step:817/1750 train_time:77368ms step_avg:94.70ms
step:818/1750 train_time:77465ms step_avg:94.70ms
step:819/1750 train_time:77562ms step_avg:94.70ms
step:820/1750 train_time:77660ms step_avg:94.71ms
step:821/1750 train_time:77758ms step_avg:94.71ms
step:822/1750 train_time:77855ms step_avg:94.71ms
step:823/1750 train_time:77952ms step_avg:94.72ms
step:824/1750 train_time:78050ms step_avg:94.72ms
step:825/1750 train_time:78148ms step_avg:94.72ms
step:826/1750 train_time:78246ms step_avg:94.73ms
step:827/1750 train_time:78343ms step_avg:94.73ms
step:828/1750 train_time:78440ms step_avg:94.73ms
step:829/1750 train_time:78538ms step_avg:94.74ms
step:830/1750 train_time:78636ms step_avg:94.74ms
step:831/1750 train_time:78734ms step_avg:94.75ms
step:832/1750 train_time:78831ms step_avg:94.75ms
step:833/1750 train_time:78928ms step_avg:94.75ms
step:834/1750 train_time:79025ms step_avg:94.75ms
step:835/1750 train_time:79122ms step_avg:94.76ms
step:836/1750 train_time:79219ms step_avg:94.76ms
step:837/1750 train_time:79318ms step_avg:94.76ms
step:838/1750 train_time:79415ms step_avg:94.77ms
step:839/1750 train_time:79512ms step_avg:94.77ms
step:840/1750 train_time:79611ms step_avg:94.78ms
step:841/1750 train_time:79709ms step_avg:94.78ms
step:842/1750 train_time:79804ms step_avg:94.78ms
step:843/1750 train_time:79902ms step_avg:94.78ms
step:844/1750 train_time:80000ms step_avg:94.79ms
step:845/1750 train_time:80097ms step_avg:94.79ms
step:846/1750 train_time:80195ms step_avg:94.79ms
step:847/1750 train_time:80292ms step_avg:94.80ms
step:848/1750 train_time:80389ms step_avg:94.80ms
step:849/1750 train_time:80485ms step_avg:94.80ms
step:850/1750 train_time:80584ms step_avg:94.80ms
step:851/1750 train_time:80681ms step_avg:94.81ms
step:852/1750 train_time:80778ms step_avg:94.81ms
step:853/1750 train_time:80877ms step_avg:94.81ms
step:854/1750 train_time:80975ms step_avg:94.82ms
step:855/1750 train_time:81072ms step_avg:94.82ms
step:856/1750 train_time:81170ms step_avg:94.82ms
step:857/1750 train_time:81267ms step_avg:94.83ms
step:858/1750 train_time:81364ms step_avg:94.83ms
step:859/1750 train_time:81461ms step_avg:94.83ms
step:860/1750 train_time:81559ms step_avg:94.84ms
step:861/1750 train_time:81658ms step_avg:94.84ms
step:862/1750 train_time:81755ms step_avg:94.84ms
step:863/1750 train_time:81851ms step_avg:94.84ms
step:864/1750 train_time:81950ms step_avg:94.85ms
step:865/1750 train_time:82048ms step_avg:94.85ms
step:866/1750 train_time:82144ms step_avg:94.86ms
step:867/1750 train_time:82241ms step_avg:94.86ms
step:868/1750 train_time:82339ms step_avg:94.86ms
step:869/1750 train_time:82436ms step_avg:94.86ms
step:870/1750 train_time:82534ms step_avg:94.87ms
step:871/1750 train_time:82632ms step_avg:94.87ms
step:872/1750 train_time:82729ms step_avg:94.87ms
step:873/1750 train_time:82826ms step_avg:94.87ms
step:874/1750 train_time:82923ms step_avg:94.88ms
step:875/1750 train_time:83021ms step_avg:94.88ms
step:875/1750 val_loss:3.7830 train_time:83127ms step_avg:95.00ms
step:876/1750 train_time:83144ms step_avg:94.91ms
step:877/1750 train_time:83224ms step_avg:94.90ms
step:878/1750 train_time:83323ms step_avg:94.90ms
step:879/1750 train_time:83421ms step_avg:94.90ms
step:880/1750 train_time:83518ms step_avg:94.91ms
step:881/1750 train_time:83615ms step_avg:94.91ms
step:882/1750 train_time:83711ms step_avg:94.91ms
step:883/1750 train_time:83809ms step_avg:94.91ms
step:884/1750 train_time:83906ms step_avg:94.92ms
step:885/1750 train_time:84002ms step_avg:94.92ms
step:886/1750 train_time:84100ms step_avg:94.92ms
step:887/1750 train_time:84202ms step_avg:94.93ms
step:888/1750 train_time:84301ms step_avg:94.93ms
step:889/1750 train_time:84398ms step_avg:94.94ms
step:890/1750 train_time:84496ms step_avg:94.94ms
step:891/1750 train_time:84591ms step_avg:94.94ms
step:892/1750 train_time:84688ms step_avg:94.94ms
step:893/1750 train_time:84785ms step_avg:94.94ms
step:894/1750 train_time:84881ms step_avg:94.95ms
step:895/1750 train_time:84978ms step_avg:94.95ms
step:896/1750 train_time:85076ms step_avg:94.95ms
step:897/1750 train_time:85175ms step_avg:94.95ms
step:898/1750 train_time:85276ms step_avg:94.96ms
step:899/1750 train_time:85373ms step_avg:94.96ms
step:900/1750 train_time:85469ms step_avg:94.97ms
step:901/1750 train_time:85567ms step_avg:94.97ms
step:902/1750 train_time:85665ms step_avg:94.97ms
step:903/1750 train_time:85762ms step_avg:94.97ms
step:904/1750 train_time:85859ms step_avg:94.98ms
step:905/1750 train_time:85957ms step_avg:94.98ms
step:906/1750 train_time:86054ms step_avg:94.98ms
step:907/1750 train_time:86151ms step_avg:94.98ms
step:908/1750 train_time:86250ms step_avg:94.99ms
step:909/1750 train_time:86346ms step_avg:94.99ms
step:910/1750 train_time:86448ms step_avg:95.00ms
step:911/1750 train_time:86546ms step_avg:95.00ms
step:912/1750 train_time:86645ms step_avg:95.01ms
step:913/1750 train_time:86744ms step_avg:95.01ms
step:914/1750 train_time:86842ms step_avg:95.01ms
step:915/1750 train_time:86942ms step_avg:95.02ms
step:916/1750 train_time:87041ms step_avg:95.02ms
step:917/1750 train_time:87139ms step_avg:95.03ms
step:918/1750 train_time:87239ms step_avg:95.03ms
step:919/1750 train_time:87339ms step_avg:95.04ms
step:920/1750 train_time:87439ms step_avg:95.04ms
step:921/1750 train_time:87540ms step_avg:95.05ms
step:922/1750 train_time:87640ms step_avg:95.05ms
step:923/1750 train_time:87739ms step_avg:95.06ms
step:924/1750 train_time:87839ms step_avg:95.06ms
step:925/1750 train_time:87937ms step_avg:95.07ms
step:926/1750 train_time:88036ms step_avg:95.07ms
step:927/1750 train_time:88133ms step_avg:95.07ms
step:928/1750 train_time:88232ms step_avg:95.08ms
step:929/1750 train_time:88329ms step_avg:95.08ms
step:930/1750 train_time:88427ms step_avg:95.08ms
step:931/1750 train_time:88526ms step_avg:95.09ms
step:932/1750 train_time:88626ms step_avg:95.09ms
step:933/1750 train_time:88725ms step_avg:95.10ms
step:934/1750 train_time:88823ms step_avg:95.10ms
step:935/1750 train_time:88922ms step_avg:95.10ms
step:936/1750 train_time:89023ms step_avg:95.11ms
step:937/1750 train_time:89121ms step_avg:95.11ms
step:938/1750 train_time:89220ms step_avg:95.12ms
step:939/1750 train_time:89319ms step_avg:95.12ms
step:940/1750 train_time:89419ms step_avg:95.13ms
step:941/1750 train_time:89518ms step_avg:95.13ms
step:942/1750 train_time:89618ms step_avg:95.14ms
step:943/1750 train_time:89717ms step_avg:95.14ms
step:944/1750 train_time:89817ms step_avg:95.14ms
step:945/1750 train_time:89914ms step_avg:95.15ms
step:946/1750 train_time:90012ms step_avg:95.15ms
step:947/1750 train_time:90109ms step_avg:95.15ms
step:948/1750 train_time:90209ms step_avg:95.16ms
step:949/1750 train_time:90307ms step_avg:95.16ms
step:950/1750 train_time:90405ms step_avg:95.16ms
step:951/1750 train_time:90504ms step_avg:95.17ms
step:952/1750 train_time:90605ms step_avg:95.17ms
step:953/1750 train_time:90704ms step_avg:95.18ms
step:954/1750 train_time:90803ms step_avg:95.18ms
step:955/1750 train_time:90902ms step_avg:95.19ms
step:956/1750 train_time:91002ms step_avg:95.19ms
step:957/1750 train_time:91101ms step_avg:95.19ms
step:958/1750 train_time:91201ms step_avg:95.20ms
step:959/1750 train_time:91300ms step_avg:95.20ms
step:960/1750 train_time:91398ms step_avg:95.21ms
step:961/1750 train_time:91498ms step_avg:95.21ms
step:962/1750 train_time:91597ms step_avg:95.22ms
step:963/1750 train_time:91697ms step_avg:95.22ms
step:964/1750 train_time:91797ms step_avg:95.23ms
step:965/1750 train_time:91897ms step_avg:95.23ms
step:966/1750 train_time:91995ms step_avg:95.23ms
step:967/1750 train_time:92093ms step_avg:95.24ms
step:968/1750 train_time:92192ms step_avg:95.24ms
step:969/1750 train_time:92291ms step_avg:95.24ms
step:970/1750 train_time:92390ms step_avg:95.25ms
step:971/1750 train_time:92487ms step_avg:95.25ms
step:972/1750 train_time:92586ms step_avg:95.25ms
step:973/1750 train_time:92686ms step_avg:95.26ms
step:974/1750 train_time:92785ms step_avg:95.26ms
step:975/1750 train_time:92884ms step_avg:95.27ms
step:976/1750 train_time:92983ms step_avg:95.27ms
step:977/1750 train_time:93083ms step_avg:95.27ms
step:978/1750 train_time:93182ms step_avg:95.28ms
step:979/1750 train_time:93281ms step_avg:95.28ms
step:980/1750 train_time:93379ms step_avg:95.28ms
step:981/1750 train_time:93478ms step_avg:95.29ms
step:982/1750 train_time:93576ms step_avg:95.29ms
step:983/1750 train_time:93676ms step_avg:95.30ms
step:984/1750 train_time:93777ms step_avg:95.30ms
step:985/1750 train_time:93878ms step_avg:95.31ms
step:986/1750 train_time:93979ms step_avg:95.31ms
step:987/1750 train_time:94080ms step_avg:95.32ms
step:988/1750 train_time:94179ms step_avg:95.32ms
step:989/1750 train_time:94277ms step_avg:95.33ms
step:990/1750 train_time:94374ms step_avg:95.33ms
step:991/1750 train_time:94473ms step_avg:95.33ms
step:992/1750 train_time:94570ms step_avg:95.33ms
step:993/1750 train_time:94669ms step_avg:95.34ms
step:994/1750 train_time:94766ms step_avg:95.34ms
step:995/1750 train_time:94866ms step_avg:95.34ms
step:996/1750 train_time:94965ms step_avg:95.35ms
step:997/1750 train_time:95064ms step_avg:95.35ms
step:998/1750 train_time:95164ms step_avg:95.35ms
step:999/1750 train_time:95264ms step_avg:95.36ms
step:1000/1750 train_time:95363ms step_avg:95.36ms
step:1000/1750 val_loss:3.7345 train_time:95471ms step_avg:95.47ms
step:1001/1750 train_time:95488ms step_avg:95.39ms
step:1002/1750 train_time:95565ms step_avg:95.37ms
step:1003/1750 train_time:95666ms step_avg:95.38ms
step:1004/1750 train_time:95765ms step_avg:95.38ms
step:1005/1750 train_time:95863ms step_avg:95.39ms
step:1006/1750 train_time:95960ms step_avg:95.39ms
step:1007/1750 train_time:96059ms step_avg:95.39ms
step:1008/1750 train_time:96157ms step_avg:95.39ms
step:1009/1750 train_time:96254ms step_avg:95.40ms
step:1010/1750 train_time:96353ms step_avg:95.40ms
step:1011/1750 train_time:96452ms step_avg:95.40ms
step:1012/1750 train_time:96553ms step_avg:95.41ms
step:1013/1750 train_time:96653ms step_avg:95.41ms
step:1014/1750 train_time:96752ms step_avg:95.42ms
step:1015/1750 train_time:96851ms step_avg:95.42ms
step:1016/1750 train_time:96951ms step_avg:95.42ms
step:1017/1750 train_time:97049ms step_avg:95.43ms
step:1018/1750 train_time:97147ms step_avg:95.43ms
step:1019/1750 train_time:97245ms step_avg:95.43ms
step:1020/1750 train_time:97344ms step_avg:95.44ms
step:1021/1750 train_time:97444ms step_avg:95.44ms
step:1022/1750 train_time:97544ms step_avg:95.44ms
step:1023/1750 train_time:97644ms step_avg:95.45ms
step:1024/1750 train_time:97743ms step_avg:95.45ms
step:1025/1750 train_time:97845ms step_avg:95.46ms
step:1026/1750 train_time:97947ms step_avg:95.46ms
step:1027/1750 train_time:98046ms step_avg:95.47ms
step:1028/1750 train_time:98145ms step_avg:95.47ms
step:1029/1750 train_time:98243ms step_avg:95.47ms
step:1030/1750 train_time:98344ms step_avg:95.48ms
step:1031/1750 train_time:98444ms step_avg:95.48ms
step:1032/1750 train_time:98543ms step_avg:95.49ms
step:1033/1750 train_time:98642ms step_avg:95.49ms
step:1034/1750 train_time:98741ms step_avg:95.49ms
step:1035/1750 train_time:98840ms step_avg:95.50ms
step:1036/1750 train_time:98939ms step_avg:95.50ms
step:1037/1750 train_time:99037ms step_avg:95.50ms
step:1038/1750 train_time:99136ms step_avg:95.51ms
step:1039/1750 train_time:99235ms step_avg:95.51ms
step:1040/1750 train_time:99334ms step_avg:95.51ms
step:1041/1750 train_time:99432ms step_avg:95.52ms
step:1042/1750 train_time:99532ms step_avg:95.52ms
step:1043/1750 train_time:99633ms step_avg:95.53ms
step:1044/1750 train_time:99732ms step_avg:95.53ms
step:1045/1750 train_time:99832ms step_avg:95.53ms
step:1046/1750 train_time:99932ms step_avg:95.54ms
step:1047/1750 train_time:100031ms step_avg:95.54ms
step:1048/1750 train_time:100129ms step_avg:95.54ms
step:1049/1750 train_time:100229ms step_avg:95.55ms
step:1050/1750 train_time:100327ms step_avg:95.55ms
step:1051/1750 train_time:100426ms step_avg:95.55ms
step:1052/1750 train_time:100526ms step_avg:95.56ms
step:1053/1750 train_time:100625ms step_avg:95.56ms
step:1054/1750 train_time:100724ms step_avg:95.56ms
step:1055/1750 train_time:100823ms step_avg:95.57ms
step:1056/1750 train_time:100921ms step_avg:95.57ms
step:1057/1750 train_time:101020ms step_avg:95.57ms
step:1058/1750 train_time:101119ms step_avg:95.58ms
step:1059/1750 train_time:101218ms step_avg:95.58ms
step:1060/1750 train_time:101316ms step_avg:95.58ms
step:1061/1750 train_time:101416ms step_avg:95.59ms
step:1062/1750 train_time:101514ms step_avg:95.59ms
step:1063/1750 train_time:101613ms step_avg:95.59ms
step:1064/1750 train_time:101712ms step_avg:95.59ms
step:1065/1750 train_time:101814ms step_avg:95.60ms
step:1066/1750 train_time:101912ms step_avg:95.60ms
step:1067/1750 train_time:102012ms step_avg:95.61ms
step:1068/1750 train_time:102111ms step_avg:95.61ms
step:1069/1750 train_time:102211ms step_avg:95.61ms
step:1070/1750 train_time:102308ms step_avg:95.62ms
step:1071/1750 train_time:102408ms step_avg:95.62ms
step:1072/1750 train_time:102507ms step_avg:95.62ms
step:1073/1750 train_time:102607ms step_avg:95.63ms
step:1074/1750 train_time:102707ms step_avg:95.63ms
step:1075/1750 train_time:102808ms step_avg:95.64ms
step:1076/1750 train_time:102906ms step_avg:95.64ms
step:1077/1750 train_time:103006ms step_avg:95.64ms
step:1078/1750 train_time:103106ms step_avg:95.65ms
step:1079/1750 train_time:103205ms step_avg:95.65ms
step:1080/1750 train_time:103304ms step_avg:95.65ms
step:1081/1750 train_time:103403ms step_avg:95.66ms
step:1082/1750 train_time:103502ms step_avg:95.66ms
step:1083/1750 train_time:103601ms step_avg:95.66ms
step:1084/1750 train_time:103700ms step_avg:95.66ms
step:1085/1750 train_time:103799ms step_avg:95.67ms
step:1086/1750 train_time:103897ms step_avg:95.67ms
step:1087/1750 train_time:103997ms step_avg:95.67ms
step:1088/1750 train_time:104094ms step_avg:95.67ms
step:1089/1750 train_time:104194ms step_avg:95.68ms
step:1090/1750 train_time:104291ms step_avg:95.68ms
step:1091/1750 train_time:104392ms step_avg:95.69ms
step:1092/1750 train_time:104493ms step_avg:95.69ms
step:1093/1750 train_time:104592ms step_avg:95.69ms
step:1094/1750 train_time:104691ms step_avg:95.70ms
step:1095/1750 train_time:104790ms step_avg:95.70ms
step:1096/1750 train_time:104889ms step_avg:95.70ms
step:1097/1750 train_time:104989ms step_avg:95.71ms
step:1098/1750 train_time:105087ms step_avg:95.71ms
step:1099/1750 train_time:105188ms step_avg:95.71ms
step:1100/1750 train_time:105288ms step_avg:95.72ms
step:1101/1750 train_time:105387ms step_avg:95.72ms
step:1102/1750 train_time:105485ms step_avg:95.72ms
step:1103/1750 train_time:105585ms step_avg:95.72ms
step:1104/1750 train_time:105684ms step_avg:95.73ms
step:1105/1750 train_time:105782ms step_avg:95.73ms
step:1106/1750 train_time:105882ms step_avg:95.73ms
step:1107/1750 train_time:105980ms step_avg:95.74ms
step:1108/1750 train_time:106079ms step_avg:95.74ms
step:1109/1750 train_time:106179ms step_avg:95.74ms
step:1110/1750 train_time:106278ms step_avg:95.75ms
step:1111/1750 train_time:106376ms step_avg:95.75ms
step:1112/1750 train_time:106476ms step_avg:95.75ms
step:1113/1750 train_time:106575ms step_avg:95.75ms
step:1114/1750 train_time:106673ms step_avg:95.76ms
step:1115/1750 train_time:106773ms step_avg:95.76ms
step:1116/1750 train_time:106873ms step_avg:95.76ms
step:1117/1750 train_time:106974ms step_avg:95.77ms
step:1118/1750 train_time:107072ms step_avg:95.77ms
step:1119/1750 train_time:107171ms step_avg:95.77ms
step:1120/1750 train_time:107269ms step_avg:95.78ms
step:1121/1750 train_time:107369ms step_avg:95.78ms
step:1122/1750 train_time:107468ms step_avg:95.78ms
step:1123/1750 train_time:107568ms step_avg:95.79ms
step:1124/1750 train_time:107667ms step_avg:95.79ms
step:1125/1750 train_time:107767ms step_avg:95.79ms
step:1125/1750 val_loss:3.6612 train_time:107875ms step_avg:95.89ms
step:1126/1750 train_time:107891ms step_avg:95.82ms
step:1127/1750 train_time:107969ms step_avg:95.80ms
step:1128/1750 train_time:108075ms step_avg:95.81ms
step:1129/1750 train_time:108171ms step_avg:95.81ms
step:1130/1750 train_time:108270ms step_avg:95.81ms
step:1131/1750 train_time:108368ms step_avg:95.82ms
step:1132/1750 train_time:108466ms step_avg:95.82ms
step:1133/1750 train_time:108565ms step_avg:95.82ms
step:1134/1750 train_time:108664ms step_avg:95.82ms
step:1135/1750 train_time:108762ms step_avg:95.83ms
step:1136/1750 train_time:108860ms step_avg:95.83ms
step:1137/1750 train_time:108962ms step_avg:95.83ms
step:1138/1750 train_time:109062ms step_avg:95.84ms
step:1139/1750 train_time:109161ms step_avg:95.84ms
step:1140/1750 train_time:109261ms step_avg:95.84ms
step:1141/1750 train_time:109360ms step_avg:95.85ms
step:1142/1750 train_time:109457ms step_avg:95.85ms
step:1143/1750 train_time:109557ms step_avg:95.85ms
step:1144/1750 train_time:109656ms step_avg:95.85ms
step:1145/1750 train_time:109753ms step_avg:95.85ms
step:1146/1750 train_time:109852ms step_avg:95.86ms
step:1147/1750 train_time:109950ms step_avg:95.86ms
step:1148/1750 train_time:110050ms step_avg:95.86ms
step:1149/1750 train_time:110149ms step_avg:95.86ms
step:1150/1750 train_time:110249ms step_avg:95.87ms
step:1151/1750 train_time:110348ms step_avg:95.87ms
step:1152/1750 train_time:110447ms step_avg:95.87ms
step:1153/1750 train_time:110547ms step_avg:95.88ms
step:1154/1750 train_time:110646ms step_avg:95.88ms
step:1155/1750 train_time:110744ms step_avg:95.88ms
step:1156/1750 train_time:110843ms step_avg:95.88ms
step:1157/1750 train_time:110941ms step_avg:95.89ms
step:1158/1750 train_time:111041ms step_avg:95.89ms
step:1159/1750 train_time:111141ms step_avg:95.89ms
step:1160/1750 train_time:111240ms step_avg:95.90ms
step:1161/1750 train_time:111340ms step_avg:95.90ms
step:1162/1750 train_time:111440ms step_avg:95.90ms
step:1163/1750 train_time:111540ms step_avg:95.91ms
step:1164/1750 train_time:111640ms step_avg:95.91ms
step:1165/1750 train_time:111741ms step_avg:95.92ms
step:1166/1750 train_time:111840ms step_avg:95.92ms
step:1167/1750 train_time:111940ms step_avg:95.92ms
step:1168/1750 train_time:112038ms step_avg:95.92ms
step:1169/1750 train_time:112141ms step_avg:95.93ms
step:1170/1750 train_time:112243ms step_avg:95.93ms
step:1171/1750 train_time:112342ms step_avg:95.94ms
step:1172/1750 train_time:112442ms step_avg:95.94ms
step:1173/1750 train_time:112543ms step_avg:95.94ms
step:1174/1750 train_time:112643ms step_avg:95.95ms
step:1175/1750 train_time:112744ms step_avg:95.95ms
step:1176/1750 train_time:112843ms step_avg:95.95ms
step:1177/1750 train_time:112942ms step_avg:95.96ms
step:1178/1750 train_time:113043ms step_avg:95.96ms
step:1179/1750 train_time:113144ms step_avg:95.97ms
step:1180/1750 train_time:113245ms step_avg:95.97ms
step:1181/1750 train_time:113347ms step_avg:95.98ms
step:1182/1750 train_time:113445ms step_avg:95.98ms
step:1183/1750 train_time:113547ms step_avg:95.98ms
step:1184/1750 train_time:113649ms step_avg:95.99ms
step:1185/1750 train_time:113749ms step_avg:95.99ms
step:1186/1750 train_time:113848ms step_avg:95.99ms
step:1187/1750 train_time:113949ms step_avg:96.00ms
step:1188/1750 train_time:114048ms step_avg:96.00ms
step:1189/1750 train_time:114149ms step_avg:96.00ms
step:1190/1750 train_time:114249ms step_avg:96.01ms
step:1191/1750 train_time:114351ms step_avg:96.01ms
step:1192/1750 train_time:114449ms step_avg:96.01ms
step:1193/1750 train_time:114550ms step_avg:96.02ms
step:1194/1750 train_time:114649ms step_avg:96.02ms
step:1195/1750 train_time:114750ms step_avg:96.02ms
step:1196/1750 train_time:114850ms step_avg:96.03ms
step:1197/1750 train_time:114949ms step_avg:96.03ms
step:1198/1750 train_time:115049ms step_avg:96.03ms
step:1199/1750 train_time:115149ms step_avg:96.04ms
step:1200/1750 train_time:115250ms step_avg:96.04ms
step:1201/1750 train_time:115350ms step_avg:96.05ms
step:1202/1750 train_time:115449ms step_avg:96.05ms
step:1203/1750 train_time:115550ms step_avg:96.05ms
step:1204/1750 train_time:115651ms step_avg:96.06ms
step:1205/1750 train_time:115750ms step_avg:96.06ms
step:1206/1750 train_time:115849ms step_avg:96.06ms
step:1207/1750 train_time:115949ms step_avg:96.06ms
step:1208/1750 train_time:116049ms step_avg:96.07ms
step:1209/1750 train_time:116149ms step_avg:96.07ms
step:1210/1750 train_time:116249ms step_avg:96.07ms
step:1211/1750 train_time:116351ms step_avg:96.08ms
step:1212/1750 train_time:116451ms step_avg:96.08ms
step:1213/1750 train_time:116549ms step_avg:96.08ms
step:1214/1750 train_time:116650ms step_avg:96.09ms
step:1215/1750 train_time:116749ms step_avg:96.09ms
step:1216/1750 train_time:116849ms step_avg:96.09ms
step:1217/1750 train_time:116951ms step_avg:96.10ms
step:1218/1750 train_time:117052ms step_avg:96.10ms
step:1219/1750 train_time:117152ms step_avg:96.11ms
step:1220/1750 train_time:117253ms step_avg:96.11ms
step:1221/1750 train_time:117351ms step_avg:96.11ms
step:1222/1750 train_time:117452ms step_avg:96.11ms
step:1223/1750 train_time:117552ms step_avg:96.12ms
step:1224/1750 train_time:117652ms step_avg:96.12ms
step:1225/1750 train_time:117752ms step_avg:96.12ms
step:1226/1750 train_time:117853ms step_avg:96.13ms
step:1227/1750 train_time:117952ms step_avg:96.13ms
step:1228/1750 train_time:118052ms step_avg:96.13ms
step:1229/1750 train_time:118152ms step_avg:96.14ms
step:1230/1750 train_time:118252ms step_avg:96.14ms
step:1231/1750 train_time:118352ms step_avg:96.14ms
step:1232/1750 train_time:118454ms step_avg:96.15ms
step:1233/1750 train_time:118553ms step_avg:96.15ms
step:1234/1750 train_time:118653ms step_avg:96.15ms
step:1235/1750 train_time:118754ms step_avg:96.16ms
step:1236/1750 train_time:118854ms step_avg:96.16ms
step:1237/1750 train_time:118955ms step_avg:96.16ms
step:1238/1750 train_time:119056ms step_avg:96.17ms
step:1239/1750 train_time:119155ms step_avg:96.17ms
step:1240/1750 train_time:119256ms step_avg:96.17ms
step:1241/1750 train_time:119354ms step_avg:96.18ms
step:1242/1750 train_time:119456ms step_avg:96.18ms
step:1243/1750 train_time:119556ms step_avg:96.18ms
step:1244/1750 train_time:119656ms step_avg:96.19ms
step:1245/1750 train_time:119755ms step_avg:96.19ms
step:1246/1750 train_time:119856ms step_avg:96.19ms
step:1247/1750 train_time:119958ms step_avg:96.20ms
step:1248/1750 train_time:120059ms step_avg:96.20ms
step:1249/1750 train_time:120161ms step_avg:96.21ms
step:1250/1750 train_time:120261ms step_avg:96.21ms
step:1250/1750 val_loss:3.6103 train_time:120370ms step_avg:96.30ms
step:1251/1750 train_time:120386ms step_avg:96.23ms
step:1252/1750 train_time:120468ms step_avg:96.22ms
step:1253/1750 train_time:120569ms step_avg:96.22ms
step:1254/1750 train_time:120668ms step_avg:96.23ms
step:1255/1750 train_time:120768ms step_avg:96.23ms
step:1256/1750 train_time:120868ms step_avg:96.23ms
step:1257/1750 train_time:120968ms step_avg:96.24ms
step:1258/1750 train_time:121068ms step_avg:96.24ms
step:1259/1750 train_time:121167ms step_avg:96.24ms
step:1260/1750 train_time:121265ms step_avg:96.24ms
step:1261/1750 train_time:121367ms step_avg:96.25ms
step:1262/1750 train_time:121472ms step_avg:96.25ms
step:1263/1750 train_time:121574ms step_avg:96.26ms
step:1264/1750 train_time:121673ms step_avg:96.26ms
step:1265/1750 train_time:121772ms step_avg:96.26ms
step:1266/1750 train_time:121872ms step_avg:96.27ms
step:1267/1750 train_time:121970ms step_avg:96.27ms
step:1268/1750 train_time:122070ms step_avg:96.27ms
step:1269/1750 train_time:122171ms step_avg:96.27ms
step:1270/1750 train_time:122271ms step_avg:96.28ms
step:1271/1750 train_time:122370ms step_avg:96.28ms
step:1272/1750 train_time:122473ms step_avg:96.28ms
step:1273/1750 train_time:122574ms step_avg:96.29ms
step:1274/1750 train_time:122673ms step_avg:96.29ms
step:1275/1750 train_time:122774ms step_avg:96.29ms
step:1276/1750 train_time:122872ms step_avg:96.30ms
step:1277/1750 train_time:122973ms step_avg:96.30ms
step:1278/1750 train_time:123072ms step_avg:96.30ms
step:1279/1750 train_time:123173ms step_avg:96.30ms
step:1280/1750 train_time:123272ms step_avg:96.31ms
step:1281/1750 train_time:123372ms step_avg:96.31ms
step:1282/1750 train_time:123472ms step_avg:96.31ms
step:1283/1750 train_time:123571ms step_avg:96.31ms
step:1284/1750 train_time:123673ms step_avg:96.32ms
step:1285/1750 train_time:123773ms step_avg:96.32ms
step:1286/1750 train_time:123872ms step_avg:96.32ms
step:1287/1750 train_time:123971ms step_avg:96.33ms
step:1288/1750 train_time:124071ms step_avg:96.33ms
step:1289/1750 train_time:124170ms step_avg:96.33ms
step:1290/1750 train_time:124271ms step_avg:96.33ms
step:1291/1750 train_time:124371ms step_avg:96.34ms
step:1292/1750 train_time:124470ms step_avg:96.34ms
step:1293/1750 train_time:124572ms step_avg:96.34ms
step:1294/1750 train_time:124674ms step_avg:96.35ms
step:1295/1750 train_time:124775ms step_avg:96.35ms
step:1296/1750 train_time:124875ms step_avg:96.35ms
step:1297/1750 train_time:124974ms step_avg:96.36ms
step:1298/1750 train_time:125073ms step_avg:96.36ms
step:1299/1750 train_time:125173ms step_avg:96.36ms
step:1300/1750 train_time:125273ms step_avg:96.36ms
step:1301/1750 train_time:125374ms step_avg:96.37ms
step:1302/1750 train_time:125475ms step_avg:96.37ms
step:1303/1750 train_time:125574ms step_avg:96.37ms
step:1304/1750 train_time:125675ms step_avg:96.38ms
step:1305/1750 train_time:125777ms step_avg:96.38ms
step:1306/1750 train_time:125874ms step_avg:96.38ms
step:1307/1750 train_time:125974ms step_avg:96.38ms
step:1308/1750 train_time:126074ms step_avg:96.39ms
step:1309/1750 train_time:126174ms step_avg:96.39ms
step:1310/1750 train_time:126274ms step_avg:96.39ms
step:1311/1750 train_time:126373ms step_avg:96.39ms
step:1312/1750 train_time:126475ms step_avg:96.40ms
step:1313/1750 train_time:126576ms step_avg:96.40ms
step:1314/1750 train_time:126676ms step_avg:96.40ms
step:1315/1750 train_time:126775ms step_avg:96.41ms
step:1316/1750 train_time:126876ms step_avg:96.41ms
step:1317/1750 train_time:126975ms step_avg:96.41ms
step:1318/1750 train_time:127074ms step_avg:96.41ms
step:1319/1750 train_time:127174ms step_avg:96.42ms
step:1320/1750 train_time:127276ms step_avg:96.42ms
step:1321/1750 train_time:127378ms step_avg:96.43ms
step:1322/1750 train_time:127477ms step_avg:96.43ms
step:1323/1750 train_time:127578ms step_avg:96.43ms
step:1324/1750 train_time:127679ms step_avg:96.43ms
step:1325/1750 train_time:127779ms step_avg:96.44ms
step:1326/1750 train_time:127882ms step_avg:96.44ms
step:1327/1750 train_time:127984ms step_avg:96.45ms
step:1328/1750 train_time:128085ms step_avg:96.45ms
step:1329/1750 train_time:128185ms step_avg:96.45ms
step:1330/1750 train_time:128285ms step_avg:96.45ms
step:1331/1750 train_time:128385ms step_avg:96.46ms
step:1332/1750 train_time:128485ms step_avg:96.46ms
step:1333/1750 train_time:128584ms step_avg:96.46ms
step:1334/1750 train_time:128687ms step_avg:96.47ms
step:1335/1750 train_time:128785ms step_avg:96.47ms
step:1336/1750 train_time:128887ms step_avg:96.47ms
step:1337/1750 train_time:128987ms step_avg:96.47ms
step:1338/1750 train_time:129086ms step_avg:96.48ms
step:1339/1750 train_time:129187ms step_avg:96.48ms
step:1340/1750 train_time:129287ms step_avg:96.48ms
step:1341/1750 train_time:129387ms step_avg:96.49ms
step:1342/1750 train_time:129488ms step_avg:96.49ms
step:1343/1750 train_time:129588ms step_avg:96.49ms
step:1344/1750 train_time:129686ms step_avg:96.49ms
step:1345/1750 train_time:129787ms step_avg:96.50ms
step:1346/1750 train_time:129888ms step_avg:96.50ms
step:1347/1750 train_time:129991ms step_avg:96.50ms
step:1348/1750 train_time:130090ms step_avg:96.51ms
step:1349/1750 train_time:130190ms step_avg:96.51ms
step:1350/1750 train_time:130289ms step_avg:96.51ms
step:1351/1750 train_time:130390ms step_avg:96.51ms
step:1352/1750 train_time:130491ms step_avg:96.52ms
step:1353/1750 train_time:130592ms step_avg:96.52ms
step:1354/1750 train_time:130690ms step_avg:96.52ms
step:1355/1750 train_time:130789ms step_avg:96.52ms
step:1356/1750 train_time:130890ms step_avg:96.53ms
step:1357/1750 train_time:130991ms step_avg:96.53ms
step:1358/1750 train_time:131090ms step_avg:96.53ms
step:1359/1750 train_time:131190ms step_avg:96.53ms
step:1360/1750 train_time:131293ms step_avg:96.54ms
step:1361/1750 train_time:131393ms step_avg:96.54ms
step:1362/1750 train_time:131491ms step_avg:96.54ms
step:1363/1750 train_time:131591ms step_avg:96.55ms
step:1364/1750 train_time:131693ms step_avg:96.55ms
step:1365/1750 train_time:131794ms step_avg:96.55ms
step:1366/1750 train_time:131893ms step_avg:96.55ms
step:1367/1750 train_time:131994ms step_avg:96.56ms
step:1368/1750 train_time:132092ms step_avg:96.56ms
step:1369/1750 train_time:132194ms step_avg:96.56ms
step:1370/1750 train_time:132294ms step_avg:96.57ms
step:1371/1750 train_time:132394ms step_avg:96.57ms
step:1372/1750 train_time:132495ms step_avg:96.57ms
step:1373/1750 train_time:132595ms step_avg:96.57ms
step:1374/1750 train_time:132694ms step_avg:96.57ms
step:1375/1750 train_time:132794ms step_avg:96.58ms
step:1375/1750 val_loss:3.5647 train_time:132904ms step_avg:96.66ms
step:1376/1750 train_time:132920ms step_avg:96.60ms
step:1377/1750 train_time:133001ms step_avg:96.59ms
step:1378/1750 train_time:133102ms step_avg:96.59ms
step:1379/1750 train_time:133202ms step_avg:96.59ms
step:1380/1750 train_time:133299ms step_avg:96.59ms
step:1381/1750 train_time:133401ms step_avg:96.60ms
step:1382/1750 train_time:133501ms step_avg:96.60ms
step:1383/1750 train_time:133600ms step_avg:96.60ms
step:1384/1750 train_time:133699ms step_avg:96.60ms
step:1385/1750 train_time:133799ms step_avg:96.61ms
step:1386/1750 train_time:133899ms step_avg:96.61ms
step:1387/1750 train_time:134000ms step_avg:96.61ms
step:1388/1750 train_time:134103ms step_avg:96.62ms
step:1389/1750 train_time:134204ms step_avg:96.62ms
step:1390/1750 train_time:134303ms step_avg:96.62ms
step:1391/1750 train_time:134402ms step_avg:96.62ms
step:1392/1750 train_time:134503ms step_avg:96.63ms
step:1393/1750 train_time:134602ms step_avg:96.63ms
step:1394/1750 train_time:134703ms step_avg:96.63ms
step:1395/1750 train_time:134802ms step_avg:96.63ms
step:1396/1750 train_time:134901ms step_avg:96.63ms
step:1397/1750 train_time:135002ms step_avg:96.64ms
step:1398/1750 train_time:135103ms step_avg:96.64ms
step:1399/1750 train_time:135203ms step_avg:96.64ms
step:1400/1750 train_time:135306ms step_avg:96.65ms
step:1401/1750 train_time:135405ms step_avg:96.65ms
step:1402/1750 train_time:135506ms step_avg:96.65ms
step:1403/1750 train_time:135606ms step_avg:96.65ms
step:1404/1750 train_time:135708ms step_avg:96.66ms
step:1405/1750 train_time:135810ms step_avg:96.66ms
step:1406/1750 train_time:135912ms step_avg:96.67ms
step:1407/1750 train_time:136013ms step_avg:96.67ms
step:1408/1750 train_time:136113ms step_avg:96.67ms
step:1409/1750 train_time:136213ms step_avg:96.67ms
step:1410/1750 train_time:136315ms step_avg:96.68ms
step:1411/1750 train_time:136416ms step_avg:96.68ms
step:1412/1750 train_time:136516ms step_avg:96.68ms
step:1413/1750 train_time:136616ms step_avg:96.69ms
step:1414/1750 train_time:136717ms step_avg:96.69ms
step:1415/1750 train_time:136817ms step_avg:96.69ms
step:1416/1750 train_time:136917ms step_avg:96.69ms
step:1417/1750 train_time:137017ms step_avg:96.70ms
step:1418/1750 train_time:137118ms step_avg:96.70ms
step:1419/1750 train_time:137218ms step_avg:96.70ms
step:1420/1750 train_time:137320ms step_avg:96.70ms
step:1421/1750 train_time:137418ms step_avg:96.71ms
step:1422/1750 train_time:137520ms step_avg:96.71ms
step:1423/1750 train_time:137618ms step_avg:96.71ms
step:1424/1750 train_time:137717ms step_avg:96.71ms
step:1425/1750 train_time:137819ms step_avg:96.72ms
step:1426/1750 train_time:137920ms step_avg:96.72ms
step:1427/1750 train_time:138020ms step_avg:96.72ms
step:1428/1750 train_time:138120ms step_avg:96.72ms
step:1429/1750 train_time:138221ms step_avg:96.73ms
step:1430/1750 train_time:138321ms step_avg:96.73ms
step:1431/1750 train_time:138424ms step_avg:96.73ms
step:1432/1750 train_time:138524ms step_avg:96.73ms
step:1433/1750 train_time:138626ms step_avg:96.74ms
step:1434/1750 train_time:138726ms step_avg:96.74ms
step:1435/1750 train_time:138825ms step_avg:96.74ms
step:1436/1750 train_time:138928ms step_avg:96.75ms
step:1437/1750 train_time:139028ms step_avg:96.75ms
step:1438/1750 train_time:139132ms step_avg:96.75ms
step:1439/1750 train_time:139232ms step_avg:96.76ms
step:1440/1750 train_time:139335ms step_avg:96.76ms
step:1441/1750 train_time:139436ms step_avg:96.76ms
step:1442/1750 train_time:139537ms step_avg:96.77ms
step:1443/1750 train_time:139636ms step_avg:96.77ms
step:1444/1750 train_time:139738ms step_avg:96.77ms
step:1445/1750 train_time:139840ms step_avg:96.78ms
step:1446/1750 train_time:139941ms step_avg:96.78ms
step:1447/1750 train_time:140042ms step_avg:96.78ms
step:1448/1750 train_time:140143ms step_avg:96.78ms
step:1449/1750 train_time:140245ms step_avg:96.79ms
step:1450/1750 train_time:140345ms step_avg:96.79ms
step:1451/1750 train_time:140450ms step_avg:96.80ms
step:1452/1750 train_time:140554ms step_avg:96.80ms
step:1453/1750 train_time:140654ms step_avg:96.80ms
step:1454/1750 train_time:140755ms step_avg:96.81ms
step:1455/1750 train_time:140858ms step_avg:96.81ms
step:1456/1750 train_time:140958ms step_avg:96.81ms
step:1457/1750 train_time:141057ms step_avg:96.81ms
step:1458/1750 train_time:141160ms step_avg:96.82ms
step:1459/1750 train_time:141260ms step_avg:96.82ms
step:1460/1750 train_time:141362ms step_avg:96.82ms
step:1461/1750 train_time:141465ms step_avg:96.83ms
step:1462/1750 train_time:141566ms step_avg:96.83ms
step:1463/1750 train_time:141667ms step_avg:96.83ms
step:1464/1750 train_time:141770ms step_avg:96.84ms
step:1465/1750 train_time:141873ms step_avg:96.84ms
step:1466/1750 train_time:141975ms step_avg:96.85ms
step:1467/1750 train_time:142076ms step_avg:96.85ms
step:1468/1750 train_time:142176ms step_avg:96.85ms
step:1469/1750 train_time:142278ms step_avg:96.85ms
step:1470/1750 train_time:142378ms step_avg:96.86ms
step:1471/1750 train_time:142480ms step_avg:96.86ms
step:1472/1750 train_time:142581ms step_avg:96.86ms
step:1473/1750 train_time:142681ms step_avg:96.86ms
step:1474/1750 train_time:142782ms step_avg:96.87ms
step:1475/1750 train_time:142883ms step_avg:96.87ms
step:1476/1750 train_time:142983ms step_avg:96.87ms
step:1477/1750 train_time:143086ms step_avg:96.88ms
step:1478/1750 train_time:143188ms step_avg:96.88ms
step:1479/1750 train_time:143292ms step_avg:96.88ms
step:1480/1750 train_time:143393ms step_avg:96.89ms
step:1481/1750 train_time:143493ms step_avg:96.89ms
step:1482/1750 train_time:143594ms step_avg:96.89ms
step:1483/1750 train_time:143696ms step_avg:96.90ms
step:1484/1750 train_time:143796ms step_avg:96.90ms
step:1485/1750 train_time:143899ms step_avg:96.90ms
step:1486/1750 train_time:144002ms step_avg:96.91ms
step:1487/1750 train_time:144103ms step_avg:96.91ms
step:1488/1750 train_time:144204ms step_avg:96.91ms
step:1489/1750 train_time:144306ms step_avg:96.91ms
step:1490/1750 train_time:144406ms step_avg:96.92ms
step:1491/1750 train_time:144509ms step_avg:96.92ms
step:1492/1750 train_time:144611ms step_avg:96.92ms
step:1493/1750 train_time:144713ms step_avg:96.93ms
step:1494/1750 train_time:144817ms step_avg:96.93ms
step:1495/1750 train_time:144917ms step_avg:96.93ms
step:1496/1750 train_time:145018ms step_avg:96.94ms
step:1497/1750 train_time:145117ms step_avg:96.94ms
step:1498/1750 train_time:145218ms step_avg:96.94ms
step:1499/1750 train_time:145321ms step_avg:96.95ms
step:1500/1750 train_time:145422ms step_avg:96.95ms
step:1500/1750 val_loss:3.5222 train_time:145534ms step_avg:97.02ms
step:1501/1750 train_time:145551ms step_avg:96.97ms
step:1502/1750 train_time:145629ms step_avg:96.96ms
step:1503/1750 train_time:145735ms step_avg:96.96ms
step:1504/1750 train_time:145835ms step_avg:96.96ms
step:1505/1750 train_time:145935ms step_avg:96.97ms
step:1506/1750 train_time:146035ms step_avg:96.97ms
step:1507/1750 train_time:146134ms step_avg:96.97ms
step:1508/1750 train_time:146233ms step_avg:96.97ms
step:1509/1750 train_time:146335ms step_avg:96.97ms
step:1510/1750 train_time:146435ms step_avg:96.98ms
step:1511/1750 train_time:146539ms step_avg:96.98ms
step:1512/1750 train_time:146643ms step_avg:96.99ms
step:1513/1750 train_time:146744ms step_avg:96.99ms
step:1514/1750 train_time:146846ms step_avg:96.99ms
step:1515/1750 train_time:146944ms step_avg:96.99ms
step:1516/1750 train_time:147047ms step_avg:97.00ms
step:1517/1750 train_time:147148ms step_avg:97.00ms
step:1518/1750 train_time:147249ms step_avg:97.00ms
step:1519/1750 train_time:147352ms step_avg:97.01ms
step:1520/1750 train_time:147455ms step_avg:97.01ms
step:1521/1750 train_time:147557ms step_avg:97.01ms
step:1522/1750 train_time:147658ms step_avg:97.02ms
step:1523/1750 train_time:147758ms step_avg:97.02ms
step:1524/1750 train_time:147858ms step_avg:97.02ms
step:1525/1750 train_time:147959ms step_avg:97.02ms
step:1526/1750 train_time:148064ms step_avg:97.03ms
step:1527/1750 train_time:148164ms step_avg:97.03ms
step:1528/1750 train_time:148266ms step_avg:97.03ms
step:1529/1750 train_time:148371ms step_avg:97.04ms
step:1530/1750 train_time:148474ms step_avg:97.04ms
step:1531/1750 train_time:148576ms step_avg:97.05ms
step:1532/1750 train_time:148677ms step_avg:97.05ms
step:1533/1750 train_time:148776ms step_avg:97.05ms
step:1534/1750 train_time:148877ms step_avg:97.05ms
step:1535/1750 train_time:148978ms step_avg:97.05ms
step:1536/1750 train_time:149077ms step_avg:97.06ms
step:1537/1750 train_time:149179ms step_avg:97.06ms
step:1538/1750 train_time:149281ms step_avg:97.06ms
step:1539/1750 train_time:149382ms step_avg:97.06ms
step:1540/1750 train_time:149484ms step_avg:97.07ms
step:1541/1750 train_time:149587ms step_avg:97.07ms
step:1542/1750 train_time:149690ms step_avg:97.07ms
step:1543/1750 train_time:149791ms step_avg:97.08ms
step:1544/1750 train_time:149893ms step_avg:97.08ms
step:1545/1750 train_time:149996ms step_avg:97.08ms
step:1546/1750 train_time:150097ms step_avg:97.09ms
step:1547/1750 train_time:150197ms step_avg:97.09ms
step:1548/1750 train_time:150297ms step_avg:97.09ms
step:1549/1750 train_time:150398ms step_avg:97.09ms
step:1550/1750 train_time:150499ms step_avg:97.10ms
step:1551/1750 train_time:150600ms step_avg:97.10ms
step:1552/1750 train_time:150702ms step_avg:97.10ms
step:1553/1750 train_time:150806ms step_avg:97.11ms
step:1554/1750 train_time:150906ms step_avg:97.11ms
step:1555/1750 train_time:151007ms step_avg:97.11ms
step:1556/1750 train_time:151108ms step_avg:97.11ms
step:1557/1750 train_time:151212ms step_avg:97.12ms
step:1558/1750 train_time:151315ms step_avg:97.12ms
step:1559/1750 train_time:151416ms step_avg:97.12ms
step:1560/1750 train_time:151518ms step_avg:97.13ms
step:1561/1750 train_time:151618ms step_avg:97.13ms
step:1562/1750 train_time:151719ms step_avg:97.13ms
step:1563/1750 train_time:151821ms step_avg:97.13ms
step:1564/1750 train_time:151927ms step_avg:97.14ms
step:1565/1750 train_time:152027ms step_avg:97.14ms
step:1566/1750 train_time:152128ms step_avg:97.14ms
step:1567/1750 train_time:152231ms step_avg:97.15ms
step:1568/1750 train_time:152334ms step_avg:97.15ms
step:1569/1750 train_time:152434ms step_avg:97.15ms
step:1570/1750 train_time:152535ms step_avg:97.16ms
step:1571/1750 train_time:152636ms step_avg:97.16ms
step:1572/1750 train_time:152736ms step_avg:97.16ms
step:1573/1750 train_time:152837ms step_avg:97.16ms
step:1574/1750 train_time:152938ms step_avg:97.17ms
step:1575/1750 train_time:153040ms step_avg:97.17ms
step:1576/1750 train_time:153142ms step_avg:97.17ms
step:1577/1750 train_time:153245ms step_avg:97.18ms
step:1578/1750 train_time:153347ms step_avg:97.18ms
step:1579/1750 train_time:153449ms step_avg:97.18ms
step:1580/1750 train_time:153550ms step_avg:97.18ms
step:1581/1750 train_time:153654ms step_avg:97.19ms
step:1582/1750 train_time:153755ms step_avg:97.19ms
step:1583/1750 train_time:153856ms step_avg:97.19ms
step:1584/1750 train_time:153957ms step_avg:97.19ms
step:1585/1750 train_time:154057ms step_avg:97.20ms
step:1586/1750 train_time:154160ms step_avg:97.20ms
step:1587/1750 train_time:154264ms step_avg:97.20ms
step:1588/1750 train_time:154363ms step_avg:97.21ms
step:1589/1750 train_time:154466ms step_avg:97.21ms
step:1590/1750 train_time:154567ms step_avg:97.21ms
step:1591/1750 train_time:154668ms step_avg:97.21ms
step:1592/1750 train_time:154772ms step_avg:97.22ms
step:1593/1750 train_time:154874ms step_avg:97.22ms
step:1594/1750 train_time:154975ms step_avg:97.22ms
step:1595/1750 train_time:155082ms step_avg:97.23ms
step:1596/1750 train_time:155183ms step_avg:97.23ms
step:1597/1750 train_time:155282ms step_avg:97.23ms
step:1598/1750 train_time:155384ms step_avg:97.24ms
step:1599/1750 train_time:155485ms step_avg:97.24ms
step:1600/1750 train_time:155584ms step_avg:97.24ms
step:1601/1750 train_time:155685ms step_avg:97.24ms
step:1602/1750 train_time:155786ms step_avg:97.24ms
step:1603/1750 train_time:155890ms step_avg:97.25ms
step:1604/1750 train_time:155994ms step_avg:97.25ms
step:1605/1750 train_time:156095ms step_avg:97.26ms
step:1606/1750 train_time:156196ms step_avg:97.26ms
step:1607/1750 train_time:156297ms step_avg:97.26ms
step:1608/1750 train_time:156396ms step_avg:97.26ms
step:1609/1750 train_time:156498ms step_avg:97.26ms
step:1610/1750 train_time:156601ms step_avg:97.27ms
step:1611/1750 train_time:156702ms step_avg:97.27ms
step:1612/1750 train_time:156803ms step_avg:97.27ms
step:1613/1750 train_time:156907ms step_avg:97.28ms
step:1614/1750 train_time:157008ms step_avg:97.28ms
step:1615/1750 train_time:157110ms step_avg:97.28ms
step:1616/1750 train_time:157214ms step_avg:97.29ms
step:1617/1750 train_time:157315ms step_avg:97.29ms
step:1618/1750 train_time:157417ms step_avg:97.29ms
step:1619/1750 train_time:157516ms step_avg:97.29ms
step:1620/1750 train_time:157616ms step_avg:97.29ms
step:1621/1750 train_time:157719ms step_avg:97.30ms
step:1622/1750 train_time:157819ms step_avg:97.30ms
step:1623/1750 train_time:157922ms step_avg:97.30ms
step:1624/1750 train_time:158022ms step_avg:97.30ms
step:1625/1750 train_time:158127ms step_avg:97.31ms
step:1625/1750 val_loss:3.4899 train_time:158239ms step_avg:97.38ms
step:1626/1750 train_time:158256ms step_avg:97.33ms
step:1627/1750 train_time:158336ms step_avg:97.32ms
step:1628/1750 train_time:158441ms step_avg:97.32ms
step:1629/1750 train_time:158542ms step_avg:97.32ms
step:1630/1750 train_time:158645ms step_avg:97.33ms
step:1631/1750 train_time:158743ms step_avg:97.33ms
step:1632/1750 train_time:158847ms step_avg:97.33ms
step:1633/1750 train_time:158947ms step_avg:97.33ms
step:1634/1750 train_time:159047ms step_avg:97.34ms
step:1635/1750 train_time:159149ms step_avg:97.34ms
step:1636/1750 train_time:159251ms step_avg:97.34ms
step:1637/1750 train_time:159354ms step_avg:97.34ms
step:1638/1750 train_time:159455ms step_avg:97.35ms
step:1639/1750 train_time:159556ms step_avg:97.35ms
step:1640/1750 train_time:159658ms step_avg:97.35ms
step:1641/1750 train_time:159758ms step_avg:97.35ms
step:1642/1750 train_time:159856ms step_avg:97.35ms
step:1643/1750 train_time:159958ms step_avg:97.36ms
step:1644/1750 train_time:160059ms step_avg:97.36ms
step:1645/1750 train_time:160162ms step_avg:97.36ms
step:1646/1750 train_time:160262ms step_avg:97.36ms
step:1647/1750 train_time:160366ms step_avg:97.37ms
step:1648/1750 train_time:160469ms step_avg:97.37ms
step:1649/1750 train_time:160572ms step_avg:97.38ms
step:1650/1750 train_time:160672ms step_avg:97.38ms
step:1651/1750 train_time:160772ms step_avg:97.38ms
step:1652/1750 train_time:160874ms step_avg:97.38ms
step:1653/1750 train_time:160976ms step_avg:97.38ms
step:1654/1750 train_time:161078ms step_avg:97.39ms
step:1655/1750 train_time:161177ms step_avg:97.39ms
step:1656/1750 train_time:161279ms step_avg:97.39ms
step:1657/1750 train_time:161382ms step_avg:97.39ms
step:1658/1750 train_time:161483ms step_avg:97.40ms
step:1659/1750 train_time:161586ms step_avg:97.40ms
step:1660/1750 train_time:161691ms step_avg:97.40ms
step:1661/1750 train_time:161792ms step_avg:97.41ms
step:1662/1750 train_time:161896ms step_avg:97.41ms
step:1663/1750 train_time:161994ms step_avg:97.41ms
step:1664/1750 train_time:162097ms step_avg:97.41ms
step:1665/1750 train_time:162201ms step_avg:97.42ms
step:1666/1750 train_time:162300ms step_avg:97.42ms
step:1667/1750 train_time:162401ms step_avg:97.42ms
step:1668/1750 train_time:162504ms step_avg:97.42ms
step:1669/1750 train_time:162607ms step_avg:97.43ms
step:1670/1750 train_time:162709ms step_avg:97.43ms
step:1671/1750 train_time:162811ms step_avg:97.43ms
step:1672/1750 train_time:162912ms step_avg:97.44ms
step:1673/1750 train_time:163013ms step_avg:97.44ms
step:1674/1750 train_time:163113ms step_avg:97.44ms
step:1675/1750 train_time:163213ms step_avg:97.44ms
step:1676/1750 train_time:163317ms step_avg:97.44ms
step:1677/1750 train_time:163418ms step_avg:97.45ms
step:1678/1750 train_time:163521ms step_avg:97.45ms
step:1679/1750 train_time:163621ms step_avg:97.45ms
step:1680/1750 train_time:163723ms step_avg:97.45ms
step:1681/1750 train_time:163826ms step_avg:97.46ms
step:1682/1750 train_time:163929ms step_avg:97.46ms
step:1683/1750 train_time:164031ms step_avg:97.46ms
step:1684/1750 train_time:164132ms step_avg:97.47ms
step:1685/1750 train_time:164232ms step_avg:97.47ms
step:1686/1750 train_time:164335ms step_avg:97.47ms
step:1687/1750 train_time:164435ms step_avg:97.47ms
step:1688/1750 train_time:164537ms step_avg:97.47ms
step:1689/1750 train_time:164639ms step_avg:97.48ms
step:1690/1750 train_time:164740ms step_avg:97.48ms
step:1691/1750 train_time:164843ms step_avg:97.48ms
step:1692/1750 train_time:164948ms step_avg:97.49ms
step:1693/1750 train_time:165051ms step_avg:97.49ms
step:1694/1750 train_time:165155ms step_avg:97.49ms
step:1695/1750 train_time:165255ms step_avg:97.50ms
step:1696/1750 train_time:165356ms step_avg:97.50ms
step:1697/1750 train_time:165460ms step_avg:97.50ms
step:1698/1750 train_time:165561ms step_avg:97.50ms
step:1699/1750 train_time:165663ms step_avg:97.51ms
step:1700/1750 train_time:165767ms step_avg:97.51ms
step:1701/1750 train_time:165871ms step_avg:97.51ms
step:1702/1750 train_time:165972ms step_avg:97.52ms
step:1703/1750 train_time:166074ms step_avg:97.52ms
step:1704/1750 train_time:166177ms step_avg:97.52ms
step:1705/1750 train_time:166278ms step_avg:97.52ms
step:1706/1750 train_time:166377ms step_avg:97.52ms
step:1707/1750 train_time:166479ms step_avg:97.53ms
step:1708/1750 train_time:166583ms step_avg:97.53ms
step:1709/1750 train_time:166685ms step_avg:97.53ms
step:1710/1750 train_time:166788ms step_avg:97.54ms
step:1711/1750 train_time:166891ms step_avg:97.54ms
step:1712/1750 train_time:166995ms step_avg:97.54ms
step:1713/1750 train_time:167096ms step_avg:97.55ms
step:1714/1750 train_time:167198ms step_avg:97.55ms
step:1715/1750 train_time:167302ms step_avg:97.55ms
step:1716/1750 train_time:167403ms step_avg:97.55ms
step:1717/1750 train_time:167509ms step_avg:97.56ms
step:1718/1750 train_time:167608ms step_avg:97.56ms
step:1719/1750 train_time:167711ms step_avg:97.56ms
step:1720/1750 train_time:167813ms step_avg:97.57ms
step:1721/1750 train_time:167914ms step_avg:97.57ms
step:1722/1750 train_time:168016ms step_avg:97.57ms
step:1723/1750 train_time:168120ms step_avg:97.57ms
step:1724/1750 train_time:168222ms step_avg:97.58ms
step:1725/1750 train_time:168327ms step_avg:97.58ms
step:1726/1750 train_time:168429ms step_avg:97.58ms
step:1727/1750 train_time:168530ms step_avg:97.59ms
step:1728/1750 train_time:168632ms step_avg:97.59ms
step:1729/1750 train_time:168735ms step_avg:97.59ms
step:1730/1750 train_time:168837ms step_avg:97.59ms
step:1731/1750 train_time:168939ms step_avg:97.60ms
step:1732/1750 train_time:169041ms step_avg:97.60ms
step:1733/1750 train_time:169143ms step_avg:97.60ms
step:1734/1750 train_time:169246ms step_avg:97.60ms
step:1735/1750 train_time:169352ms step_avg:97.61ms
step:1736/1750 train_time:169453ms step_avg:97.61ms
step:1737/1750 train_time:169553ms step_avg:97.61ms
step:1738/1750 train_time:169657ms step_avg:97.62ms
step:1739/1750 train_time:169760ms step_avg:97.62ms
step:1740/1750 train_time:169860ms step_avg:97.62ms
step:1741/1750 train_time:169960ms step_avg:97.62ms
step:1742/1750 train_time:170066ms step_avg:97.63ms
step:1743/1750 train_time:170169ms step_avg:97.63ms
step:1744/1750 train_time:170270ms step_avg:97.63ms
step:1745/1750 train_time:170373ms step_avg:97.64ms
step:1746/1750 train_time:170475ms step_avg:97.64ms
step:1747/1750 train_time:170578ms step_avg:97.64ms
step:1748/1750 train_time:170679ms step_avg:97.64ms
step:1749/1750 train_time:170782ms step_avg:97.65ms
step:1750/1750 train_time:170884ms step_avg:97.65ms
step:1750/1750 val_loss:3.4651 train_time:170996ms step_avg:97.71ms
peak memory allocated: 35009 MiB reserved: 49934 MiB
