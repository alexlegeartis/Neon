import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X
'''
import torch.utils.dlpack as thd
from cupyx.scipy.sparse.linalg import svds as cupyx_svds

def several_sv_svds_approximation(W_torch, k, num_iter=50):
    """SVD approximation using the top k singular values and corresponding vectors."""
    # Store original device and dtype
    original_device = W_torch.device
    original_dtype = W_torch.dtype
    
    W = cp.from_dlpack(thd.to_dlpack(W_torch)).astype(cp.float32)
    U, S, Vt = cupyx_svds(W, k=min([k, W.shape[0] - 1, W.shape[1] - 1]), maxiter=num_iter, which='LM')

    # Convert back to torch tensors and ensure they're on the correct device
    approx_torch_U = thd.from_dlpack(U.toDlpack()).to(device=original_device, dtype=original_dtype)
    approx_torch_S = thd.from_dlpack(S.toDlpack()).to(device=original_device, dtype=original_dtype)
    approx_torch_Vt = thd.from_dlpack(Vt.toDlpack()).to(device=original_device, dtype=original_dtype)
    
    return approx_torch_U, approx_torch_S, approx_torch_Vt

class NormNeon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    # v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    u, s, vt = several_sv_svds_approximation(grad.bfloat16(), 5)
                    p.add_(other=u@vt, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()
'''
class FastNormMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, sgd_coeff=0):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, sgd_coeff=sgd_coeff)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        self.sgd_coeff = sgd_coeff

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            sgd_coeff = group["sgd_coeff"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    # assume: grad is a Tensor, momentum_buffer exists, momentum, sgd_coeff, eps, eff_lr are defined
                    with torch.no_grad():
                        # inplace blend with momentum (keeps original semantic: grad becomes the lerped tensor)
                        grad.lerp_(momentum_buffer, momentum)

                        # compute zeropower part from the (post-lerp) grad, in bfloat16 to match the original code
                        if sgd_coeff != 1:
                            # create update_part in bfloat16 (may allocate once per call; unavoidable if zeropower expects bfloat16)
                            update_part = zeropower_via_newtonschulz5(grad.to(torch.bfloat16), 5)

                            # upcast update_part to grad dtype to do mixed-dtype blending without extra temporaries later
                            if update_part.dtype != grad.dtype:
                                update_part = update_part.to(grad.dtype)

                        # compute inverse norm scalar efficiently (no large temp tensors)
                        # - view(-1) is cheap (no copy)
                        # - dot(view, view) is a single reduction
                        norm_sq = grad.view(-1).dot(grad.view(-1))
                        inv_norm = (norm_sq + eps).rsqrt()  # stable rsqrt on the scalar

                        # normalize grad in-place (so we avoid allocating g_normalized)
                        grad.mul_(inv_norm)   # now grad == g_normalized (in-place)

                        # build final update (reuse update_part buffer if available)
                        if sgd_coeff != 1:
                            # update_part := (1 - sgd_coeff) * update_part + sgd_coeff * grad
                            # do it in-place on update_part to avoid creating another temp
                            update_part.mul_(1 - sgd_coeff)    # in-place scale
                            update_part.lerp_(grad, sgd_coeff) # in-place linear interpolation
                            update = update_part
                        else:
                            # if sgd_coeff == 1 we can use grad directly (already normalized in-place)
                            update = grad

                        # apply update (same as original)
                        p.add_(other=update, alpha=-eff_lr)
                    '''
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    eps = 1e-12
                    g_normalized = grad / (grad.norm() + eps)
                    if sgd_coeff != 1:
                        update_part = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                        update = (1 - sgd_coeff) * update_part + sgd_coeff * g_normalized
                    else:
                        update = sgd_coeff * g_normalized
                    p.add_(other=update, alpha=-eff_lr)
                    '''
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class NormMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, sgd_coeff=0):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, sgd_coeff=sgd_coeff)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        self.sgd_coeff = sgd_coeff

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            sgd_coeff = group["sgd_coeff"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    eps = 1e-12
                    g_normalized = grad / (grad.norm() + eps)
                    if sgd_coeff != 1:
                        update_part = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                        update = (1 - sgd_coeff) * update_part + sgd_coeff * g_normalized
                    else:
                        update = sgd_coeff * g_normalized
                    p.add_(other=update, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()
class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)
                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_reduce_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 27.5
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos : pos + max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()
    starts = []
    for i in range(1, len(boundary_positions)):
        end = boundary_positions[i].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == dist.get_world_size():
                return starts, end - pos
            start = end
    assert False # increase max_batch_span if necessary

def distributed_data_generator(filename_pattern: str, batch_size: int, align_to_bos: bool):
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    max_batch_span = 2 * batch_size if align_to_bos else batch_size # provide buffer to handle samples up to length local_batch_size
    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            batch_span = batch_size
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1750 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
from datetime import datetime
if master_process:
    # run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_id = str(int(time.time() * 1000))
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0)
optimizer2 = NormMuon(hidden_matrix_params, lr=0.05, momentum=0.98, weight_decay=0, sgd_coeff=0.5)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.11 | packaged by conda-forge | (main, Jun  4 2025, 14:45:31) [GCC 13.3.0]
Running PyTorch 2.9.0.dev20250824+cu126 compiled for CUDA 12.6
Sun Aug 24 18:23:58 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:04:00.0 Off |                    0 |
| N/A   41C    P0            122W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:23:00.0 Off |                    0 |
| N/A   34C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:43:00.0 Off |                    0 |
| N/A   40C    P0            125W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   34C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:84:00.0 Off |                    0 |
| N/A   39C    P0            123W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:A3:00.0 Off |                    0 |
| N/A   34C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:C3:00.0 Off |                    0 |
| N/A   41C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E4:00.0 Off |                    0 |
| N/A   33C    P0            114W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.06ms
step:1/1750 train_time:146ms step_avg:145.75ms
step:2/1750 train_time:166ms step_avg:82.92ms
step:3/1750 train_time:247ms step_avg:82.43ms
step:4/1750 train_time:339ms step_avg:84.65ms
step:5/1750 train_time:432ms step_avg:86.33ms
step:6/1750 train_time:524ms step_avg:87.37ms
step:7/1750 train_time:617ms step_avg:88.10ms
step:8/1750 train_time:709ms step_avg:88.65ms
step:9/1750 train_time:802ms step_avg:89.10ms
step:10/1750 train_time:894ms step_avg:89.43ms
step:11/1750 train_time:987ms step_avg:89.70ms
step:12/1750 train_time:1082ms step_avg:90.18ms
step:13/1750 train_time:1179ms step_avg:90.68ms
step:14/1750 train_time:1276ms step_avg:91.14ms
step:15/1750 train_time:1369ms step_avg:91.30ms
step:16/1750 train_time:1462ms step_avg:91.38ms
step:17/1750 train_time:1555ms step_avg:91.48ms
step:18/1750 train_time:1648ms step_avg:91.55ms
step:19/1750 train_time:1741ms step_avg:91.61ms
step:20/1750 train_time:1834ms step_avg:91.71ms
step:21/1750 train_time:1927ms step_avg:91.78ms
step:22/1750 train_time:2021ms step_avg:91.84ms
step:23/1750 train_time:2115ms step_avg:91.97ms
step:24/1750 train_time:2211ms step_avg:92.11ms
step:25/1750 train_time:2306ms step_avg:92.22ms
step:26/1750 train_time:2399ms step_avg:92.27ms
step:27/1750 train_time:2493ms step_avg:92.32ms
step:28/1750 train_time:2586ms step_avg:92.34ms
step:29/1750 train_time:2678ms step_avg:92.36ms
step:30/1750 train_time:2772ms step_avg:92.40ms
step:31/1750 train_time:2865ms step_avg:92.41ms
step:32/1750 train_time:2957ms step_avg:92.41ms
step:33/1750 train_time:3051ms step_avg:92.44ms
step:34/1750 train_time:3145ms step_avg:92.49ms
step:35/1750 train_time:3239ms step_avg:92.55ms
step:36/1750 train_time:3334ms step_avg:92.60ms
step:37/1750 train_time:3428ms step_avg:92.64ms
step:38/1750 train_time:3521ms step_avg:92.66ms
step:39/1750 train_time:3615ms step_avg:92.68ms
step:40/1750 train_time:3708ms step_avg:92.69ms
step:41/1750 train_time:3801ms step_avg:92.70ms
step:42/1750 train_time:3894ms step_avg:92.72ms
step:43/1750 train_time:3988ms step_avg:92.75ms
step:44/1750 train_time:4081ms step_avg:92.75ms
step:45/1750 train_time:4175ms step_avg:92.78ms
step:46/1750 train_time:4270ms step_avg:92.83ms
step:47/1750 train_time:4364ms step_avg:92.85ms
step:48/1750 train_time:4458ms step_avg:92.87ms
step:49/1750 train_time:4551ms step_avg:92.89ms
step:50/1750 train_time:4645ms step_avg:92.90ms
step:51/1750 train_time:4738ms step_avg:92.90ms
step:52/1750 train_time:4832ms step_avg:92.92ms
step:53/1750 train_time:4925ms step_avg:92.93ms
step:54/1750 train_time:5018ms step_avg:92.93ms
step:55/1750 train_time:5112ms step_avg:92.95ms
step:56/1750 train_time:5207ms step_avg:92.98ms
step:57/1750 train_time:5300ms step_avg:92.99ms
step:58/1750 train_time:5394ms step_avg:93.00ms
step:59/1750 train_time:5488ms step_avg:93.01ms
step:60/1750 train_time:5581ms step_avg:93.02ms
step:61/1750 train_time:5675ms step_avg:93.03ms
step:62/1750 train_time:5768ms step_avg:93.04ms
step:63/1750 train_time:5862ms step_avg:93.04ms
step:64/1750 train_time:5955ms step_avg:93.04ms
step:65/1750 train_time:6048ms step_avg:93.05ms
step:66/1750 train_time:6142ms step_avg:93.06ms
step:67/1750 train_time:6236ms step_avg:93.07ms
step:68/1750 train_time:6330ms step_avg:93.08ms
step:69/1750 train_time:6423ms step_avg:93.08ms
step:70/1750 train_time:6516ms step_avg:93.08ms
step:71/1750 train_time:6610ms step_avg:93.09ms
step:72/1750 train_time:6702ms step_avg:93.09ms
step:73/1750 train_time:6796ms step_avg:93.09ms
step:74/1750 train_time:6889ms step_avg:93.09ms
step:75/1750 train_time:6982ms step_avg:93.09ms
step:76/1750 train_time:7075ms step_avg:93.09ms
step:77/1750 train_time:7170ms step_avg:93.12ms
step:78/1750 train_time:7263ms step_avg:93.12ms
step:79/1750 train_time:7357ms step_avg:93.12ms
step:80/1750 train_time:7451ms step_avg:93.14ms
step:81/1750 train_time:7544ms step_avg:93.14ms
step:82/1750 train_time:7638ms step_avg:93.15ms
step:83/1750 train_time:7732ms step_avg:93.16ms
step:84/1750 train_time:7826ms step_avg:93.17ms
step:85/1750 train_time:7919ms step_avg:93.17ms
step:86/1750 train_time:8013ms step_avg:93.17ms
step:87/1750 train_time:8106ms step_avg:93.17ms
step:88/1750 train_time:8200ms step_avg:93.18ms
step:89/1750 train_time:8294ms step_avg:93.19ms
step:90/1750 train_time:8388ms step_avg:93.20ms
step:91/1750 train_time:8481ms step_avg:93.20ms
step:92/1750 train_time:8574ms step_avg:93.19ms
step:93/1750 train_time:8667ms step_avg:93.19ms
step:94/1750 train_time:8760ms step_avg:93.19ms
step:95/1750 train_time:8854ms step_avg:93.20ms
step:96/1750 train_time:8947ms step_avg:93.20ms
step:97/1750 train_time:9041ms step_avg:93.21ms
step:98/1750 train_time:9135ms step_avg:93.21ms
step:99/1750 train_time:9229ms step_avg:93.22ms
step:100/1750 train_time:9322ms step_avg:93.22ms
step:101/1750 train_time:9416ms step_avg:93.23ms
step:102/1750 train_time:9510ms step_avg:93.23ms
step:103/1750 train_time:9602ms step_avg:93.22ms
step:104/1750 train_time:9696ms step_avg:93.23ms
step:105/1750 train_time:9791ms step_avg:93.25ms
step:106/1750 train_time:9884ms step_avg:93.25ms
step:107/1750 train_time:9977ms step_avg:93.24ms
step:108/1750 train_time:10071ms step_avg:93.25ms
step:109/1750 train_time:10165ms step_avg:93.25ms
step:110/1750 train_time:10258ms step_avg:93.26ms
step:111/1750 train_time:10351ms step_avg:93.25ms
step:112/1750 train_time:10444ms step_avg:93.25ms
step:113/1750 train_time:10538ms step_avg:93.25ms
step:114/1750 train_time:10631ms step_avg:93.25ms
step:115/1750 train_time:10724ms step_avg:93.25ms
step:116/1750 train_time:10818ms step_avg:93.26ms
step:117/1750 train_time:10911ms step_avg:93.26ms
step:118/1750 train_time:11005ms step_avg:93.27ms
step:119/1750 train_time:11099ms step_avg:93.27ms
step:120/1750 train_time:11193ms step_avg:93.28ms
step:121/1750 train_time:11286ms step_avg:93.27ms
step:122/1750 train_time:11379ms step_avg:93.27ms
step:123/1750 train_time:11473ms step_avg:93.27ms
step:124/1750 train_time:11565ms step_avg:93.27ms
step:125/1750 train_time:11659ms step_avg:93.27ms
step:125/1750 val_loss:4.6821 train_time:11742ms step_avg:93.94ms
step:126/1750 train_time:11764ms step_avg:93.36ms
step:127/1750 train_time:11857ms step_avg:93.36ms
step:128/1750 train_time:11960ms step_avg:93.44ms
step:129/1750 train_time:12055ms step_avg:93.45ms
step:130/1750 train_time:12148ms step_avg:93.44ms
step:131/1750 train_time:12240ms step_avg:93.43ms
step:132/1750 train_time:12333ms step_avg:93.43ms
step:133/1750 train_time:12426ms step_avg:93.43ms
step:134/1750 train_time:12519ms step_avg:93.43ms
step:135/1750 train_time:12612ms step_avg:93.42ms
step:136/1750 train_time:12706ms step_avg:93.43ms
step:137/1750 train_time:12799ms step_avg:93.42ms
step:138/1750 train_time:12895ms step_avg:93.45ms
step:139/1750 train_time:12991ms step_avg:93.46ms
step:140/1750 train_time:13086ms step_avg:93.47ms
step:141/1750 train_time:13180ms step_avg:93.47ms
step:142/1750 train_time:13273ms step_avg:93.47ms
step:143/1750 train_time:13367ms step_avg:93.47ms
step:144/1750 train_time:13460ms step_avg:93.47ms
step:145/1750 train_time:13553ms step_avg:93.47ms
step:146/1750 train_time:13646ms step_avg:93.47ms
step:147/1750 train_time:13740ms step_avg:93.47ms
step:148/1750 train_time:13834ms step_avg:93.47ms
step:149/1750 train_time:13929ms step_avg:93.48ms
step:150/1750 train_time:14025ms step_avg:93.50ms
step:151/1750 train_time:14119ms step_avg:93.50ms
step:152/1750 train_time:14213ms step_avg:93.50ms
step:153/1750 train_time:14306ms step_avg:93.50ms
step:154/1750 train_time:14400ms step_avg:93.51ms
step:155/1750 train_time:14493ms step_avg:93.50ms
step:156/1750 train_time:14586ms step_avg:93.50ms
step:157/1750 train_time:14681ms step_avg:93.51ms
step:158/1750 train_time:14774ms step_avg:93.51ms
step:159/1750 train_time:14868ms step_avg:93.51ms
step:160/1750 train_time:14962ms step_avg:93.52ms
step:161/1750 train_time:15056ms step_avg:93.52ms
step:162/1750 train_time:15150ms step_avg:93.52ms
step:163/1750 train_time:15245ms step_avg:93.53ms
step:164/1750 train_time:15339ms step_avg:93.53ms
step:165/1750 train_time:15432ms step_avg:93.53ms
step:166/1750 train_time:15526ms step_avg:93.53ms
step:167/1750 train_time:15620ms step_avg:93.53ms
step:168/1750 train_time:15713ms step_avg:93.53ms
step:169/1750 train_time:15807ms step_avg:93.53ms
step:170/1750 train_time:15901ms step_avg:93.54ms
step:171/1750 train_time:15996ms step_avg:93.55ms
step:172/1750 train_time:16091ms step_avg:93.55ms
step:173/1750 train_time:16185ms step_avg:93.56ms
step:174/1750 train_time:16279ms step_avg:93.56ms
step:175/1750 train_time:16372ms step_avg:93.55ms
step:176/1750 train_time:16466ms step_avg:93.55ms
step:177/1750 train_time:16559ms step_avg:93.55ms
step:178/1750 train_time:16652ms step_avg:93.55ms
step:179/1750 train_time:16746ms step_avg:93.55ms
step:180/1750 train_time:16840ms step_avg:93.56ms
step:181/1750 train_time:16935ms step_avg:93.56ms
step:182/1750 train_time:17029ms step_avg:93.57ms
step:183/1750 train_time:17124ms step_avg:93.57ms
step:184/1750 train_time:17218ms step_avg:93.58ms
step:185/1750 train_time:17311ms step_avg:93.57ms
step:186/1750 train_time:17405ms step_avg:93.57ms
step:187/1750 train_time:17499ms step_avg:93.58ms
step:188/1750 train_time:17592ms step_avg:93.58ms
step:189/1750 train_time:17686ms step_avg:93.58ms
step:190/1750 train_time:17780ms step_avg:93.58ms
step:191/1750 train_time:17873ms step_avg:93.58ms
step:192/1750 train_time:17967ms step_avg:93.58ms
step:193/1750 train_time:18062ms step_avg:93.58ms
step:194/1750 train_time:18156ms step_avg:93.59ms
step:195/1750 train_time:18250ms step_avg:93.59ms
step:196/1750 train_time:18344ms step_avg:93.59ms
step:197/1750 train_time:18439ms step_avg:93.60ms
step:198/1750 train_time:18532ms step_avg:93.60ms
step:199/1750 train_time:18626ms step_avg:93.60ms
step:200/1750 train_time:18720ms step_avg:93.60ms
step:201/1750 train_time:18813ms step_avg:93.60ms
step:202/1750 train_time:18907ms step_avg:93.60ms
step:203/1750 train_time:19001ms step_avg:93.60ms
step:204/1750 train_time:19095ms step_avg:93.60ms
step:205/1750 train_time:19189ms step_avg:93.60ms
step:206/1750 train_time:19283ms step_avg:93.61ms
step:207/1750 train_time:19376ms step_avg:93.61ms
step:208/1750 train_time:19470ms step_avg:93.61ms
step:209/1750 train_time:19564ms step_avg:93.61ms
step:210/1750 train_time:19658ms step_avg:93.61ms
step:211/1750 train_time:19751ms step_avg:93.60ms
step:212/1750 train_time:19845ms step_avg:93.61ms
step:213/1750 train_time:19939ms step_avg:93.61ms
step:214/1750 train_time:20033ms step_avg:93.61ms
step:215/1750 train_time:20126ms step_avg:93.61ms
step:216/1750 train_time:20221ms step_avg:93.62ms
step:217/1750 train_time:20314ms step_avg:93.61ms
step:218/1750 train_time:20408ms step_avg:93.62ms
step:219/1750 train_time:20503ms step_avg:93.62ms
step:220/1750 train_time:20597ms step_avg:93.62ms
step:221/1750 train_time:20690ms step_avg:93.62ms
step:222/1750 train_time:20784ms step_avg:93.62ms
step:223/1750 train_time:20878ms step_avg:93.62ms
step:224/1750 train_time:20972ms step_avg:93.62ms
step:225/1750 train_time:21066ms step_avg:93.63ms
step:226/1750 train_time:21160ms step_avg:93.63ms
step:227/1750 train_time:21254ms step_avg:93.63ms
step:228/1750 train_time:21347ms step_avg:93.63ms
step:229/1750 train_time:21441ms step_avg:93.63ms
step:230/1750 train_time:21535ms step_avg:93.63ms
step:231/1750 train_time:21629ms step_avg:93.63ms
step:232/1750 train_time:21723ms step_avg:93.64ms
step:233/1750 train_time:21817ms step_avg:93.64ms
step:234/1750 train_time:21911ms step_avg:93.64ms
step:235/1750 train_time:22006ms step_avg:93.64ms
step:236/1750 train_time:22100ms step_avg:93.64ms
step:237/1750 train_time:22194ms step_avg:93.65ms
step:238/1750 train_time:22287ms step_avg:93.64ms
step:239/1750 train_time:22382ms step_avg:93.65ms
step:240/1750 train_time:22476ms step_avg:93.65ms
step:241/1750 train_time:22569ms step_avg:93.65ms
step:242/1750 train_time:22664ms step_avg:93.65ms
step:243/1750 train_time:22758ms step_avg:93.66ms
step:244/1750 train_time:22851ms step_avg:93.65ms
step:245/1750 train_time:22946ms step_avg:93.66ms
step:246/1750 train_time:23039ms step_avg:93.66ms
step:247/1750 train_time:23133ms step_avg:93.66ms
step:248/1750 train_time:23227ms step_avg:93.66ms
step:249/1750 train_time:23321ms step_avg:93.66ms
step:250/1750 train_time:23415ms step_avg:93.66ms
step:250/1750 val_loss:4.1077 train_time:23498ms step_avg:93.99ms
step:251/1750 train_time:23520ms step_avg:93.71ms
step:252/1750 train_time:23609ms step_avg:93.69ms
step:253/1750 train_time:23709ms step_avg:93.71ms
step:254/1750 train_time:23803ms step_avg:93.71ms
step:255/1750 train_time:23898ms step_avg:93.72ms
step:256/1750 train_time:23991ms step_avg:93.71ms
step:257/1750 train_time:24084ms step_avg:93.71ms
step:258/1750 train_time:24177ms step_avg:93.71ms
step:259/1750 train_time:24269ms step_avg:93.70ms
step:260/1750 train_time:24362ms step_avg:93.70ms
step:261/1750 train_time:24456ms step_avg:93.70ms
step:262/1750 train_time:24551ms step_avg:93.71ms
step:263/1750 train_time:24647ms step_avg:93.72ms
step:264/1750 train_time:24743ms step_avg:93.72ms
step:265/1750 train_time:24838ms step_avg:93.73ms
step:266/1750 train_time:24932ms step_avg:93.73ms
step:267/1750 train_time:25026ms step_avg:93.73ms
step:268/1750 train_time:25119ms step_avg:93.73ms
step:269/1750 train_time:25213ms step_avg:93.73ms
step:270/1750 train_time:25306ms step_avg:93.73ms
step:271/1750 train_time:25400ms step_avg:93.73ms
step:272/1750 train_time:25495ms step_avg:93.73ms
step:273/1750 train_time:25589ms step_avg:93.73ms
step:274/1750 train_time:25686ms step_avg:93.74ms
step:275/1750 train_time:25781ms step_avg:93.75ms
step:276/1750 train_time:25875ms step_avg:93.75ms
step:277/1750 train_time:25969ms step_avg:93.75ms
step:278/1750 train_time:26064ms step_avg:93.76ms
step:279/1750 train_time:26158ms step_avg:93.76ms
step:280/1750 train_time:26251ms step_avg:93.75ms
step:281/1750 train_time:26346ms step_avg:93.76ms
step:282/1750 train_time:26439ms step_avg:93.76ms
step:283/1750 train_time:26534ms step_avg:93.76ms
step:284/1750 train_time:26628ms step_avg:93.76ms
step:285/1750 train_time:26723ms step_avg:93.76ms
step:286/1750 train_time:26818ms step_avg:93.77ms
step:287/1750 train_time:26913ms step_avg:93.77ms
step:288/1750 train_time:27007ms step_avg:93.78ms
step:289/1750 train_time:27101ms step_avg:93.78ms
step:290/1750 train_time:27195ms step_avg:93.78ms
step:291/1750 train_time:27289ms step_avg:93.78ms
step:292/1750 train_time:27383ms step_avg:93.78ms
step:293/1750 train_time:27478ms step_avg:93.78ms
step:294/1750 train_time:27571ms step_avg:93.78ms
step:295/1750 train_time:27666ms step_avg:93.78ms
step:296/1750 train_time:27760ms step_avg:93.79ms
step:297/1750 train_time:27855ms step_avg:93.79ms
step:298/1750 train_time:27949ms step_avg:93.79ms
step:299/1750 train_time:28044ms step_avg:93.79ms
step:300/1750 train_time:28139ms step_avg:93.80ms
step:301/1750 train_time:28232ms step_avg:93.80ms
step:302/1750 train_time:28326ms step_avg:93.80ms
step:303/1750 train_time:28421ms step_avg:93.80ms
step:304/1750 train_time:28515ms step_avg:93.80ms
step:305/1750 train_time:28609ms step_avg:93.80ms
step:306/1750 train_time:28703ms step_avg:93.80ms
step:307/1750 train_time:28798ms step_avg:93.81ms
step:308/1750 train_time:28893ms step_avg:93.81ms
step:309/1750 train_time:28987ms step_avg:93.81ms
step:310/1750 train_time:29081ms step_avg:93.81ms
step:311/1750 train_time:29175ms step_avg:93.81ms
step:312/1750 train_time:29269ms step_avg:93.81ms
step:313/1750 train_time:29364ms step_avg:93.81ms
step:314/1750 train_time:29459ms step_avg:93.82ms
step:315/1750 train_time:29553ms step_avg:93.82ms
step:316/1750 train_time:29646ms step_avg:93.82ms
step:317/1750 train_time:29741ms step_avg:93.82ms
step:318/1750 train_time:29836ms step_avg:93.82ms
step:319/1750 train_time:29930ms step_avg:93.82ms
step:320/1750 train_time:30024ms step_avg:93.83ms
step:321/1750 train_time:30119ms step_avg:93.83ms
step:322/1750 train_time:30212ms step_avg:93.83ms
step:323/1750 train_time:30306ms step_avg:93.83ms
step:324/1750 train_time:30400ms step_avg:93.83ms
step:325/1750 train_time:30494ms step_avg:93.83ms
step:326/1750 train_time:30588ms step_avg:93.83ms
step:327/1750 train_time:30682ms step_avg:93.83ms
step:328/1750 train_time:30777ms step_avg:93.83ms
step:329/1750 train_time:30870ms step_avg:93.83ms
step:330/1750 train_time:30964ms step_avg:93.83ms
step:331/1750 train_time:31058ms step_avg:93.83ms
step:332/1750 train_time:31153ms step_avg:93.83ms
step:333/1750 train_time:31247ms step_avg:93.83ms
step:334/1750 train_time:31342ms step_avg:93.84ms
step:335/1750 train_time:31436ms step_avg:93.84ms
step:336/1750 train_time:31530ms step_avg:93.84ms
step:337/1750 train_time:31624ms step_avg:93.84ms
step:338/1750 train_time:31719ms step_avg:93.84ms
step:339/1750 train_time:31813ms step_avg:93.84ms
step:340/1750 train_time:31907ms step_avg:93.84ms
step:341/1750 train_time:32001ms step_avg:93.84ms
step:342/1750 train_time:32097ms step_avg:93.85ms
step:343/1750 train_time:32191ms step_avg:93.85ms
step:344/1750 train_time:32285ms step_avg:93.85ms
step:345/1750 train_time:32379ms step_avg:93.85ms
step:346/1750 train_time:32474ms step_avg:93.85ms
step:347/1750 train_time:32568ms step_avg:93.86ms
step:348/1750 train_time:32662ms step_avg:93.86ms
step:349/1750 train_time:32757ms step_avg:93.86ms
step:350/1750 train_time:32852ms step_avg:93.86ms
step:351/1750 train_time:32945ms step_avg:93.86ms
step:352/1750 train_time:33040ms step_avg:93.86ms
step:353/1750 train_time:33134ms step_avg:93.86ms
step:354/1750 train_time:33228ms step_avg:93.86ms
step:355/1750 train_time:33322ms step_avg:93.86ms
step:356/1750 train_time:33417ms step_avg:93.87ms
step:357/1750 train_time:33510ms step_avg:93.87ms
step:358/1750 train_time:33604ms step_avg:93.87ms
step:359/1750 train_time:33699ms step_avg:93.87ms
step:360/1750 train_time:33793ms step_avg:93.87ms
step:361/1750 train_time:33888ms step_avg:93.87ms
step:362/1750 train_time:33982ms step_avg:93.87ms
step:363/1750 train_time:34076ms step_avg:93.87ms
step:364/1750 train_time:34170ms step_avg:93.87ms
step:365/1750 train_time:34265ms step_avg:93.88ms
step:366/1750 train_time:34359ms step_avg:93.88ms
step:367/1750 train_time:34453ms step_avg:93.88ms
step:368/1750 train_time:34547ms step_avg:93.88ms
step:369/1750 train_time:34642ms step_avg:93.88ms
step:370/1750 train_time:34736ms step_avg:93.88ms
step:371/1750 train_time:34830ms step_avg:93.88ms
step:372/1750 train_time:34924ms step_avg:93.88ms
step:373/1750 train_time:35018ms step_avg:93.88ms
step:374/1750 train_time:35113ms step_avg:93.89ms
step:375/1750 train_time:35207ms step_avg:93.89ms
step:375/1750 val_loss:3.8947 train_time:35291ms step_avg:94.11ms
step:376/1750 train_time:35312ms step_avg:93.91ms
step:377/1750 train_time:35403ms step_avg:93.91ms
step:378/1750 train_time:35498ms step_avg:93.91ms
step:379/1750 train_time:35593ms step_avg:93.91ms
step:380/1750 train_time:35686ms step_avg:93.91ms
step:381/1750 train_time:35780ms step_avg:93.91ms
step:382/1750 train_time:35873ms step_avg:93.91ms
step:383/1750 train_time:35967ms step_avg:93.91ms
step:384/1750 train_time:36060ms step_avg:93.91ms
step:385/1750 train_time:36154ms step_avg:93.91ms
step:386/1750 train_time:36249ms step_avg:93.91ms
step:387/1750 train_time:36344ms step_avg:93.91ms
step:388/1750 train_time:36439ms step_avg:93.92ms
step:389/1750 train_time:36534ms step_avg:93.92ms
step:390/1750 train_time:36630ms step_avg:93.92ms
step:391/1750 train_time:36726ms step_avg:93.93ms
step:392/1750 train_time:36822ms step_avg:93.93ms
step:393/1750 train_time:36917ms step_avg:93.94ms
step:394/1750 train_time:37012ms step_avg:93.94ms
step:395/1750 train_time:37107ms step_avg:93.94ms
step:396/1750 train_time:37203ms step_avg:93.95ms
step:397/1750 train_time:37300ms step_avg:93.95ms
step:398/1750 train_time:37397ms step_avg:93.96ms
step:399/1750 train_time:37494ms step_avg:93.97ms
step:400/1750 train_time:37591ms step_avg:93.98ms
step:401/1750 train_time:37688ms step_avg:93.98ms
step:402/1750 train_time:37784ms step_avg:93.99ms
step:403/1750 train_time:37879ms step_avg:93.99ms
step:404/1750 train_time:37975ms step_avg:94.00ms
step:405/1750 train_time:38070ms step_avg:94.00ms
step:406/1750 train_time:38166ms step_avg:94.01ms
step:407/1750 train_time:38262ms step_avg:94.01ms
step:408/1750 train_time:38358ms step_avg:94.01ms
step:409/1750 train_time:38454ms step_avg:94.02ms
step:410/1750 train_time:38552ms step_avg:94.03ms
step:411/1750 train_time:38648ms step_avg:94.03ms
step:412/1750 train_time:38743ms step_avg:94.04ms
step:413/1750 train_time:38839ms step_avg:94.04ms
step:414/1750 train_time:38935ms step_avg:94.05ms
step:415/1750 train_time:39031ms step_avg:94.05ms
step:416/1750 train_time:39127ms step_avg:94.05ms
step:417/1750 train_time:39223ms step_avg:94.06ms
step:418/1750 train_time:39319ms step_avg:94.06ms
step:419/1750 train_time:39415ms step_avg:94.07ms
step:420/1750 train_time:39513ms step_avg:94.08ms
step:421/1750 train_time:39609ms step_avg:94.08ms
step:422/1750 train_time:39705ms step_avg:94.09ms
step:423/1750 train_time:39801ms step_avg:94.09ms
step:424/1750 train_time:39897ms step_avg:94.10ms
step:425/1750 train_time:39993ms step_avg:94.10ms
step:426/1750 train_time:40090ms step_avg:94.11ms
step:427/1750 train_time:40186ms step_avg:94.11ms
step:428/1750 train_time:40282ms step_avg:94.12ms
step:429/1750 train_time:40379ms step_avg:94.12ms
step:430/1750 train_time:40475ms step_avg:94.13ms
step:431/1750 train_time:40571ms step_avg:94.13ms
step:432/1750 train_time:40668ms step_avg:94.14ms
step:433/1750 train_time:40764ms step_avg:94.14ms
step:434/1750 train_time:40861ms step_avg:94.15ms
step:435/1750 train_time:40958ms step_avg:94.16ms
step:436/1750 train_time:41053ms step_avg:94.16ms
step:437/1750 train_time:41149ms step_avg:94.16ms
step:438/1750 train_time:41245ms step_avg:94.17ms
step:439/1750 train_time:41342ms step_avg:94.17ms
step:440/1750 train_time:41438ms step_avg:94.18ms
step:441/1750 train_time:41534ms step_avg:94.18ms
step:442/1750 train_time:41630ms step_avg:94.19ms
step:443/1750 train_time:41726ms step_avg:94.19ms
step:444/1750 train_time:41822ms step_avg:94.19ms
step:445/1750 train_time:41918ms step_avg:94.20ms
step:446/1750 train_time:42014ms step_avg:94.20ms
step:447/1750 train_time:42110ms step_avg:94.21ms
step:448/1750 train_time:42206ms step_avg:94.21ms
step:449/1750 train_time:42301ms step_avg:94.21ms
step:450/1750 train_time:42398ms step_avg:94.22ms
step:451/1750 train_time:42494ms step_avg:94.22ms
step:452/1750 train_time:42590ms step_avg:94.23ms
step:453/1750 train_time:42686ms step_avg:94.23ms
step:454/1750 train_time:42783ms step_avg:94.23ms
step:455/1750 train_time:42878ms step_avg:94.24ms
step:456/1750 train_time:42974ms step_avg:94.24ms
step:457/1750 train_time:43071ms step_avg:94.25ms
step:458/1750 train_time:43166ms step_avg:94.25ms
step:459/1750 train_time:43263ms step_avg:94.25ms
step:460/1750 train_time:43358ms step_avg:94.26ms
step:461/1750 train_time:43454ms step_avg:94.26ms
step:462/1750 train_time:43550ms step_avg:94.26ms
step:463/1750 train_time:43646ms step_avg:94.27ms
step:464/1750 train_time:43742ms step_avg:94.27ms
step:465/1750 train_time:43838ms step_avg:94.28ms
step:466/1750 train_time:43934ms step_avg:94.28ms
step:467/1750 train_time:44030ms step_avg:94.28ms
step:468/1750 train_time:44127ms step_avg:94.29ms
step:469/1750 train_time:44223ms step_avg:94.29ms
step:470/1750 train_time:44318ms step_avg:94.29ms
step:471/1750 train_time:44414ms step_avg:94.30ms
step:472/1750 train_time:44510ms step_avg:94.30ms
step:473/1750 train_time:44607ms step_avg:94.31ms
step:474/1750 train_time:44704ms step_avg:94.31ms
step:475/1750 train_time:44800ms step_avg:94.32ms
step:476/1750 train_time:44895ms step_avg:94.32ms
step:477/1750 train_time:44992ms step_avg:94.32ms
step:478/1750 train_time:45088ms step_avg:94.33ms
step:479/1750 train_time:45183ms step_avg:94.33ms
step:480/1750 train_time:45279ms step_avg:94.33ms
step:481/1750 train_time:45376ms step_avg:94.34ms
step:482/1750 train_time:45472ms step_avg:94.34ms
step:483/1750 train_time:45568ms step_avg:94.34ms
step:484/1750 train_time:45664ms step_avg:94.35ms
step:485/1750 train_time:45760ms step_avg:94.35ms
step:486/1750 train_time:45856ms step_avg:94.35ms
step:487/1750 train_time:45953ms step_avg:94.36ms
step:488/1750 train_time:46049ms step_avg:94.36ms
step:489/1750 train_time:46145ms step_avg:94.37ms
step:490/1750 train_time:46241ms step_avg:94.37ms
step:491/1750 train_time:46337ms step_avg:94.37ms
step:492/1750 train_time:46432ms step_avg:94.37ms
step:493/1750 train_time:46529ms step_avg:94.38ms
step:494/1750 train_time:46625ms step_avg:94.38ms
step:495/1750 train_time:46721ms step_avg:94.39ms
step:496/1750 train_time:46817ms step_avg:94.39ms
step:497/1750 train_time:46913ms step_avg:94.39ms
step:498/1750 train_time:47009ms step_avg:94.40ms
step:499/1750 train_time:47105ms step_avg:94.40ms
step:500/1750 train_time:47201ms step_avg:94.40ms
step:500/1750 val_loss:3.7471 train_time:47287ms step_avg:94.57ms
step:501/1750 train_time:47308ms step_avg:94.43ms
step:502/1750 train_time:47401ms step_avg:94.43ms
step:503/1750 train_time:47500ms step_avg:94.43ms
step:504/1750 train_time:47596ms step_avg:94.44ms
step:505/1750 train_time:47692ms step_avg:94.44ms
step:506/1750 train_time:47788ms step_avg:94.44ms
step:507/1750 train_time:47883ms step_avg:94.44ms
step:508/1750 train_time:47978ms step_avg:94.44ms
step:509/1750 train_time:48073ms step_avg:94.45ms
step:510/1750 train_time:48168ms step_avg:94.45ms
step:511/1750 train_time:48264ms step_avg:94.45ms
step:512/1750 train_time:48362ms step_avg:94.46ms
step:513/1750 train_time:48459ms step_avg:94.46ms
step:514/1750 train_time:48556ms step_avg:94.47ms
step:515/1750 train_time:48652ms step_avg:94.47ms
step:516/1750 train_time:48748ms step_avg:94.47ms
step:517/1750 train_time:48844ms step_avg:94.48ms
step:518/1750 train_time:48939ms step_avg:94.48ms
step:519/1750 train_time:49034ms step_avg:94.48ms
step:520/1750 train_time:49130ms step_avg:94.48ms
step:521/1750 train_time:49226ms step_avg:94.48ms
step:522/1750 train_time:49322ms step_avg:94.49ms
step:523/1750 train_time:49419ms step_avg:94.49ms
step:524/1750 train_time:49517ms step_avg:94.50ms
step:525/1750 train_time:49614ms step_avg:94.50ms
step:526/1750 train_time:49710ms step_avg:94.51ms
step:527/1750 train_time:49807ms step_avg:94.51ms
step:528/1750 train_time:49903ms step_avg:94.51ms
step:529/1750 train_time:49999ms step_avg:94.52ms
step:530/1750 train_time:50095ms step_avg:94.52ms
step:531/1750 train_time:50191ms step_avg:94.52ms
step:532/1750 train_time:50288ms step_avg:94.53ms
step:533/1750 train_time:50385ms step_avg:94.53ms
step:534/1750 train_time:50482ms step_avg:94.54ms
step:535/1750 train_time:50579ms step_avg:94.54ms
step:536/1750 train_time:50676ms step_avg:94.54ms
step:537/1750 train_time:50773ms step_avg:94.55ms
step:538/1750 train_time:50869ms step_avg:94.55ms
step:539/1750 train_time:50967ms step_avg:94.56ms
step:540/1750 train_time:51062ms step_avg:94.56ms
step:541/1750 train_time:51159ms step_avg:94.56ms
step:542/1750 train_time:51255ms step_avg:94.57ms
step:543/1750 train_time:51351ms step_avg:94.57ms
step:544/1750 train_time:51449ms step_avg:94.58ms
step:545/1750 train_time:51547ms step_avg:94.58ms
step:546/1750 train_time:51643ms step_avg:94.58ms
step:547/1750 train_time:51740ms step_avg:94.59ms
step:548/1750 train_time:51836ms step_avg:94.59ms
step:549/1750 train_time:51932ms step_avg:94.59ms
step:550/1750 train_time:52028ms step_avg:94.60ms
step:551/1750 train_time:52124ms step_avg:94.60ms
step:552/1750 train_time:52220ms step_avg:94.60ms
step:553/1750 train_time:52317ms step_avg:94.61ms
step:554/1750 train_time:52415ms step_avg:94.61ms
step:555/1750 train_time:52511ms step_avg:94.61ms
step:556/1750 train_time:52609ms step_avg:94.62ms
step:557/1750 train_time:52706ms step_avg:94.62ms
step:558/1750 train_time:52803ms step_avg:94.63ms
step:559/1750 train_time:52899ms step_avg:94.63ms
step:560/1750 train_time:52995ms step_avg:94.63ms
step:561/1750 train_time:53091ms step_avg:94.64ms
step:562/1750 train_time:53187ms step_avg:94.64ms
step:563/1750 train_time:53283ms step_avg:94.64ms
step:564/1750 train_time:53380ms step_avg:94.65ms
step:565/1750 train_time:53477ms step_avg:94.65ms
step:566/1750 train_time:53574ms step_avg:94.65ms
step:567/1750 train_time:53671ms step_avg:94.66ms
step:568/1750 train_time:53767ms step_avg:94.66ms
step:569/1750 train_time:53864ms step_avg:94.66ms
step:570/1750 train_time:53960ms step_avg:94.67ms
step:571/1750 train_time:54056ms step_avg:94.67ms
step:572/1750 train_time:54153ms step_avg:94.67ms
step:573/1750 train_time:54250ms step_avg:94.68ms
step:574/1750 train_time:54347ms step_avg:94.68ms
step:575/1750 train_time:54445ms step_avg:94.69ms
step:576/1750 train_time:54541ms step_avg:94.69ms
step:577/1750 train_time:54637ms step_avg:94.69ms
step:578/1750 train_time:54734ms step_avg:94.69ms
step:579/1750 train_time:54830ms step_avg:94.70ms
step:580/1750 train_time:54926ms step_avg:94.70ms
step:581/1750 train_time:55022ms step_avg:94.70ms
step:582/1750 train_time:55119ms step_avg:94.71ms
step:583/1750 train_time:55215ms step_avg:94.71ms
step:584/1750 train_time:55311ms step_avg:94.71ms
step:585/1750 train_time:55409ms step_avg:94.72ms
step:586/1750 train_time:55505ms step_avg:94.72ms
step:587/1750 train_time:55602ms step_avg:94.72ms
step:588/1750 train_time:55698ms step_avg:94.73ms
step:589/1750 train_time:55795ms step_avg:94.73ms
step:590/1750 train_time:55892ms step_avg:94.73ms
step:591/1750 train_time:55988ms step_avg:94.73ms
step:592/1750 train_time:56085ms step_avg:94.74ms
step:593/1750 train_time:56182ms step_avg:94.74ms
step:594/1750 train_time:56279ms step_avg:94.75ms
step:595/1750 train_time:56375ms step_avg:94.75ms
step:596/1750 train_time:56472ms step_avg:94.75ms
step:597/1750 train_time:56569ms step_avg:94.76ms
step:598/1750 train_time:56666ms step_avg:94.76ms
step:599/1750 train_time:56762ms step_avg:94.76ms
step:600/1750 train_time:56859ms step_avg:94.77ms
step:601/1750 train_time:56956ms step_avg:94.77ms
step:602/1750 train_time:57051ms step_avg:94.77ms
step:603/1750 train_time:57149ms step_avg:94.77ms
step:604/1750 train_time:57246ms step_avg:94.78ms
step:605/1750 train_time:57342ms step_avg:94.78ms
step:606/1750 train_time:57439ms step_avg:94.78ms
step:607/1750 train_time:57535ms step_avg:94.79ms
step:608/1750 train_time:57632ms step_avg:94.79ms
step:609/1750 train_time:57729ms step_avg:94.79ms
step:610/1750 train_time:57825ms step_avg:94.80ms
step:611/1750 train_time:57922ms step_avg:94.80ms
step:612/1750 train_time:58018ms step_avg:94.80ms
step:613/1750 train_time:58114ms step_avg:94.80ms
step:614/1750 train_time:58210ms step_avg:94.81ms
step:615/1750 train_time:58308ms step_avg:94.81ms
step:616/1750 train_time:58405ms step_avg:94.81ms
step:617/1750 train_time:58501ms step_avg:94.82ms
step:618/1750 train_time:58597ms step_avg:94.82ms
step:619/1750 train_time:58694ms step_avg:94.82ms
step:620/1750 train_time:58790ms step_avg:94.82ms
step:621/1750 train_time:58887ms step_avg:94.83ms
step:622/1750 train_time:58984ms step_avg:94.83ms
step:623/1750 train_time:59080ms step_avg:94.83ms
step:624/1750 train_time:59177ms step_avg:94.83ms
step:625/1750 train_time:59273ms step_avg:94.84ms
step:625/1750 val_loss:3.6600 train_time:59359ms step_avg:94.97ms
step:626/1750 train_time:59380ms step_avg:94.86ms
step:627/1750 train_time:59474ms step_avg:94.85ms
step:628/1750 train_time:59574ms step_avg:94.86ms
step:629/1750 train_time:59672ms step_avg:94.87ms
step:630/1750 train_time:59767ms step_avg:94.87ms
step:631/1750 train_time:59863ms step_avg:94.87ms
step:632/1750 train_time:59958ms step_avg:94.87ms
step:633/1750 train_time:60054ms step_avg:94.87ms
step:634/1750 train_time:60150ms step_avg:94.87ms
step:635/1750 train_time:60245ms step_avg:94.87ms
step:636/1750 train_time:60342ms step_avg:94.88ms
step:637/1750 train_time:60439ms step_avg:94.88ms
step:638/1750 train_time:60537ms step_avg:94.89ms
step:639/1750 train_time:60635ms step_avg:94.89ms
step:640/1750 train_time:60731ms step_avg:94.89ms
step:641/1750 train_time:60828ms step_avg:94.90ms
step:642/1750 train_time:60924ms step_avg:94.90ms
step:643/1750 train_time:61020ms step_avg:94.90ms
step:644/1750 train_time:61115ms step_avg:94.90ms
step:645/1750 train_time:61211ms step_avg:94.90ms
step:646/1750 train_time:61307ms step_avg:94.90ms
step:647/1750 train_time:61404ms step_avg:94.91ms
step:648/1750 train_time:61502ms step_avg:94.91ms
step:649/1750 train_time:61600ms step_avg:94.92ms
step:650/1750 train_time:61698ms step_avg:94.92ms
step:651/1750 train_time:61796ms step_avg:94.92ms
step:652/1750 train_time:61894ms step_avg:94.93ms
step:653/1750 train_time:61992ms step_avg:94.93ms
step:654/1750 train_time:62089ms step_avg:94.94ms
step:655/1750 train_time:62186ms step_avg:94.94ms
step:656/1750 train_time:62284ms step_avg:94.94ms
step:657/1750 train_time:62381ms step_avg:94.95ms
step:658/1750 train_time:62478ms step_avg:94.95ms
step:659/1750 train_time:62576ms step_avg:94.96ms
step:660/1750 train_time:62675ms step_avg:94.96ms
step:661/1750 train_time:62773ms step_avg:94.97ms
step:662/1750 train_time:62871ms step_avg:94.97ms
step:663/1750 train_time:62968ms step_avg:94.97ms
step:664/1750 train_time:63066ms step_avg:94.98ms
step:665/1750 train_time:63164ms step_avg:94.98ms
step:666/1750 train_time:63261ms step_avg:94.99ms
step:667/1750 train_time:63359ms step_avg:94.99ms
step:668/1750 train_time:63456ms step_avg:94.99ms
step:669/1750 train_time:63555ms step_avg:95.00ms
step:670/1750 train_time:63653ms step_avg:95.00ms
step:671/1750 train_time:63751ms step_avg:95.01ms
step:672/1750 train_time:63848ms step_avg:95.01ms
step:673/1750 train_time:63946ms step_avg:95.02ms
step:674/1750 train_time:64044ms step_avg:95.02ms
step:675/1750 train_time:64142ms step_avg:95.03ms
step:676/1750 train_time:64240ms step_avg:95.03ms
step:677/1750 train_time:64337ms step_avg:95.03ms
step:678/1750 train_time:64435ms step_avg:95.04ms
step:679/1750 train_time:64533ms step_avg:95.04ms
step:680/1750 train_time:64631ms step_avg:95.05ms
step:681/1750 train_time:64730ms step_avg:95.05ms
step:682/1750 train_time:64828ms step_avg:95.06ms
step:683/1750 train_time:64926ms step_avg:95.06ms
step:684/1750 train_time:65023ms step_avg:95.06ms
step:685/1750 train_time:65121ms step_avg:95.07ms
step:686/1750 train_time:65219ms step_avg:95.07ms
step:687/1750 train_time:65316ms step_avg:95.07ms
step:688/1750 train_time:65414ms step_avg:95.08ms
step:689/1750 train_time:65512ms step_avg:95.08ms
step:690/1750 train_time:65611ms step_avg:95.09ms
step:691/1750 train_time:65708ms step_avg:95.09ms
step:692/1750 train_time:65807ms step_avg:95.10ms
step:693/1750 train_time:65905ms step_avg:95.10ms
step:694/1750 train_time:66004ms step_avg:95.11ms
step:695/1750 train_time:66102ms step_avg:95.11ms
step:696/1750 train_time:66199ms step_avg:95.11ms
step:697/1750 train_time:66295ms step_avg:95.12ms
step:698/1750 train_time:66392ms step_avg:95.12ms
step:699/1750 train_time:66491ms step_avg:95.12ms
step:700/1750 train_time:66589ms step_avg:95.13ms
step:701/1750 train_time:66686ms step_avg:95.13ms
step:702/1750 train_time:66784ms step_avg:95.13ms
step:703/1750 train_time:66883ms step_avg:95.14ms
step:704/1750 train_time:66981ms step_avg:95.14ms
step:705/1750 train_time:67079ms step_avg:95.15ms
step:706/1750 train_time:67177ms step_avg:95.15ms
step:707/1750 train_time:67275ms step_avg:95.16ms
step:708/1750 train_time:67372ms step_avg:95.16ms
step:709/1750 train_time:67471ms step_avg:95.16ms
step:710/1750 train_time:67568ms step_avg:95.17ms
step:711/1750 train_time:67666ms step_avg:95.17ms
step:712/1750 train_time:67765ms step_avg:95.18ms
step:713/1750 train_time:67864ms step_avg:95.18ms
step:714/1750 train_time:67963ms step_avg:95.19ms
step:715/1750 train_time:68060ms step_avg:95.19ms
step:716/1750 train_time:68157ms step_avg:95.19ms
step:717/1750 train_time:68255ms step_avg:95.20ms
step:718/1750 train_time:68353ms step_avg:95.20ms
step:719/1750 train_time:68451ms step_avg:95.20ms
step:720/1750 train_time:68548ms step_avg:95.21ms
step:721/1750 train_time:68645ms step_avg:95.21ms
step:722/1750 train_time:68743ms step_avg:95.21ms
step:723/1750 train_time:68840ms step_avg:95.21ms
step:724/1750 train_time:68938ms step_avg:95.22ms
step:725/1750 train_time:69036ms step_avg:95.22ms
step:726/1750 train_time:69135ms step_avg:95.23ms
step:727/1750 train_time:69233ms step_avg:95.23ms
step:728/1750 train_time:69330ms step_avg:95.23ms
step:729/1750 train_time:69427ms step_avg:95.24ms
step:730/1750 train_time:69525ms step_avg:95.24ms
step:731/1750 train_time:69622ms step_avg:95.24ms
step:732/1750 train_time:69720ms step_avg:95.25ms
step:733/1750 train_time:69818ms step_avg:95.25ms
step:734/1750 train_time:69915ms step_avg:95.25ms
step:735/1750 train_time:70014ms step_avg:95.26ms
step:736/1750 train_time:70112ms step_avg:95.26ms
step:737/1750 train_time:70209ms step_avg:95.26ms
step:738/1750 train_time:70307ms step_avg:95.27ms
step:739/1750 train_time:70405ms step_avg:95.27ms
step:740/1750 train_time:70503ms step_avg:95.27ms
step:741/1750 train_time:70602ms step_avg:95.28ms
step:742/1750 train_time:70700ms step_avg:95.28ms
step:743/1750 train_time:70797ms step_avg:95.29ms
step:744/1750 train_time:70895ms step_avg:95.29ms
step:745/1750 train_time:70992ms step_avg:95.29ms
step:746/1750 train_time:71092ms step_avg:95.30ms
step:747/1750 train_time:71189ms step_avg:95.30ms
step:748/1750 train_time:71287ms step_avg:95.30ms
step:749/1750 train_time:71384ms step_avg:95.31ms
step:750/1750 train_time:71482ms step_avg:95.31ms
step:750/1750 val_loss:3.5966 train_time:71569ms step_avg:95.43ms
step:751/1750 train_time:71590ms step_avg:95.33ms
step:752/1750 train_time:71689ms step_avg:95.33ms
step:753/1750 train_time:71787ms step_avg:95.34ms
step:754/1750 train_time:71885ms step_avg:95.34ms
step:755/1750 train_time:71983ms step_avg:95.34ms
step:756/1750 train_time:72080ms step_avg:95.34ms
step:757/1750 train_time:72176ms step_avg:95.34ms
step:758/1750 train_time:72273ms step_avg:95.35ms
step:759/1750 train_time:72370ms step_avg:95.35ms
step:760/1750 train_time:72466ms step_avg:95.35ms
step:761/1750 train_time:72566ms step_avg:95.36ms
step:762/1750 train_time:72668ms step_avg:95.36ms
step:763/1750 train_time:72767ms step_avg:95.37ms
step:764/1750 train_time:72866ms step_avg:95.37ms
step:765/1750 train_time:72965ms step_avg:95.38ms
step:766/1750 train_time:73062ms step_avg:95.38ms
step:767/1750 train_time:73159ms step_avg:95.38ms
step:768/1750 train_time:73256ms step_avg:95.39ms
step:769/1750 train_time:73353ms step_avg:95.39ms
step:770/1750 train_time:73450ms step_avg:95.39ms
step:771/1750 train_time:73549ms step_avg:95.39ms
step:772/1750 train_time:73648ms step_avg:95.40ms
step:773/1750 train_time:73747ms step_avg:95.40ms
step:774/1750 train_time:73847ms step_avg:95.41ms
step:775/1750 train_time:73945ms step_avg:95.41ms
step:776/1750 train_time:74042ms step_avg:95.41ms
step:777/1750 train_time:74139ms step_avg:95.42ms
step:778/1750 train_time:74237ms step_avg:95.42ms
step:779/1750 train_time:74334ms step_avg:95.42ms
step:780/1750 train_time:74432ms step_avg:95.43ms
step:781/1750 train_time:74531ms step_avg:95.43ms
step:782/1750 train_time:74629ms step_avg:95.43ms
step:783/1750 train_time:74727ms step_avg:95.44ms
step:784/1750 train_time:74827ms step_avg:95.44ms
step:785/1750 train_time:74926ms step_avg:95.45ms
step:786/1750 train_time:75024ms step_avg:95.45ms
step:787/1750 train_time:75123ms step_avg:95.45ms
step:788/1750 train_time:75221ms step_avg:95.46ms
step:789/1750 train_time:75319ms step_avg:95.46ms
step:790/1750 train_time:75417ms step_avg:95.46ms
step:791/1750 train_time:75515ms step_avg:95.47ms
step:792/1750 train_time:75613ms step_avg:95.47ms
step:793/1750 train_time:75711ms step_avg:95.47ms
step:794/1750 train_time:75810ms step_avg:95.48ms
step:795/1750 train_time:75909ms step_avg:95.48ms
step:796/1750 train_time:76008ms step_avg:95.49ms
step:797/1750 train_time:76107ms step_avg:95.49ms
step:798/1750 train_time:76204ms step_avg:95.49ms
step:799/1750 train_time:76304ms step_avg:95.50ms
step:800/1750 train_time:76402ms step_avg:95.50ms
step:801/1750 train_time:76500ms step_avg:95.51ms
step:802/1750 train_time:76597ms step_avg:95.51ms
step:803/1750 train_time:76695ms step_avg:95.51ms
step:804/1750 train_time:76792ms step_avg:95.51ms
step:805/1750 train_time:76891ms step_avg:95.52ms
step:806/1750 train_time:76990ms step_avg:95.52ms
step:807/1750 train_time:77088ms step_avg:95.52ms
step:808/1750 train_time:77186ms step_avg:95.53ms
step:809/1750 train_time:77285ms step_avg:95.53ms
step:810/1750 train_time:77384ms step_avg:95.54ms
step:811/1750 train_time:77483ms step_avg:95.54ms
step:812/1750 train_time:77581ms step_avg:95.54ms
step:813/1750 train_time:77679ms step_avg:95.55ms
step:814/1750 train_time:77777ms step_avg:95.55ms
step:815/1750 train_time:77875ms step_avg:95.55ms
step:816/1750 train_time:77972ms step_avg:95.55ms
step:817/1750 train_time:78071ms step_avg:95.56ms
step:818/1750 train_time:78170ms step_avg:95.56ms
step:819/1750 train_time:78269ms step_avg:95.57ms
step:820/1750 train_time:78367ms step_avg:95.57ms
step:821/1750 train_time:78466ms step_avg:95.57ms
step:822/1750 train_time:78564ms step_avg:95.58ms
step:823/1750 train_time:78662ms step_avg:95.58ms
step:824/1750 train_time:78761ms step_avg:95.58ms
step:825/1750 train_time:78859ms step_avg:95.59ms
step:826/1750 train_time:78956ms step_avg:95.59ms
step:827/1750 train_time:79054ms step_avg:95.59ms
step:828/1750 train_time:79152ms step_avg:95.59ms
step:829/1750 train_time:79250ms step_avg:95.60ms
step:830/1750 train_time:79349ms step_avg:95.60ms
step:831/1750 train_time:79448ms step_avg:95.61ms
step:832/1750 train_time:79546ms step_avg:95.61ms
step:833/1750 train_time:79645ms step_avg:95.61ms
step:834/1750 train_time:79743ms step_avg:95.62ms
step:835/1750 train_time:79842ms step_avg:95.62ms
step:836/1750 train_time:79941ms step_avg:95.62ms
step:837/1750 train_time:80041ms step_avg:95.63ms
step:838/1750 train_time:80139ms step_avg:95.63ms
step:839/1750 train_time:80237ms step_avg:95.63ms
step:840/1750 train_time:80335ms step_avg:95.64ms
step:841/1750 train_time:80434ms step_avg:95.64ms
step:842/1750 train_time:80532ms step_avg:95.64ms
step:843/1750 train_time:80631ms step_avg:95.65ms
step:844/1750 train_time:80729ms step_avg:95.65ms
step:845/1750 train_time:80827ms step_avg:95.65ms
step:846/1750 train_time:80927ms step_avg:95.66ms
step:847/1750 train_time:81026ms step_avg:95.66ms
step:848/1750 train_time:81126ms step_avg:95.67ms
step:849/1750 train_time:81225ms step_avg:95.67ms
step:850/1750 train_time:81323ms step_avg:95.67ms
step:851/1750 train_time:81421ms step_avg:95.68ms
step:852/1750 train_time:81520ms step_avg:95.68ms
step:853/1750 train_time:81617ms step_avg:95.68ms
step:854/1750 train_time:81715ms step_avg:95.69ms
step:855/1750 train_time:81813ms step_avg:95.69ms
step:856/1750 train_time:81912ms step_avg:95.69ms
step:857/1750 train_time:82010ms step_avg:95.69ms
step:858/1750 train_time:82109ms step_avg:95.70ms
step:859/1750 train_time:82208ms step_avg:95.70ms
step:860/1750 train_time:82306ms step_avg:95.71ms
step:861/1750 train_time:82404ms step_avg:95.71ms
step:862/1750 train_time:82501ms step_avg:95.71ms
step:863/1750 train_time:82599ms step_avg:95.71ms
step:864/1750 train_time:82697ms step_avg:95.71ms
step:865/1750 train_time:82795ms step_avg:95.72ms
step:866/1750 train_time:82893ms step_avg:95.72ms
step:867/1750 train_time:82991ms step_avg:95.72ms
step:868/1750 train_time:83089ms step_avg:95.72ms
step:869/1750 train_time:83188ms step_avg:95.73ms
step:870/1750 train_time:83286ms step_avg:95.73ms
step:871/1750 train_time:83387ms step_avg:95.74ms
step:872/1750 train_time:83484ms step_avg:95.74ms
step:873/1750 train_time:83583ms step_avg:95.74ms
step:874/1750 train_time:83681ms step_avg:95.75ms
step:875/1750 train_time:83779ms step_avg:95.75ms
step:875/1750 val_loss:3.5486 train_time:83866ms step_avg:95.85ms
step:876/1750 train_time:83886ms step_avg:95.76ms
step:877/1750 train_time:83982ms step_avg:95.76ms
step:878/1750 train_time:84082ms step_avg:95.77ms
step:879/1750 train_time:84180ms step_avg:95.77ms
step:880/1750 train_time:84279ms step_avg:95.77ms
step:881/1750 train_time:84377ms step_avg:95.77ms
step:882/1750 train_time:84475ms step_avg:95.78ms
step:883/1750 train_time:84572ms step_avg:95.78ms
step:884/1750 train_time:84669ms step_avg:95.78ms
step:885/1750 train_time:84766ms step_avg:95.78ms
step:886/1750 train_time:84865ms step_avg:95.78ms
step:887/1750 train_time:84965ms step_avg:95.79ms
step:888/1750 train_time:85063ms step_avg:95.79ms
step:889/1750 train_time:85162ms step_avg:95.79ms
step:890/1750 train_time:85260ms step_avg:95.80ms
step:891/1750 train_time:85358ms step_avg:95.80ms
step:892/1750 train_time:85456ms step_avg:95.80ms
step:893/1750 train_time:85553ms step_avg:95.80ms
step:894/1750 train_time:85650ms step_avg:95.80ms
step:895/1750 train_time:85748ms step_avg:95.81ms
step:896/1750 train_time:85846ms step_avg:95.81ms
step:897/1750 train_time:85944ms step_avg:95.81ms
step:898/1750 train_time:86043ms step_avg:95.82ms
step:899/1750 train_time:86142ms step_avg:95.82ms
step:900/1750 train_time:86241ms step_avg:95.82ms
step:901/1750 train_time:86339ms step_avg:95.83ms
step:902/1750 train_time:86437ms step_avg:95.83ms
step:903/1750 train_time:86534ms step_avg:95.83ms
step:904/1750 train_time:86632ms step_avg:95.83ms
step:905/1750 train_time:86730ms step_avg:95.83ms
step:906/1750 train_time:86828ms step_avg:95.84ms
step:907/1750 train_time:86926ms step_avg:95.84ms
step:908/1750 train_time:87025ms step_avg:95.84ms
step:909/1750 train_time:87124ms step_avg:95.85ms
step:910/1750 train_time:87223ms step_avg:95.85ms
step:911/1750 train_time:87323ms step_avg:95.85ms
step:912/1750 train_time:87423ms step_avg:95.86ms
step:913/1750 train_time:87523ms step_avg:95.86ms
step:914/1750 train_time:87621ms step_avg:95.87ms
step:915/1750 train_time:87721ms step_avg:95.87ms
step:916/1750 train_time:87820ms step_avg:95.87ms
step:917/1750 train_time:87922ms step_avg:95.88ms
step:918/1750 train_time:88021ms step_avg:95.88ms
step:919/1750 train_time:88121ms step_avg:95.89ms
step:920/1750 train_time:88221ms step_avg:95.89ms
step:921/1750 train_time:88320ms step_avg:95.90ms
step:922/1750 train_time:88420ms step_avg:95.90ms
step:923/1750 train_time:88519ms step_avg:95.90ms
step:924/1750 train_time:88618ms step_avg:95.91ms
step:925/1750 train_time:88716ms step_avg:95.91ms
step:926/1750 train_time:88816ms step_avg:95.91ms
step:927/1750 train_time:88916ms step_avg:95.92ms
step:928/1750 train_time:89016ms step_avg:95.92ms
step:929/1750 train_time:89117ms step_avg:95.93ms
step:930/1750 train_time:89216ms step_avg:95.93ms
step:931/1750 train_time:89317ms step_avg:95.94ms
step:932/1750 train_time:89417ms step_avg:95.94ms
step:933/1750 train_time:89516ms step_avg:95.94ms
step:934/1750 train_time:89616ms step_avg:95.95ms
step:935/1750 train_time:89714ms step_avg:95.95ms
step:936/1750 train_time:89815ms step_avg:95.96ms
step:937/1750 train_time:89914ms step_avg:95.96ms
step:938/1750 train_time:90014ms step_avg:95.96ms
step:939/1750 train_time:90114ms step_avg:95.97ms
step:940/1750 train_time:90214ms step_avg:95.97ms
step:941/1750 train_time:90314ms step_avg:95.98ms
step:942/1750 train_time:90414ms step_avg:95.98ms
step:943/1750 train_time:90514ms step_avg:95.99ms
step:944/1750 train_time:90613ms step_avg:95.99ms
step:945/1750 train_time:90713ms step_avg:95.99ms
step:946/1750 train_time:90812ms step_avg:96.00ms
step:947/1750 train_time:90912ms step_avg:96.00ms
step:948/1750 train_time:91011ms step_avg:96.00ms
step:949/1750 train_time:91112ms step_avg:96.01ms
step:950/1750 train_time:91212ms step_avg:96.01ms
step:951/1750 train_time:91312ms step_avg:96.02ms
step:952/1750 train_time:91411ms step_avg:96.02ms
step:953/1750 train_time:91511ms step_avg:96.02ms
step:954/1750 train_time:91611ms step_avg:96.03ms
step:955/1750 train_time:91710ms step_avg:96.03ms
step:956/1750 train_time:91811ms step_avg:96.04ms
step:957/1750 train_time:91910ms step_avg:96.04ms
step:958/1750 train_time:92010ms step_avg:96.04ms
step:959/1750 train_time:92110ms step_avg:96.05ms
step:960/1750 train_time:92211ms step_avg:96.05ms
step:961/1750 train_time:92312ms step_avg:96.06ms
step:962/1750 train_time:92412ms step_avg:96.06ms
step:963/1750 train_time:92512ms step_avg:96.07ms
step:964/1750 train_time:92611ms step_avg:96.07ms
step:965/1750 train_time:92711ms step_avg:96.07ms
step:966/1750 train_time:92810ms step_avg:96.08ms
step:967/1750 train_time:92910ms step_avg:96.08ms
step:968/1750 train_time:93011ms step_avg:96.09ms
step:969/1750 train_time:93111ms step_avg:96.09ms
step:970/1750 train_time:93210ms step_avg:96.09ms
step:971/1750 train_time:93311ms step_avg:96.10ms
step:972/1750 train_time:93411ms step_avg:96.10ms
step:973/1750 train_time:93512ms step_avg:96.11ms
step:974/1750 train_time:93611ms step_avg:96.11ms
step:975/1750 train_time:93711ms step_avg:96.11ms
step:976/1750 train_time:93810ms step_avg:96.12ms
step:977/1750 train_time:93910ms step_avg:96.12ms
step:978/1750 train_time:94009ms step_avg:96.12ms
step:979/1750 train_time:94110ms step_avg:96.13ms
step:980/1750 train_time:94210ms step_avg:96.13ms
step:981/1750 train_time:94309ms step_avg:96.14ms
step:982/1750 train_time:94409ms step_avg:96.14ms
step:983/1750 train_time:94511ms step_avg:96.15ms
step:984/1750 train_time:94610ms step_avg:96.15ms
step:985/1750 train_time:94710ms step_avg:96.15ms
step:986/1750 train_time:94809ms step_avg:96.16ms
step:987/1750 train_time:94910ms step_avg:96.16ms
step:988/1750 train_time:95009ms step_avg:96.16ms
step:989/1750 train_time:95110ms step_avg:96.17ms
step:990/1750 train_time:95209ms step_avg:96.17ms
step:991/1750 train_time:95310ms step_avg:96.18ms
step:992/1750 train_time:95410ms step_avg:96.18ms
step:993/1750 train_time:95510ms step_avg:96.18ms
step:994/1750 train_time:95609ms step_avg:96.19ms
step:995/1750 train_time:95710ms step_avg:96.19ms
step:996/1750 train_time:95810ms step_avg:96.19ms
step:997/1750 train_time:95909ms step_avg:96.20ms
step:998/1750 train_time:96009ms step_avg:96.20ms
step:999/1750 train_time:96110ms step_avg:96.21ms
step:1000/1750 train_time:96211ms step_avg:96.21ms
step:1000/1750 val_loss:3.5071 train_time:96299ms step_avg:96.30ms
step:1001/1750 train_time:96320ms step_avg:96.22ms
step:1002/1750 train_time:96416ms step_avg:96.22ms
step:1003/1750 train_time:96515ms step_avg:96.23ms
step:1004/1750 train_time:96614ms step_avg:96.23ms
step:1005/1750 train_time:96714ms step_avg:96.23ms
step:1006/1750 train_time:96813ms step_avg:96.24ms
step:1007/1750 train_time:96911ms step_avg:96.24ms
step:1008/1750 train_time:97009ms step_avg:96.24ms
step:1009/1750 train_time:97108ms step_avg:96.24ms
step:1010/1750 train_time:97207ms step_avg:96.24ms
step:1011/1750 train_time:97309ms step_avg:96.25ms
step:1012/1750 train_time:97410ms step_avg:96.25ms
step:1013/1750 train_time:97510ms step_avg:96.26ms
step:1014/1750 train_time:97610ms step_avg:96.26ms
step:1015/1750 train_time:97709ms step_avg:96.27ms
step:1016/1750 train_time:97808ms step_avg:96.27ms
step:1017/1750 train_time:97907ms step_avg:96.27ms
step:1018/1750 train_time:98005ms step_avg:96.27ms
step:1019/1750 train_time:98104ms step_avg:96.27ms
step:1020/1750 train_time:98204ms step_avg:96.28ms
step:1021/1750 train_time:98304ms step_avg:96.28ms
step:1022/1750 train_time:98405ms step_avg:96.29ms
step:1023/1750 train_time:98506ms step_avg:96.29ms
step:1024/1750 train_time:98608ms step_avg:96.30ms
step:1025/1750 train_time:98707ms step_avg:96.30ms
step:1026/1750 train_time:98806ms step_avg:96.30ms
step:1027/1750 train_time:98905ms step_avg:96.30ms
step:1028/1750 train_time:99003ms step_avg:96.31ms
step:1029/1750 train_time:99103ms step_avg:96.31ms
step:1030/1750 train_time:99202ms step_avg:96.31ms
step:1031/1750 train_time:99302ms step_avg:96.32ms
step:1032/1750 train_time:99402ms step_avg:96.32ms
step:1033/1750 train_time:99502ms step_avg:96.32ms
step:1034/1750 train_time:99603ms step_avg:96.33ms
step:1035/1750 train_time:99703ms step_avg:96.33ms
step:1036/1750 train_time:99802ms step_avg:96.33ms
step:1037/1750 train_time:99902ms step_avg:96.34ms
step:1038/1750 train_time:100001ms step_avg:96.34ms
step:1039/1750 train_time:100101ms step_avg:96.34ms
step:1040/1750 train_time:100200ms step_avg:96.35ms
step:1041/1750 train_time:100300ms step_avg:96.35ms
step:1042/1750 train_time:100400ms step_avg:96.35ms
step:1043/1750 train_time:100501ms step_avg:96.36ms
step:1044/1750 train_time:100602ms step_avg:96.36ms
step:1045/1750 train_time:100701ms step_avg:96.36ms
step:1046/1750 train_time:100802ms step_avg:96.37ms
step:1047/1750 train_time:100901ms step_avg:96.37ms
step:1048/1750 train_time:101001ms step_avg:96.38ms
step:1049/1750 train_time:101100ms step_avg:96.38ms
step:1050/1750 train_time:101201ms step_avg:96.38ms
step:1051/1750 train_time:101560ms step_avg:96.63ms
step:1052/1750 train_time:101657ms step_avg:96.63ms
step:1053/1750 train_time:101756ms step_avg:96.63ms
step:1054/1750 train_time:101854ms step_avg:96.64ms
step:1055/1750 train_time:101953ms step_avg:96.64ms
step:1056/1750 train_time:102051ms step_avg:96.64ms
step:1057/1750 train_time:102149ms step_avg:96.64ms
step:1058/1750 train_time:102248ms step_avg:96.64ms
step:1059/1750 train_time:102345ms step_avg:96.64ms
step:1060/1750 train_time:102446ms step_avg:96.65ms
step:1061/1750 train_time:102801ms step_avg:96.89ms
step:1062/1750 train_time:102899ms step_avg:96.89ms
step:1063/1750 train_time:102997ms step_avg:96.89ms
step:1064/1750 train_time:103096ms step_avg:96.89ms
step:1065/1750 train_time:103194ms step_avg:96.90ms
step:1066/1750 train_time:103292ms step_avg:96.90ms
step:1067/1750 train_time:103391ms step_avg:96.90ms
step:1068/1750 train_time:103489ms step_avg:96.90ms
step:1069/1750 train_time:103588ms step_avg:96.90ms
step:1070/1750 train_time:103690ms step_avg:96.91ms
step:1071/1750 train_time:103796ms step_avg:96.92ms
step:1072/1750 train_time:103897ms step_avg:96.92ms
step:1073/1750 train_time:103996ms step_avg:96.92ms
step:1074/1750 train_time:104095ms step_avg:96.92ms
step:1075/1750 train_time:104194ms step_avg:96.92ms
step:1076/1750 train_time:104293ms step_avg:96.93ms
step:1077/1750 train_time:104392ms step_avg:96.93ms
step:1078/1750 train_time:104490ms step_avg:96.93ms
step:1079/1750 train_time:104590ms step_avg:96.93ms
step:1080/1750 train_time:104690ms step_avg:96.94ms
step:1081/1750 train_time:105079ms step_avg:97.21ms
step:1082/1750 train_time:105177ms step_avg:97.21ms
step:1083/1750 train_time:105275ms step_avg:97.21ms
step:1084/1750 train_time:105373ms step_avg:97.21ms
step:1085/1750 train_time:105472ms step_avg:97.21ms
step:1086/1750 train_time:105571ms step_avg:97.21ms
step:1087/1750 train_time:105670ms step_avg:97.21ms
step:1088/1750 train_time:105768ms step_avg:97.21ms
step:1089/1750 train_time:105866ms step_avg:97.21ms
step:1090/1750 train_time:105969ms step_avg:97.22ms
step:1091/1750 train_time:106072ms step_avg:97.22ms
step:1092/1750 train_time:106175ms step_avg:97.23ms
step:1093/1750 train_time:106274ms step_avg:97.23ms
step:1094/1750 train_time:106374ms step_avg:97.23ms
step:1095/1750 train_time:106473ms step_avg:97.24ms
step:1096/1750 train_time:106573ms step_avg:97.24ms
step:1097/1750 train_time:106672ms step_avg:97.24ms
step:1098/1750 train_time:106772ms step_avg:97.24ms
step:1099/1750 train_time:106872ms step_avg:97.24ms
step:1100/1750 train_time:106973ms step_avg:97.25ms
step:1101/1750 train_time:107075ms step_avg:97.25ms
step:1102/1750 train_time:107176ms step_avg:97.26ms
step:1103/1750 train_time:107276ms step_avg:97.26ms
step:1104/1750 train_time:107375ms step_avg:97.26ms
step:1105/1750 train_time:107474ms step_avg:97.26ms
step:1106/1750 train_time:107574ms step_avg:97.26ms
step:1107/1750 train_time:107674ms step_avg:97.27ms
step:1108/1750 train_time:107773ms step_avg:97.27ms
step:1109/1750 train_time:107872ms step_avg:97.27ms
step:1110/1750 train_time:107972ms step_avg:97.27ms
step:1111/1750 train_time:108073ms step_avg:97.28ms
step:1112/1750 train_time:108174ms step_avg:97.28ms
step:1113/1750 train_time:108274ms step_avg:97.28ms
step:1114/1750 train_time:108375ms step_avg:97.28ms
step:1115/1750 train_time:108474ms step_avg:97.29ms
step:1116/1750 train_time:108574ms step_avg:97.29ms
step:1117/1750 train_time:108674ms step_avg:97.29ms
step:1118/1750 train_time:108773ms step_avg:97.29ms
step:1119/1750 train_time:108873ms step_avg:97.29ms
step:1120/1750 train_time:108973ms step_avg:97.30ms
step:1121/1750 train_time:109074ms step_avg:97.30ms
step:1122/1750 train_time:109174ms step_avg:97.30ms
step:1123/1750 train_time:109275ms step_avg:97.31ms
step:1124/1750 train_time:109374ms step_avg:97.31ms
step:1125/1750 train_time:109474ms step_avg:97.31ms
step:1125/1750 val_loss:3.4564 train_time:109563ms step_avg:97.39ms
step:1126/1750 train_time:109584ms step_avg:97.32ms
step:1127/1750 train_time:109685ms step_avg:97.32ms
step:1128/1750 train_time:109784ms step_avg:97.33ms
step:1129/1750 train_time:109884ms step_avg:97.33ms
step:1130/1750 train_time:109982ms step_avg:97.33ms
step:1131/1750 train_time:110081ms step_avg:97.33ms
step:1132/1750 train_time:110180ms step_avg:97.33ms
step:1133/1750 train_time:110278ms step_avg:97.33ms
step:1134/1750 train_time:110377ms step_avg:97.33ms
step:1135/1750 train_time:110476ms step_avg:97.34ms
step:1136/1750 train_time:110575ms step_avg:97.34ms
step:1137/1750 train_time:110676ms step_avg:97.34ms
step:1138/1750 train_time:110778ms step_avg:97.34ms
step:1139/1750 train_time:110879ms step_avg:97.35ms
step:1140/1750 train_time:110979ms step_avg:97.35ms
step:1141/1750 train_time:111077ms step_avg:97.35ms
step:1142/1750 train_time:111176ms step_avg:97.35ms
step:1143/1750 train_time:111275ms step_avg:97.35ms
step:1144/1750 train_time:111374ms step_avg:97.35ms
step:1145/1750 train_time:111473ms step_avg:97.36ms
step:1146/1750 train_time:111574ms step_avg:97.36ms
step:1147/1750 train_time:111674ms step_avg:97.36ms
step:1148/1750 train_time:111775ms step_avg:97.37ms
step:1149/1750 train_time:111877ms step_avg:97.37ms
step:1150/1750 train_time:111977ms step_avg:97.37ms
step:1151/1750 train_time:112078ms step_avg:97.37ms
step:1152/1750 train_time:112177ms step_avg:97.38ms
step:1153/1750 train_time:112276ms step_avg:97.38ms
step:1154/1750 train_time:112375ms step_avg:97.38ms
step:1155/1750 train_time:112473ms step_avg:97.38ms
step:1156/1750 train_time:112572ms step_avg:97.38ms
step:1157/1750 train_time:112674ms step_avg:97.38ms
step:1158/1750 train_time:112774ms step_avg:97.39ms
step:1159/1750 train_time:112874ms step_avg:97.39ms
step:1160/1750 train_time:112975ms step_avg:97.39ms
step:1161/1750 train_time:113075ms step_avg:97.39ms
step:1162/1750 train_time:113174ms step_avg:97.40ms
step:1163/1750 train_time:113274ms step_avg:97.40ms
step:1164/1750 train_time:113374ms step_avg:97.40ms
step:1165/1750 train_time:113473ms step_avg:97.40ms
step:1166/1750 train_time:113573ms step_avg:97.40ms
step:1167/1750 train_time:113989ms step_avg:97.68ms
step:1168/1750 train_time:114087ms step_avg:97.68ms
step:1169/1750 train_time:114187ms step_avg:97.68ms
step:1170/1750 train_time:114287ms step_avg:97.68ms
step:1171/1750 train_time:114386ms step_avg:97.68ms
step:1172/1750 train_time:114486ms step_avg:97.68ms
step:1173/1750 train_time:114586ms step_avg:97.69ms
step:1174/1750 train_time:114686ms step_avg:97.69ms
step:1175/1750 train_time:114787ms step_avg:97.69ms
step:1176/1750 train_time:114896ms step_avg:97.70ms
step:1177/1750 train_time:114998ms step_avg:97.70ms
step:1178/1750 train_time:115099ms step_avg:97.71ms
step:1179/1750 train_time:115201ms step_avg:97.71ms
step:1180/1750 train_time:115301ms step_avg:97.71ms
step:1181/1750 train_time:115400ms step_avg:97.71ms
step:1182/1750 train_time:115500ms step_avg:97.72ms
step:1183/1750 train_time:115600ms step_avg:97.72ms
step:1184/1750 train_time:115701ms step_avg:97.72ms
step:1185/1750 train_time:115801ms step_avg:97.72ms
step:1186/1750 train_time:115902ms step_avg:97.73ms
step:1187/1750 train_time:116003ms step_avg:97.73ms
step:1188/1750 train_time:116104ms step_avg:97.73ms
step:1189/1750 train_time:116205ms step_avg:97.73ms
step:1190/1750 train_time:116305ms step_avg:97.74ms
step:1191/1750 train_time:116406ms step_avg:97.74ms
step:1192/1750 train_time:116507ms step_avg:97.74ms
step:1193/1750 train_time:116608ms step_avg:97.74ms
step:1194/1750 train_time:116708ms step_avg:97.75ms
step:1195/1750 train_time:116809ms step_avg:97.75ms
step:1196/1750 train_time:116910ms step_avg:97.75ms
step:1197/1750 train_time:117013ms step_avg:97.75ms
step:1198/1750 train_time:117114ms step_avg:97.76ms
step:1199/1750 train_time:117215ms step_avg:97.76ms
step:1200/1750 train_time:117316ms step_avg:97.76ms
step:1201/1750 train_time:117417ms step_avg:97.77ms
step:1202/1750 train_time:117856ms step_avg:98.05ms
step:1203/1750 train_time:117920ms step_avg:98.02ms
step:1204/1750 train_time:118018ms step_avg:98.02ms
step:1205/1750 train_time:118418ms step_avg:98.27ms
step:1206/1750 train_time:118517ms step_avg:98.27ms
step:1207/1750 train_time:118617ms step_avg:98.27ms
step:1208/1750 train_time:118716ms step_avg:98.27ms
step:1209/1750 train_time:118815ms step_avg:98.28ms
step:1210/1750 train_time:118915ms step_avg:98.28ms
step:1211/1750 train_time:119015ms step_avg:98.28ms
step:1212/1750 train_time:119114ms step_avg:98.28ms
step:1213/1750 train_time:119214ms step_avg:98.28ms
step:1214/1750 train_time:119320ms step_avg:98.29ms
step:1215/1750 train_time:119785ms step_avg:98.59ms
step:1216/1750 train_time:119886ms step_avg:98.59ms
step:1217/1750 train_time:119986ms step_avg:98.59ms
step:1218/1750 train_time:120086ms step_avg:98.59ms
step:1219/1750 train_time:120185ms step_avg:98.59ms
step:1220/1750 train_time:120284ms step_avg:98.59ms
step:1221/1750 train_time:120384ms step_avg:98.59ms
step:1222/1750 train_time:120483ms step_avg:98.60ms
step:1223/1750 train_time:120583ms step_avg:98.60ms
step:1224/1750 train_time:120690ms step_avg:98.60ms
step:1225/1750 train_time:120796ms step_avg:98.61ms
step:1226/1750 train_time:120898ms step_avg:98.61ms
step:1227/1750 train_time:120998ms step_avg:98.61ms
step:1228/1750 train_time:121098ms step_avg:98.61ms
step:1229/1750 train_time:121198ms step_avg:98.62ms
step:1230/1750 train_time:121299ms step_avg:98.62ms
step:1231/1750 train_time:121399ms step_avg:98.62ms
step:1232/1750 train_time:121500ms step_avg:98.62ms
step:1233/1750 train_time:121601ms step_avg:98.62ms
step:1234/1750 train_time:121702ms step_avg:98.62ms
step:1235/1750 train_time:121804ms step_avg:98.63ms
step:1236/1750 train_time:121905ms step_avg:98.63ms
step:1237/1750 train_time:122007ms step_avg:98.63ms
step:1238/1750 train_time:122108ms step_avg:98.63ms
step:1239/1750 train_time:122209ms step_avg:98.64ms
step:1240/1750 train_time:122309ms step_avg:98.64ms
step:1241/1750 train_time:122412ms step_avg:98.64ms
step:1242/1750 train_time:122512ms step_avg:98.64ms
step:1243/1750 train_time:122614ms step_avg:98.64ms
step:1244/1750 train_time:122715ms step_avg:98.65ms
step:1245/1750 train_time:122816ms step_avg:98.65ms
step:1246/1750 train_time:122917ms step_avg:98.65ms
step:1247/1750 train_time:123018ms step_avg:98.65ms
step:1248/1750 train_time:123120ms step_avg:98.65ms
step:1249/1750 train_time:123220ms step_avg:98.66ms
step:1250/1750 train_time:123320ms step_avg:98.66ms
step:1250/1750 val_loss:3.4115 train_time:123409ms step_avg:98.73ms
step:1251/1750 train_time:123429ms step_avg:98.66ms
step:1252/1750 train_time:123532ms step_avg:98.67ms
step:1253/1750 train_time:123635ms step_avg:98.67ms
step:1254/1750 train_time:123735ms step_avg:98.67ms
step:1255/1750 train_time:123835ms step_avg:98.67ms
step:1256/1750 train_time:123936ms step_avg:98.67ms
step:1257/1750 train_time:124035ms step_avg:98.68ms
step:1258/1750 train_time:124135ms step_avg:98.68ms
step:1259/1750 train_time:124234ms step_avg:98.68ms
step:1260/1750 train_time:124334ms step_avg:98.68ms
step:1261/1750 train_time:124437ms step_avg:98.68ms
step:1262/1750 train_time:124541ms step_avg:98.69ms
step:1263/1750 train_time:124643ms step_avg:98.69ms
step:1264/1750 train_time:124744ms step_avg:98.69ms
step:1265/1750 train_time:124844ms step_avg:98.69ms
step:1266/1750 train_time:124944ms step_avg:98.69ms
step:1267/1750 train_time:125045ms step_avg:98.69ms
step:1268/1750 train_time:125146ms step_avg:98.70ms
step:1269/1750 train_time:125247ms step_avg:98.70ms
step:1270/1750 train_time:125348ms step_avg:98.70ms
step:1271/1750 train_time:125450ms step_avg:98.70ms
step:1272/1750 train_time:125550ms step_avg:98.70ms
step:1273/1750 train_time:125651ms step_avg:98.70ms
step:1274/1750 train_time:125752ms step_avg:98.71ms
step:1275/1750 train_time:125853ms step_avg:98.71ms
step:1276/1750 train_time:125955ms step_avg:98.71ms
step:1277/1750 train_time:126056ms step_avg:98.71ms
step:1278/1750 train_time:126158ms step_avg:98.71ms
step:1279/1750 train_time:126258ms step_avg:98.72ms
step:1280/1750 train_time:126358ms step_avg:98.72ms
step:1281/1750 train_time:126460ms step_avg:98.72ms
step:1282/1750 train_time:126561ms step_avg:98.72ms
step:1283/1750 train_time:126662ms step_avg:98.72ms
step:1284/1750 train_time:126763ms step_avg:98.73ms
step:1285/1750 train_time:126864ms step_avg:98.73ms
step:1286/1750 train_time:126965ms step_avg:98.73ms
step:1287/1750 train_time:127066ms step_avg:98.73ms
step:1288/1750 train_time:127166ms step_avg:98.73ms
step:1289/1750 train_time:127268ms step_avg:98.73ms
step:1290/1750 train_time:127369ms step_avg:98.74ms
step:1291/1750 train_time:127468ms step_avg:98.74ms
step:1292/1750 train_time:127569ms step_avg:98.74ms
step:1293/1750 train_time:127669ms step_avg:98.74ms
step:1294/1750 train_time:127771ms step_avg:98.74ms
step:1295/1750 train_time:127873ms step_avg:98.74ms
step:1296/1750 train_time:127976ms step_avg:98.75ms
step:1297/1750 train_time:128077ms step_avg:98.75ms
step:1298/1750 train_time:128179ms step_avg:98.75ms
step:1299/1750 train_time:128280ms step_avg:98.75ms
step:1300/1750 train_time:128381ms step_avg:98.75ms
step:1301/1750 train_time:128482ms step_avg:98.76ms
step:1302/1750 train_time:128583ms step_avg:98.76ms
step:1303/1750 train_time:128686ms step_avg:98.76ms
step:1304/1750 train_time:128788ms step_avg:98.76ms
step:1305/1750 train_time:128889ms step_avg:98.77ms
step:1306/1750 train_time:128990ms step_avg:98.77ms
step:1307/1750 train_time:129090ms step_avg:98.77ms
step:1308/1750 train_time:129190ms step_avg:98.77ms
step:1309/1750 train_time:129291ms step_avg:98.77ms
step:1310/1750 train_time:129393ms step_avg:98.77ms
step:1311/1750 train_time:129494ms step_avg:98.77ms
step:1312/1750 train_time:129595ms step_avg:98.78ms
step:1313/1750 train_time:129698ms step_avg:98.78ms
step:1314/1750 train_time:129799ms step_avg:98.78ms
step:1315/1750 train_time:129900ms step_avg:98.78ms
step:1316/1750 train_time:130001ms step_avg:98.78ms
step:1317/1750 train_time:130103ms step_avg:98.79ms
step:1318/1750 train_time:130204ms step_avg:98.79ms
step:1319/1750 train_time:130305ms step_avg:98.79ms
step:1320/1750 train_time:130408ms step_avg:98.79ms
step:1321/1750 train_time:130508ms step_avg:98.80ms
step:1322/1750 train_time:130609ms step_avg:98.80ms
step:1323/1750 train_time:130710ms step_avg:98.80ms
step:1324/1750 train_time:130810ms step_avg:98.80ms
step:1325/1750 train_time:130910ms step_avg:98.80ms
step:1326/1750 train_time:131012ms step_avg:98.80ms
step:1327/1750 train_time:131114ms step_avg:98.80ms
step:1328/1750 train_time:131216ms step_avg:98.81ms
step:1329/1750 train_time:131316ms step_avg:98.81ms
step:1330/1750 train_time:131417ms step_avg:98.81ms
step:1331/1750 train_time:131519ms step_avg:98.81ms
step:1332/1750 train_time:131619ms step_avg:98.81ms
step:1333/1750 train_time:131721ms step_avg:98.82ms
step:1334/1750 train_time:131823ms step_avg:98.82ms
step:1335/1750 train_time:131923ms step_avg:98.82ms
step:1336/1750 train_time:132025ms step_avg:98.82ms
step:1337/1750 train_time:132127ms step_avg:98.82ms
step:1338/1750 train_time:132228ms step_avg:98.83ms
step:1339/1750 train_time:132329ms step_avg:98.83ms
step:1340/1750 train_time:132429ms step_avg:98.83ms
step:1341/1750 train_time:132530ms step_avg:98.83ms
step:1342/1750 train_time:132630ms step_avg:98.83ms
step:1343/1750 train_time:132731ms step_avg:98.83ms
step:1344/1750 train_time:132833ms step_avg:98.83ms
step:1345/1750 train_time:132934ms step_avg:98.84ms
step:1346/1750 train_time:133037ms step_avg:98.84ms
step:1347/1750 train_time:133139ms step_avg:98.84ms
step:1348/1750 train_time:133241ms step_avg:98.84ms
step:1349/1750 train_time:133342ms step_avg:98.85ms
step:1350/1750 train_time:133443ms step_avg:98.85ms
step:1351/1750 train_time:133543ms step_avg:98.85ms
step:1352/1750 train_time:133645ms step_avg:98.85ms
step:1353/1750 train_time:133746ms step_avg:98.85ms
step:1354/1750 train_time:133847ms step_avg:98.85ms
step:1355/1750 train_time:133948ms step_avg:98.85ms
step:1356/1750 train_time:134049ms step_avg:98.86ms
step:1357/1750 train_time:134150ms step_avg:98.86ms
step:1358/1750 train_time:134250ms step_avg:98.86ms
step:1359/1750 train_time:134351ms step_avg:98.86ms
step:1360/1750 train_time:134451ms step_avg:98.86ms
step:1361/1750 train_time:134551ms step_avg:98.86ms
step:1362/1750 train_time:134653ms step_avg:98.86ms
step:1363/1750 train_time:134756ms step_avg:98.87ms
step:1364/1750 train_time:134857ms step_avg:98.87ms
step:1365/1750 train_time:134958ms step_avg:98.87ms
step:1366/1750 train_time:135059ms step_avg:98.87ms
step:1367/1750 train_time:135159ms step_avg:98.87ms
step:1368/1750 train_time:135263ms step_avg:98.88ms
step:1369/1750 train_time:135364ms step_avg:98.88ms
step:1370/1750 train_time:135464ms step_avg:98.88ms
step:1371/1750 train_time:135565ms step_avg:98.88ms
step:1372/1750 train_time:135666ms step_avg:98.88ms
step:1373/1750 train_time:135768ms step_avg:98.88ms
step:1374/1750 train_time:135869ms step_avg:98.89ms
step:1375/1750 train_time:135971ms step_avg:98.89ms
step:1375/1750 val_loss:3.3715 train_time:136060ms step_avg:98.95ms
step:1376/1750 train_time:136081ms step_avg:98.90ms
step:1377/1750 train_time:136184ms step_avg:98.90ms
step:1378/1750 train_time:136286ms step_avg:98.90ms
step:1379/1750 train_time:136386ms step_avg:98.90ms
step:1380/1750 train_time:136489ms step_avg:98.91ms
step:1381/1750 train_time:136589ms step_avg:98.91ms
step:1382/1750 train_time:136689ms step_avg:98.91ms
step:1383/1750 train_time:136789ms step_avg:98.91ms
step:1384/1750 train_time:136888ms step_avg:98.91ms
step:1385/1750 train_time:136989ms step_avg:98.91ms
step:1386/1750 train_time:137093ms step_avg:98.91ms
step:1387/1750 train_time:137194ms step_avg:98.91ms
step:1388/1750 train_time:137296ms step_avg:98.92ms
step:1389/1750 train_time:137397ms step_avg:98.92ms
step:1390/1750 train_time:137498ms step_avg:98.92ms
step:1391/1750 train_time:137599ms step_avg:98.92ms
step:1392/1750 train_time:137700ms step_avg:98.92ms
step:1393/1750 train_time:137801ms step_avg:98.92ms
step:1394/1750 train_time:137902ms step_avg:98.93ms
step:1395/1750 train_time:138004ms step_avg:98.93ms
step:1396/1750 train_time:138107ms step_avg:98.93ms
step:1397/1750 train_time:138208ms step_avg:98.93ms
step:1398/1750 train_time:138309ms step_avg:98.93ms
step:1399/1750 train_time:138411ms step_avg:98.94ms
step:1400/1750 train_time:138512ms step_avg:98.94ms
step:1401/1750 train_time:138613ms step_avg:98.94ms
step:1402/1750 train_time:138713ms step_avg:98.94ms
step:1403/1750 train_time:138814ms step_avg:98.94ms
step:1404/1750 train_time:138917ms step_avg:98.94ms
step:1405/1750 train_time:139017ms step_avg:98.94ms
step:1406/1750 train_time:139119ms step_avg:98.95ms
step:1407/1750 train_time:139221ms step_avg:98.95ms
step:1408/1750 train_time:139323ms step_avg:98.95ms
step:1409/1750 train_time:139427ms step_avg:98.95ms
step:1410/1750 train_time:139528ms step_avg:98.96ms
step:1411/1750 train_time:139629ms step_avg:98.96ms
step:1412/1750 train_time:139730ms step_avg:98.96ms
step:1413/1750 train_time:139830ms step_avg:98.96ms
step:1414/1750 train_time:139931ms step_avg:98.96ms
step:1415/1750 train_time:140032ms step_avg:98.96ms
step:1416/1750 train_time:140133ms step_avg:98.96ms
step:1417/1750 train_time:140234ms step_avg:98.97ms
step:1418/1750 train_time:140334ms step_avg:98.97ms
step:1419/1750 train_time:140435ms step_avg:98.97ms
step:1420/1750 train_time:140536ms step_avg:98.97ms
step:1421/1750 train_time:140637ms step_avg:98.97ms
step:1422/1750 train_time:140738ms step_avg:98.97ms
step:1423/1750 train_time:140839ms step_avg:98.97ms
step:1424/1750 train_time:140940ms step_avg:98.97ms
step:1425/1750 train_time:141041ms step_avg:98.98ms
step:1426/1750 train_time:141143ms step_avg:98.98ms
step:1427/1750 train_time:141244ms step_avg:98.98ms
step:1428/1750 train_time:141346ms step_avg:98.98ms
step:1429/1750 train_time:141447ms step_avg:98.98ms
step:1430/1750 train_time:141551ms step_avg:98.99ms
step:1431/1750 train_time:141654ms step_avg:98.99ms
step:1432/1750 train_time:141756ms step_avg:98.99ms
step:1433/1750 train_time:141856ms step_avg:98.99ms
step:1434/1750 train_time:141957ms step_avg:98.99ms
step:1435/1750 train_time:142059ms step_avg:99.00ms
step:1436/1750 train_time:142161ms step_avg:99.00ms
step:1437/1750 train_time:142263ms step_avg:99.00ms
step:1438/1750 train_time:142365ms step_avg:99.00ms
step:1439/1750 train_time:142469ms step_avg:99.01ms
step:1440/1750 train_time:142572ms step_avg:99.01ms
step:1441/1750 train_time:142674ms step_avg:99.01ms
step:1442/1750 train_time:142774ms step_avg:99.01ms
step:1443/1750 train_time:142875ms step_avg:99.01ms
step:1444/1750 train_time:142976ms step_avg:99.01ms
step:1445/1750 train_time:143077ms step_avg:99.02ms
step:1446/1750 train_time:143179ms step_avg:99.02ms
step:1447/1750 train_time:143282ms step_avg:99.02ms
step:1448/1750 train_time:143387ms step_avg:99.02ms
step:1449/1750 train_time:143488ms step_avg:99.03ms
step:1450/1750 train_time:143590ms step_avg:99.03ms
step:1451/1750 train_time:143692ms step_avg:99.03ms
step:1452/1750 train_time:143793ms step_avg:99.03ms
step:1453/1750 train_time:143895ms step_avg:99.03ms
step:1454/1750 train_time:143998ms step_avg:99.04ms
step:1455/1750 train_time:144100ms step_avg:99.04ms
step:1456/1750 train_time:144201ms step_avg:99.04ms
step:1457/1750 train_time:144303ms step_avg:99.04ms
step:1458/1750 train_time:144407ms step_avg:99.04ms
step:1459/1750 train_time:144510ms step_avg:99.05ms
step:1460/1750 train_time:144611ms step_avg:99.05ms
step:1461/1750 train_time:144713ms step_avg:99.05ms
step:1462/1750 train_time:144815ms step_avg:99.05ms
step:1463/1750 train_time:144917ms step_avg:99.05ms
step:1464/1750 train_time:145018ms step_avg:99.06ms
step:1465/1750 train_time:145120ms step_avg:99.06ms
step:1466/1750 train_time:145221ms step_avg:99.06ms
step:1467/1750 train_time:145323ms step_avg:99.06ms
step:1468/1750 train_time:145427ms step_avg:99.06ms
step:1469/1750 train_time:145530ms step_avg:99.07ms
step:1470/1750 train_time:145632ms step_avg:99.07ms
step:1471/1750 train_time:145734ms step_avg:99.07ms
step:1472/1750 train_time:145835ms step_avg:99.07ms
step:1473/1750 train_time:145935ms step_avg:99.07ms
step:1474/1750 train_time:146036ms step_avg:99.07ms
step:1475/1750 train_time:146136ms step_avg:99.08ms
step:1476/1750 train_time:146239ms step_avg:99.08ms
step:1477/1750 train_time:146343ms step_avg:99.08ms
step:1478/1750 train_time:146446ms step_avg:99.08ms
step:1479/1750 train_time:146547ms step_avg:99.09ms
step:1480/1750 train_time:146649ms step_avg:99.09ms
step:1481/1750 train_time:146751ms step_avg:99.09ms
step:1482/1750 train_time:146852ms step_avg:99.09ms
step:1483/1750 train_time:146954ms step_avg:99.09ms
step:1484/1750 train_time:147055ms step_avg:99.09ms
step:1485/1750 train_time:147156ms step_avg:99.10ms
step:1486/1750 train_time:147259ms step_avg:99.10ms
step:1487/1750 train_time:147362ms step_avg:99.10ms
step:1488/1750 train_time:147465ms step_avg:99.10ms
step:1489/1750 train_time:147567ms step_avg:99.10ms
step:1490/1750 train_time:147669ms step_avg:99.11ms
step:1491/1750 train_time:147771ms step_avg:99.11ms
step:1492/1750 train_time:147872ms step_avg:99.11ms
step:1493/1750 train_time:147973ms step_avg:99.11ms
step:1494/1750 train_time:148074ms step_avg:99.11ms
step:1495/1750 train_time:148174ms step_avg:99.11ms
step:1496/1750 train_time:148277ms step_avg:99.12ms
step:1497/1750 train_time:148378ms step_avg:99.12ms
step:1498/1750 train_time:148482ms step_avg:99.12ms
step:1499/1750 train_time:148584ms step_avg:99.12ms
step:1500/1750 train_time:148686ms step_avg:99.12ms
step:1500/1750 val_loss:3.3358 train_time:148776ms step_avg:99.18ms
step:1501/1750 train_time:148799ms step_avg:99.13ms
step:1502/1750 train_time:148899ms step_avg:99.13ms
step:1503/1750 train_time:149000ms step_avg:99.14ms
step:1504/1750 train_time:149102ms step_avg:99.14ms
step:1505/1750 train_time:149202ms step_avg:99.14ms
step:1506/1750 train_time:149302ms step_avg:99.14ms
step:1507/1750 train_time:149403ms step_avg:99.14ms
step:1508/1750 train_time:149504ms step_avg:99.14ms
step:1509/1750 train_time:149605ms step_avg:99.14ms
step:1510/1750 train_time:149706ms step_avg:99.14ms
step:1511/1750 train_time:149812ms step_avg:99.15ms
step:1512/1750 train_time:149915ms step_avg:99.15ms
step:1513/1750 train_time:150017ms step_avg:99.15ms
step:1514/1750 train_time:150119ms step_avg:99.15ms
step:1515/1750 train_time:150224ms step_avg:99.16ms
step:1516/1750 train_time:150324ms step_avg:99.16ms
step:1517/1750 train_time:150424ms step_avg:99.16ms
step:1518/1750 train_time:150524ms step_avg:99.16ms
step:1519/1750 train_time:150626ms step_avg:99.16ms
step:1520/1750 train_time:150729ms step_avg:99.16ms
step:1521/1750 train_time:150830ms step_avg:99.17ms
step:1522/1750 train_time:150933ms step_avg:99.17ms
step:1523/1750 train_time:151035ms step_avg:99.17ms
step:1524/1750 train_time:151140ms step_avg:99.17ms
step:1525/1750 train_time:151243ms step_avg:99.18ms
step:1526/1750 train_time:151344ms step_avg:99.18ms
step:1527/1750 train_time:151446ms step_avg:99.18ms
step:1528/1750 train_time:151550ms step_avg:99.18ms
step:1529/1750 train_time:151651ms step_avg:99.18ms
step:1530/1750 train_time:151754ms step_avg:99.19ms
step:1531/1750 train_time:151856ms step_avg:99.19ms
step:1532/1750 train_time:151958ms step_avg:99.19ms
step:1533/1750 train_time:152060ms step_avg:99.19ms
step:1534/1750 train_time:152162ms step_avg:99.19ms
step:1535/1750 train_time:152263ms step_avg:99.19ms
step:1536/1750 train_time:152364ms step_avg:99.20ms
step:1537/1750 train_time:152465ms step_avg:99.20ms
step:1538/1750 train_time:152566ms step_avg:99.20ms
step:1539/1750 train_time:152668ms step_avg:99.20ms
step:1540/1750 train_time:152770ms step_avg:99.20ms
step:1541/1750 train_time:152873ms step_avg:99.20ms
step:1542/1750 train_time:152977ms step_avg:99.21ms
step:1543/1750 train_time:153080ms step_avg:99.21ms
step:1544/1750 train_time:153182ms step_avg:99.21ms
step:1545/1750 train_time:153284ms step_avg:99.21ms
step:1546/1750 train_time:153385ms step_avg:99.21ms
step:1547/1750 train_time:153486ms step_avg:99.22ms
step:1548/1750 train_time:153587ms step_avg:99.22ms
step:1549/1750 train_time:153689ms step_avg:99.22ms
step:1550/1750 train_time:153791ms step_avg:99.22ms
step:1551/1750 train_time:153894ms step_avg:99.22ms
step:1552/1750 train_time:153997ms step_avg:99.22ms
step:1553/1750 train_time:154100ms step_avg:99.23ms
step:1554/1750 train_time:154201ms step_avg:99.23ms
step:1555/1750 train_time:154303ms step_avg:99.23ms
step:1556/1750 train_time:154405ms step_avg:99.23ms
step:1557/1750 train_time:154506ms step_avg:99.23ms
step:1558/1750 train_time:154609ms step_avg:99.24ms
step:1559/1750 train_time:154710ms step_avg:99.24ms
step:1560/1750 train_time:154812ms step_avg:99.24ms
step:1561/1750 train_time:154914ms step_avg:99.24ms
step:1562/1750 train_time:155018ms step_avg:99.24ms
step:1563/1750 train_time:155123ms step_avg:99.25ms
step:1564/1750 train_time:155225ms step_avg:99.25ms
step:1565/1750 train_time:155325ms step_avg:99.25ms
step:1566/1750 train_time:155427ms step_avg:99.25ms
step:1567/1750 train_time:155528ms step_avg:99.25ms
step:1568/1750 train_time:155629ms step_avg:99.25ms
step:1569/1750 train_time:155730ms step_avg:99.25ms
step:1570/1750 train_time:155833ms step_avg:99.26ms
step:1571/1750 train_time:155935ms step_avg:99.26ms
step:1572/1750 train_time:156037ms step_avg:99.26ms
step:1573/1750 train_time:156140ms step_avg:99.26ms
step:1574/1750 train_time:156242ms step_avg:99.26ms
step:1575/1750 train_time:156344ms step_avg:99.27ms
step:1576/1750 train_time:156446ms step_avg:99.27ms
step:1577/1750 train_time:156549ms step_avg:99.27ms
step:1578/1750 train_time:156649ms step_avg:99.27ms
step:1579/1750 train_time:156750ms step_avg:99.27ms
step:1580/1750 train_time:156853ms step_avg:99.27ms
step:1581/1750 train_time:156954ms step_avg:99.28ms
step:1582/1750 train_time:157056ms step_avg:99.28ms
step:1583/1750 train_time:157159ms step_avg:99.28ms
step:1584/1750 train_time:157262ms step_avg:99.28ms
step:1585/1750 train_time:157364ms step_avg:99.28ms
step:1586/1750 train_time:157466ms step_avg:99.29ms
step:1587/1750 train_time:157567ms step_avg:99.29ms
step:1588/1750 train_time:157669ms step_avg:99.29ms
step:1589/1750 train_time:157770ms step_avg:99.29ms
step:1590/1750 train_time:157871ms step_avg:99.29ms
step:1591/1750 train_time:157972ms step_avg:99.29ms
step:1592/1750 train_time:158077ms step_avg:99.29ms
step:1593/1750 train_time:158179ms step_avg:99.30ms
step:1594/1750 train_time:158285ms step_avg:99.30ms
step:1595/1750 train_time:158387ms step_avg:99.30ms
step:1596/1750 train_time:158489ms step_avg:99.30ms
step:1597/1750 train_time:158590ms step_avg:99.30ms
step:1598/1750 train_time:158693ms step_avg:99.31ms
step:1599/1750 train_time:158793ms step_avg:99.31ms
step:1600/1750 train_time:158895ms step_avg:99.31ms
step:1601/1750 train_time:158997ms step_avg:99.31ms
step:1602/1750 train_time:159099ms step_avg:99.31ms
step:1603/1750 train_time:159201ms step_avg:99.31ms
step:1604/1750 train_time:159303ms step_avg:99.32ms
step:1605/1750 train_time:159405ms step_avg:99.32ms
step:1606/1750 train_time:159506ms step_avg:99.32ms
step:1607/1750 train_time:159607ms step_avg:99.32ms
step:1608/1750 train_time:159708ms step_avg:99.32ms
step:1609/1750 train_time:159810ms step_avg:99.32ms
step:1610/1750 train_time:159913ms step_avg:99.33ms
step:1611/1750 train_time:160016ms step_avg:99.33ms
step:1612/1750 train_time:160119ms step_avg:99.33ms
step:1613/1750 train_time:160221ms step_avg:99.33ms
step:1614/1750 train_time:160322ms step_avg:99.33ms
step:1615/1750 train_time:160423ms step_avg:99.33ms
step:1616/1750 train_time:160526ms step_avg:99.34ms
step:1617/1750 train_time:160627ms step_avg:99.34ms
step:1618/1750 train_time:160729ms step_avg:99.34ms
step:1619/1750 train_time:160830ms step_avg:99.34ms
step:1620/1750 train_time:160933ms step_avg:99.34ms
step:1621/1750 train_time:161035ms step_avg:99.34ms
step:1622/1750 train_time:161137ms step_avg:99.34ms
step:1623/1750 train_time:161240ms step_avg:99.35ms
step:1624/1750 train_time:161342ms step_avg:99.35ms
step:1625/1750 train_time:161445ms step_avg:99.35ms
step:1625/1750 val_loss:3.3056 train_time:161534ms step_avg:99.41ms
step:1626/1750 train_time:161555ms step_avg:99.36ms
step:1627/1750 train_time:161657ms step_avg:99.36ms
step:1628/1750 train_time:161758ms step_avg:99.36ms
step:1629/1750 train_time:161860ms step_avg:99.36ms
step:1630/1750 train_time:161960ms step_avg:99.36ms
step:1631/1750 train_time:162062ms step_avg:99.36ms
step:1632/1750 train_time:162163ms step_avg:99.36ms
step:1633/1750 train_time:162264ms step_avg:99.37ms
step:1634/1750 train_time:162369ms step_avg:99.37ms
step:1635/1750 train_time:162471ms step_avg:99.37ms
step:1636/1750 train_time:162574ms step_avg:99.37ms
step:1637/1750 train_time:162676ms step_avg:99.37ms
step:1638/1750 train_time:162778ms step_avg:99.38ms
step:1639/1750 train_time:162880ms step_avg:99.38ms
step:1640/1750 train_time:162981ms step_avg:99.38ms
step:1641/1750 train_time:163082ms step_avg:99.38ms
step:1642/1750 train_time:163184ms step_avg:99.38ms
step:1643/1750 train_time:163285ms step_avg:99.38ms
step:1644/1750 train_time:163386ms step_avg:99.38ms
step:1645/1750 train_time:163489ms step_avg:99.39ms
step:1646/1750 train_time:163593ms step_avg:99.39ms
step:1647/1750 train_time:163697ms step_avg:99.39ms
step:1648/1750 train_time:163800ms step_avg:99.39ms
step:1649/1750 train_time:163902ms step_avg:99.39ms
step:1650/1750 train_time:164003ms step_avg:99.40ms
step:1651/1750 train_time:164105ms step_avg:99.40ms
step:1652/1750 train_time:164207ms step_avg:99.40ms
step:1653/1750 train_time:164309ms step_avg:99.40ms
step:1654/1750 train_time:164410ms step_avg:99.40ms
step:1655/1750 train_time:164512ms step_avg:99.40ms
step:1656/1750 train_time:164615ms step_avg:99.41ms
step:1657/1750 train_time:164717ms step_avg:99.41ms
step:1658/1750 train_time:164819ms step_avg:99.41ms
step:1659/1750 train_time:164923ms step_avg:99.41ms
step:1660/1750 train_time:165024ms step_avg:99.41ms
step:1661/1750 train_time:165127ms step_avg:99.41ms
step:1662/1750 train_time:165230ms step_avg:99.42ms
step:1663/1750 train_time:165333ms step_avg:99.42ms
step:1664/1750 train_time:165435ms step_avg:99.42ms
step:1665/1750 train_time:165538ms step_avg:99.42ms
step:1666/1750 train_time:165640ms step_avg:99.42ms
step:1667/1750 train_time:165743ms step_avg:99.43ms
step:1668/1750 train_time:165847ms step_avg:99.43ms
step:1669/1750 train_time:165950ms step_avg:99.43ms
step:1670/1750 train_time:166052ms step_avg:99.43ms
step:1671/1750 train_time:166154ms step_avg:99.43ms
step:1672/1750 train_time:166255ms step_avg:99.43ms
step:1673/1750 train_time:166356ms step_avg:99.44ms
step:1674/1750 train_time:166457ms step_avg:99.44ms
step:1675/1750 train_time:166559ms step_avg:99.44ms
step:1676/1750 train_time:166663ms step_avg:99.44ms
step:1677/1750 train_time:166765ms step_avg:99.44ms
step:1678/1750 train_time:166869ms step_avg:99.45ms
step:1679/1750 train_time:166971ms step_avg:99.45ms
step:1680/1750 train_time:167072ms step_avg:99.45ms
step:1681/1750 train_time:167175ms step_avg:99.45ms
step:1682/1750 train_time:167278ms step_avg:99.45ms
step:1683/1750 train_time:167379ms step_avg:99.45ms
step:1684/1750 train_time:167482ms step_avg:99.45ms
step:1685/1750 train_time:167584ms step_avg:99.46ms
step:1686/1750 train_time:167686ms step_avg:99.46ms
step:1687/1750 train_time:167789ms step_avg:99.46ms
step:1688/1750 train_time:167891ms step_avg:99.46ms
step:1689/1750 train_time:167993ms step_avg:99.46ms
step:1690/1750 train_time:168097ms step_avg:99.47ms
step:1691/1750 train_time:168198ms step_avg:99.47ms
step:1692/1750 train_time:168300ms step_avg:99.47ms
step:1693/1750 train_time:168403ms step_avg:99.47ms
step:1694/1750 train_time:168507ms step_avg:99.47ms
step:1695/1750 train_time:168611ms step_avg:99.48ms
step:1696/1750 train_time:168713ms step_avg:99.48ms
step:1697/1750 train_time:168817ms step_avg:99.48ms
step:1698/1750 train_time:168919ms step_avg:99.48ms
step:1699/1750 train_time:169022ms step_avg:99.48ms
step:1700/1750 train_time:169126ms step_avg:99.49ms
step:1701/1750 train_time:169229ms step_avg:99.49ms
step:1702/1750 train_time:169333ms step_avg:99.49ms
step:1703/1750 train_time:169436ms step_avg:99.49ms
step:1704/1750 train_time:169538ms step_avg:99.49ms
step:1705/1750 train_time:169640ms step_avg:99.50ms
step:1706/1750 train_time:169744ms step_avg:99.50ms
step:1707/1750 train_time:169847ms step_avg:99.50ms
step:1708/1750 train_time:169952ms step_avg:99.50ms
step:1709/1750 train_time:170054ms step_avg:99.50ms
step:1710/1750 train_time:170155ms step_avg:99.51ms
step:1711/1750 train_time:170260ms step_avg:99.51ms
step:1712/1750 train_time:170362ms step_avg:99.51ms
step:1713/1750 train_time:170467ms step_avg:99.51ms
step:1714/1750 train_time:170569ms step_avg:99.52ms
step:1715/1750 train_time:170674ms step_avg:99.52ms
step:1716/1750 train_time:170776ms step_avg:99.52ms
step:1717/1750 train_time:170878ms step_avg:99.52ms
step:1718/1750 train_time:170981ms step_avg:99.52ms
step:1719/1750 train_time:171089ms step_avg:99.53ms
step:1720/1750 train_time:171191ms step_avg:99.53ms
step:1721/1750 train_time:171294ms step_avg:99.53ms
step:1722/1750 train_time:171399ms step_avg:99.53ms
step:1723/1750 train_time:171501ms step_avg:99.54ms
step:1724/1750 train_time:171605ms step_avg:99.54ms
step:1725/1750 train_time:171709ms step_avg:99.54ms
step:1726/1750 train_time:171811ms step_avg:99.54ms
step:1727/1750 train_time:171914ms step_avg:99.54ms
step:1728/1750 train_time:172017ms step_avg:99.55ms
step:1729/1750 train_time:172121ms step_avg:99.55ms
step:1730/1750 train_time:172224ms step_avg:99.55ms
step:1731/1750 train_time:172328ms step_avg:99.55ms
step:1732/1750 train_time:172431ms step_avg:99.56ms
step:1733/1750 train_time:172534ms step_avg:99.56ms
step:1734/1750 train_time:172638ms step_avg:99.56ms
step:1735/1750 train_time:172740ms step_avg:99.56ms
step:1736/1750 train_time:172843ms step_avg:99.56ms
step:1737/1750 train_time:172946ms step_avg:99.57ms
step:1738/1750 train_time:173049ms step_avg:99.57ms
step:1739/1750 train_time:173152ms step_avg:99.57ms
step:1740/1750 train_time:173254ms step_avg:99.57ms
step:1741/1750 train_time:173359ms step_avg:99.57ms
step:1742/1750 train_time:173462ms step_avg:99.58ms
step:1743/1750 train_time:173565ms step_avg:99.58ms
step:1744/1750 train_time:173668ms step_avg:99.58ms
step:1745/1750 train_time:173770ms step_avg:99.58ms
step:1746/1750 train_time:173872ms step_avg:99.58ms
step:1747/1750 train_time:173976ms step_avg:99.59ms
step:1748/1750 train_time:174079ms step_avg:99.59ms
step:1749/1750 train_time:174181ms step_avg:99.59ms
step:1750/1750 train_time:174285ms step_avg:99.59ms
step:1750/1750 val_loss:3.2822 train_time:174376ms step_avg:99.64ms
peak memory allocated: 33278 MiB reserved: 49174 MiB
