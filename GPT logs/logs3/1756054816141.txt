import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


class NormMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, sgd_coeff=0):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, sgd_coeff=sgd_coeff)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        self.sgd_coeff = sgd_coeff

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            sgd_coeff = group["sgd_coeff"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    eps = 1e-12
                    g_normalized = grad / (grad.norm() + eps)
                    if sgd_coeff != 1:
                        update_part = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                        update = (1 - sgd_coeff) * update_part + sgd_coeff * g_normalized
                    else:
                        update = sgd_coeff * g_normalized
                    p.add_(other=update, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()
class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)
                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_reduce_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 27.5
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos : pos + max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()
    starts = []
    for i in range(1, len(boundary_positions)):
        end = boundary_positions[i].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == dist.get_world_size():
                return starts, end - pos
            start = end
    assert False # increase max_batch_span if necessary

def distributed_data_generator(filename_pattern: str, batch_size: int, align_to_bos: bool):
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    max_batch_span = 2 * batch_size if align_to_bos else batch_size # provide buffer to handle samples up to length local_batch_size
    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            batch_span = batch_size
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1750 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
from datetime import datetime
if master_process:
    # run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_id = str(int(time.time() * 1000))
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0)
optimizer2 = NormMuon(hidden_matrix_params, lr=0.06, momentum=0.97, weight_decay=0.0, sgd_coeff=0.5)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.11 | packaged by conda-forge | (main, Jun  4 2025, 14:45:31) [GCC 13.3.0]
Running PyTorch 2.9.0.dev20250824+cu126 compiled for CUDA 12.6
Sun Aug 24 17:00:16 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:04:00.0 Off |                    0 |
| N/A   37C    P0            119W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:23:00.0 Off |                    0 |
| N/A   31C    P0            114W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:43:00.0 Off |                    0 |
| N/A   36C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   31C    P0            115W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:84:00.0 Off |                    0 |
| N/A   36C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:A3:00.0 Off |                    0 |
| N/A   32C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:C3:00.0 Off |                    0 |
| N/A   37C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E4:00.0 Off |                    0 |
| N/A   30C    P0            112W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.07ms
step:1/1750 train_time:141ms step_avg:141.25ms
step:2/1750 train_time:162ms step_avg:80.93ms
step:3/1750 train_time:243ms step_avg:81.07ms
step:4/1750 train_time:335ms step_avg:83.77ms
step:5/1750 train_time:427ms step_avg:85.44ms
step:6/1750 train_time:519ms step_avg:86.56ms
step:7/1750 train_time:612ms step_avg:87.41ms
step:8/1750 train_time:704ms step_avg:88.04ms
step:9/1750 train_time:796ms step_avg:88.45ms
step:10/1750 train_time:889ms step_avg:88.89ms
step:11/1750 train_time:981ms step_avg:89.17ms
step:12/1750 train_time:1076ms step_avg:89.66ms
step:13/1750 train_time:1171ms step_avg:90.11ms
step:14/1750 train_time:1266ms step_avg:90.41ms
step:15/1750 train_time:1360ms step_avg:90.65ms
step:16/1750 train_time:1453ms step_avg:90.80ms
step:17/1750 train_time:1545ms step_avg:90.90ms
step:18/1750 train_time:1639ms step_avg:91.04ms
step:19/1750 train_time:1732ms step_avg:91.14ms
step:20/1750 train_time:1825ms step_avg:91.24ms
step:21/1750 train_time:1917ms step_avg:91.30ms
step:22/1750 train_time:2010ms step_avg:91.37ms
step:23/1750 train_time:2104ms step_avg:91.47ms
step:24/1750 train_time:2199ms step_avg:91.61ms
step:25/1750 train_time:2293ms step_avg:91.71ms
step:26/1750 train_time:2387ms step_avg:91.80ms
step:27/1750 train_time:2480ms step_avg:91.87ms
step:28/1750 train_time:2574ms step_avg:91.92ms
step:29/1750 train_time:2667ms step_avg:91.95ms
step:30/1750 train_time:2760ms step_avg:91.99ms
step:31/1750 train_time:2852ms step_avg:92.01ms
step:32/1750 train_time:2946ms step_avg:92.05ms
step:33/1750 train_time:3039ms step_avg:92.11ms
step:34/1750 train_time:3133ms step_avg:92.16ms
step:35/1750 train_time:3227ms step_avg:92.19ms
step:36/1750 train_time:3320ms step_avg:92.23ms
step:37/1750 train_time:3414ms step_avg:92.27ms
step:38/1750 train_time:3507ms step_avg:92.30ms
step:39/1750 train_time:3601ms step_avg:92.33ms
step:40/1750 train_time:3694ms step_avg:92.36ms
step:41/1750 train_time:3787ms step_avg:92.37ms
step:42/1750 train_time:3880ms step_avg:92.39ms
step:43/1750 train_time:3974ms step_avg:92.43ms
step:44/1750 train_time:4067ms step_avg:92.44ms
step:45/1750 train_time:4161ms step_avg:92.46ms
step:46/1750 train_time:4255ms step_avg:92.50ms
step:47/1750 train_time:4350ms step_avg:92.54ms
step:48/1750 train_time:4444ms step_avg:92.58ms
step:49/1750 train_time:4537ms step_avg:92.59ms
step:50/1750 train_time:4630ms step_avg:92.61ms
step:51/1750 train_time:4723ms step_avg:92.61ms
step:52/1750 train_time:4816ms step_avg:92.62ms
step:53/1750 train_time:4910ms step_avg:92.64ms
step:54/1750 train_time:5003ms step_avg:92.65ms
step:55/1750 train_time:5097ms step_avg:92.67ms
step:56/1750 train_time:5190ms step_avg:92.67ms
step:57/1750 train_time:5283ms step_avg:92.69ms
step:58/1750 train_time:5377ms step_avg:92.70ms
step:59/1750 train_time:5470ms step_avg:92.71ms
step:60/1750 train_time:5563ms step_avg:92.71ms
step:61/1750 train_time:5657ms step_avg:92.74ms
step:62/1750 train_time:5751ms step_avg:92.75ms
step:63/1750 train_time:5844ms step_avg:92.76ms
step:64/1750 train_time:5938ms step_avg:92.77ms
step:65/1750 train_time:6031ms step_avg:92.78ms
step:66/1750 train_time:6123ms step_avg:92.78ms
step:67/1750 train_time:6217ms step_avg:92.79ms
step:68/1750 train_time:6310ms step_avg:92.79ms
step:69/1750 train_time:6403ms step_avg:92.79ms
step:70/1750 train_time:6496ms step_avg:92.80ms
step:71/1750 train_time:6590ms step_avg:92.81ms
step:72/1750 train_time:6683ms step_avg:92.83ms
step:73/1750 train_time:6777ms step_avg:92.84ms
step:74/1750 train_time:6870ms step_avg:92.84ms
step:75/1750 train_time:6964ms step_avg:92.85ms
step:76/1750 train_time:7058ms step_avg:92.86ms
step:77/1750 train_time:7152ms step_avg:92.89ms
step:78/1750 train_time:7247ms step_avg:92.91ms
step:79/1750 train_time:7340ms step_avg:92.91ms
step:80/1750 train_time:7434ms step_avg:92.92ms
step:81/1750 train_time:7528ms step_avg:92.93ms
step:82/1750 train_time:7621ms step_avg:92.94ms
step:83/1750 train_time:7714ms step_avg:92.94ms
step:84/1750 train_time:7807ms step_avg:92.94ms
step:85/1750 train_time:7900ms step_avg:92.94ms
step:86/1750 train_time:7994ms step_avg:92.96ms
step:87/1750 train_time:8087ms step_avg:92.95ms
step:88/1750 train_time:8180ms step_avg:92.96ms
step:89/1750 train_time:8274ms step_avg:92.97ms
step:90/1750 train_time:8367ms step_avg:92.96ms
step:91/1750 train_time:8460ms step_avg:92.97ms
step:92/1750 train_time:8553ms step_avg:92.97ms
step:93/1750 train_time:8647ms step_avg:92.98ms
step:94/1750 train_time:8740ms step_avg:92.98ms
step:95/1750 train_time:8833ms step_avg:92.98ms
step:96/1750 train_time:8926ms step_avg:92.98ms
step:97/1750 train_time:9020ms step_avg:92.99ms
step:98/1750 train_time:9114ms step_avg:93.00ms
step:99/1750 train_time:9207ms step_avg:93.00ms
step:100/1750 train_time:9300ms step_avg:93.00ms
step:101/1750 train_time:9394ms step_avg:93.01ms
step:102/1750 train_time:9488ms step_avg:93.02ms
step:103/1750 train_time:9580ms step_avg:93.01ms
step:104/1750 train_time:9673ms step_avg:93.01ms
step:105/1750 train_time:9766ms step_avg:93.01ms
step:106/1750 train_time:9859ms step_avg:93.01ms
step:107/1750 train_time:9953ms step_avg:93.02ms
step:108/1750 train_time:10045ms step_avg:93.01ms
step:109/1750 train_time:10139ms step_avg:93.02ms
step:110/1750 train_time:10233ms step_avg:93.02ms
step:111/1750 train_time:10326ms step_avg:93.03ms
step:112/1750 train_time:10419ms step_avg:93.03ms
step:113/1750 train_time:10513ms step_avg:93.03ms
step:114/1750 train_time:10606ms step_avg:93.04ms
step:115/1750 train_time:10699ms step_avg:93.03ms
step:116/1750 train_time:10792ms step_avg:93.03ms
step:117/1750 train_time:10885ms step_avg:93.04ms
step:118/1750 train_time:10979ms step_avg:93.04ms
step:119/1750 train_time:11072ms step_avg:93.04ms
step:120/1750 train_time:11166ms step_avg:93.05ms
step:121/1750 train_time:11259ms step_avg:93.05ms
step:122/1750 train_time:11353ms step_avg:93.06ms
step:123/1750 train_time:11446ms step_avg:93.06ms
step:124/1750 train_time:11539ms step_avg:93.06ms
step:125/1750 train_time:11633ms step_avg:93.06ms
step:125/1750 val_loss:4.6506 train_time:11715ms step_avg:93.72ms
step:126/1750 train_time:11737ms step_avg:93.15ms
step:127/1750 train_time:11825ms step_avg:93.11ms
step:128/1750 train_time:11927ms step_avg:93.18ms
step:129/1750 train_time:12021ms step_avg:93.19ms
step:130/1750 train_time:12116ms step_avg:93.20ms
step:131/1750 train_time:12210ms step_avg:93.21ms
step:132/1750 train_time:12303ms step_avg:93.21ms
step:133/1750 train_time:12396ms step_avg:93.20ms
step:134/1750 train_time:12488ms step_avg:93.20ms
step:135/1750 train_time:12581ms step_avg:93.19ms
step:136/1750 train_time:12674ms step_avg:93.19ms
step:137/1750 train_time:12767ms step_avg:93.19ms
step:138/1750 train_time:12863ms step_avg:93.21ms
step:139/1750 train_time:12959ms step_avg:93.23ms
step:140/1750 train_time:13053ms step_avg:93.24ms
step:141/1750 train_time:13148ms step_avg:93.25ms
step:142/1750 train_time:13242ms step_avg:93.25ms
step:143/1750 train_time:13336ms step_avg:93.26ms
step:144/1750 train_time:13429ms step_avg:93.26ms
step:145/1750 train_time:13522ms step_avg:93.26ms
step:146/1750 train_time:13615ms step_avg:93.25ms
step:147/1750 train_time:13708ms step_avg:93.25ms
step:148/1750 train_time:13801ms step_avg:93.25ms
step:149/1750 train_time:13895ms step_avg:93.26ms
step:150/1750 train_time:13990ms step_avg:93.27ms
step:151/1750 train_time:14084ms step_avg:93.27ms
step:152/1750 train_time:14178ms step_avg:93.28ms
step:153/1750 train_time:14273ms step_avg:93.29ms
step:154/1750 train_time:14367ms step_avg:93.29ms
step:155/1750 train_time:14460ms step_avg:93.29ms
step:156/1750 train_time:14553ms step_avg:93.29ms
step:157/1750 train_time:14646ms step_avg:93.29ms
step:158/1750 train_time:14740ms step_avg:93.29ms
step:159/1750 train_time:14833ms step_avg:93.29ms
step:160/1750 train_time:14928ms step_avg:93.30ms
step:161/1750 train_time:15022ms step_avg:93.31ms
step:162/1750 train_time:15116ms step_avg:93.31ms
step:163/1750 train_time:15210ms step_avg:93.31ms
step:164/1750 train_time:15304ms step_avg:93.32ms
step:165/1750 train_time:15398ms step_avg:93.32ms
step:166/1750 train_time:15492ms step_avg:93.33ms
step:167/1750 train_time:15586ms step_avg:93.33ms
step:168/1750 train_time:15679ms step_avg:93.33ms
step:169/1750 train_time:15773ms step_avg:93.33ms
step:170/1750 train_time:15867ms step_avg:93.34ms
step:171/1750 train_time:15961ms step_avg:93.34ms
step:172/1750 train_time:16055ms step_avg:93.35ms
step:173/1750 train_time:16149ms step_avg:93.35ms
step:174/1750 train_time:16243ms step_avg:93.35ms
step:175/1750 train_time:16338ms step_avg:93.36ms
step:176/1750 train_time:16433ms step_avg:93.37ms
step:177/1750 train_time:16527ms step_avg:93.37ms
step:178/1750 train_time:16620ms step_avg:93.37ms
step:179/1750 train_time:16713ms step_avg:93.37ms
step:180/1750 train_time:16807ms step_avg:93.37ms
step:181/1750 train_time:16901ms step_avg:93.37ms
step:182/1750 train_time:16994ms step_avg:93.37ms
step:183/1750 train_time:17088ms step_avg:93.38ms
step:184/1750 train_time:17182ms step_avg:93.38ms
step:185/1750 train_time:17276ms step_avg:93.38ms
step:186/1750 train_time:17370ms step_avg:93.39ms
step:187/1750 train_time:17464ms step_avg:93.39ms
step:188/1750 train_time:17557ms step_avg:93.39ms
step:189/1750 train_time:17651ms step_avg:93.39ms
step:190/1750 train_time:17744ms step_avg:93.39ms
step:191/1750 train_time:17838ms step_avg:93.39ms
step:192/1750 train_time:17932ms step_avg:93.40ms
step:193/1750 train_time:18026ms step_avg:93.40ms
step:194/1750 train_time:18121ms step_avg:93.41ms
step:195/1750 train_time:18214ms step_avg:93.41ms
step:196/1750 train_time:18308ms step_avg:93.41ms
step:197/1750 train_time:18402ms step_avg:93.41ms
step:198/1750 train_time:18496ms step_avg:93.42ms
step:199/1750 train_time:18591ms step_avg:93.42ms
step:200/1750 train_time:18685ms step_avg:93.42ms
step:201/1750 train_time:18778ms step_avg:93.42ms
step:202/1750 train_time:18872ms step_avg:93.42ms
step:203/1750 train_time:18967ms step_avg:93.43ms
step:204/1750 train_time:19061ms step_avg:93.44ms
step:205/1750 train_time:19155ms step_avg:93.44ms
step:206/1750 train_time:19248ms step_avg:93.44ms
step:207/1750 train_time:19343ms step_avg:93.44ms
step:208/1750 train_time:19437ms step_avg:93.45ms
step:209/1750 train_time:19531ms step_avg:93.45ms
step:210/1750 train_time:19625ms step_avg:93.45ms
step:211/1750 train_time:19718ms step_avg:93.45ms
step:212/1750 train_time:19812ms step_avg:93.45ms
step:213/1750 train_time:19905ms step_avg:93.45ms
step:214/1750 train_time:19999ms step_avg:93.45ms
step:215/1750 train_time:20092ms step_avg:93.45ms
step:216/1750 train_time:20186ms step_avg:93.46ms
step:217/1750 train_time:20281ms step_avg:93.46ms
step:218/1750 train_time:20375ms step_avg:93.46ms
step:219/1750 train_time:20468ms step_avg:93.46ms
step:220/1750 train_time:20562ms step_avg:93.46ms
step:221/1750 train_time:20656ms step_avg:93.46ms
step:222/1750 train_time:20749ms step_avg:93.46ms
step:223/1750 train_time:20843ms step_avg:93.47ms
step:224/1750 train_time:20937ms step_avg:93.47ms
step:225/1750 train_time:21032ms step_avg:93.47ms
step:226/1750 train_time:21125ms step_avg:93.47ms
step:227/1750 train_time:21218ms step_avg:93.47ms
step:228/1750 train_time:21311ms step_avg:93.47ms
step:229/1750 train_time:21405ms step_avg:93.47ms
step:230/1750 train_time:21499ms step_avg:93.47ms
step:231/1750 train_time:21592ms step_avg:93.47ms
step:232/1750 train_time:21686ms step_avg:93.47ms
step:233/1750 train_time:21780ms step_avg:93.47ms
step:234/1750 train_time:21875ms step_avg:93.48ms
step:235/1750 train_time:21968ms step_avg:93.48ms
step:236/1750 train_time:22062ms step_avg:93.48ms
step:237/1750 train_time:22155ms step_avg:93.48ms
step:238/1750 train_time:22249ms step_avg:93.48ms
step:239/1750 train_time:22343ms step_avg:93.49ms
step:240/1750 train_time:22437ms step_avg:93.49ms
step:241/1750 train_time:22531ms step_avg:93.49ms
step:242/1750 train_time:22624ms step_avg:93.49ms
step:243/1750 train_time:22718ms step_avg:93.49ms
step:244/1750 train_time:22812ms step_avg:93.49ms
step:245/1750 train_time:22905ms step_avg:93.49ms
step:246/1750 train_time:22999ms step_avg:93.49ms
step:247/1750 train_time:23093ms step_avg:93.49ms
step:248/1750 train_time:23187ms step_avg:93.49ms
step:249/1750 train_time:23280ms step_avg:93.49ms
step:250/1750 train_time:23374ms step_avg:93.49ms
step:250/1750 val_loss:4.1047 train_time:23457ms step_avg:93.83ms
step:251/1750 train_time:23481ms step_avg:93.55ms
step:252/1750 train_time:23569ms step_avg:93.53ms
step:253/1750 train_time:23670ms step_avg:93.56ms
step:254/1750 train_time:23764ms step_avg:93.56ms
step:255/1750 train_time:23858ms step_avg:93.56ms
step:256/1750 train_time:23951ms step_avg:93.56ms
step:257/1750 train_time:24044ms step_avg:93.55ms
step:258/1750 train_time:24136ms step_avg:93.55ms
step:259/1750 train_time:24229ms step_avg:93.55ms
step:260/1750 train_time:24321ms step_avg:93.54ms
step:261/1750 train_time:24415ms step_avg:93.54ms
step:262/1750 train_time:24509ms step_avg:93.55ms
step:263/1750 train_time:24605ms step_avg:93.55ms
step:264/1750 train_time:24700ms step_avg:93.56ms
step:265/1750 train_time:24796ms step_avg:93.57ms
step:266/1750 train_time:24889ms step_avg:93.57ms
step:267/1750 train_time:24984ms step_avg:93.57ms
step:268/1750 train_time:25077ms step_avg:93.57ms
step:269/1750 train_time:25171ms step_avg:93.57ms
step:270/1750 train_time:25264ms step_avg:93.57ms
step:271/1750 train_time:25358ms step_avg:93.57ms
step:272/1750 train_time:25452ms step_avg:93.57ms
step:273/1750 train_time:25546ms step_avg:93.58ms
step:274/1750 train_time:25641ms step_avg:93.58ms
step:275/1750 train_time:25736ms step_avg:93.59ms
step:276/1750 train_time:25831ms step_avg:93.59ms
step:277/1750 train_time:25925ms step_avg:93.59ms
step:278/1750 train_time:26019ms step_avg:93.59ms
step:279/1750 train_time:26113ms step_avg:93.59ms
step:280/1750 train_time:26207ms step_avg:93.60ms
step:281/1750 train_time:26300ms step_avg:93.60ms
step:282/1750 train_time:26394ms step_avg:93.59ms
step:283/1750 train_time:26488ms step_avg:93.60ms
step:284/1750 train_time:26582ms step_avg:93.60ms
step:285/1750 train_time:26676ms step_avg:93.60ms
step:286/1750 train_time:26771ms step_avg:93.60ms
step:287/1750 train_time:26865ms step_avg:93.61ms
step:288/1750 train_time:26959ms step_avg:93.61ms
step:289/1750 train_time:27053ms step_avg:93.61ms
step:290/1750 train_time:27147ms step_avg:93.61ms
step:291/1750 train_time:27240ms step_avg:93.61ms
step:292/1750 train_time:27333ms step_avg:93.61ms
step:293/1750 train_time:27428ms step_avg:93.61ms
step:294/1750 train_time:27521ms step_avg:93.61ms
step:295/1750 train_time:27615ms step_avg:93.61ms
step:296/1750 train_time:27710ms step_avg:93.61ms
step:297/1750 train_time:27804ms step_avg:93.62ms
step:298/1750 train_time:27899ms step_avg:93.62ms
step:299/1750 train_time:27993ms step_avg:93.62ms
step:300/1750 train_time:28087ms step_avg:93.62ms
step:301/1750 train_time:28180ms step_avg:93.62ms
step:302/1750 train_time:28274ms step_avg:93.62ms
step:303/1750 train_time:28368ms step_avg:93.62ms
step:304/1750 train_time:28461ms step_avg:93.62ms
step:305/1750 train_time:28555ms step_avg:93.62ms
step:306/1750 train_time:28649ms step_avg:93.63ms
step:307/1750 train_time:28744ms step_avg:93.63ms
step:308/1750 train_time:28838ms step_avg:93.63ms
step:309/1750 train_time:28933ms step_avg:93.63ms
step:310/1750 train_time:29027ms step_avg:93.64ms
step:311/1750 train_time:29121ms step_avg:93.64ms
step:312/1750 train_time:29215ms step_avg:93.64ms
step:313/1750 train_time:29309ms step_avg:93.64ms
step:314/1750 train_time:29403ms step_avg:93.64ms
step:315/1750 train_time:29496ms step_avg:93.64ms
step:316/1750 train_time:29590ms step_avg:93.64ms
step:317/1750 train_time:29684ms step_avg:93.64ms
step:318/1750 train_time:29779ms step_avg:93.64ms
step:319/1750 train_time:29873ms step_avg:93.65ms
step:320/1750 train_time:29967ms step_avg:93.65ms
step:321/1750 train_time:30061ms step_avg:93.65ms
step:322/1750 train_time:30155ms step_avg:93.65ms
step:323/1750 train_time:30250ms step_avg:93.65ms
step:324/1750 train_time:30344ms step_avg:93.65ms
step:325/1750 train_time:30437ms step_avg:93.65ms
step:326/1750 train_time:30531ms step_avg:93.65ms
step:327/1750 train_time:30626ms step_avg:93.66ms
step:328/1750 train_time:30719ms step_avg:93.66ms
step:329/1750 train_time:30814ms step_avg:93.66ms
step:330/1750 train_time:30908ms step_avg:93.66ms
step:331/1750 train_time:31003ms step_avg:93.67ms
step:332/1750 train_time:31097ms step_avg:93.67ms
step:333/1750 train_time:31191ms step_avg:93.67ms
step:334/1750 train_time:31286ms step_avg:93.67ms
step:335/1750 train_time:31380ms step_avg:93.67ms
step:336/1750 train_time:31474ms step_avg:93.67ms
step:337/1750 train_time:31568ms step_avg:93.67ms
step:338/1750 train_time:31662ms step_avg:93.67ms
step:339/1750 train_time:31756ms step_avg:93.68ms
step:340/1750 train_time:31850ms step_avg:93.68ms
step:341/1750 train_time:31944ms step_avg:93.68ms
step:342/1750 train_time:32038ms step_avg:93.68ms
step:343/1750 train_time:32133ms step_avg:93.68ms
step:344/1750 train_time:32227ms step_avg:93.68ms
step:345/1750 train_time:32321ms step_avg:93.68ms
step:346/1750 train_time:32415ms step_avg:93.68ms
step:347/1750 train_time:32509ms step_avg:93.69ms
step:348/1750 train_time:32603ms step_avg:93.69ms
step:349/1750 train_time:32697ms step_avg:93.69ms
step:350/1750 train_time:32791ms step_avg:93.69ms
step:351/1750 train_time:32885ms step_avg:93.69ms
step:352/1750 train_time:32980ms step_avg:93.69ms
step:353/1750 train_time:33073ms step_avg:93.69ms
step:354/1750 train_time:33168ms step_avg:93.69ms
step:355/1750 train_time:33262ms step_avg:93.70ms
step:356/1750 train_time:33356ms step_avg:93.70ms
step:357/1750 train_time:33449ms step_avg:93.70ms
step:358/1750 train_time:33543ms step_avg:93.70ms
step:359/1750 train_time:33637ms step_avg:93.70ms
step:360/1750 train_time:33731ms step_avg:93.70ms
step:361/1750 train_time:33826ms step_avg:93.70ms
step:362/1750 train_time:33920ms step_avg:93.70ms
step:363/1750 train_time:34014ms step_avg:93.70ms
step:364/1750 train_time:34109ms step_avg:93.71ms
step:365/1750 train_time:34203ms step_avg:93.71ms
step:366/1750 train_time:34296ms step_avg:93.71ms
step:367/1750 train_time:34390ms step_avg:93.71ms
step:368/1750 train_time:34484ms step_avg:93.71ms
step:369/1750 train_time:34579ms step_avg:93.71ms
step:370/1750 train_time:34673ms step_avg:93.71ms
step:371/1750 train_time:34767ms step_avg:93.71ms
step:372/1750 train_time:34861ms step_avg:93.71ms
step:373/1750 train_time:34955ms step_avg:93.71ms
step:374/1750 train_time:35049ms step_avg:93.71ms
step:375/1750 train_time:35143ms step_avg:93.72ms
step:375/1750 val_loss:3.8910 train_time:35227ms step_avg:93.94ms
step:376/1750 train_time:35249ms step_avg:93.75ms
step:377/1750 train_time:35339ms step_avg:93.74ms
step:378/1750 train_time:35437ms step_avg:93.75ms
step:379/1750 train_time:35534ms step_avg:93.76ms
step:380/1750 train_time:35628ms step_avg:93.76ms
step:381/1750 train_time:35721ms step_avg:93.76ms
step:382/1750 train_time:35814ms step_avg:93.75ms
step:383/1750 train_time:35907ms step_avg:93.75ms
step:384/1750 train_time:36000ms step_avg:93.75ms
step:385/1750 train_time:36093ms step_avg:93.75ms
step:386/1750 train_time:36189ms step_avg:93.75ms
step:387/1750 train_time:36285ms step_avg:93.76ms
step:388/1750 train_time:36381ms step_avg:93.77ms
step:389/1750 train_time:36477ms step_avg:93.77ms
step:390/1750 train_time:36572ms step_avg:93.77ms
step:391/1750 train_time:36667ms step_avg:93.78ms
step:392/1750 train_time:36764ms step_avg:93.78ms
step:393/1750 train_time:36859ms step_avg:93.79ms
step:394/1750 train_time:36955ms step_avg:93.79ms
step:395/1750 train_time:37049ms step_avg:93.80ms
step:396/1750 train_time:37145ms step_avg:93.80ms
step:397/1750 train_time:37241ms step_avg:93.81ms
step:398/1750 train_time:37339ms step_avg:93.82ms
step:399/1750 train_time:37436ms step_avg:93.82ms
step:400/1750 train_time:37532ms step_avg:93.83ms
step:401/1750 train_time:37627ms step_avg:93.83ms
step:402/1750 train_time:37723ms step_avg:93.84ms
step:403/1750 train_time:37819ms step_avg:93.84ms
step:404/1750 train_time:37914ms step_avg:93.85ms
step:405/1750 train_time:38009ms step_avg:93.85ms
step:406/1750 train_time:38105ms step_avg:93.85ms
step:407/1750 train_time:38201ms step_avg:93.86ms
step:408/1750 train_time:38297ms step_avg:93.87ms
step:409/1750 train_time:38395ms step_avg:93.87ms
step:410/1750 train_time:38491ms step_avg:93.88ms
step:411/1750 train_time:38587ms step_avg:93.89ms
step:412/1750 train_time:38683ms step_avg:93.89ms
step:413/1750 train_time:38779ms step_avg:93.90ms
step:414/1750 train_time:38875ms step_avg:93.90ms
step:415/1750 train_time:38971ms step_avg:93.91ms
step:416/1750 train_time:39066ms step_avg:93.91ms
step:417/1750 train_time:39161ms step_avg:93.91ms
step:418/1750 train_time:39257ms step_avg:93.92ms
step:419/1750 train_time:39353ms step_avg:93.92ms
step:420/1750 train_time:39450ms step_avg:93.93ms
step:421/1750 train_time:39546ms step_avg:93.93ms
step:422/1750 train_time:39642ms step_avg:93.94ms
step:423/1750 train_time:39738ms step_avg:93.94ms
step:424/1750 train_time:39835ms step_avg:93.95ms
step:425/1750 train_time:39930ms step_avg:93.95ms
step:426/1750 train_time:40026ms step_avg:93.96ms
step:427/1750 train_time:40121ms step_avg:93.96ms
step:428/1750 train_time:40216ms step_avg:93.96ms
step:429/1750 train_time:40312ms step_avg:93.97ms
step:430/1750 train_time:40409ms step_avg:93.97ms
step:431/1750 train_time:40505ms step_avg:93.98ms
step:432/1750 train_time:40602ms step_avg:93.98ms
step:433/1750 train_time:40698ms step_avg:93.99ms
step:434/1750 train_time:40794ms step_avg:94.00ms
step:435/1750 train_time:40890ms step_avg:94.00ms
step:436/1750 train_time:40985ms step_avg:94.00ms
step:437/1750 train_time:41081ms step_avg:94.01ms
step:438/1750 train_time:41176ms step_avg:94.01ms
step:439/1750 train_time:41272ms step_avg:94.01ms
step:440/1750 train_time:41369ms step_avg:94.02ms
step:441/1750 train_time:41465ms step_avg:94.03ms
step:442/1750 train_time:41562ms step_avg:94.03ms
step:443/1750 train_time:41658ms step_avg:94.04ms
step:444/1750 train_time:41753ms step_avg:94.04ms
step:445/1750 train_time:41850ms step_avg:94.04ms
step:446/1750 train_time:41946ms step_avg:94.05ms
step:447/1750 train_time:42041ms step_avg:94.05ms
step:448/1750 train_time:42137ms step_avg:94.06ms
step:449/1750 train_time:42234ms step_avg:94.06ms
step:450/1750 train_time:42329ms step_avg:94.07ms
step:451/1750 train_time:42426ms step_avg:94.07ms
step:452/1750 train_time:42522ms step_avg:94.07ms
step:453/1750 train_time:42618ms step_avg:94.08ms
step:454/1750 train_time:42716ms step_avg:94.09ms
step:455/1750 train_time:42812ms step_avg:94.09ms
step:456/1750 train_time:42908ms step_avg:94.10ms
step:457/1750 train_time:43004ms step_avg:94.10ms
step:458/1750 train_time:43100ms step_avg:94.11ms
step:459/1750 train_time:43196ms step_avg:94.11ms
step:460/1750 train_time:43292ms step_avg:94.11ms
step:461/1750 train_time:43388ms step_avg:94.12ms
step:462/1750 train_time:43484ms step_avg:94.12ms
step:463/1750 train_time:43580ms step_avg:94.12ms
step:464/1750 train_time:43676ms step_avg:94.13ms
step:465/1750 train_time:43772ms step_avg:94.13ms
step:466/1750 train_time:43867ms step_avg:94.14ms
step:467/1750 train_time:43963ms step_avg:94.14ms
step:468/1750 train_time:44059ms step_avg:94.14ms
step:469/1750 train_time:44155ms step_avg:94.15ms
step:470/1750 train_time:44251ms step_avg:94.15ms
step:471/1750 train_time:44347ms step_avg:94.16ms
step:472/1750 train_time:44444ms step_avg:94.16ms
step:473/1750 train_time:44540ms step_avg:94.17ms
step:474/1750 train_time:44636ms step_avg:94.17ms
step:475/1750 train_time:44732ms step_avg:94.17ms
step:476/1750 train_time:44828ms step_avg:94.18ms
step:477/1750 train_time:44924ms step_avg:94.18ms
step:478/1750 train_time:45020ms step_avg:94.18ms
step:479/1750 train_time:45115ms step_avg:94.19ms
step:480/1750 train_time:45211ms step_avg:94.19ms
step:481/1750 train_time:45307ms step_avg:94.19ms
step:482/1750 train_time:45403ms step_avg:94.20ms
step:483/1750 train_time:45499ms step_avg:94.20ms
step:484/1750 train_time:45595ms step_avg:94.21ms
step:485/1750 train_time:45692ms step_avg:94.21ms
step:486/1750 train_time:45788ms step_avg:94.21ms
step:487/1750 train_time:45884ms step_avg:94.22ms
step:488/1750 train_time:45981ms step_avg:94.22ms
step:489/1750 train_time:46076ms step_avg:94.23ms
step:490/1750 train_time:46173ms step_avg:94.23ms
step:491/1750 train_time:46269ms step_avg:94.23ms
step:492/1750 train_time:46367ms step_avg:94.24ms
step:493/1750 train_time:46464ms step_avg:94.25ms
step:494/1750 train_time:46560ms step_avg:94.25ms
step:495/1750 train_time:46656ms step_avg:94.25ms
step:496/1750 train_time:46751ms step_avg:94.26ms
step:497/1750 train_time:46848ms step_avg:94.26ms
step:498/1750 train_time:46944ms step_avg:94.27ms
step:499/1750 train_time:47040ms step_avg:94.27ms
step:500/1750 train_time:47135ms step_avg:94.27ms
step:500/1750 val_loss:3.7469 train_time:47220ms step_avg:94.44ms
step:501/1750 train_time:47242ms step_avg:94.30ms
step:502/1750 train_time:47335ms step_avg:94.29ms
step:503/1750 train_time:47435ms step_avg:94.30ms
step:504/1750 train_time:47530ms step_avg:94.31ms
step:505/1750 train_time:47626ms step_avg:94.31ms
step:506/1750 train_time:47721ms step_avg:94.31ms
step:507/1750 train_time:47816ms step_avg:94.31ms
step:508/1750 train_time:47912ms step_avg:94.31ms
step:509/1750 train_time:48007ms step_avg:94.32ms
step:510/1750 train_time:48102ms step_avg:94.32ms
step:511/1750 train_time:48198ms step_avg:94.32ms
step:512/1750 train_time:48296ms step_avg:94.33ms
step:513/1750 train_time:48393ms step_avg:94.33ms
step:514/1750 train_time:48489ms step_avg:94.34ms
step:515/1750 train_time:48585ms step_avg:94.34ms
step:516/1750 train_time:48681ms step_avg:94.34ms
step:517/1750 train_time:48777ms step_avg:94.35ms
step:518/1750 train_time:48872ms step_avg:94.35ms
step:519/1750 train_time:48967ms step_avg:94.35ms
step:520/1750 train_time:49063ms step_avg:94.35ms
step:521/1750 train_time:49160ms step_avg:94.36ms
step:522/1750 train_time:49257ms step_avg:94.36ms
step:523/1750 train_time:49355ms step_avg:94.37ms
step:524/1750 train_time:49452ms step_avg:94.37ms
step:525/1750 train_time:49549ms step_avg:94.38ms
step:526/1750 train_time:49646ms step_avg:94.38ms
step:527/1750 train_time:49742ms step_avg:94.39ms
step:528/1750 train_time:49838ms step_avg:94.39ms
step:529/1750 train_time:49934ms step_avg:94.39ms
step:530/1750 train_time:50029ms step_avg:94.39ms
step:531/1750 train_time:50125ms step_avg:94.40ms
step:532/1750 train_time:50223ms step_avg:94.40ms
step:533/1750 train_time:50320ms step_avg:94.41ms
step:534/1750 train_time:50417ms step_avg:94.41ms
step:535/1750 train_time:50514ms step_avg:94.42ms
step:536/1750 train_time:50611ms step_avg:94.42ms
step:537/1750 train_time:50708ms step_avg:94.43ms
step:538/1750 train_time:50804ms step_avg:94.43ms
step:539/1750 train_time:50899ms step_avg:94.43ms
step:540/1750 train_time:50995ms step_avg:94.43ms
step:541/1750 train_time:51091ms step_avg:94.44ms
step:542/1750 train_time:51187ms step_avg:94.44ms
step:543/1750 train_time:51285ms step_avg:94.45ms
step:544/1750 train_time:51381ms step_avg:94.45ms
step:545/1750 train_time:51478ms step_avg:94.46ms
step:546/1750 train_time:51575ms step_avg:94.46ms
step:547/1750 train_time:51672ms step_avg:94.46ms
step:548/1750 train_time:51767ms step_avg:94.47ms
step:549/1750 train_time:51863ms step_avg:94.47ms
step:550/1750 train_time:51959ms step_avg:94.47ms
step:551/1750 train_time:52057ms step_avg:94.48ms
step:552/1750 train_time:52153ms step_avg:94.48ms
step:553/1750 train_time:52249ms step_avg:94.48ms
step:554/1750 train_time:52346ms step_avg:94.49ms
step:555/1750 train_time:52443ms step_avg:94.49ms
step:556/1750 train_time:52540ms step_avg:94.50ms
step:557/1750 train_time:52637ms step_avg:94.50ms
step:558/1750 train_time:52733ms step_avg:94.50ms
step:559/1750 train_time:52829ms step_avg:94.51ms
step:560/1750 train_time:52925ms step_avg:94.51ms
step:561/1750 train_time:53021ms step_avg:94.51ms
step:562/1750 train_time:53118ms step_avg:94.52ms
step:563/1750 train_time:53215ms step_avg:94.52ms
step:564/1750 train_time:53312ms step_avg:94.52ms
step:565/1750 train_time:53408ms step_avg:94.53ms
step:566/1750 train_time:53506ms step_avg:94.53ms
step:567/1750 train_time:53603ms step_avg:94.54ms
step:568/1750 train_time:53700ms step_avg:94.54ms
step:569/1750 train_time:53796ms step_avg:94.55ms
step:570/1750 train_time:53893ms step_avg:94.55ms
step:571/1750 train_time:53989ms step_avg:94.55ms
step:572/1750 train_time:54086ms step_avg:94.56ms
step:573/1750 train_time:54183ms step_avg:94.56ms
step:574/1750 train_time:54280ms step_avg:94.56ms
step:575/1750 train_time:54377ms step_avg:94.57ms
step:576/1750 train_time:54474ms step_avg:94.57ms
step:577/1750 train_time:54570ms step_avg:94.58ms
step:578/1750 train_time:54668ms step_avg:94.58ms
step:579/1750 train_time:54765ms step_avg:94.59ms
step:580/1750 train_time:54861ms step_avg:94.59ms
step:581/1750 train_time:54958ms step_avg:94.59ms
step:582/1750 train_time:55054ms step_avg:94.59ms
step:583/1750 train_time:55150ms step_avg:94.60ms
step:584/1750 train_time:55247ms step_avg:94.60ms
step:585/1750 train_time:55343ms step_avg:94.60ms
step:586/1750 train_time:55440ms step_avg:94.61ms
step:587/1750 train_time:55537ms step_avg:94.61ms
step:588/1750 train_time:55634ms step_avg:94.62ms
step:589/1750 train_time:55731ms step_avg:94.62ms
step:590/1750 train_time:55827ms step_avg:94.62ms
step:591/1750 train_time:55923ms step_avg:94.62ms
step:592/1750 train_time:56020ms step_avg:94.63ms
step:593/1750 train_time:56116ms step_avg:94.63ms
step:594/1750 train_time:56213ms step_avg:94.63ms
step:595/1750 train_time:56309ms step_avg:94.64ms
step:596/1750 train_time:56406ms step_avg:94.64ms
step:597/1750 train_time:56503ms step_avg:94.64ms
step:598/1750 train_time:56600ms step_avg:94.65ms
step:599/1750 train_time:56696ms step_avg:94.65ms
step:600/1750 train_time:56791ms step_avg:94.65ms
step:601/1750 train_time:56888ms step_avg:94.65ms
step:602/1750 train_time:56985ms step_avg:94.66ms
step:603/1750 train_time:57081ms step_avg:94.66ms
step:604/1750 train_time:57178ms step_avg:94.67ms
step:605/1750 train_time:57274ms step_avg:94.67ms
step:606/1750 train_time:57370ms step_avg:94.67ms
step:607/1750 train_time:57467ms step_avg:94.67ms
step:608/1750 train_time:57564ms step_avg:94.68ms
step:609/1750 train_time:57662ms step_avg:94.68ms
step:610/1750 train_time:57758ms step_avg:94.69ms
step:611/1750 train_time:57854ms step_avg:94.69ms
step:612/1750 train_time:57951ms step_avg:94.69ms
step:613/1750 train_time:58047ms step_avg:94.69ms
step:614/1750 train_time:58143ms step_avg:94.70ms
step:615/1750 train_time:58240ms step_avg:94.70ms
step:616/1750 train_time:58337ms step_avg:94.70ms
step:617/1750 train_time:58434ms step_avg:94.71ms
step:618/1750 train_time:58531ms step_avg:94.71ms
step:619/1750 train_time:58627ms step_avg:94.71ms
step:620/1750 train_time:58724ms step_avg:94.72ms
step:621/1750 train_time:58820ms step_avg:94.72ms
step:622/1750 train_time:58916ms step_avg:94.72ms
step:623/1750 train_time:59013ms step_avg:94.72ms
step:624/1750 train_time:59110ms step_avg:94.73ms
step:625/1750 train_time:59207ms step_avg:94.73ms
step:625/1750 val_loss:3.6577 train_time:59292ms step_avg:94.87ms
step:626/1750 train_time:59314ms step_avg:94.75ms
step:627/1750 train_time:59407ms step_avg:94.75ms
step:628/1750 train_time:59506ms step_avg:94.75ms
step:629/1750 train_time:59603ms step_avg:94.76ms
step:630/1750 train_time:59699ms step_avg:94.76ms
step:631/1750 train_time:59795ms step_avg:94.76ms
step:632/1750 train_time:59890ms step_avg:94.76ms
step:633/1750 train_time:59985ms step_avg:94.76ms
step:634/1750 train_time:60082ms step_avg:94.77ms
step:635/1750 train_time:60177ms step_avg:94.77ms
step:636/1750 train_time:60274ms step_avg:94.77ms
step:637/1750 train_time:60372ms step_avg:94.77ms
step:638/1750 train_time:60469ms step_avg:94.78ms
step:639/1750 train_time:60567ms step_avg:94.78ms
step:640/1750 train_time:60664ms step_avg:94.79ms
step:641/1750 train_time:60761ms step_avg:94.79ms
step:642/1750 train_time:60856ms step_avg:94.79ms
step:643/1750 train_time:60951ms step_avg:94.79ms
step:644/1750 train_time:61046ms step_avg:94.79ms
step:645/1750 train_time:61142ms step_avg:94.79ms
step:646/1750 train_time:61240ms step_avg:94.80ms
step:647/1750 train_time:61337ms step_avg:94.80ms
step:648/1750 train_time:61434ms step_avg:94.81ms
step:649/1750 train_time:61532ms step_avg:94.81ms
step:650/1750 train_time:61630ms step_avg:94.81ms
step:651/1750 train_time:61728ms step_avg:94.82ms
step:652/1750 train_time:61825ms step_avg:94.82ms
step:653/1750 train_time:61923ms step_avg:94.83ms
step:654/1750 train_time:62021ms step_avg:94.83ms
step:655/1750 train_time:62118ms step_avg:94.84ms
step:656/1750 train_time:62217ms step_avg:94.84ms
step:657/1750 train_time:62315ms step_avg:94.85ms
step:658/1750 train_time:62413ms step_avg:94.85ms
step:659/1750 train_time:62512ms step_avg:94.86ms
step:660/1750 train_time:62611ms step_avg:94.86ms
step:661/1750 train_time:62709ms step_avg:94.87ms
step:662/1750 train_time:62807ms step_avg:94.87ms
step:663/1750 train_time:62905ms step_avg:94.88ms
step:664/1750 train_time:63002ms step_avg:94.88ms
step:665/1750 train_time:63099ms step_avg:94.89ms
step:666/1750 train_time:63196ms step_avg:94.89ms
step:667/1750 train_time:63293ms step_avg:94.89ms
step:668/1750 train_time:63391ms step_avg:94.90ms
step:669/1750 train_time:63489ms step_avg:94.90ms
step:670/1750 train_time:63587ms step_avg:94.91ms
step:671/1750 train_time:63685ms step_avg:94.91ms
step:672/1750 train_time:63783ms step_avg:94.92ms
step:673/1750 train_time:63881ms step_avg:94.92ms
step:674/1750 train_time:63978ms step_avg:94.92ms
step:675/1750 train_time:64075ms step_avg:94.93ms
step:676/1750 train_time:64172ms step_avg:94.93ms
step:677/1750 train_time:64270ms step_avg:94.93ms
step:678/1750 train_time:64368ms step_avg:94.94ms
step:679/1750 train_time:64467ms step_avg:94.94ms
step:680/1750 train_time:64565ms step_avg:94.95ms
step:681/1750 train_time:64663ms step_avg:94.95ms
step:682/1750 train_time:64761ms step_avg:94.96ms
step:683/1750 train_time:64858ms step_avg:94.96ms
step:684/1750 train_time:64956ms step_avg:94.97ms
step:685/1750 train_time:65055ms step_avg:94.97ms
step:686/1750 train_time:65153ms step_avg:94.97ms
step:687/1750 train_time:65251ms step_avg:94.98ms
step:688/1750 train_time:65348ms step_avg:94.98ms
step:689/1750 train_time:65446ms step_avg:94.99ms
step:690/1750 train_time:65544ms step_avg:94.99ms
step:691/1750 train_time:65643ms step_avg:95.00ms
step:692/1750 train_time:65741ms step_avg:95.00ms
step:693/1750 train_time:65839ms step_avg:95.01ms
step:694/1750 train_time:65937ms step_avg:95.01ms
step:695/1750 train_time:66035ms step_avg:95.01ms
step:696/1750 train_time:66132ms step_avg:95.02ms
step:697/1750 train_time:66229ms step_avg:95.02ms
step:698/1750 train_time:66327ms step_avg:95.02ms
step:699/1750 train_time:66424ms step_avg:95.03ms
step:700/1750 train_time:66523ms step_avg:95.03ms
step:701/1750 train_time:66621ms step_avg:95.04ms
step:702/1750 train_time:66718ms step_avg:95.04ms
step:703/1750 train_time:66816ms step_avg:95.04ms
step:704/1750 train_time:66914ms step_avg:95.05ms
step:705/1750 train_time:67012ms step_avg:95.05ms
step:706/1750 train_time:67110ms step_avg:95.06ms
step:707/1750 train_time:67208ms step_avg:95.06ms
step:708/1750 train_time:67306ms step_avg:95.06ms
step:709/1750 train_time:67405ms step_avg:95.07ms
step:710/1750 train_time:67503ms step_avg:95.07ms
step:711/1750 train_time:67600ms step_avg:95.08ms
step:712/1750 train_time:67697ms step_avg:95.08ms
step:713/1750 train_time:67796ms step_avg:95.09ms
step:714/1750 train_time:67894ms step_avg:95.09ms
step:715/1750 train_time:67991ms step_avg:95.09ms
step:716/1750 train_time:68089ms step_avg:95.10ms
step:717/1750 train_time:68186ms step_avg:95.10ms
step:718/1750 train_time:68284ms step_avg:95.10ms
step:719/1750 train_time:68381ms step_avg:95.11ms
step:720/1750 train_time:68479ms step_avg:95.11ms
step:721/1750 train_time:68577ms step_avg:95.11ms
step:722/1750 train_time:68675ms step_avg:95.12ms
step:723/1750 train_time:68772ms step_avg:95.12ms
step:724/1750 train_time:68869ms step_avg:95.12ms
step:725/1750 train_time:68967ms step_avg:95.13ms
step:726/1750 train_time:69066ms step_avg:95.13ms
step:727/1750 train_time:69163ms step_avg:95.14ms
step:728/1750 train_time:69261ms step_avg:95.14ms
step:729/1750 train_time:69359ms step_avg:95.14ms
step:730/1750 train_time:69457ms step_avg:95.15ms
step:731/1750 train_time:69555ms step_avg:95.15ms
step:732/1750 train_time:69653ms step_avg:95.15ms
step:733/1750 train_time:69750ms step_avg:95.16ms
step:734/1750 train_time:69848ms step_avg:95.16ms
step:735/1750 train_time:69946ms step_avg:95.16ms
step:736/1750 train_time:70044ms step_avg:95.17ms
step:737/1750 train_time:70142ms step_avg:95.17ms
step:738/1750 train_time:70239ms step_avg:95.18ms
step:739/1750 train_time:70337ms step_avg:95.18ms
step:740/1750 train_time:70436ms step_avg:95.18ms
step:741/1750 train_time:70534ms step_avg:95.19ms
step:742/1750 train_time:70633ms step_avg:95.19ms
step:743/1750 train_time:70730ms step_avg:95.20ms
step:744/1750 train_time:70827ms step_avg:95.20ms
step:745/1750 train_time:70925ms step_avg:95.20ms
step:746/1750 train_time:71023ms step_avg:95.20ms
step:747/1750 train_time:71120ms step_avg:95.21ms
step:748/1750 train_time:71217ms step_avg:95.21ms
step:749/1750 train_time:71315ms step_avg:95.21ms
step:750/1750 train_time:71413ms step_avg:95.22ms
step:750/1750 val_loss:3.5968 train_time:71500ms step_avg:95.33ms
step:751/1750 train_time:71522ms step_avg:95.24ms
step:752/1750 train_time:71618ms step_avg:95.24ms
step:753/1750 train_time:71720ms step_avg:95.25ms
step:754/1750 train_time:71818ms step_avg:95.25ms
step:755/1750 train_time:71916ms step_avg:95.25ms
step:756/1750 train_time:72013ms step_avg:95.26ms
step:757/1750 train_time:72110ms step_avg:95.26ms
step:758/1750 train_time:72207ms step_avg:95.26ms
step:759/1750 train_time:72305ms step_avg:95.26ms
step:760/1750 train_time:72401ms step_avg:95.27ms
step:761/1750 train_time:72499ms step_avg:95.27ms
step:762/1750 train_time:72599ms step_avg:95.27ms
step:763/1750 train_time:72698ms step_avg:95.28ms
step:764/1750 train_time:72796ms step_avg:95.28ms
step:765/1750 train_time:72894ms step_avg:95.29ms
step:766/1750 train_time:72991ms step_avg:95.29ms
step:767/1750 train_time:73089ms step_avg:95.29ms
step:768/1750 train_time:73186ms step_avg:95.29ms
step:769/1750 train_time:73284ms step_avg:95.30ms
step:770/1750 train_time:73381ms step_avg:95.30ms
step:771/1750 train_time:73479ms step_avg:95.30ms
step:772/1750 train_time:73576ms step_avg:95.31ms
step:773/1750 train_time:73675ms step_avg:95.31ms
step:774/1750 train_time:73774ms step_avg:95.31ms
step:775/1750 train_time:73871ms step_avg:95.32ms
step:776/1750 train_time:73969ms step_avg:95.32ms
step:777/1750 train_time:74067ms step_avg:95.32ms
step:778/1750 train_time:74164ms step_avg:95.33ms
step:779/1750 train_time:74262ms step_avg:95.33ms
step:780/1750 train_time:74361ms step_avg:95.33ms
step:781/1750 train_time:74460ms step_avg:95.34ms
step:782/1750 train_time:74558ms step_avg:95.34ms
step:783/1750 train_time:74657ms step_avg:95.35ms
step:784/1750 train_time:74756ms step_avg:95.35ms
step:785/1750 train_time:74855ms step_avg:95.36ms
step:786/1750 train_time:74954ms step_avg:95.36ms
step:787/1750 train_time:75052ms step_avg:95.36ms
step:788/1750 train_time:75150ms step_avg:95.37ms
step:789/1750 train_time:75248ms step_avg:95.37ms
step:790/1750 train_time:75346ms step_avg:95.38ms
step:791/1750 train_time:75444ms step_avg:95.38ms
step:792/1750 train_time:75542ms step_avg:95.38ms
step:793/1750 train_time:75640ms step_avg:95.38ms
step:794/1750 train_time:75739ms step_avg:95.39ms
step:795/1750 train_time:75838ms step_avg:95.39ms
step:796/1750 train_time:75938ms step_avg:95.40ms
step:797/1750 train_time:76037ms step_avg:95.40ms
step:798/1750 train_time:76134ms step_avg:95.41ms
step:799/1750 train_time:76232ms step_avg:95.41ms
step:800/1750 train_time:76331ms step_avg:95.41ms
step:801/1750 train_time:76430ms step_avg:95.42ms
step:802/1750 train_time:76529ms step_avg:95.42ms
step:803/1750 train_time:76627ms step_avg:95.43ms
step:804/1750 train_time:76728ms step_avg:95.43ms
step:805/1750 train_time:76826ms step_avg:95.44ms
step:806/1750 train_time:76926ms step_avg:95.44ms
step:807/1750 train_time:77026ms step_avg:95.45ms
step:808/1750 train_time:77124ms step_avg:95.45ms
step:809/1750 train_time:77222ms step_avg:95.45ms
step:810/1750 train_time:77321ms step_avg:95.46ms
step:811/1750 train_time:77421ms step_avg:95.46ms
step:812/1750 train_time:77520ms step_avg:95.47ms
step:813/1750 train_time:77619ms step_avg:95.47ms
step:814/1750 train_time:77717ms step_avg:95.47ms
step:815/1750 train_time:77815ms step_avg:95.48ms
step:816/1750 train_time:77913ms step_avg:95.48ms
step:817/1750 train_time:78011ms step_avg:95.48ms
step:818/1750 train_time:78110ms step_avg:95.49ms
step:819/1750 train_time:78209ms step_avg:95.49ms
step:820/1750 train_time:78307ms step_avg:95.50ms
step:821/1750 train_time:78406ms step_avg:95.50ms
step:822/1750 train_time:78504ms step_avg:95.50ms
step:823/1750 train_time:78602ms step_avg:95.51ms
step:824/1750 train_time:78700ms step_avg:95.51ms
step:825/1750 train_time:78799ms step_avg:95.51ms
step:826/1750 train_time:78897ms step_avg:95.52ms
step:827/1750 train_time:78995ms step_avg:95.52ms
step:828/1750 train_time:79093ms step_avg:95.52ms
step:829/1750 train_time:79191ms step_avg:95.53ms
step:830/1750 train_time:79288ms step_avg:95.53ms
step:831/1750 train_time:79387ms step_avg:95.53ms
step:832/1750 train_time:79485ms step_avg:95.54ms
step:833/1750 train_time:79584ms step_avg:95.54ms
step:834/1750 train_time:79681ms step_avg:95.54ms
step:835/1750 train_time:79779ms step_avg:95.54ms
step:836/1750 train_time:79878ms step_avg:95.55ms
step:837/1750 train_time:79977ms step_avg:95.55ms
step:838/1750 train_time:80075ms step_avg:95.56ms
step:839/1750 train_time:80174ms step_avg:95.56ms
step:840/1750 train_time:80272ms step_avg:95.56ms
step:841/1750 train_time:80369ms step_avg:95.56ms
step:842/1750 train_time:80468ms step_avg:95.57ms
step:843/1750 train_time:80566ms step_avg:95.57ms
step:844/1750 train_time:80664ms step_avg:95.57ms
step:845/1750 train_time:80763ms step_avg:95.58ms
step:846/1750 train_time:80861ms step_avg:95.58ms
step:847/1750 train_time:80960ms step_avg:95.58ms
step:848/1750 train_time:81058ms step_avg:95.59ms
step:849/1750 train_time:81156ms step_avg:95.59ms
step:850/1750 train_time:81255ms step_avg:95.59ms
step:851/1750 train_time:81353ms step_avg:95.60ms
step:852/1750 train_time:81450ms step_avg:95.60ms
step:853/1750 train_time:81547ms step_avg:95.60ms
step:854/1750 train_time:81645ms step_avg:95.60ms
step:855/1750 train_time:81744ms step_avg:95.61ms
step:856/1750 train_time:81842ms step_avg:95.61ms
step:857/1750 train_time:81941ms step_avg:95.61ms
step:858/1750 train_time:82038ms step_avg:95.62ms
step:859/1750 train_time:82136ms step_avg:95.62ms
step:860/1750 train_time:82235ms step_avg:95.62ms
step:861/1750 train_time:82334ms step_avg:95.63ms
step:862/1750 train_time:82431ms step_avg:95.63ms
step:863/1750 train_time:82529ms step_avg:95.63ms
step:864/1750 train_time:82627ms step_avg:95.63ms
step:865/1750 train_time:82725ms step_avg:95.64ms
step:866/1750 train_time:82822ms step_avg:95.64ms
step:867/1750 train_time:82920ms step_avg:95.64ms
step:868/1750 train_time:83019ms step_avg:95.64ms
step:869/1750 train_time:83117ms step_avg:95.65ms
step:870/1750 train_time:83215ms step_avg:95.65ms
step:871/1750 train_time:83314ms step_avg:95.65ms
step:872/1750 train_time:83412ms step_avg:95.66ms
step:873/1750 train_time:83510ms step_avg:95.66ms
step:874/1750 train_time:83609ms step_avg:95.66ms
step:875/1750 train_time:83707ms step_avg:95.67ms
step:875/1750 val_loss:3.5481 train_time:83794ms step_avg:95.76ms
step:876/1750 train_time:83816ms step_avg:95.68ms
step:877/1750 train_time:83914ms step_avg:95.68ms
step:878/1750 train_time:84017ms step_avg:95.69ms
step:879/1750 train_time:84116ms step_avg:95.69ms
step:880/1750 train_time:84213ms step_avg:95.70ms
step:881/1750 train_time:84310ms step_avg:95.70ms
step:882/1750 train_time:84407ms step_avg:95.70ms
step:883/1750 train_time:84505ms step_avg:95.70ms
step:884/1750 train_time:84602ms step_avg:95.70ms
step:885/1750 train_time:84700ms step_avg:95.71ms
step:886/1750 train_time:84798ms step_avg:95.71ms
step:887/1750 train_time:84898ms step_avg:95.71ms
step:888/1750 train_time:84998ms step_avg:95.72ms
step:889/1750 train_time:85098ms step_avg:95.72ms
step:890/1750 train_time:85197ms step_avg:95.73ms
step:891/1750 train_time:85295ms step_avg:95.73ms
step:892/1750 train_time:85393ms step_avg:95.73ms
step:893/1750 train_time:85490ms step_avg:95.73ms
step:894/1750 train_time:85587ms step_avg:95.74ms
step:895/1750 train_time:85685ms step_avg:95.74ms
step:896/1750 train_time:85782ms step_avg:95.74ms
step:897/1750 train_time:85881ms step_avg:95.74ms
step:898/1750 train_time:85980ms step_avg:95.75ms
step:899/1750 train_time:86079ms step_avg:95.75ms
step:900/1750 train_time:86178ms step_avg:95.75ms
step:901/1750 train_time:86277ms step_avg:95.76ms
step:902/1750 train_time:86376ms step_avg:95.76ms
step:903/1750 train_time:86474ms step_avg:95.76ms
step:904/1750 train_time:86572ms step_avg:95.77ms
step:905/1750 train_time:86670ms step_avg:95.77ms
step:906/1750 train_time:86767ms step_avg:95.77ms
step:907/1750 train_time:86866ms step_avg:95.77ms
step:908/1750 train_time:86966ms step_avg:95.78ms
step:909/1750 train_time:87066ms step_avg:95.78ms
step:910/1750 train_time:87166ms step_avg:95.79ms
step:911/1750 train_time:87266ms step_avg:95.79ms
step:912/1750 train_time:87366ms step_avg:95.80ms
step:913/1750 train_time:87466ms step_avg:95.80ms
step:914/1750 train_time:87566ms step_avg:95.81ms
step:915/1750 train_time:87665ms step_avg:95.81ms
step:916/1750 train_time:87764ms step_avg:95.81ms
step:917/1750 train_time:87863ms step_avg:95.82ms
step:918/1750 train_time:87963ms step_avg:95.82ms
step:919/1750 train_time:88062ms step_avg:95.82ms
step:920/1750 train_time:88162ms step_avg:95.83ms
step:921/1750 train_time:88262ms step_avg:95.83ms
step:922/1750 train_time:88362ms step_avg:95.84ms
step:923/1750 train_time:88462ms step_avg:95.84ms
step:924/1750 train_time:88562ms step_avg:95.85ms
step:925/1750 train_time:88661ms step_avg:95.85ms
step:926/1750 train_time:88761ms step_avg:95.85ms
step:927/1750 train_time:88861ms step_avg:95.86ms
step:928/1750 train_time:88960ms step_avg:95.86ms
step:929/1750 train_time:89059ms step_avg:95.87ms
step:930/1750 train_time:89158ms step_avg:95.87ms
step:931/1750 train_time:89257ms step_avg:95.87ms
step:932/1750 train_time:89357ms step_avg:95.88ms
step:933/1750 train_time:89457ms step_avg:95.88ms
step:934/1750 train_time:89556ms step_avg:95.88ms
step:935/1750 train_time:89655ms step_avg:95.89ms
step:936/1750 train_time:89754ms step_avg:95.89ms
step:937/1750 train_time:89855ms step_avg:95.90ms
step:938/1750 train_time:89954ms step_avg:95.90ms
step:939/1750 train_time:90054ms step_avg:95.90ms
step:940/1750 train_time:90153ms step_avg:95.91ms
step:941/1750 train_time:90253ms step_avg:95.91ms
step:942/1750 train_time:90351ms step_avg:95.91ms
step:943/1750 train_time:90452ms step_avg:95.92ms
step:944/1750 train_time:90551ms step_avg:95.92ms
step:945/1750 train_time:90652ms step_avg:95.93ms
step:946/1750 train_time:90751ms step_avg:95.93ms
step:947/1750 train_time:90851ms step_avg:95.94ms
step:948/1750 train_time:90950ms step_avg:95.94ms
step:949/1750 train_time:91050ms step_avg:95.94ms
step:950/1750 train_time:91151ms step_avg:95.95ms
step:951/1750 train_time:91250ms step_avg:95.95ms
step:952/1750 train_time:91350ms step_avg:95.96ms
step:953/1750 train_time:91451ms step_avg:95.96ms
step:954/1750 train_time:91550ms step_avg:95.96ms
step:955/1750 train_time:91651ms step_avg:95.97ms
step:956/1750 train_time:91750ms step_avg:95.97ms
step:957/1750 train_time:91849ms step_avg:95.98ms
step:958/1750 train_time:91949ms step_avg:95.98ms
step:959/1750 train_time:92048ms step_avg:95.98ms
step:960/1750 train_time:92148ms step_avg:95.99ms
step:961/1750 train_time:92247ms step_avg:95.99ms
step:962/1750 train_time:92347ms step_avg:95.99ms
step:963/1750 train_time:92446ms step_avg:96.00ms
step:964/1750 train_time:92547ms step_avg:96.00ms
step:965/1750 train_time:92647ms step_avg:96.01ms
step:966/1750 train_time:92746ms step_avg:96.01ms
step:967/1750 train_time:92846ms step_avg:96.01ms
step:968/1750 train_time:92946ms step_avg:96.02ms
step:969/1750 train_time:93046ms step_avg:96.02ms
step:970/1750 train_time:93146ms step_avg:96.03ms
step:971/1750 train_time:93246ms step_avg:96.03ms
step:972/1750 train_time:93345ms step_avg:96.03ms
step:973/1750 train_time:93445ms step_avg:96.04ms
step:974/1750 train_time:93544ms step_avg:96.04ms
step:975/1750 train_time:93644ms step_avg:96.04ms
step:976/1750 train_time:93744ms step_avg:96.05ms
step:977/1750 train_time:93843ms step_avg:96.05ms
step:978/1750 train_time:93943ms step_avg:96.06ms
step:979/1750 train_time:94042ms step_avg:96.06ms
step:980/1750 train_time:94142ms step_avg:96.06ms
step:981/1750 train_time:94242ms step_avg:96.07ms
step:982/1750 train_time:94341ms step_avg:96.07ms
step:983/1750 train_time:94442ms step_avg:96.07ms
step:984/1750 train_time:94541ms step_avg:96.08ms
step:985/1750 train_time:94640ms step_avg:96.08ms
step:986/1750 train_time:94740ms step_avg:96.09ms
step:987/1750 train_time:94841ms step_avg:96.09ms
step:988/1750 train_time:94941ms step_avg:96.09ms
step:989/1750 train_time:95040ms step_avg:96.10ms
step:990/1750 train_time:95141ms step_avg:96.10ms
step:991/1750 train_time:95240ms step_avg:96.11ms
step:992/1750 train_time:95340ms step_avg:96.11ms
step:993/1750 train_time:95441ms step_avg:96.11ms
step:994/1750 train_time:95540ms step_avg:96.12ms
step:995/1750 train_time:95639ms step_avg:96.12ms
step:996/1750 train_time:95740ms step_avg:96.12ms
step:997/1750 train_time:95840ms step_avg:96.13ms
step:998/1750 train_time:95939ms step_avg:96.13ms
step:999/1750 train_time:96040ms step_avg:96.14ms
step:1000/1750 train_time:96139ms step_avg:96.14ms
step:1000/1750 val_loss:3.5074 train_time:96228ms step_avg:96.23ms
step:1001/1750 train_time:96250ms step_avg:96.15ms
step:1002/1750 train_time:96351ms step_avg:96.16ms
step:1003/1750 train_time:96455ms step_avg:96.17ms
step:1004/1750 train_time:96555ms step_avg:96.17ms
step:1005/1750 train_time:96655ms step_avg:96.17ms
step:1006/1750 train_time:96754ms step_avg:96.18ms
step:1007/1750 train_time:96853ms step_avg:96.18ms
step:1008/1750 train_time:96951ms step_avg:96.18ms
step:1009/1750 train_time:97049ms step_avg:96.18ms
step:1010/1750 train_time:97147ms step_avg:96.19ms
step:1011/1750 train_time:97248ms step_avg:96.19ms
step:1012/1750 train_time:97349ms step_avg:96.19ms
step:1013/1750 train_time:97450ms step_avg:96.20ms
step:1014/1750 train_time:97551ms step_avg:96.20ms
step:1015/1750 train_time:97651ms step_avg:96.21ms
step:1016/1750 train_time:97750ms step_avg:96.21ms
step:1017/1750 train_time:97849ms step_avg:96.21ms
step:1018/1750 train_time:97947ms step_avg:96.22ms
step:1019/1750 train_time:98046ms step_avg:96.22ms
step:1020/1750 train_time:98145ms step_avg:96.22ms
step:1021/1750 train_time:98244ms step_avg:96.22ms
step:1022/1750 train_time:98344ms step_avg:96.23ms
step:1023/1750 train_time:98445ms step_avg:96.23ms
step:1024/1750 train_time:98547ms step_avg:96.24ms
step:1025/1750 train_time:98646ms step_avg:96.24ms
step:1026/1750 train_time:98746ms step_avg:96.24ms
step:1027/1750 train_time:98845ms step_avg:96.25ms
step:1028/1750 train_time:98945ms step_avg:96.25ms
step:1029/1750 train_time:99044ms step_avg:96.25ms
step:1030/1750 train_time:99143ms step_avg:96.26ms
step:1031/1750 train_time:99243ms step_avg:96.26ms
step:1032/1750 train_time:99342ms step_avg:96.26ms
step:1033/1750 train_time:99442ms step_avg:96.27ms
step:1034/1750 train_time:99541ms step_avg:96.27ms
step:1035/1750 train_time:99641ms step_avg:96.27ms
step:1036/1750 train_time:99741ms step_avg:96.28ms
step:1037/1750 train_time:99841ms step_avg:96.28ms
step:1038/1750 train_time:99941ms step_avg:96.28ms
step:1039/1750 train_time:100039ms step_avg:96.28ms
step:1040/1750 train_time:100140ms step_avg:96.29ms
step:1041/1750 train_time:100240ms step_avg:96.29ms
step:1042/1750 train_time:100341ms step_avg:96.30ms
step:1043/1750 train_time:100440ms step_avg:96.30ms
step:1044/1750 train_time:100540ms step_avg:96.30ms
step:1045/1750 train_time:100641ms step_avg:96.31ms
step:1046/1750 train_time:100742ms step_avg:96.31ms
step:1047/1750 train_time:100842ms step_avg:96.31ms
step:1048/1750 train_time:100942ms step_avg:96.32ms
step:1049/1750 train_time:101041ms step_avg:96.32ms
step:1050/1750 train_time:101142ms step_avg:96.33ms
step:1051/1750 train_time:101500ms step_avg:96.57ms
step:1052/1750 train_time:101598ms step_avg:96.58ms
step:1053/1750 train_time:101697ms step_avg:96.58ms
step:1054/1750 train_time:101795ms step_avg:96.58ms
step:1055/1750 train_time:101894ms step_avg:96.58ms
step:1056/1750 train_time:101992ms step_avg:96.58ms
step:1057/1750 train_time:102090ms step_avg:96.58ms
step:1058/1750 train_time:102189ms step_avg:96.59ms
step:1059/1750 train_time:102286ms step_avg:96.59ms
step:1060/1750 train_time:102386ms step_avg:96.59ms
step:1061/1750 train_time:102490ms step_avg:96.60ms
step:1062/1750 train_time:102591ms step_avg:96.60ms
step:1063/1750 train_time:102690ms step_avg:96.60ms
step:1064/1750 train_time:102791ms step_avg:96.61ms
step:1065/1750 train_time:102889ms step_avg:96.61ms
step:1066/1750 train_time:102988ms step_avg:96.61ms
step:1067/1750 train_time:103088ms step_avg:96.61ms
step:1068/1750 train_time:103186ms step_avg:96.62ms
step:1069/1750 train_time:103284ms step_avg:96.62ms
step:1070/1750 train_time:103386ms step_avg:96.62ms
step:1071/1750 train_time:103487ms step_avg:96.63ms
step:1072/1750 train_time:103587ms step_avg:96.63ms
step:1073/1750 train_time:103687ms step_avg:96.63ms
step:1074/1750 train_time:103786ms step_avg:96.63ms
step:1075/1750 train_time:103885ms step_avg:96.64ms
step:1076/1750 train_time:103984ms step_avg:96.64ms
step:1077/1750 train_time:104085ms step_avg:96.64ms
step:1078/1750 train_time:104183ms step_avg:96.64ms
step:1079/1750 train_time:104282ms step_avg:96.65ms
step:1080/1750 train_time:104380ms step_avg:96.65ms
step:1081/1750 train_time:104481ms step_avg:96.65ms
step:1082/1750 train_time:104581ms step_avg:96.66ms
step:1083/1750 train_time:104681ms step_avg:96.66ms
step:1084/1750 train_time:104782ms step_avg:96.66ms
step:1085/1750 train_time:104882ms step_avg:96.67ms
step:1086/1750 train_time:104982ms step_avg:96.67ms
step:1087/1750 train_time:105455ms step_avg:97.02ms
step:1088/1750 train_time:105517ms step_avg:96.98ms
step:1089/1750 train_time:105613ms step_avg:96.98ms
step:1090/1750 train_time:105711ms step_avg:96.98ms
step:1091/1750 train_time:105810ms step_avg:96.98ms
step:1092/1750 train_time:105908ms step_avg:96.99ms
step:1093/1750 train_time:106006ms step_avg:96.99ms
step:1094/1750 train_time:106105ms step_avg:96.99ms
step:1095/1750 train_time:106203ms step_avg:96.99ms
step:1096/1750 train_time:106302ms step_avg:96.99ms
step:1097/1750 train_time:106404ms step_avg:97.00ms
step:1098/1750 train_time:106509ms step_avg:97.00ms
step:1099/1750 train_time:106609ms step_avg:97.01ms
step:1100/1750 train_time:106708ms step_avg:97.01ms
step:1101/1750 train_time:106808ms step_avg:97.01ms
step:1102/1750 train_time:106907ms step_avg:97.01ms
step:1103/1750 train_time:107005ms step_avg:97.01ms
step:1104/1750 train_time:107104ms step_avg:97.01ms
step:1105/1750 train_time:107202ms step_avg:97.02ms
step:1106/1750 train_time:107302ms step_avg:97.02ms
step:1107/1750 train_time:107404ms step_avg:97.02ms
step:1108/1750 train_time:107507ms step_avg:97.03ms
step:1109/1750 train_time:107607ms step_avg:97.03ms
step:1110/1750 train_time:107708ms step_avg:97.03ms
step:1111/1750 train_time:107808ms step_avg:97.04ms
step:1112/1750 train_time:107908ms step_avg:97.04ms
step:1113/1750 train_time:108007ms step_avg:97.04ms
step:1114/1750 train_time:108106ms step_avg:97.04ms
step:1115/1750 train_time:108205ms step_avg:97.04ms
step:1116/1750 train_time:108304ms step_avg:97.05ms
step:1117/1750 train_time:108404ms step_avg:97.05ms
step:1118/1750 train_time:108504ms step_avg:97.05ms
step:1119/1750 train_time:108604ms step_avg:97.05ms
step:1120/1750 train_time:108705ms step_avg:97.06ms
step:1121/1750 train_time:108804ms step_avg:97.06ms
step:1122/1750 train_time:108904ms step_avg:97.06ms
step:1123/1750 train_time:109003ms step_avg:97.06ms
step:1124/1750 train_time:109104ms step_avg:97.07ms
step:1125/1750 train_time:109205ms step_avg:97.07ms
step:1125/1750 val_loss:3.4553 train_time:109293ms step_avg:97.15ms
step:1126/1750 train_time:109315ms step_avg:97.08ms
step:1127/1750 train_time:109414ms step_avg:97.08ms
step:1128/1750 train_time:109518ms step_avg:97.09ms
step:1129/1750 train_time:109619ms step_avg:97.09ms
step:1130/1750 train_time:109719ms step_avg:97.10ms
step:1131/1750 train_time:109818ms step_avg:97.10ms
step:1132/1750 train_time:109917ms step_avg:97.10ms
step:1133/1750 train_time:110016ms step_avg:97.10ms
step:1134/1750 train_time:110115ms step_avg:97.10ms
step:1135/1750 train_time:110213ms step_avg:97.10ms
step:1136/1750 train_time:110314ms step_avg:97.11ms
step:1137/1750 train_time:110415ms step_avg:97.11ms
step:1138/1750 train_time:110517ms step_avg:97.11ms
step:1139/1750 train_time:110617ms step_avg:97.12ms
step:1140/1750 train_time:110974ms step_avg:97.35ms
step:1141/1750 train_time:111072ms step_avg:97.35ms
step:1142/1750 train_time:111171ms step_avg:97.35ms
step:1143/1750 train_time:111269ms step_avg:97.35ms
step:1144/1750 train_time:111368ms step_avg:97.35ms
step:1145/1750 train_time:111466ms step_avg:97.35ms
step:1146/1750 train_time:111564ms step_avg:97.35ms
step:1147/1750 train_time:111662ms step_avg:97.35ms
step:1148/1750 train_time:111761ms step_avg:97.35ms
step:1149/1750 train_time:111864ms step_avg:97.36ms
step:1150/1750 train_time:111967ms step_avg:97.36ms
step:1151/1750 train_time:112068ms step_avg:97.37ms
step:1152/1750 train_time:112168ms step_avg:97.37ms
step:1153/1750 train_time:112267ms step_avg:97.37ms
step:1154/1750 train_time:112367ms step_avg:97.37ms
step:1155/1750 train_time:112465ms step_avg:97.37ms
step:1156/1750 train_time:112563ms step_avg:97.37ms
step:1157/1750 train_time:112662ms step_avg:97.37ms
step:1158/1750 train_time:112760ms step_avg:97.38ms
step:1159/1750 train_time:112861ms step_avg:97.38ms
step:1160/1750 train_time:112961ms step_avg:97.38ms
step:1161/1750 train_time:113062ms step_avg:97.38ms
step:1162/1750 train_time:113162ms step_avg:97.39ms
step:1163/1750 train_time:113261ms step_avg:97.39ms
step:1164/1750 train_time:113361ms step_avg:97.39ms
step:1165/1750 train_time:113459ms step_avg:97.39ms
step:1166/1750 train_time:113558ms step_avg:97.39ms
step:1167/1750 train_time:113917ms step_avg:97.62ms
step:1168/1750 train_time:114016ms step_avg:97.62ms
step:1169/1750 train_time:114116ms step_avg:97.62ms
step:1170/1750 train_time:114215ms step_avg:97.62ms
step:1171/1750 train_time:114315ms step_avg:97.62ms
step:1172/1750 train_time:114415ms step_avg:97.62ms
step:1173/1750 train_time:114515ms step_avg:97.63ms
step:1174/1750 train_time:114615ms step_avg:97.63ms
step:1175/1750 train_time:114714ms step_avg:97.63ms
step:1176/1750 train_time:114822ms step_avg:97.64ms
step:1177/1750 train_time:114926ms step_avg:97.64ms
step:1178/1750 train_time:115303ms step_avg:97.88ms
step:1179/1750 train_time:115404ms step_avg:97.88ms
step:1180/1750 train_time:115503ms step_avg:97.88ms
step:1181/1750 train_time:115602ms step_avg:97.89ms
step:1182/1750 train_time:115703ms step_avg:97.89ms
step:1183/1750 train_time:115802ms step_avg:97.89ms
step:1184/1750 train_time:115902ms step_avg:97.89ms
step:1185/1750 train_time:116002ms step_avg:97.89ms
step:1186/1750 train_time:116102ms step_avg:97.89ms
step:1187/1750 train_time:116204ms step_avg:97.90ms
step:1188/1750 train_time:116309ms step_avg:97.90ms
step:1189/1750 train_time:116409ms step_avg:97.90ms
step:1190/1750 train_time:116509ms step_avg:97.91ms
step:1191/1750 train_time:116610ms step_avg:97.91ms
step:1192/1750 train_time:116711ms step_avg:97.91ms
step:1193/1750 train_time:116812ms step_avg:97.91ms
step:1194/1750 train_time:116912ms step_avg:97.92ms
step:1195/1750 train_time:117012ms step_avg:97.92ms
step:1196/1750 train_time:117112ms step_avg:97.92ms
step:1197/1750 train_time:117215ms step_avg:97.92ms
step:1198/1750 train_time:117316ms step_avg:97.93ms
step:1199/1750 train_time:117419ms step_avg:97.93ms
step:1200/1750 train_time:117521ms step_avg:97.93ms
step:1201/1750 train_time:117622ms step_avg:97.94ms
step:1202/1750 train_time:117724ms step_avg:97.94ms
step:1203/1750 train_time:117824ms step_avg:97.94ms
step:1204/1750 train_time:117924ms step_avg:97.94ms
step:1205/1750 train_time:118024ms step_avg:97.95ms
step:1206/1750 train_time:118124ms step_avg:97.95ms
step:1207/1750 train_time:118225ms step_avg:97.95ms
step:1208/1750 train_time:118326ms step_avg:97.95ms
step:1209/1750 train_time:118729ms step_avg:98.20ms
step:1210/1750 train_time:118827ms step_avg:98.20ms
step:1211/1750 train_time:118926ms step_avg:98.21ms
step:1212/1750 train_time:119025ms step_avg:98.21ms
step:1213/1750 train_time:119124ms step_avg:98.21ms
step:1214/1750 train_time:119223ms step_avg:98.21ms
step:1215/1750 train_time:119323ms step_avg:98.21ms
step:1216/1750 train_time:119424ms step_avg:98.21ms
step:1217/1750 train_time:119524ms step_avg:98.21ms
step:1218/1750 train_time:119629ms step_avg:98.22ms
step:1219/1750 train_time:119734ms step_avg:98.22ms
step:1220/1750 train_time:119835ms step_avg:98.23ms
step:1221/1750 train_time:120237ms step_avg:98.47ms
step:1222/1750 train_time:120335ms step_avg:98.47ms
step:1223/1750 train_time:120436ms step_avg:98.48ms
step:1224/1750 train_time:120535ms step_avg:98.48ms
step:1225/1750 train_time:120634ms step_avg:98.48ms
step:1226/1750 train_time:120733ms step_avg:98.48ms
step:1227/1750 train_time:120833ms step_avg:98.48ms
step:1228/1750 train_time:120932ms step_avg:98.48ms
step:1229/1750 train_time:121032ms step_avg:98.48ms
step:1230/1750 train_time:121135ms step_avg:98.48ms
step:1231/1750 train_time:121240ms step_avg:98.49ms
step:1232/1750 train_time:121342ms step_avg:98.49ms
step:1233/1750 train_time:121441ms step_avg:98.49ms
step:1234/1750 train_time:121543ms step_avg:98.49ms
step:1235/1750 train_time:121642ms step_avg:98.50ms
step:1236/1750 train_time:121742ms step_avg:98.50ms
step:1237/1750 train_time:121843ms step_avg:98.50ms
step:1238/1750 train_time:121943ms step_avg:98.50ms
step:1239/1750 train_time:122044ms step_avg:98.50ms
step:1240/1750 train_time:122145ms step_avg:98.50ms
step:1241/1750 train_time:122246ms step_avg:98.51ms
step:1242/1750 train_time:122348ms step_avg:98.51ms
step:1243/1750 train_time:122449ms step_avg:98.51ms
step:1244/1750 train_time:122550ms step_avg:98.51ms
step:1245/1750 train_time:122651ms step_avg:98.52ms
step:1246/1750 train_time:122752ms step_avg:98.52ms
step:1247/1750 train_time:122852ms step_avg:98.52ms
step:1248/1750 train_time:122954ms step_avg:98.52ms
step:1249/1750 train_time:123055ms step_avg:98.52ms
step:1250/1750 train_time:123157ms step_avg:98.53ms
step:1250/1750 val_loss:3.4108 train_time:123248ms step_avg:98.60ms
step:1251/1750 train_time:123270ms step_avg:98.54ms
step:1252/1750 train_time:123371ms step_avg:98.54ms
step:1253/1750 train_time:123474ms step_avg:98.54ms
step:1254/1750 train_time:123575ms step_avg:98.54ms
step:1255/1750 train_time:123674ms step_avg:98.55ms
step:1256/1750 train_time:123774ms step_avg:98.55ms
step:1257/1750 train_time:123874ms step_avg:98.55ms
step:1258/1750 train_time:123974ms step_avg:98.55ms
step:1259/1750 train_time:124073ms step_avg:98.55ms
step:1260/1750 train_time:124173ms step_avg:98.55ms
step:1261/1750 train_time:124276ms step_avg:98.55ms
step:1262/1750 train_time:124378ms step_avg:98.56ms
step:1263/1750 train_time:124481ms step_avg:98.56ms
step:1264/1750 train_time:124580ms step_avg:98.56ms
step:1265/1750 train_time:124681ms step_avg:98.56ms
step:1266/1750 train_time:124781ms step_avg:98.56ms
step:1267/1750 train_time:124882ms step_avg:98.57ms
step:1268/1750 train_time:124983ms step_avg:98.57ms
step:1269/1750 train_time:125083ms step_avg:98.57ms
step:1270/1750 train_time:125183ms step_avg:98.57ms
step:1271/1750 train_time:125285ms step_avg:98.57ms
step:1272/1750 train_time:125386ms step_avg:98.57ms
step:1273/1750 train_time:125487ms step_avg:98.58ms
step:1274/1750 train_time:125587ms step_avg:98.58ms
step:1275/1750 train_time:125688ms step_avg:98.58ms
step:1276/1750 train_time:125790ms step_avg:98.58ms
step:1277/1750 train_time:125891ms step_avg:98.58ms
step:1278/1750 train_time:125994ms step_avg:98.59ms
step:1279/1750 train_time:126095ms step_avg:98.59ms
step:1280/1750 train_time:126195ms step_avg:98.59ms
step:1281/1750 train_time:126296ms step_avg:98.59ms
step:1282/1750 train_time:126396ms step_avg:98.59ms
step:1283/1750 train_time:126497ms step_avg:98.59ms
step:1284/1750 train_time:126598ms step_avg:98.60ms
step:1285/1750 train_time:126699ms step_avg:98.60ms
step:1286/1750 train_time:126800ms step_avg:98.60ms
step:1287/1750 train_time:126901ms step_avg:98.60ms
step:1288/1750 train_time:127002ms step_avg:98.60ms
step:1289/1750 train_time:127103ms step_avg:98.61ms
step:1290/1750 train_time:127204ms step_avg:98.61ms
step:1291/1750 train_time:127305ms step_avg:98.61ms
step:1292/1750 train_time:127405ms step_avg:98.61ms
step:1293/1750 train_time:127506ms step_avg:98.61ms
step:1294/1750 train_time:127608ms step_avg:98.62ms
step:1295/1750 train_time:127710ms step_avg:98.62ms
step:1296/1750 train_time:127811ms step_avg:98.62ms
step:1297/1750 train_time:127912ms step_avg:98.62ms
step:1298/1750 train_time:128013ms step_avg:98.62ms
step:1299/1750 train_time:128114ms step_avg:98.63ms
step:1300/1750 train_time:128215ms step_avg:98.63ms
step:1301/1750 train_time:128316ms step_avg:98.63ms
step:1302/1750 train_time:128417ms step_avg:98.63ms
step:1303/1750 train_time:128518ms step_avg:98.63ms
step:1304/1750 train_time:128620ms step_avg:98.63ms
step:1305/1750 train_time:128721ms step_avg:98.64ms
step:1306/1750 train_time:128824ms step_avg:98.64ms
step:1307/1750 train_time:128925ms step_avg:98.64ms
step:1308/1750 train_time:129026ms step_avg:98.64ms
step:1309/1750 train_time:129127ms step_avg:98.65ms
step:1310/1750 train_time:129228ms step_avg:98.65ms
step:1311/1750 train_time:129330ms step_avg:98.65ms
step:1312/1750 train_time:129431ms step_avg:98.65ms
step:1313/1750 train_time:129535ms step_avg:98.66ms
step:1314/1750 train_time:129636ms step_avg:98.66ms
step:1315/1750 train_time:129736ms step_avg:98.66ms
step:1316/1750 train_time:129837ms step_avg:98.66ms
step:1317/1750 train_time:129938ms step_avg:98.66ms
step:1318/1750 train_time:130039ms step_avg:98.66ms
step:1319/1750 train_time:130142ms step_avg:98.67ms
step:1320/1750 train_time:130245ms step_avg:98.67ms
step:1321/1750 train_time:130346ms step_avg:98.67ms
step:1322/1750 train_time:130446ms step_avg:98.67ms
step:1323/1750 train_time:130546ms step_avg:98.67ms
step:1324/1750 train_time:130647ms step_avg:98.68ms
step:1325/1750 train_time:130748ms step_avg:98.68ms
step:1326/1750 train_time:130849ms step_avg:98.68ms
step:1327/1750 train_time:130952ms step_avg:98.68ms
step:1328/1750 train_time:131053ms step_avg:98.68ms
step:1329/1750 train_time:131155ms step_avg:98.69ms
step:1330/1750 train_time:131255ms step_avg:98.69ms
step:1331/1750 train_time:131355ms step_avg:98.69ms
step:1332/1750 train_time:131456ms step_avg:98.69ms
step:1333/1750 train_time:131558ms step_avg:98.69ms
step:1334/1750 train_time:131659ms step_avg:98.69ms
step:1335/1750 train_time:131761ms step_avg:98.70ms
step:1336/1750 train_time:131862ms step_avg:98.70ms
step:1337/1750 train_time:131964ms step_avg:98.70ms
step:1338/1750 train_time:132064ms step_avg:98.70ms
step:1339/1750 train_time:132165ms step_avg:98.70ms
step:1340/1750 train_time:132267ms step_avg:98.71ms
step:1341/1750 train_time:132367ms step_avg:98.71ms
step:1342/1750 train_time:132468ms step_avg:98.71ms
step:1343/1750 train_time:132568ms step_avg:98.71ms
step:1344/1750 train_time:132670ms step_avg:98.71ms
step:1345/1750 train_time:132772ms step_avg:98.72ms
step:1346/1750 train_time:132874ms step_avg:98.72ms
step:1347/1750 train_time:132975ms step_avg:98.72ms
step:1348/1750 train_time:133076ms step_avg:98.72ms
step:1349/1750 train_time:133176ms step_avg:98.72ms
step:1350/1750 train_time:133277ms step_avg:98.72ms
step:1351/1750 train_time:133378ms step_avg:98.73ms
step:1352/1750 train_time:133479ms step_avg:98.73ms
step:1353/1750 train_time:133580ms step_avg:98.73ms
step:1354/1750 train_time:133681ms step_avg:98.73ms
step:1355/1750 train_time:133783ms step_avg:98.73ms
step:1356/1750 train_time:133884ms step_avg:98.73ms
step:1357/1750 train_time:133984ms step_avg:98.74ms
step:1358/1750 train_time:134085ms step_avg:98.74ms
step:1359/1750 train_time:134185ms step_avg:98.74ms
step:1360/1750 train_time:134286ms step_avg:98.74ms
step:1361/1750 train_time:134385ms step_avg:98.74ms
step:1362/1750 train_time:134486ms step_avg:98.74ms
step:1363/1750 train_time:134588ms step_avg:98.74ms
step:1364/1750 train_time:134688ms step_avg:98.75ms
step:1365/1750 train_time:134789ms step_avg:98.75ms
step:1366/1750 train_time:134891ms step_avg:98.75ms
step:1367/1750 train_time:134992ms step_avg:98.75ms
step:1368/1750 train_time:135094ms step_avg:98.75ms
step:1369/1750 train_time:135195ms step_avg:98.75ms
step:1370/1750 train_time:135296ms step_avg:98.76ms
step:1371/1750 train_time:135396ms step_avg:98.76ms
step:1372/1750 train_time:135496ms step_avg:98.76ms
step:1373/1750 train_time:135598ms step_avg:98.76ms
step:1374/1750 train_time:135699ms step_avg:98.76ms
step:1375/1750 train_time:135802ms step_avg:98.77ms
step:1375/1750 val_loss:3.3715 train_time:135892ms step_avg:98.83ms
step:1376/1750 train_time:135914ms step_avg:98.77ms
step:1377/1750 train_time:136012ms step_avg:98.77ms
step:1378/1750 train_time:136114ms step_avg:98.78ms
step:1379/1750 train_time:136213ms step_avg:98.78ms
step:1380/1750 train_time:136316ms step_avg:98.78ms
step:1381/1750 train_time:136416ms step_avg:98.78ms
step:1382/1750 train_time:136515ms step_avg:98.78ms
step:1383/1750 train_time:136615ms step_avg:98.78ms
step:1384/1750 train_time:136716ms step_avg:98.78ms
step:1385/1750 train_time:136818ms step_avg:98.79ms
step:1386/1750 train_time:136923ms step_avg:98.79ms
step:1387/1750 train_time:137025ms step_avg:98.79ms
step:1388/1750 train_time:137127ms step_avg:98.79ms
step:1389/1750 train_time:137229ms step_avg:98.80ms
step:1390/1750 train_time:137329ms step_avg:98.80ms
step:1391/1750 train_time:137430ms step_avg:98.80ms
step:1392/1750 train_time:137531ms step_avg:98.80ms
step:1393/1750 train_time:137631ms step_avg:98.80ms
step:1394/1750 train_time:137733ms step_avg:98.80ms
step:1395/1750 train_time:137834ms step_avg:98.81ms
step:1396/1750 train_time:137935ms step_avg:98.81ms
step:1397/1750 train_time:138038ms step_avg:98.81ms
step:1398/1750 train_time:138140ms step_avg:98.81ms
step:1399/1750 train_time:138242ms step_avg:98.81ms
step:1400/1750 train_time:138342ms step_avg:98.82ms
step:1401/1750 train_time:138443ms step_avg:98.82ms
step:1402/1750 train_time:138544ms step_avg:98.82ms
step:1403/1750 train_time:138646ms step_avg:98.82ms
step:1404/1750 train_time:138747ms step_avg:98.82ms
step:1405/1750 train_time:138848ms step_avg:98.82ms
step:1406/1750 train_time:138949ms step_avg:98.83ms
step:1407/1750 train_time:139051ms step_avg:98.83ms
step:1408/1750 train_time:139153ms step_avg:98.83ms
step:1409/1750 train_time:139254ms step_avg:98.83ms
step:1410/1750 train_time:139353ms step_avg:98.83ms
step:1411/1750 train_time:139455ms step_avg:98.83ms
step:1412/1750 train_time:139559ms step_avg:98.84ms
step:1413/1750 train_time:139659ms step_avg:98.84ms
step:1414/1750 train_time:139761ms step_avg:98.84ms
step:1415/1750 train_time:139862ms step_avg:98.84ms
step:1416/1750 train_time:139962ms step_avg:98.84ms
step:1417/1750 train_time:140064ms step_avg:98.85ms
step:1418/1750 train_time:140164ms step_avg:98.85ms
step:1419/1750 train_time:140266ms step_avg:98.85ms
step:1420/1750 train_time:140368ms step_avg:98.85ms
step:1421/1750 train_time:140470ms step_avg:98.85ms
step:1422/1750 train_time:140571ms step_avg:98.85ms
step:1423/1750 train_time:140672ms step_avg:98.86ms
step:1424/1750 train_time:140773ms step_avg:98.86ms
step:1425/1750 train_time:140873ms step_avg:98.86ms
step:1426/1750 train_time:140974ms step_avg:98.86ms
step:1427/1750 train_time:141075ms step_avg:98.86ms
step:1428/1750 train_time:141178ms step_avg:98.86ms
step:1429/1750 train_time:141281ms step_avg:98.87ms
step:1430/1750 train_time:141383ms step_avg:98.87ms
step:1431/1750 train_time:141486ms step_avg:98.87ms
step:1432/1750 train_time:141587ms step_avg:98.87ms
step:1433/1750 train_time:141689ms step_avg:98.88ms
step:1434/1750 train_time:141789ms step_avg:98.88ms
step:1435/1750 train_time:141891ms step_avg:98.88ms
step:1436/1750 train_time:141995ms step_avg:98.88ms
step:1437/1750 train_time:142097ms step_avg:98.88ms
step:1438/1750 train_time:142198ms step_avg:98.89ms
step:1439/1750 train_time:142302ms step_avg:98.89ms
step:1440/1750 train_time:142406ms step_avg:98.89ms
step:1441/1750 train_time:142508ms step_avg:98.90ms
step:1442/1750 train_time:142609ms step_avg:98.90ms
step:1443/1750 train_time:142709ms step_avg:98.90ms
step:1444/1750 train_time:142811ms step_avg:98.90ms
step:1445/1750 train_time:142912ms step_avg:98.90ms
step:1446/1750 train_time:143013ms step_avg:98.90ms
step:1447/1750 train_time:143114ms step_avg:98.90ms
step:1448/1750 train_time:143218ms step_avg:98.91ms
step:1449/1750 train_time:143320ms step_avg:98.91ms
step:1450/1750 train_time:143422ms step_avg:98.91ms
step:1451/1750 train_time:143524ms step_avg:98.91ms
step:1452/1750 train_time:143626ms step_avg:98.92ms
step:1453/1750 train_time:143728ms step_avg:98.92ms
step:1454/1750 train_time:143832ms step_avg:98.92ms
step:1455/1750 train_time:143932ms step_avg:98.92ms
step:1456/1750 train_time:144033ms step_avg:98.92ms
step:1457/1750 train_time:144135ms step_avg:98.93ms
step:1458/1750 train_time:144238ms step_avg:98.93ms
step:1459/1750 train_time:144341ms step_avg:98.93ms
step:1460/1750 train_time:144443ms step_avg:98.93ms
step:1461/1750 train_time:144546ms step_avg:98.94ms
step:1462/1750 train_time:144648ms step_avg:98.94ms
step:1463/1750 train_time:144749ms step_avg:98.94ms
step:1464/1750 train_time:144850ms step_avg:98.94ms
step:1465/1750 train_time:144952ms step_avg:98.94ms
step:1466/1750 train_time:145053ms step_avg:98.94ms
step:1467/1750 train_time:145154ms step_avg:98.95ms
step:1468/1750 train_time:145257ms step_avg:98.95ms
step:1469/1750 train_time:145359ms step_avg:98.95ms
step:1470/1750 train_time:145462ms step_avg:98.95ms
step:1471/1750 train_time:145564ms step_avg:98.96ms
step:1472/1750 train_time:145665ms step_avg:98.96ms
step:1473/1750 train_time:145767ms step_avg:98.96ms
step:1474/1750 train_time:145868ms step_avg:98.96ms
step:1475/1750 train_time:145969ms step_avg:98.96ms
step:1476/1750 train_time:146072ms step_avg:98.96ms
step:1477/1750 train_time:146174ms step_avg:98.97ms
step:1478/1750 train_time:146277ms step_avg:98.97ms
step:1479/1750 train_time:146378ms step_avg:98.97ms
step:1480/1750 train_time:146481ms step_avg:98.97ms
step:1481/1750 train_time:146583ms step_avg:98.98ms
step:1482/1750 train_time:146687ms step_avg:98.98ms
step:1483/1750 train_time:146788ms step_avg:98.98ms
step:1484/1750 train_time:146891ms step_avg:98.98ms
step:1485/1750 train_time:146993ms step_avg:98.99ms
step:1486/1750 train_time:147094ms step_avg:98.99ms
step:1487/1750 train_time:147197ms step_avg:98.99ms
step:1488/1750 train_time:147300ms step_avg:98.99ms
step:1489/1750 train_time:147402ms step_avg:98.99ms
step:1490/1750 train_time:147503ms step_avg:99.00ms
step:1491/1750 train_time:147605ms step_avg:99.00ms
step:1492/1750 train_time:147706ms step_avg:99.00ms
step:1493/1750 train_time:147807ms step_avg:99.00ms
step:1494/1750 train_time:147909ms step_avg:99.00ms
step:1495/1750 train_time:148010ms step_avg:99.00ms
step:1496/1750 train_time:148112ms step_avg:99.01ms
step:1497/1750 train_time:148212ms step_avg:99.01ms
step:1498/1750 train_time:148314ms step_avg:99.01ms
step:1499/1750 train_time:148414ms step_avg:99.01ms
step:1500/1750 train_time:148517ms step_avg:99.01ms
step:1500/1750 val_loss:3.3352 train_time:148608ms step_avg:99.07ms
step:1501/1750 train_time:148630ms step_avg:99.02ms
step:1502/1750 train_time:148732ms step_avg:99.02ms
step:1503/1750 train_time:148834ms step_avg:99.02ms
step:1504/1750 train_time:148936ms step_avg:99.03ms
step:1505/1750 train_time:149037ms step_avg:99.03ms
step:1506/1750 train_time:149138ms step_avg:99.03ms
step:1507/1750 train_time:149240ms step_avg:99.03ms
step:1508/1750 train_time:149341ms step_avg:99.03ms
step:1509/1750 train_time:149443ms step_avg:99.03ms
step:1510/1750 train_time:149544ms step_avg:99.04ms
step:1511/1750 train_time:149650ms step_avg:99.04ms
step:1512/1750 train_time:149753ms step_avg:99.04ms
step:1513/1750 train_time:149855ms step_avg:99.04ms
step:1514/1750 train_time:149956ms step_avg:99.05ms
step:1515/1750 train_time:150061ms step_avg:99.05ms
step:1516/1750 train_time:150163ms step_avg:99.05ms
step:1517/1750 train_time:150264ms step_avg:99.05ms
step:1518/1750 train_time:150364ms step_avg:99.05ms
step:1519/1750 train_time:150466ms step_avg:99.06ms
step:1520/1750 train_time:150568ms step_avg:99.06ms
step:1521/1750 train_time:150670ms step_avg:99.06ms
step:1522/1750 train_time:150772ms step_avg:99.06ms
step:1523/1750 train_time:150874ms step_avg:99.06ms
step:1524/1750 train_time:150979ms step_avg:99.07ms
step:1525/1750 train_time:151082ms step_avg:99.07ms
step:1526/1750 train_time:151183ms step_avg:99.07ms
step:1527/1750 train_time:151285ms step_avg:99.07ms
step:1528/1750 train_time:151388ms step_avg:99.08ms
step:1529/1750 train_time:151489ms step_avg:99.08ms
step:1530/1750 train_time:151593ms step_avg:99.08ms
step:1531/1750 train_time:151694ms step_avg:99.08ms
step:1532/1750 train_time:151796ms step_avg:99.08ms
step:1533/1750 train_time:151898ms step_avg:99.09ms
step:1534/1750 train_time:152000ms step_avg:99.09ms
step:1535/1750 train_time:152102ms step_avg:99.09ms
step:1536/1750 train_time:152203ms step_avg:99.09ms
step:1537/1750 train_time:152306ms step_avg:99.09ms
step:1538/1750 train_time:152406ms step_avg:99.09ms
step:1539/1750 train_time:152508ms step_avg:99.10ms
step:1540/1750 train_time:152611ms step_avg:99.10ms
step:1541/1750 train_time:152714ms step_avg:99.10ms
step:1542/1750 train_time:152818ms step_avg:99.10ms
step:1543/1750 train_time:152920ms step_avg:99.11ms
step:1544/1750 train_time:153022ms step_avg:99.11ms
step:1545/1750 train_time:153124ms step_avg:99.11ms
step:1546/1750 train_time:153225ms step_avg:99.11ms
step:1547/1750 train_time:153327ms step_avg:99.11ms
step:1548/1750 train_time:153428ms step_avg:99.11ms
step:1549/1750 train_time:153530ms step_avg:99.12ms
step:1550/1750 train_time:153631ms step_avg:99.12ms
step:1551/1750 train_time:153735ms step_avg:99.12ms
step:1552/1750 train_time:153837ms step_avg:99.12ms
step:1553/1750 train_time:153940ms step_avg:99.12ms
step:1554/1750 train_time:154040ms step_avg:99.12ms
step:1555/1750 train_time:154142ms step_avg:99.13ms
step:1556/1750 train_time:154243ms step_avg:99.13ms
step:1557/1750 train_time:154345ms step_avg:99.13ms
step:1558/1750 train_time:154447ms step_avg:99.13ms
step:1559/1750 train_time:154550ms step_avg:99.13ms
step:1560/1750 train_time:154652ms step_avg:99.14ms
step:1561/1750 train_time:154753ms step_avg:99.14ms
step:1562/1750 train_time:154855ms step_avg:99.14ms
step:1563/1750 train_time:154961ms step_avg:99.14ms
step:1564/1750 train_time:155062ms step_avg:99.14ms
step:1565/1750 train_time:155163ms step_avg:99.15ms
step:1566/1750 train_time:155265ms step_avg:99.15ms
step:1567/1750 train_time:155366ms step_avg:99.15ms
step:1568/1750 train_time:155468ms step_avg:99.15ms
step:1569/1750 train_time:155570ms step_avg:99.15ms
step:1570/1750 train_time:155675ms step_avg:99.16ms
step:1571/1750 train_time:155776ms step_avg:99.16ms
step:1572/1750 train_time:155877ms step_avg:99.16ms
step:1573/1750 train_time:155979ms step_avg:99.16ms
step:1574/1750 train_time:156081ms step_avg:99.16ms
step:1575/1750 train_time:156183ms step_avg:99.16ms
step:1576/1750 train_time:156285ms step_avg:99.17ms
step:1577/1750 train_time:156388ms step_avg:99.17ms
step:1578/1750 train_time:156489ms step_avg:99.17ms
step:1579/1750 train_time:156591ms step_avg:99.17ms
step:1580/1750 train_time:156694ms step_avg:99.17ms
step:1581/1750 train_time:156795ms step_avg:99.17ms
step:1582/1750 train_time:156897ms step_avg:99.18ms
step:1583/1750 train_time:157000ms step_avg:99.18ms
step:1584/1750 train_time:157103ms step_avg:99.18ms
step:1585/1750 train_time:157204ms step_avg:99.18ms
step:1586/1750 train_time:157307ms step_avg:99.18ms
step:1587/1750 train_time:157408ms step_avg:99.19ms
step:1588/1750 train_time:157511ms step_avg:99.19ms
step:1589/1750 train_time:157612ms step_avg:99.19ms
step:1590/1750 train_time:157714ms step_avg:99.19ms
step:1591/1750 train_time:157815ms step_avg:99.19ms
step:1592/1750 train_time:157917ms step_avg:99.19ms
step:1593/1750 train_time:158019ms step_avg:99.20ms
step:1594/1750 train_time:158124ms step_avg:99.20ms
step:1595/1750 train_time:158226ms step_avg:99.20ms
step:1596/1750 train_time:158328ms step_avg:99.20ms
step:1597/1750 train_time:158429ms step_avg:99.20ms
step:1598/1750 train_time:158533ms step_avg:99.21ms
step:1599/1750 train_time:158634ms step_avg:99.21ms
step:1600/1750 train_time:158735ms step_avg:99.21ms
step:1601/1750 train_time:158838ms step_avg:99.21ms
step:1602/1750 train_time:158940ms step_avg:99.21ms
step:1603/1750 train_time:159042ms step_avg:99.21ms
step:1604/1750 train_time:159143ms step_avg:99.22ms
step:1605/1750 train_time:159245ms step_avg:99.22ms
step:1606/1750 train_time:159348ms step_avg:99.22ms
step:1607/1750 train_time:159450ms step_avg:99.22ms
step:1608/1750 train_time:159551ms step_avg:99.22ms
step:1609/1750 train_time:159653ms step_avg:99.22ms
step:1610/1750 train_time:159756ms step_avg:99.23ms
step:1611/1750 train_time:159858ms step_avg:99.23ms
step:1612/1750 train_time:159961ms step_avg:99.23ms
step:1613/1750 train_time:160062ms step_avg:99.23ms
step:1614/1750 train_time:160163ms step_avg:99.23ms
step:1615/1750 train_time:160264ms step_avg:99.23ms
step:1616/1750 train_time:160367ms step_avg:99.24ms
step:1617/1750 train_time:160470ms step_avg:99.24ms
step:1618/1750 train_time:160572ms step_avg:99.24ms
step:1619/1750 train_time:160674ms step_avg:99.24ms
step:1620/1750 train_time:160777ms step_avg:99.25ms
step:1621/1750 train_time:160878ms step_avg:99.25ms
step:1622/1750 train_time:160980ms step_avg:99.25ms
step:1623/1750 train_time:161082ms step_avg:99.25ms
step:1624/1750 train_time:161185ms step_avg:99.25ms
step:1625/1750 train_time:161289ms step_avg:99.25ms
step:1625/1750 val_loss:3.3056 train_time:161380ms step_avg:99.31ms
step:1626/1750 train_time:161402ms step_avg:99.26ms
step:1627/1750 train_time:161500ms step_avg:99.26ms
step:1628/1750 train_time:161603ms step_avg:99.27ms
step:1629/1750 train_time:161705ms step_avg:99.27ms
step:1630/1750 train_time:161805ms step_avg:99.27ms
step:1631/1750 train_time:161907ms step_avg:99.27ms
step:1632/1750 train_time:162007ms step_avg:99.27ms
step:1633/1750 train_time:162107ms step_avg:99.27ms
step:1634/1750 train_time:162210ms step_avg:99.27ms
step:1635/1750 train_time:162313ms step_avg:99.27ms
step:1636/1750 train_time:162417ms step_avg:99.28ms
step:1637/1750 train_time:162521ms step_avg:99.28ms
step:1638/1750 train_time:162623ms step_avg:99.28ms
step:1639/1750 train_time:162725ms step_avg:99.28ms
step:1640/1750 train_time:162827ms step_avg:99.28ms
step:1641/1750 train_time:162928ms step_avg:99.29ms
step:1642/1750 train_time:163028ms step_avg:99.29ms
step:1643/1750 train_time:163130ms step_avg:99.29ms
step:1644/1750 train_time:163232ms step_avg:99.29ms
step:1645/1750 train_time:163334ms step_avg:99.29ms
step:1646/1750 train_time:163438ms step_avg:99.29ms
step:1647/1750 train_time:163543ms step_avg:99.30ms
step:1648/1750 train_time:163646ms step_avg:99.30ms
step:1649/1750 train_time:163748ms step_avg:99.30ms
step:1650/1750 train_time:163848ms step_avg:99.30ms
step:1651/1750 train_time:163950ms step_avg:99.30ms
step:1652/1750 train_time:164051ms step_avg:99.30ms
step:1653/1750 train_time:164153ms step_avg:99.31ms
step:1654/1750 train_time:164254ms step_avg:99.31ms
step:1655/1750 train_time:164357ms step_avg:99.31ms
step:1656/1750 train_time:164459ms step_avg:99.31ms
step:1657/1750 train_time:164561ms step_avg:99.31ms
step:1658/1750 train_time:164664ms step_avg:99.31ms
step:1659/1750 train_time:164769ms step_avg:99.32ms
step:1660/1750 train_time:164870ms step_avg:99.32ms
step:1661/1750 train_time:164973ms step_avg:99.32ms
step:1662/1750 train_time:165076ms step_avg:99.32ms
step:1663/1750 train_time:165177ms step_avg:99.32ms
step:1664/1750 train_time:165279ms step_avg:99.33ms
step:1665/1750 train_time:165384ms step_avg:99.33ms
step:1666/1750 train_time:165486ms step_avg:99.33ms
step:1667/1750 train_time:165588ms step_avg:99.33ms
step:1668/1750 train_time:165691ms step_avg:99.34ms
step:1669/1750 train_time:165793ms step_avg:99.34ms
step:1670/1750 train_time:165894ms step_avg:99.34ms
step:1671/1750 train_time:165995ms step_avg:99.34ms
step:1672/1750 train_time:166097ms step_avg:99.34ms
step:1673/1750 train_time:166198ms step_avg:99.34ms
step:1674/1750 train_time:166300ms step_avg:99.34ms
step:1675/1750 train_time:166402ms step_avg:99.34ms
step:1676/1750 train_time:166506ms step_avg:99.35ms
step:1677/1750 train_time:166608ms step_avg:99.35ms
step:1678/1750 train_time:166710ms step_avg:99.35ms
step:1679/1750 train_time:166814ms step_avg:99.35ms
step:1680/1750 train_time:166915ms step_avg:99.35ms
step:1681/1750 train_time:167017ms step_avg:99.36ms
step:1682/1750 train_time:167121ms step_avg:99.36ms
step:1683/1750 train_time:167222ms step_avg:99.36ms
step:1684/1750 train_time:167325ms step_avg:99.36ms
step:1685/1750 train_time:167427ms step_avg:99.36ms
step:1686/1750 train_time:167529ms step_avg:99.36ms
step:1687/1750 train_time:167631ms step_avg:99.37ms
step:1688/1750 train_time:167733ms step_avg:99.37ms
step:1689/1750 train_time:167835ms step_avg:99.37ms
step:1690/1750 train_time:167937ms step_avg:99.37ms
step:1691/1750 train_time:168039ms step_avg:99.37ms
step:1692/1750 train_time:168142ms step_avg:99.37ms
step:1693/1750 train_time:168246ms step_avg:99.38ms
step:1694/1750 train_time:168349ms step_avg:99.38ms
step:1695/1750 train_time:168453ms step_avg:99.38ms
step:1696/1750 train_time:168555ms step_avg:99.38ms
step:1697/1750 train_time:168660ms step_avg:99.39ms
step:1698/1750 train_time:168763ms step_avg:99.39ms
step:1699/1750 train_time:168865ms step_avg:99.39ms
step:1700/1750 train_time:168968ms step_avg:99.39ms
step:1701/1750 train_time:169070ms step_avg:99.39ms
step:1702/1750 train_time:169175ms step_avg:99.40ms
step:1703/1750 train_time:169276ms step_avg:99.40ms
step:1704/1750 train_time:169379ms step_avg:99.40ms
step:1705/1750 train_time:169482ms step_avg:99.40ms
step:1706/1750 train_time:169586ms step_avg:99.41ms
step:1707/1750 train_time:169689ms step_avg:99.41ms
step:1708/1750 train_time:169793ms step_avg:99.41ms
step:1709/1750 train_time:169895ms step_avg:99.41ms
step:1710/1750 train_time:169998ms step_avg:99.41ms
step:1711/1750 train_time:170103ms step_avg:99.42ms
step:1712/1750 train_time:170206ms step_avg:99.42ms
step:1713/1750 train_time:170309ms step_avg:99.42ms
step:1714/1750 train_time:170412ms step_avg:99.42ms
step:1715/1750 train_time:170516ms step_avg:99.43ms
step:1716/1750 train_time:170618ms step_avg:99.43ms
step:1717/1750 train_time:170721ms step_avg:99.43ms
step:1718/1750 train_time:170825ms step_avg:99.43ms
step:1719/1750 train_time:170931ms step_avg:99.44ms
step:1720/1750 train_time:171032ms step_avg:99.44ms
step:1721/1750 train_time:171136ms step_avg:99.44ms
step:1722/1750 train_time:171239ms step_avg:99.44ms
step:1723/1750 train_time:171342ms step_avg:99.44ms
step:1724/1750 train_time:171447ms step_avg:99.45ms
step:1725/1750 train_time:171551ms step_avg:99.45ms
step:1726/1750 train_time:171653ms step_avg:99.45ms
step:1727/1750 train_time:171756ms step_avg:99.45ms
step:1728/1750 train_time:171860ms step_avg:99.46ms
step:1729/1750 train_time:171964ms step_avg:99.46ms
step:1730/1750 train_time:172067ms step_avg:99.46ms
step:1731/1750 train_time:172170ms step_avg:99.46ms
step:1732/1750 train_time:172273ms step_avg:99.47ms
step:1733/1750 train_time:172376ms step_avg:99.47ms
step:1734/1750 train_time:172480ms step_avg:99.47ms
step:1735/1750 train_time:172583ms step_avg:99.47ms
step:1736/1750 train_time:172685ms step_avg:99.47ms
step:1737/1750 train_time:172789ms step_avg:99.48ms
step:1738/1750 train_time:172892ms step_avg:99.48ms
step:1739/1750 train_time:172994ms step_avg:99.48ms
step:1740/1750 train_time:173097ms step_avg:99.48ms
step:1741/1750 train_time:173203ms step_avg:99.48ms
step:1742/1750 train_time:173306ms step_avg:99.49ms
step:1743/1750 train_time:173409ms step_avg:99.49ms
step:1744/1750 train_time:173512ms step_avg:99.49ms
step:1745/1750 train_time:173613ms step_avg:99.49ms
step:1746/1750 train_time:173716ms step_avg:99.49ms
step:1747/1750 train_time:173819ms step_avg:99.50ms
step:1748/1750 train_time:173922ms step_avg:99.50ms
step:1749/1750 train_time:174025ms step_avg:99.50ms
step:1750/1750 train_time:174128ms step_avg:99.50ms
step:1750/1750 val_loss:3.2821 train_time:174218ms step_avg:99.55ms
peak memory allocated: 33278 MiB reserved: 49134 MiB
