import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X
'''
import torch.utils.dlpack as thd
from cupyx.scipy.sparse.linalg import svds as cupyx_svds

def several_sv_svds_approximation(W_torch, k, num_iter=50):
    """SVD approximation using the top k singular values and corresponding vectors."""
    # Store original device and dtype
    original_device = W_torch.device
    original_dtype = W_torch.dtype
    
    W = cp.from_dlpack(thd.to_dlpack(W_torch)).astype(cp.float32)
    U, S, Vt = cupyx_svds(W, k=min([k, W.shape[0] - 1, W.shape[1] - 1]), maxiter=num_iter, which='LM')

    # Convert back to torch tensors and ensure they're on the correct device
    approx_torch_U = thd.from_dlpack(U.toDlpack()).to(device=original_device, dtype=original_dtype)
    approx_torch_S = thd.from_dlpack(S.toDlpack()).to(device=original_device, dtype=original_dtype)
    approx_torch_Vt = thd.from_dlpack(Vt.toDlpack()).to(device=original_device, dtype=original_dtype)
    
    return approx_torch_U, approx_torch_S, approx_torch_Vt

class NormNeon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    # v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    u, s, vt = several_sv_svds_approximation(grad.bfloat16(), 5)
                    p.add_(other=u@vt, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()
'''
class FastNormMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, sgd_coeff=0):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, sgd_coeff=sgd_coeff)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        self.sgd_coeff = sgd_coeff

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            sgd_coeff = group["sgd_coeff"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    # assume: grad is a Tensor, momentum_buffer exists, momentum, sgd_coeff, eps, eff_lr are defined
                    with torch.no_grad():
                        # inplace blend with momentum (keeps original semantic: grad becomes the lerped tensor)
                        grad.lerp_(momentum_buffer, momentum)

                        # compute zeropower part from the (post-lerp) grad, in bfloat16 to match the original code
                        if sgd_coeff != 1:
                            # create update_part in bfloat16 (may allocate once per call; unavoidable if zeropower expects bfloat16)
                            update_part = zeropower_via_newtonschulz5(grad.to(torch.bfloat16), 5)

                            # upcast update_part to grad dtype to do mixed-dtype blending without extra temporaries later
                            if update_part.dtype != grad.dtype:
                                update_part = update_part.to(grad.dtype)

                        # compute inverse norm scalar efficiently (no large temp tensors)
                        # - view(-1) is cheap (no copy)
                        # - dot(view, view) is a single reduction
                        norm_sq = grad.view(-1).dot(grad.view(-1))
                        inv_norm = (norm_sq + eps).rsqrt()  # stable rsqrt on the scalar

                        # normalize grad in-place (so we avoid allocating g_normalized)
                        grad.mul_(inv_norm)   # now grad == g_normalized (in-place)

                        # build final update (reuse update_part buffer if available)
                        if sgd_coeff != 1:
                            # update_part := (1 - sgd_coeff) * update_part + sgd_coeff * grad
                            # do it in-place on update_part to avoid creating another temp
                            update_part.mul_(1 - sgd_coeff)    # in-place scale
                            update_part.lerp_(grad, sgd_coeff) # in-place linear interpolation
                            update = update_part
                        else:
                            # if sgd_coeff == 1 we can use grad directly (already normalized in-place)
                            update = grad

                        # apply update (same as original)
                        p.add_(other=update, alpha=-eff_lr)
                    '''
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    eps = 1e-12
                    g_normalized = grad / (grad.norm() + eps)
                    if sgd_coeff != 1:
                        update_part = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                        update = (1 - sgd_coeff) * update_part + sgd_coeff * g_normalized
                    else:
                        update = sgd_coeff * g_normalized
                    p.add_(other=update, alpha=-eff_lr)
                    '''
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class NormMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, sgd_coeff=0):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, sgd_coeff=sgd_coeff)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        self.sgd_coeff = sgd_coeff

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            sgd_coeff = group["sgd_coeff"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    eps = 1e-12
                    g_normalized = grad / (grad.norm() + eps)
                    if sgd_coeff != 1:
                        update_part = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                        update = (1 - sgd_coeff) * update_part + sgd_coeff * g_normalized
                    else:
                        update = sgd_coeff * g_normalized
                    p.add_(other=update, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()
class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)
                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_reduce_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 27.5
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos : pos + max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()
    starts = []
    for i in range(1, len(boundary_positions)):
        end = boundary_positions[i].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == dist.get_world_size():
                return starts, end - pos
            start = end
    assert False # increase max_batch_span if necessary

def distributed_data_generator(filename_pattern: str, batch_size: int, align_to_bos: bool):
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    max_batch_span = 2 * batch_size if align_to_bos else batch_size # provide buffer to handle samples up to length local_batch_size
    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            batch_span = batch_size
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1750 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
from datetime import datetime
if master_process:
    # run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_id = str(int(time.time() * 1000))
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0)
optimizer2 = NormMuon(hidden_matrix_params, lr=0.07, momentum=0.94, weight_decay=0, sgd_coeff=0.5)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.11 | packaged by conda-forge | (main, Jun  4 2025, 14:45:31) [GCC 13.3.0]
Running PyTorch 2.9.0.dev20250824+cu126 compiled for CUDA 12.6
Sun Aug 24 18:15:25 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:04:00.0 Off |                    0 |
| N/A   41C    P0            122W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:23:00.0 Off |                    0 |
| N/A   34C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:43:00.0 Off |                    0 |
| N/A   39C    P0            123W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   33C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:84:00.0 Off |                    0 |
| N/A   39C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:A3:00.0 Off |                    0 |
| N/A   34C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:C3:00.0 Off |                    0 |
| N/A   41C    P0            121W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E4:00.0 Off |                    0 |
| N/A   33C    P0            113W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.06ms
step:1/1750 train_time:144ms step_avg:144.23ms
step:2/1750 train_time:166ms step_avg:82.78ms
step:3/1750 train_time:245ms step_avg:81.82ms
step:4/1750 train_time:337ms step_avg:84.31ms
step:5/1750 train_time:430ms step_avg:85.96ms
step:6/1750 train_time:523ms step_avg:87.10ms
step:7/1750 train_time:615ms step_avg:87.89ms
step:8/1750 train_time:708ms step_avg:88.47ms
step:9/1750 train_time:800ms step_avg:88.93ms
step:10/1750 train_time:893ms step_avg:89.31ms
step:11/1750 train_time:987ms step_avg:89.71ms
step:12/1750 train_time:1083ms step_avg:90.25ms
step:13/1750 train_time:1178ms step_avg:90.59ms
step:14/1750 train_time:1272ms step_avg:90.84ms
step:15/1750 train_time:1365ms step_avg:90.99ms
step:16/1750 train_time:1458ms step_avg:91.14ms
step:17/1750 train_time:1551ms step_avg:91.25ms
step:18/1750 train_time:1644ms step_avg:91.35ms
step:19/1750 train_time:1737ms step_avg:91.42ms
step:20/1750 train_time:1830ms step_avg:91.48ms
step:21/1750 train_time:1923ms step_avg:91.59ms
step:22/1750 train_time:2017ms step_avg:91.67ms
step:23/1750 train_time:2111ms step_avg:91.80ms
step:24/1750 train_time:2207ms step_avg:91.95ms
step:25/1750 train_time:2301ms step_avg:92.03ms
step:26/1750 train_time:2395ms step_avg:92.10ms
step:27/1750 train_time:2488ms step_avg:92.16ms
step:28/1750 train_time:2581ms step_avg:92.19ms
step:29/1750 train_time:2674ms step_avg:92.22ms
step:30/1750 train_time:2767ms step_avg:92.23ms
step:31/1750 train_time:2860ms step_avg:92.26ms
step:32/1750 train_time:2953ms step_avg:92.27ms
step:33/1750 train_time:3047ms step_avg:92.33ms
step:34/1750 train_time:3141ms step_avg:92.37ms
step:35/1750 train_time:3234ms step_avg:92.41ms
step:36/1750 train_time:3329ms step_avg:92.48ms
step:37/1750 train_time:3423ms step_avg:92.50ms
step:38/1750 train_time:3516ms step_avg:92.53ms
step:39/1750 train_time:3609ms step_avg:92.55ms
step:40/1750 train_time:3702ms step_avg:92.56ms
step:41/1750 train_time:3796ms step_avg:92.59ms
step:42/1750 train_time:3889ms step_avg:92.60ms
step:43/1750 train_time:3982ms step_avg:92.61ms
step:44/1750 train_time:4076ms step_avg:92.65ms
step:45/1750 train_time:4170ms step_avg:92.66ms
step:46/1750 train_time:4263ms step_avg:92.68ms
step:47/1750 train_time:4357ms step_avg:92.71ms
step:48/1750 train_time:4451ms step_avg:92.72ms
step:49/1750 train_time:4544ms step_avg:92.74ms
step:50/1750 train_time:4637ms step_avg:92.73ms
step:51/1750 train_time:4731ms step_avg:92.77ms
step:52/1750 train_time:4825ms step_avg:92.79ms
step:53/1750 train_time:4918ms step_avg:92.79ms
step:54/1750 train_time:5011ms step_avg:92.80ms
step:55/1750 train_time:5105ms step_avg:92.81ms
step:56/1750 train_time:5197ms step_avg:92.81ms
step:57/1750 train_time:5292ms step_avg:92.84ms
step:58/1750 train_time:5385ms step_avg:92.85ms
step:59/1750 train_time:5478ms step_avg:92.85ms
step:60/1750 train_time:5572ms step_avg:92.86ms
step:61/1750 train_time:5666ms step_avg:92.89ms
step:62/1750 train_time:5759ms step_avg:92.89ms
step:63/1750 train_time:5852ms step_avg:92.89ms
step:64/1750 train_time:5946ms step_avg:92.90ms
step:65/1750 train_time:6039ms step_avg:92.90ms
step:66/1750 train_time:6132ms step_avg:92.91ms
step:67/1750 train_time:6227ms step_avg:92.94ms
step:68/1750 train_time:6320ms step_avg:92.94ms
step:69/1750 train_time:6414ms step_avg:92.96ms
step:70/1750 train_time:6507ms step_avg:92.96ms
step:71/1750 train_time:6601ms step_avg:92.97ms
step:72/1750 train_time:6694ms step_avg:92.98ms
step:73/1750 train_time:6788ms step_avg:92.99ms
step:74/1750 train_time:6881ms step_avg:92.99ms
step:75/1750 train_time:6975ms step_avg:93.00ms
step:76/1750 train_time:7068ms step_avg:92.99ms
step:77/1750 train_time:7161ms step_avg:93.00ms
step:78/1750 train_time:7254ms step_avg:93.00ms
step:79/1750 train_time:7348ms step_avg:93.01ms
step:80/1750 train_time:7441ms step_avg:93.01ms
step:81/1750 train_time:7534ms step_avg:93.01ms
step:82/1750 train_time:7628ms step_avg:93.03ms
step:83/1750 train_time:7721ms step_avg:93.03ms
step:84/1750 train_time:7815ms step_avg:93.03ms
step:85/1750 train_time:7908ms step_avg:93.04ms
step:86/1750 train_time:8001ms step_avg:93.04ms
step:87/1750 train_time:8094ms step_avg:93.04ms
step:88/1750 train_time:8188ms step_avg:93.05ms
step:89/1750 train_time:8281ms step_avg:93.05ms
step:90/1750 train_time:8375ms step_avg:93.05ms
step:91/1750 train_time:8468ms step_avg:93.05ms
step:92/1750 train_time:8561ms step_avg:93.06ms
step:93/1750 train_time:8655ms step_avg:93.06ms
step:94/1750 train_time:8748ms step_avg:93.07ms
step:95/1750 train_time:8841ms step_avg:93.06ms
step:96/1750 train_time:8934ms step_avg:93.06ms
step:97/1750 train_time:9028ms step_avg:93.07ms
step:98/1750 train_time:9121ms step_avg:93.07ms
step:99/1750 train_time:9214ms step_avg:93.07ms
step:100/1750 train_time:9308ms step_avg:93.08ms
step:101/1750 train_time:9401ms step_avg:93.08ms
step:102/1750 train_time:9494ms step_avg:93.08ms
step:103/1750 train_time:9587ms step_avg:93.08ms
step:104/1750 train_time:9681ms step_avg:93.09ms
step:105/1750 train_time:9774ms step_avg:93.08ms
step:106/1750 train_time:9867ms step_avg:93.08ms
step:107/1750 train_time:9959ms step_avg:93.08ms
step:108/1750 train_time:10053ms step_avg:93.09ms
step:109/1750 train_time:10147ms step_avg:93.09ms
step:110/1750 train_time:10240ms step_avg:93.09ms
step:111/1750 train_time:10333ms step_avg:93.09ms
step:112/1750 train_time:10427ms step_avg:93.10ms
step:113/1750 train_time:10520ms step_avg:93.10ms
step:114/1750 train_time:10613ms step_avg:93.10ms
step:115/1750 train_time:10707ms step_avg:93.11ms
step:116/1750 train_time:10800ms step_avg:93.10ms
step:117/1750 train_time:10893ms step_avg:93.10ms
step:118/1750 train_time:10987ms step_avg:93.11ms
step:119/1750 train_time:11080ms step_avg:93.11ms
step:120/1750 train_time:11173ms step_avg:93.11ms
step:121/1750 train_time:11267ms step_avg:93.11ms
step:122/1750 train_time:11360ms step_avg:93.12ms
step:123/1750 train_time:11454ms step_avg:93.12ms
step:124/1750 train_time:11547ms step_avg:93.12ms
step:125/1750 train_time:11640ms step_avg:93.12ms
step:125/1750 val_loss:4.6575 train_time:11723ms step_avg:93.78ms
step:126/1750 train_time:11745ms step_avg:93.21ms
step:127/1750 train_time:11832ms step_avg:93.16ms
step:128/1750 train_time:11933ms step_avg:93.23ms
step:129/1750 train_time:12027ms step_avg:93.23ms
step:130/1750 train_time:12119ms step_avg:93.22ms
step:131/1750 train_time:12212ms step_avg:93.22ms
step:132/1750 train_time:12305ms step_avg:93.22ms
step:133/1750 train_time:12397ms step_avg:93.21ms
step:134/1750 train_time:12490ms step_avg:93.21ms
step:135/1750 train_time:12583ms step_avg:93.21ms
step:136/1750 train_time:12676ms step_avg:93.21ms
step:137/1750 train_time:12770ms step_avg:93.21ms
step:138/1750 train_time:12866ms step_avg:93.23ms
step:139/1750 train_time:12962ms step_avg:93.25ms
step:140/1750 train_time:13056ms step_avg:93.26ms
step:141/1750 train_time:13149ms step_avg:93.25ms
step:142/1750 train_time:13242ms step_avg:93.25ms
step:143/1750 train_time:13335ms step_avg:93.26ms
step:144/1750 train_time:13429ms step_avg:93.26ms
step:145/1750 train_time:13521ms step_avg:93.25ms
step:146/1750 train_time:13614ms step_avg:93.25ms
step:147/1750 train_time:13707ms step_avg:93.25ms
step:148/1750 train_time:13802ms step_avg:93.26ms
step:149/1750 train_time:13897ms step_avg:93.27ms
step:150/1750 train_time:13993ms step_avg:93.28ms
step:151/1750 train_time:14086ms step_avg:93.29ms
step:152/1750 train_time:14181ms step_avg:93.30ms
step:153/1750 train_time:14274ms step_avg:93.30ms
step:154/1750 train_time:14368ms step_avg:93.30ms
step:155/1750 train_time:14461ms step_avg:93.30ms
step:156/1750 train_time:14554ms step_avg:93.30ms
step:157/1750 train_time:14647ms step_avg:93.29ms
step:158/1750 train_time:14741ms step_avg:93.30ms
step:159/1750 train_time:14835ms step_avg:93.30ms
step:160/1750 train_time:14928ms step_avg:93.30ms
step:161/1750 train_time:15022ms step_avg:93.31ms
step:162/1750 train_time:15117ms step_avg:93.31ms
step:163/1750 train_time:15210ms step_avg:93.32ms
step:164/1750 train_time:15303ms step_avg:93.31ms
step:165/1750 train_time:15397ms step_avg:93.31ms
step:166/1750 train_time:15490ms step_avg:93.31ms
step:167/1750 train_time:15583ms step_avg:93.31ms
step:168/1750 train_time:15677ms step_avg:93.32ms
step:169/1750 train_time:15771ms step_avg:93.32ms
step:170/1750 train_time:15865ms step_avg:93.32ms
step:171/1750 train_time:15958ms step_avg:93.32ms
step:172/1750 train_time:16052ms step_avg:93.33ms
step:173/1750 train_time:16146ms step_avg:93.33ms
step:174/1750 train_time:16241ms step_avg:93.34ms
step:175/1750 train_time:16335ms step_avg:93.34ms
step:176/1750 train_time:16429ms step_avg:93.34ms
step:177/1750 train_time:16522ms step_avg:93.34ms
step:178/1750 train_time:16615ms step_avg:93.34ms
step:179/1750 train_time:16709ms step_avg:93.34ms
step:180/1750 train_time:16802ms step_avg:93.34ms
step:181/1750 train_time:16896ms step_avg:93.35ms
step:182/1750 train_time:16990ms step_avg:93.35ms
step:183/1750 train_time:17083ms step_avg:93.35ms
step:184/1750 train_time:17178ms step_avg:93.36ms
step:185/1750 train_time:17272ms step_avg:93.36ms
step:186/1750 train_time:17366ms step_avg:93.36ms
step:187/1750 train_time:17459ms step_avg:93.37ms
step:188/1750 train_time:17552ms step_avg:93.36ms
step:189/1750 train_time:17646ms step_avg:93.37ms
step:190/1750 train_time:17739ms step_avg:93.37ms
step:191/1750 train_time:17834ms step_avg:93.37ms
step:192/1750 train_time:17927ms step_avg:93.37ms
step:193/1750 train_time:18020ms step_avg:93.37ms
step:194/1750 train_time:18115ms step_avg:93.38ms
step:195/1750 train_time:18209ms step_avg:93.38ms
step:196/1750 train_time:18303ms step_avg:93.38ms
step:197/1750 train_time:18397ms step_avg:93.39ms
step:198/1750 train_time:18491ms step_avg:93.39ms
step:199/1750 train_time:18584ms step_avg:93.39ms
step:200/1750 train_time:18677ms step_avg:93.39ms
step:201/1750 train_time:18772ms step_avg:93.39ms
step:202/1750 train_time:18865ms step_avg:93.39ms
step:203/1750 train_time:18959ms step_avg:93.39ms
step:204/1750 train_time:19052ms step_avg:93.39ms
step:205/1750 train_time:19146ms step_avg:93.40ms
step:206/1750 train_time:19240ms step_avg:93.40ms
step:207/1750 train_time:19334ms step_avg:93.40ms
step:208/1750 train_time:19428ms step_avg:93.40ms
step:209/1750 train_time:19521ms step_avg:93.40ms
step:210/1750 train_time:19615ms step_avg:93.41ms
step:211/1750 train_time:19709ms step_avg:93.41ms
step:212/1750 train_time:19803ms step_avg:93.41ms
step:213/1750 train_time:19896ms step_avg:93.41ms
step:214/1750 train_time:19990ms step_avg:93.41ms
step:215/1750 train_time:20084ms step_avg:93.41ms
step:216/1750 train_time:20178ms step_avg:93.42ms
step:217/1750 train_time:20272ms step_avg:93.42ms
step:218/1750 train_time:20366ms step_avg:93.42ms
step:219/1750 train_time:20460ms step_avg:93.42ms
step:220/1750 train_time:20554ms step_avg:93.43ms
step:221/1750 train_time:20647ms step_avg:93.42ms
step:222/1750 train_time:20741ms step_avg:93.43ms
step:223/1750 train_time:20834ms step_avg:93.43ms
step:224/1750 train_time:20927ms step_avg:93.43ms
step:225/1750 train_time:21021ms step_avg:93.43ms
step:226/1750 train_time:21116ms step_avg:93.43ms
step:227/1750 train_time:21210ms step_avg:93.44ms
step:228/1750 train_time:21304ms step_avg:93.44ms
step:229/1750 train_time:21398ms step_avg:93.44ms
step:230/1750 train_time:21492ms step_avg:93.44ms
step:231/1750 train_time:21585ms step_avg:93.44ms
step:232/1750 train_time:21678ms step_avg:93.44ms
step:233/1750 train_time:21771ms step_avg:93.44ms
step:234/1750 train_time:21865ms step_avg:93.44ms
step:235/1750 train_time:21959ms step_avg:93.44ms
step:236/1750 train_time:22053ms step_avg:93.45ms
step:237/1750 train_time:22146ms step_avg:93.45ms
step:238/1750 train_time:22241ms step_avg:93.45ms
step:239/1750 train_time:22335ms step_avg:93.45ms
step:240/1750 train_time:22428ms step_avg:93.45ms
step:241/1750 train_time:22522ms step_avg:93.45ms
step:242/1750 train_time:22616ms step_avg:93.45ms
step:243/1750 train_time:22710ms step_avg:93.46ms
step:244/1750 train_time:22803ms step_avg:93.46ms
step:245/1750 train_time:22897ms step_avg:93.46ms
step:246/1750 train_time:22991ms step_avg:93.46ms
step:247/1750 train_time:23085ms step_avg:93.46ms
step:248/1750 train_time:23178ms step_avg:93.46ms
step:249/1750 train_time:23273ms step_avg:93.47ms
step:250/1750 train_time:23366ms step_avg:93.46ms
step:250/1750 val_loss:4.0984 train_time:23450ms step_avg:93.80ms
step:251/1750 train_time:23471ms step_avg:93.51ms
step:252/1750 train_time:23561ms step_avg:93.49ms
step:253/1750 train_time:23657ms step_avg:93.50ms
step:254/1750 train_time:23750ms step_avg:93.51ms
step:255/1750 train_time:23843ms step_avg:93.50ms
step:256/1750 train_time:23935ms step_avg:93.50ms
step:257/1750 train_time:24028ms step_avg:93.49ms
step:258/1750 train_time:24121ms step_avg:93.49ms
step:259/1750 train_time:24214ms step_avg:93.49ms
step:260/1750 train_time:24307ms step_avg:93.49ms
step:261/1750 train_time:24401ms step_avg:93.49ms
step:262/1750 train_time:24497ms step_avg:93.50ms
step:263/1750 train_time:24593ms step_avg:93.51ms
step:264/1750 train_time:24688ms step_avg:93.51ms
step:265/1750 train_time:24782ms step_avg:93.52ms
step:266/1750 train_time:24876ms step_avg:93.52ms
step:267/1750 train_time:24969ms step_avg:93.52ms
step:268/1750 train_time:25062ms step_avg:93.52ms
step:269/1750 train_time:25157ms step_avg:93.52ms
step:270/1750 train_time:25250ms step_avg:93.52ms
step:271/1750 train_time:25343ms step_avg:93.52ms
step:272/1750 train_time:25438ms step_avg:93.52ms
step:273/1750 train_time:25533ms step_avg:93.53ms
step:274/1750 train_time:25629ms step_avg:93.53ms
step:275/1750 train_time:25723ms step_avg:93.54ms
step:276/1750 train_time:25818ms step_avg:93.54ms
step:277/1750 train_time:25912ms step_avg:93.55ms
step:278/1750 train_time:26006ms step_avg:93.55ms
step:279/1750 train_time:26100ms step_avg:93.55ms
step:280/1750 train_time:26193ms step_avg:93.55ms
step:281/1750 train_time:26288ms step_avg:93.55ms
step:282/1750 train_time:26382ms step_avg:93.55ms
step:283/1750 train_time:26476ms step_avg:93.56ms
step:284/1750 train_time:26571ms step_avg:93.56ms
step:285/1750 train_time:26665ms step_avg:93.56ms
step:286/1750 train_time:26759ms step_avg:93.56ms
step:287/1750 train_time:26854ms step_avg:93.57ms
step:288/1750 train_time:26947ms step_avg:93.57ms
step:289/1750 train_time:27041ms step_avg:93.57ms
step:290/1750 train_time:27135ms step_avg:93.57ms
step:291/1750 train_time:27229ms step_avg:93.57ms
step:292/1750 train_time:27323ms step_avg:93.57ms
step:293/1750 train_time:27417ms step_avg:93.57ms
step:294/1750 train_time:27511ms step_avg:93.58ms
step:295/1750 train_time:27605ms step_avg:93.58ms
step:296/1750 train_time:27700ms step_avg:93.58ms
step:297/1750 train_time:27795ms step_avg:93.58ms
step:298/1750 train_time:27888ms step_avg:93.58ms
step:299/1750 train_time:27982ms step_avg:93.59ms
step:300/1750 train_time:28076ms step_avg:93.59ms
step:301/1750 train_time:28170ms step_avg:93.59ms
step:302/1750 train_time:28264ms step_avg:93.59ms
step:303/1750 train_time:28358ms step_avg:93.59ms
step:304/1750 train_time:28452ms step_avg:93.59ms
step:305/1750 train_time:28547ms step_avg:93.60ms
step:306/1750 train_time:28641ms step_avg:93.60ms
step:307/1750 train_time:28736ms step_avg:93.60ms
step:308/1750 train_time:28830ms step_avg:93.60ms
step:309/1750 train_time:28924ms step_avg:93.60ms
step:310/1750 train_time:29017ms step_avg:93.60ms
step:311/1750 train_time:29111ms step_avg:93.61ms
step:312/1750 train_time:29205ms step_avg:93.61ms
step:313/1750 train_time:29299ms step_avg:93.61ms
step:314/1750 train_time:29393ms step_avg:93.61ms
step:315/1750 train_time:29488ms step_avg:93.61ms
step:316/1750 train_time:29582ms step_avg:93.61ms
step:317/1750 train_time:29677ms step_avg:93.62ms
step:318/1750 train_time:29771ms step_avg:93.62ms
step:319/1750 train_time:29865ms step_avg:93.62ms
step:320/1750 train_time:29959ms step_avg:93.62ms
step:321/1750 train_time:30053ms step_avg:93.62ms
step:322/1750 train_time:30147ms step_avg:93.62ms
step:323/1750 train_time:30241ms step_avg:93.63ms
step:324/1750 train_time:30754ms step_avg:94.92ms
step:325/1750 train_time:30811ms step_avg:94.80ms
step:326/1750 train_time:30903ms step_avg:94.79ms
step:327/1750 train_time:30996ms step_avg:94.79ms
step:328/1750 train_time:31089ms step_avg:94.78ms
step:329/1750 train_time:31182ms step_avg:94.78ms
step:330/1750 train_time:31275ms step_avg:94.77ms
step:331/1750 train_time:31368ms step_avg:94.77ms
step:332/1750 train_time:31461ms step_avg:94.76ms
step:333/1750 train_time:31555ms step_avg:94.76ms
step:334/1750 train_time:31649ms step_avg:94.76ms
step:335/1750 train_time:31748ms step_avg:94.77ms
step:336/1750 train_time:31846ms step_avg:94.78ms
step:337/1750 train_time:31941ms step_avg:94.78ms
step:338/1750 train_time:32035ms step_avg:94.78ms
step:339/1750 train_time:32129ms step_avg:94.78ms
step:340/1750 train_time:32223ms step_avg:94.77ms
step:341/1750 train_time:32317ms step_avg:94.77ms
step:342/1750 train_time:32410ms step_avg:94.77ms
step:343/1750 train_time:32504ms step_avg:94.76ms
step:344/1750 train_time:32598ms step_avg:94.76ms
step:345/1750 train_time:32693ms step_avg:94.76ms
step:346/1750 train_time:32788ms step_avg:94.76ms
step:347/1750 train_time:32883ms step_avg:94.76ms
step:348/1750 train_time:32977ms step_avg:94.76ms
step:349/1750 train_time:33071ms step_avg:94.76ms
step:350/1750 train_time:33164ms step_avg:94.76ms
step:351/1750 train_time:33258ms step_avg:94.75ms
step:352/1750 train_time:33351ms step_avg:94.75ms
step:353/1750 train_time:33444ms step_avg:94.74ms
step:354/1750 train_time:33538ms step_avg:94.74ms
step:355/1750 train_time:33632ms step_avg:94.74ms
step:356/1750 train_time:33727ms step_avg:94.74ms
step:357/1750 train_time:33822ms step_avg:94.74ms
step:358/1750 train_time:33917ms step_avg:94.74ms
step:359/1750 train_time:34012ms step_avg:94.74ms
step:360/1750 train_time:34106ms step_avg:94.74ms
step:361/1750 train_time:34200ms step_avg:94.74ms
step:362/1750 train_time:34294ms step_avg:94.73ms
step:363/1750 train_time:34388ms step_avg:94.73ms
step:364/1750 train_time:34482ms step_avg:94.73ms
step:365/1750 train_time:34576ms step_avg:94.73ms
step:366/1750 train_time:34670ms step_avg:94.73ms
step:367/1750 train_time:34764ms step_avg:94.73ms
step:368/1750 train_time:34859ms step_avg:94.73ms
step:369/1750 train_time:34954ms step_avg:94.73ms
step:370/1750 train_time:35050ms step_avg:94.73ms
step:371/1750 train_time:35143ms step_avg:94.73ms
step:372/1750 train_time:35237ms step_avg:94.72ms
step:373/1750 train_time:35333ms step_avg:94.73ms
step:374/1750 train_time:35426ms step_avg:94.72ms
step:375/1750 train_time:35520ms step_avg:94.72ms
step:375/1750 val_loss:3.8952 train_time:35604ms step_avg:94.94ms
step:376/1750 train_time:35626ms step_avg:94.75ms
step:377/1750 train_time:35715ms step_avg:94.73ms
step:378/1750 train_time:35811ms step_avg:94.74ms
step:379/1750 train_time:35906ms step_avg:94.74ms
step:380/1750 train_time:36001ms step_avg:94.74ms
step:381/1750 train_time:36094ms step_avg:94.73ms
step:382/1750 train_time:36188ms step_avg:94.73ms
step:383/1750 train_time:36281ms step_avg:94.73ms
step:384/1750 train_time:36374ms step_avg:94.72ms
step:385/1750 train_time:36468ms step_avg:94.72ms
step:386/1750 train_time:36562ms step_avg:94.72ms
step:387/1750 train_time:36658ms step_avg:94.72ms
step:388/1750 train_time:36754ms step_avg:94.73ms
step:389/1750 train_time:36849ms step_avg:94.73ms
step:390/1750 train_time:36944ms step_avg:94.73ms
step:391/1750 train_time:37040ms step_avg:94.73ms
step:392/1750 train_time:37136ms step_avg:94.73ms
step:393/1750 train_time:37231ms step_avg:94.74ms
step:394/1750 train_time:37326ms step_avg:94.74ms
step:395/1750 train_time:37421ms step_avg:94.74ms
step:396/1750 train_time:37516ms step_avg:94.74ms
step:397/1750 train_time:37613ms step_avg:94.74ms
step:398/1750 train_time:37709ms step_avg:94.75ms
step:399/1750 train_time:37806ms step_avg:94.75ms
step:400/1750 train_time:37903ms step_avg:94.76ms
step:401/1750 train_time:37998ms step_avg:94.76ms
step:402/1750 train_time:38094ms step_avg:94.76ms
step:403/1750 train_time:38190ms step_avg:94.76ms
step:404/1750 train_time:38285ms step_avg:94.76ms
step:405/1750 train_time:38380ms step_avg:94.76ms
step:406/1750 train_time:38476ms step_avg:94.77ms
step:407/1750 train_time:38571ms step_avg:94.77ms
step:408/1750 train_time:38667ms step_avg:94.77ms
step:409/1750 train_time:38764ms step_avg:94.78ms
step:410/1750 train_time:38861ms step_avg:94.78ms
step:411/1750 train_time:38957ms step_avg:94.79ms
step:412/1750 train_time:39054ms step_avg:94.79ms
step:413/1750 train_time:39151ms step_avg:94.80ms
step:414/1750 train_time:39247ms step_avg:94.80ms
step:415/1750 train_time:39342ms step_avg:94.80ms
step:416/1750 train_time:39437ms step_avg:94.80ms
step:417/1750 train_time:39533ms step_avg:94.80ms
step:418/1750 train_time:39630ms step_avg:94.81ms
step:419/1750 train_time:39727ms step_avg:94.81ms
step:420/1750 train_time:39824ms step_avg:94.82ms
step:421/1750 train_time:39920ms step_avg:94.82ms
step:422/1750 train_time:40017ms step_avg:94.83ms
step:423/1750 train_time:40113ms step_avg:94.83ms
step:424/1750 train_time:40209ms step_avg:94.83ms
step:425/1750 train_time:40305ms step_avg:94.83ms
step:426/1750 train_time:40400ms step_avg:94.84ms
step:427/1750 train_time:40496ms step_avg:94.84ms
step:428/1750 train_time:40592ms step_avg:94.84ms
step:429/1750 train_time:40688ms step_avg:94.84ms
step:430/1750 train_time:40784ms step_avg:94.85ms
step:431/1750 train_time:40880ms step_avg:94.85ms
step:432/1750 train_time:40976ms step_avg:94.85ms
step:433/1750 train_time:41071ms step_avg:94.85ms
step:434/1750 train_time:41168ms step_avg:94.86ms
step:435/1750 train_time:41264ms step_avg:94.86ms
step:436/1750 train_time:41361ms step_avg:94.86ms
step:437/1750 train_time:41456ms step_avg:94.87ms
step:438/1750 train_time:41552ms step_avg:94.87ms
step:439/1750 train_time:41647ms step_avg:94.87ms
step:440/1750 train_time:41743ms step_avg:94.87ms
step:441/1750 train_time:41840ms step_avg:94.87ms
step:442/1750 train_time:41936ms step_avg:94.88ms
step:443/1750 train_time:42032ms step_avg:94.88ms
step:444/1750 train_time:42128ms step_avg:94.88ms
step:445/1750 train_time:42224ms step_avg:94.89ms
step:446/1750 train_time:42320ms step_avg:94.89ms
step:447/1750 train_time:42416ms step_avg:94.89ms
step:448/1750 train_time:42512ms step_avg:94.89ms
step:449/1750 train_time:42608ms step_avg:94.89ms
step:450/1750 train_time:42703ms step_avg:94.90ms
step:451/1750 train_time:42799ms step_avg:94.90ms
step:452/1750 train_time:42895ms step_avg:94.90ms
step:453/1750 train_time:42991ms step_avg:94.90ms
step:454/1750 train_time:43087ms step_avg:94.91ms
step:455/1750 train_time:43184ms step_avg:94.91ms
step:456/1750 train_time:43279ms step_avg:94.91ms
step:457/1750 train_time:43374ms step_avg:94.91ms
step:458/1750 train_time:43471ms step_avg:94.91ms
step:459/1750 train_time:43567ms step_avg:94.92ms
step:460/1750 train_time:43663ms step_avg:94.92ms
step:461/1750 train_time:43758ms step_avg:94.92ms
step:462/1750 train_time:43854ms step_avg:94.92ms
step:463/1750 train_time:43950ms step_avg:94.93ms
step:464/1750 train_time:44047ms step_avg:94.93ms
step:465/1750 train_time:44142ms step_avg:94.93ms
step:466/1750 train_time:44238ms step_avg:94.93ms
step:467/1750 train_time:44334ms step_avg:94.93ms
step:468/1750 train_time:44430ms step_avg:94.94ms
step:469/1750 train_time:44526ms step_avg:94.94ms
step:470/1750 train_time:44621ms step_avg:94.94ms
step:471/1750 train_time:44718ms step_avg:94.94ms
step:472/1750 train_time:44814ms step_avg:94.95ms
step:473/1750 train_time:44910ms step_avg:94.95ms
step:474/1750 train_time:45006ms step_avg:94.95ms
step:475/1750 train_time:45102ms step_avg:94.95ms
step:476/1750 train_time:45198ms step_avg:94.95ms
step:477/1750 train_time:45293ms step_avg:94.95ms
step:478/1750 train_time:45389ms step_avg:94.96ms
step:479/1750 train_time:45486ms step_avg:94.96ms
step:480/1750 train_time:45581ms step_avg:94.96ms
step:481/1750 train_time:45677ms step_avg:94.96ms
step:482/1750 train_time:45772ms step_avg:94.96ms
step:483/1750 train_time:45868ms step_avg:94.97ms
step:484/1750 train_time:45964ms step_avg:94.97ms
step:485/1750 train_time:46060ms step_avg:94.97ms
step:486/1750 train_time:46156ms step_avg:94.97ms
step:487/1750 train_time:46252ms step_avg:94.97ms
step:488/1750 train_time:46348ms step_avg:94.98ms
step:489/1750 train_time:46445ms step_avg:94.98ms
step:490/1750 train_time:46540ms step_avg:94.98ms
step:491/1750 train_time:46636ms step_avg:94.98ms
step:492/1750 train_time:46732ms step_avg:94.98ms
step:493/1750 train_time:46829ms step_avg:94.99ms
step:494/1750 train_time:46925ms step_avg:94.99ms
step:495/1750 train_time:47021ms step_avg:94.99ms
step:496/1750 train_time:47117ms step_avg:94.99ms
step:497/1750 train_time:47213ms step_avg:95.00ms
step:498/1750 train_time:47309ms step_avg:95.00ms
step:499/1750 train_time:47405ms step_avg:95.00ms
step:500/1750 train_time:47501ms step_avg:95.00ms
step:500/1750 val_loss:3.7466 train_time:47586ms step_avg:95.17ms
step:501/1750 train_time:47608ms step_avg:95.03ms
step:502/1750 train_time:47703ms step_avg:95.02ms
step:503/1750 train_time:47802ms step_avg:95.03ms
step:504/1750 train_time:47898ms step_avg:95.04ms
step:505/1750 train_time:47994ms step_avg:95.04ms
step:506/1750 train_time:48089ms step_avg:95.04ms
step:507/1750 train_time:48185ms step_avg:95.04ms
step:508/1750 train_time:48280ms step_avg:95.04ms
step:509/1750 train_time:48375ms step_avg:95.04ms
step:510/1750 train_time:48471ms step_avg:95.04ms
step:511/1750 train_time:48567ms step_avg:95.04ms
step:512/1750 train_time:48664ms step_avg:95.05ms
step:513/1750 train_time:48761ms step_avg:95.05ms
step:514/1750 train_time:48857ms step_avg:95.05ms
step:515/1750 train_time:48953ms step_avg:95.05ms
step:516/1750 train_time:49049ms step_avg:95.06ms
step:517/1750 train_time:49146ms step_avg:95.06ms
step:518/1750 train_time:49242ms step_avg:95.06ms
step:519/1750 train_time:49337ms step_avg:95.06ms
step:520/1750 train_time:49433ms step_avg:95.06ms
step:521/1750 train_time:49528ms step_avg:95.06ms
step:522/1750 train_time:49625ms step_avg:95.07ms
step:523/1750 train_time:49721ms step_avg:95.07ms
step:524/1750 train_time:49818ms step_avg:95.07ms
step:525/1750 train_time:49915ms step_avg:95.08ms
step:526/1750 train_time:50011ms step_avg:95.08ms
step:527/1750 train_time:50107ms step_avg:95.08ms
step:528/1750 train_time:50204ms step_avg:95.08ms
step:529/1750 train_time:50300ms step_avg:95.08ms
step:530/1750 train_time:50395ms step_avg:95.09ms
step:531/1750 train_time:50491ms step_avg:95.09ms
step:532/1750 train_time:50587ms step_avg:95.09ms
step:533/1750 train_time:50683ms step_avg:95.09ms
step:534/1750 train_time:50780ms step_avg:95.09ms
step:535/1750 train_time:50877ms step_avg:95.10ms
step:536/1750 train_time:50974ms step_avg:95.10ms
step:537/1750 train_time:51071ms step_avg:95.10ms
step:538/1750 train_time:51168ms step_avg:95.11ms
step:539/1750 train_time:51265ms step_avg:95.11ms
step:540/1750 train_time:51361ms step_avg:95.11ms
step:541/1750 train_time:51457ms step_avg:95.11ms
step:542/1750 train_time:51553ms step_avg:95.12ms
step:543/1750 train_time:51649ms step_avg:95.12ms
step:544/1750 train_time:51747ms step_avg:95.12ms
step:545/1750 train_time:51844ms step_avg:95.13ms
step:546/1750 train_time:51940ms step_avg:95.13ms
step:547/1750 train_time:52036ms step_avg:95.13ms
step:548/1750 train_time:52133ms step_avg:95.13ms
step:549/1750 train_time:52229ms step_avg:95.14ms
step:550/1750 train_time:52327ms step_avg:95.14ms
step:551/1750 train_time:52423ms step_avg:95.14ms
step:552/1750 train_time:52519ms step_avg:95.14ms
step:553/1750 train_time:52615ms step_avg:95.15ms
step:554/1750 train_time:52712ms step_avg:95.15ms
step:555/1750 train_time:52808ms step_avg:95.15ms
step:556/1750 train_time:52905ms step_avg:95.15ms
step:557/1750 train_time:53001ms step_avg:95.16ms
step:558/1750 train_time:53098ms step_avg:95.16ms
step:559/1750 train_time:53195ms step_avg:95.16ms
step:560/1750 train_time:53291ms step_avg:95.16ms
step:561/1750 train_time:53388ms step_avg:95.17ms
step:562/1750 train_time:53483ms step_avg:95.17ms
step:563/1750 train_time:53579ms step_avg:95.17ms
step:564/1750 train_time:53676ms step_avg:95.17ms
step:565/1750 train_time:53774ms step_avg:95.17ms
step:566/1750 train_time:53871ms step_avg:95.18ms
step:567/1750 train_time:53967ms step_avg:95.18ms
step:568/1750 train_time:54064ms step_avg:95.18ms
step:569/1750 train_time:54160ms step_avg:95.18ms
step:570/1750 train_time:54256ms step_avg:95.19ms
step:571/1750 train_time:54353ms step_avg:95.19ms
step:572/1750 train_time:54449ms step_avg:95.19ms
step:573/1750 train_time:54546ms step_avg:95.19ms
step:574/1750 train_time:54643ms step_avg:95.20ms
step:575/1750 train_time:54740ms step_avg:95.20ms
step:576/1750 train_time:54837ms step_avg:95.20ms
step:577/1750 train_time:54933ms step_avg:95.21ms
step:578/1750 train_time:55030ms step_avg:95.21ms
step:579/1750 train_time:55127ms step_avg:95.21ms
step:580/1750 train_time:55224ms step_avg:95.21ms
step:581/1750 train_time:55320ms step_avg:95.22ms
step:582/1750 train_time:55416ms step_avg:95.22ms
step:583/1750 train_time:55513ms step_avg:95.22ms
step:584/1750 train_time:55609ms step_avg:95.22ms
step:585/1750 train_time:55706ms step_avg:95.22ms
step:586/1750 train_time:55803ms step_avg:95.23ms
step:587/1750 train_time:55899ms step_avg:95.23ms
step:588/1750 train_time:55996ms step_avg:95.23ms
step:589/1750 train_time:56093ms step_avg:95.23ms
step:590/1750 train_time:56189ms step_avg:95.24ms
step:591/1750 train_time:56285ms step_avg:95.24ms
step:592/1750 train_time:56382ms step_avg:95.24ms
step:593/1750 train_time:56478ms step_avg:95.24ms
step:594/1750 train_time:56574ms step_avg:95.24ms
step:595/1750 train_time:56670ms step_avg:95.24ms
step:596/1750 train_time:56768ms step_avg:95.25ms
step:597/1750 train_time:56865ms step_avg:95.25ms
step:598/1750 train_time:56961ms step_avg:95.25ms
step:599/1750 train_time:57057ms step_avg:95.25ms
step:600/1750 train_time:57153ms step_avg:95.26ms
step:601/1750 train_time:57249ms step_avg:95.26ms
step:602/1750 train_time:57346ms step_avg:95.26ms
step:603/1750 train_time:57443ms step_avg:95.26ms
step:604/1750 train_time:57539ms step_avg:95.26ms
step:605/1750 train_time:57636ms step_avg:95.27ms
step:606/1750 train_time:57733ms step_avg:95.27ms
step:607/1750 train_time:57830ms step_avg:95.27ms
step:608/1750 train_time:57927ms step_avg:95.27ms
step:609/1750 train_time:58023ms step_avg:95.28ms
step:610/1750 train_time:58120ms step_avg:95.28ms
step:611/1750 train_time:58217ms step_avg:95.28ms
step:612/1750 train_time:58314ms step_avg:95.28ms
step:613/1750 train_time:58410ms step_avg:95.29ms
step:614/1750 train_time:58507ms step_avg:95.29ms
step:615/1750 train_time:58603ms step_avg:95.29ms
step:616/1750 train_time:58699ms step_avg:95.29ms
step:617/1750 train_time:58796ms step_avg:95.29ms
step:618/1750 train_time:58893ms step_avg:95.30ms
step:619/1750 train_time:58989ms step_avg:95.30ms
step:620/1750 train_time:59087ms step_avg:95.30ms
step:621/1750 train_time:59183ms step_avg:95.30ms
step:622/1750 train_time:59279ms step_avg:95.30ms
step:623/1750 train_time:59376ms step_avg:95.31ms
step:624/1750 train_time:59473ms step_avg:95.31ms
step:625/1750 train_time:59569ms step_avg:95.31ms
step:625/1750 val_loss:3.6601 train_time:59656ms step_avg:95.45ms
step:626/1750 train_time:59677ms step_avg:95.33ms
step:627/1750 train_time:59773ms step_avg:95.33ms
step:628/1750 train_time:59872ms step_avg:95.34ms
step:629/1750 train_time:59968ms step_avg:95.34ms
step:630/1750 train_time:60064ms step_avg:95.34ms
step:631/1750 train_time:60160ms step_avg:95.34ms
step:632/1750 train_time:60255ms step_avg:95.34ms
step:633/1750 train_time:60351ms step_avg:95.34ms
step:634/1750 train_time:60447ms step_avg:95.34ms
step:635/1750 train_time:60543ms step_avg:95.34ms
step:636/1750 train_time:60640ms step_avg:95.35ms
step:637/1750 train_time:60738ms step_avg:95.35ms
step:638/1750 train_time:60834ms step_avg:95.35ms
step:639/1750 train_time:60931ms step_avg:95.35ms
step:640/1750 train_time:61028ms step_avg:95.36ms
step:641/1750 train_time:61123ms step_avg:95.36ms
step:642/1750 train_time:61220ms step_avg:95.36ms
step:643/1750 train_time:61316ms step_avg:95.36ms
step:644/1750 train_time:61411ms step_avg:95.36ms
step:645/1750 train_time:61507ms step_avg:95.36ms
step:646/1750 train_time:61603ms step_avg:95.36ms
step:647/1750 train_time:61701ms step_avg:95.36ms
step:648/1750 train_time:61799ms step_avg:95.37ms
step:649/1750 train_time:61897ms step_avg:95.37ms
step:650/1750 train_time:61993ms step_avg:95.37ms
step:651/1750 train_time:62091ms step_avg:95.38ms
step:652/1750 train_time:62191ms step_avg:95.38ms
step:653/1750 train_time:62289ms step_avg:95.39ms
step:654/1750 train_time:62386ms step_avg:95.39ms
step:655/1750 train_time:62483ms step_avg:95.39ms
step:656/1750 train_time:62581ms step_avg:95.40ms
step:657/1750 train_time:62679ms step_avg:95.40ms
step:658/1750 train_time:62776ms step_avg:95.40ms
step:659/1750 train_time:62875ms step_avg:95.41ms
step:660/1750 train_time:62972ms step_avg:95.41ms
step:661/1750 train_time:63071ms step_avg:95.42ms
step:662/1750 train_time:63168ms step_avg:95.42ms
step:663/1750 train_time:63266ms step_avg:95.42ms
step:664/1750 train_time:63364ms step_avg:95.43ms
step:665/1750 train_time:63461ms step_avg:95.43ms
step:666/1750 train_time:63559ms step_avg:95.43ms
step:667/1750 train_time:63656ms step_avg:95.44ms
step:668/1750 train_time:63754ms step_avg:95.44ms
step:669/1750 train_time:63852ms step_avg:95.44ms
step:670/1750 train_time:63950ms step_avg:95.45ms
step:671/1750 train_time:64049ms step_avg:95.45ms
step:672/1750 train_time:64146ms step_avg:95.45ms
step:673/1750 train_time:64243ms step_avg:95.46ms
step:674/1750 train_time:64340ms step_avg:95.46ms
step:675/1750 train_time:64437ms step_avg:95.46ms
step:676/1750 train_time:64535ms step_avg:95.47ms
step:677/1750 train_time:64633ms step_avg:95.47ms
step:678/1750 train_time:64731ms step_avg:95.47ms
step:679/1750 train_time:64829ms step_avg:95.48ms
step:680/1750 train_time:64927ms step_avg:95.48ms
step:681/1750 train_time:65026ms step_avg:95.49ms
step:682/1750 train_time:65124ms step_avg:95.49ms
step:683/1750 train_time:65222ms step_avg:95.49ms
step:684/1750 train_time:65320ms step_avg:95.50ms
step:685/1750 train_time:65417ms step_avg:95.50ms
step:686/1750 train_time:65515ms step_avg:95.50ms
step:687/1750 train_time:65613ms step_avg:95.51ms
step:688/1750 train_time:65712ms step_avg:95.51ms
step:689/1750 train_time:65810ms step_avg:95.51ms
step:690/1750 train_time:65908ms step_avg:95.52ms
step:691/1750 train_time:66006ms step_avg:95.52ms
step:692/1750 train_time:66104ms step_avg:95.53ms
step:693/1750 train_time:66202ms step_avg:95.53ms
step:694/1750 train_time:66299ms step_avg:95.53ms
step:695/1750 train_time:66396ms step_avg:95.53ms
step:696/1750 train_time:66493ms step_avg:95.54ms
step:697/1750 train_time:66591ms step_avg:95.54ms
step:698/1750 train_time:66690ms step_avg:95.54ms
step:699/1750 train_time:66787ms step_avg:95.55ms
step:700/1750 train_time:66885ms step_avg:95.55ms
step:701/1750 train_time:66983ms step_avg:95.55ms
step:702/1750 train_time:67082ms step_avg:95.56ms
step:703/1750 train_time:67180ms step_avg:95.56ms
step:704/1750 train_time:67278ms step_avg:95.57ms
step:705/1750 train_time:67375ms step_avg:95.57ms
step:706/1750 train_time:67473ms step_avg:95.57ms
step:707/1750 train_time:67570ms step_avg:95.57ms
step:708/1750 train_time:67668ms step_avg:95.58ms
step:709/1750 train_time:67766ms step_avg:95.58ms
step:710/1750 train_time:67864ms step_avg:95.58ms
step:711/1750 train_time:67962ms step_avg:95.59ms
step:712/1750 train_time:68060ms step_avg:95.59ms
step:713/1750 train_time:68158ms step_avg:95.59ms
step:714/1750 train_time:68256ms step_avg:95.60ms
step:715/1750 train_time:68353ms step_avg:95.60ms
step:716/1750 train_time:68451ms step_avg:95.60ms
step:717/1750 train_time:68549ms step_avg:95.60ms
step:718/1750 train_time:68646ms step_avg:95.61ms
step:719/1750 train_time:68744ms step_avg:95.61ms
step:720/1750 train_time:68842ms step_avg:95.61ms
step:721/1750 train_time:68939ms step_avg:95.62ms
step:722/1750 train_time:69038ms step_avg:95.62ms
step:723/1750 train_time:69135ms step_avg:95.62ms
step:724/1750 train_time:69233ms step_avg:95.63ms
step:725/1750 train_time:69331ms step_avg:95.63ms
step:726/1750 train_time:69429ms step_avg:95.63ms
step:727/1750 train_time:69526ms step_avg:95.63ms
step:728/1750 train_time:69624ms step_avg:95.64ms
step:729/1750 train_time:69722ms step_avg:95.64ms
step:730/1750 train_time:69820ms step_avg:95.64ms
step:731/1750 train_time:69917ms step_avg:95.65ms
step:732/1750 train_time:70015ms step_avg:95.65ms
step:733/1750 train_time:70112ms step_avg:95.65ms
step:734/1750 train_time:70210ms step_avg:95.65ms
step:735/1750 train_time:70308ms step_avg:95.66ms
step:736/1750 train_time:70405ms step_avg:95.66ms
step:737/1750 train_time:70504ms step_avg:95.66ms
step:738/1750 train_time:70602ms step_avg:95.67ms
step:739/1750 train_time:70701ms step_avg:95.67ms
step:740/1750 train_time:70799ms step_avg:95.67ms
step:741/1750 train_time:70897ms step_avg:95.68ms
step:742/1750 train_time:70995ms step_avg:95.68ms
step:743/1750 train_time:71093ms step_avg:95.68ms
step:744/1750 train_time:71190ms step_avg:95.69ms
step:745/1750 train_time:71288ms step_avg:95.69ms
step:746/1750 train_time:71386ms step_avg:95.69ms
step:747/1750 train_time:71484ms step_avg:95.69ms
step:748/1750 train_time:71582ms step_avg:95.70ms
step:749/1750 train_time:71680ms step_avg:95.70ms
step:750/1750 train_time:71778ms step_avg:95.70ms
step:750/1750 val_loss:3.5966 train_time:71865ms step_avg:95.82ms
step:751/1750 train_time:71886ms step_avg:95.72ms
step:752/1750 train_time:71982ms step_avg:95.72ms
step:753/1750 train_time:72081ms step_avg:95.73ms
step:754/1750 train_time:72179ms step_avg:95.73ms
step:755/1750 train_time:72276ms step_avg:95.73ms
step:756/1750 train_time:72374ms step_avg:95.73ms
step:757/1750 train_time:72470ms step_avg:95.73ms
step:758/1750 train_time:72567ms step_avg:95.74ms
step:759/1750 train_time:72664ms step_avg:95.74ms
step:760/1750 train_time:72761ms step_avg:95.74ms
step:761/1750 train_time:72860ms step_avg:95.74ms
step:762/1750 train_time:72959ms step_avg:95.75ms
step:763/1750 train_time:73058ms step_avg:95.75ms
step:764/1750 train_time:73155ms step_avg:95.75ms
step:765/1750 train_time:73253ms step_avg:95.76ms
step:766/1750 train_time:73351ms step_avg:95.76ms
step:767/1750 train_time:73448ms step_avg:95.76ms
step:768/1750 train_time:73546ms step_avg:95.76ms
step:769/1750 train_time:73643ms step_avg:95.76ms
step:770/1750 train_time:73740ms step_avg:95.77ms
step:771/1750 train_time:73838ms step_avg:95.77ms
step:772/1750 train_time:73936ms step_avg:95.77ms
step:773/1750 train_time:74036ms step_avg:95.78ms
step:774/1750 train_time:74134ms step_avg:95.78ms
step:775/1750 train_time:74232ms step_avg:95.78ms
step:776/1750 train_time:74330ms step_avg:95.79ms
step:777/1750 train_time:74427ms step_avg:95.79ms
step:778/1750 train_time:74525ms step_avg:95.79ms
step:779/1750 train_time:74623ms step_avg:95.79ms
step:780/1750 train_time:74720ms step_avg:95.79ms
step:781/1750 train_time:74818ms step_avg:95.80ms
step:782/1750 train_time:74917ms step_avg:95.80ms
step:783/1750 train_time:75016ms step_avg:95.81ms
step:784/1750 train_time:75114ms step_avg:95.81ms
step:785/1750 train_time:75212ms step_avg:95.81ms
step:786/1750 train_time:75310ms step_avg:95.81ms
step:787/1750 train_time:75408ms step_avg:95.82ms
step:788/1750 train_time:75506ms step_avg:95.82ms
step:789/1750 train_time:75604ms step_avg:95.82ms
step:790/1750 train_time:75702ms step_avg:95.83ms
step:791/1750 train_time:75800ms step_avg:95.83ms
step:792/1750 train_time:75899ms step_avg:95.83ms
step:793/1750 train_time:75997ms step_avg:95.83ms
step:794/1750 train_time:76095ms step_avg:95.84ms
step:795/1750 train_time:76193ms step_avg:95.84ms
step:796/1750 train_time:76292ms step_avg:95.84ms
step:797/1750 train_time:76389ms step_avg:95.85ms
step:798/1750 train_time:76488ms step_avg:95.85ms
step:799/1750 train_time:76587ms step_avg:95.85ms
step:800/1750 train_time:76684ms step_avg:95.86ms
step:801/1750 train_time:76782ms step_avg:95.86ms
step:802/1750 train_time:76879ms step_avg:95.86ms
step:803/1750 train_time:76977ms step_avg:95.86ms
step:804/1750 train_time:77075ms step_avg:95.86ms
step:805/1750 train_time:77173ms step_avg:95.87ms
step:806/1750 train_time:77271ms step_avg:95.87ms
step:807/1750 train_time:77369ms step_avg:95.87ms
step:808/1750 train_time:77467ms step_avg:95.87ms
step:809/1750 train_time:77565ms step_avg:95.88ms
step:810/1750 train_time:77663ms step_avg:95.88ms
step:811/1750 train_time:77761ms step_avg:95.88ms
step:812/1750 train_time:77858ms step_avg:95.88ms
step:813/1750 train_time:77957ms step_avg:95.89ms
step:814/1750 train_time:78056ms step_avg:95.89ms
step:815/1750 train_time:78153ms step_avg:95.89ms
step:816/1750 train_time:78251ms step_avg:95.90ms
step:817/1750 train_time:78349ms step_avg:95.90ms
step:818/1750 train_time:78447ms step_avg:95.90ms
step:819/1750 train_time:78546ms step_avg:95.90ms
step:820/1750 train_time:78644ms step_avg:95.91ms
step:821/1750 train_time:78741ms step_avg:95.91ms
step:822/1750 train_time:78839ms step_avg:95.91ms
step:823/1750 train_time:78938ms step_avg:95.91ms
step:824/1750 train_time:79036ms step_avg:95.92ms
step:825/1750 train_time:79134ms step_avg:95.92ms
step:826/1750 train_time:79231ms step_avg:95.92ms
step:827/1750 train_time:79329ms step_avg:95.92ms
step:828/1750 train_time:79428ms step_avg:95.93ms
step:829/1750 train_time:79526ms step_avg:95.93ms
step:830/1750 train_time:79623ms step_avg:95.93ms
step:831/1750 train_time:79722ms step_avg:95.93ms
step:832/1750 train_time:79819ms step_avg:95.94ms
step:833/1750 train_time:79917ms step_avg:95.94ms
step:834/1750 train_time:80015ms step_avg:95.94ms
step:835/1750 train_time:80113ms step_avg:95.94ms
step:836/1750 train_time:80211ms step_avg:95.95ms
step:837/1750 train_time:80308ms step_avg:95.95ms
step:838/1750 train_time:80407ms step_avg:95.95ms
step:839/1750 train_time:80505ms step_avg:95.95ms
step:840/1750 train_time:80603ms step_avg:95.96ms
step:841/1750 train_time:80701ms step_avg:95.96ms
step:842/1750 train_time:80798ms step_avg:95.96ms
step:843/1750 train_time:80896ms step_avg:95.96ms
step:844/1750 train_time:80995ms step_avg:95.97ms
step:845/1750 train_time:81093ms step_avg:95.97ms
step:846/1750 train_time:81191ms step_avg:95.97ms
step:847/1750 train_time:81289ms step_avg:95.97ms
step:848/1750 train_time:81388ms step_avg:95.98ms
step:849/1750 train_time:81486ms step_avg:95.98ms
step:850/1750 train_time:81583ms step_avg:95.98ms
step:851/1750 train_time:81681ms step_avg:95.98ms
step:852/1750 train_time:81778ms step_avg:95.98ms
step:853/1750 train_time:81875ms step_avg:95.99ms
step:854/1750 train_time:81973ms step_avg:95.99ms
step:855/1750 train_time:82071ms step_avg:95.99ms
step:856/1750 train_time:82169ms step_avg:95.99ms
step:857/1750 train_time:82268ms step_avg:96.00ms
step:858/1750 train_time:82366ms step_avg:96.00ms
step:859/1750 train_time:82464ms step_avg:96.00ms
step:860/1750 train_time:82563ms step_avg:96.00ms
step:861/1750 train_time:82660ms step_avg:96.00ms
step:862/1750 train_time:82758ms step_avg:96.01ms
step:863/1750 train_time:82856ms step_avg:96.01ms
step:864/1750 train_time:82954ms step_avg:96.01ms
step:865/1750 train_time:83052ms step_avg:96.01ms
step:866/1750 train_time:83149ms step_avg:96.02ms
step:867/1750 train_time:83248ms step_avg:96.02ms
step:868/1750 train_time:83346ms step_avg:96.02ms
step:869/1750 train_time:83443ms step_avg:96.02ms
step:870/1750 train_time:83542ms step_avg:96.03ms
step:871/1750 train_time:83640ms step_avg:96.03ms
step:872/1750 train_time:83738ms step_avg:96.03ms
step:873/1750 train_time:83835ms step_avg:96.03ms
step:874/1750 train_time:83933ms step_avg:96.03ms
step:875/1750 train_time:84031ms step_avg:96.04ms
step:875/1750 val_loss:3.5496 train_time:84118ms step_avg:96.13ms
step:876/1750 train_time:84139ms step_avg:96.05ms
step:877/1750 train_time:84235ms step_avg:96.05ms
step:878/1750 train_time:84333ms step_avg:96.05ms
step:879/1750 train_time:84431ms step_avg:96.05ms
step:880/1750 train_time:84528ms step_avg:96.06ms
step:881/1750 train_time:84626ms step_avg:96.06ms
step:882/1750 train_time:84724ms step_avg:96.06ms
step:883/1750 train_time:84821ms step_avg:96.06ms
step:884/1750 train_time:84918ms step_avg:96.06ms
step:885/1750 train_time:85017ms step_avg:96.06ms
step:886/1750 train_time:85117ms step_avg:96.07ms
step:887/1750 train_time:85216ms step_avg:96.07ms
step:888/1750 train_time:85315ms step_avg:96.07ms
step:889/1750 train_time:85413ms step_avg:96.08ms
step:890/1750 train_time:85511ms step_avg:96.08ms
step:891/1750 train_time:85609ms step_avg:96.08ms
step:892/1750 train_time:85707ms step_avg:96.08ms
step:893/1750 train_time:85804ms step_avg:96.09ms
step:894/1750 train_time:85902ms step_avg:96.09ms
step:895/1750 train_time:86000ms step_avg:96.09ms
step:896/1750 train_time:86099ms step_avg:96.09ms
step:897/1750 train_time:86198ms step_avg:96.10ms
step:898/1750 train_time:86296ms step_avg:96.10ms
step:899/1750 train_time:86396ms step_avg:96.10ms
step:900/1750 train_time:86496ms step_avg:96.11ms
step:901/1750 train_time:86594ms step_avg:96.11ms
step:902/1750 train_time:86692ms step_avg:96.11ms
step:903/1750 train_time:86790ms step_avg:96.11ms
step:904/1750 train_time:86887ms step_avg:96.11ms
step:905/1750 train_time:86985ms step_avg:96.12ms
step:906/1750 train_time:87083ms step_avg:96.12ms
step:907/1750 train_time:87181ms step_avg:96.12ms
step:908/1750 train_time:87279ms step_avg:96.12ms
step:909/1750 train_time:87377ms step_avg:96.12ms
step:910/1750 train_time:87478ms step_avg:96.13ms
step:911/1750 train_time:87577ms step_avg:96.13ms
step:912/1750 train_time:87677ms step_avg:96.14ms
step:913/1750 train_time:87777ms step_avg:96.14ms
step:914/1750 train_time:87877ms step_avg:96.15ms
step:915/1750 train_time:87978ms step_avg:96.15ms
step:916/1750 train_time:88078ms step_avg:96.16ms
step:917/1750 train_time:88177ms step_avg:96.16ms
step:918/1750 train_time:88276ms step_avg:96.16ms
step:919/1750 train_time:88376ms step_avg:96.17ms
step:920/1750 train_time:88476ms step_avg:96.17ms
step:921/1750 train_time:88575ms step_avg:96.17ms
step:922/1750 train_time:88675ms step_avg:96.18ms
step:923/1750 train_time:88774ms step_avg:96.18ms
step:924/1750 train_time:88874ms step_avg:96.18ms
step:925/1750 train_time:88974ms step_avg:96.19ms
step:926/1750 train_time:89075ms step_avg:96.19ms
step:927/1750 train_time:89174ms step_avg:96.20ms
step:928/1750 train_time:89274ms step_avg:96.20ms
step:929/1750 train_time:89373ms step_avg:96.20ms
step:930/1750 train_time:89473ms step_avg:96.21ms
step:931/1750 train_time:89573ms step_avg:96.21ms
step:932/1750 train_time:89672ms step_avg:96.21ms
step:933/1750 train_time:89772ms step_avg:96.22ms
step:934/1750 train_time:89871ms step_avg:96.22ms
step:935/1750 train_time:89972ms step_avg:96.23ms
step:936/1750 train_time:90072ms step_avg:96.23ms
step:937/1750 train_time:90172ms step_avg:96.23ms
step:938/1750 train_time:90271ms step_avg:96.24ms
step:939/1750 train_time:90372ms step_avg:96.24ms
step:940/1750 train_time:90472ms step_avg:96.25ms
step:941/1750 train_time:90571ms step_avg:96.25ms
step:942/1750 train_time:90670ms step_avg:96.25ms
step:943/1750 train_time:90770ms step_avg:96.26ms
step:944/1750 train_time:90869ms step_avg:96.26ms
step:945/1750 train_time:90969ms step_avg:96.26ms
step:946/1750 train_time:91069ms step_avg:96.27ms
step:947/1750 train_time:91169ms step_avg:96.27ms
step:948/1750 train_time:91269ms step_avg:96.28ms
step:949/1750 train_time:91370ms step_avg:96.28ms
step:950/1750 train_time:91471ms step_avg:96.28ms
step:951/1750 train_time:91571ms step_avg:96.29ms
step:952/1750 train_time:91670ms step_avg:96.29ms
step:953/1750 train_time:91768ms step_avg:96.29ms
step:954/1750 train_time:91867ms step_avg:96.30ms
step:955/1750 train_time:91967ms step_avg:96.30ms
step:956/1750 train_time:92067ms step_avg:96.30ms
step:957/1750 train_time:92167ms step_avg:96.31ms
step:958/1750 train_time:92268ms step_avg:96.31ms
step:959/1750 train_time:92368ms step_avg:96.32ms
step:960/1750 train_time:92469ms step_avg:96.32ms
step:961/1750 train_time:92569ms step_avg:96.33ms
step:962/1750 train_time:92669ms step_avg:96.33ms
step:963/1750 train_time:92769ms step_avg:96.33ms
step:964/1750 train_time:92869ms step_avg:96.34ms
step:965/1750 train_time:92969ms step_avg:96.34ms
step:966/1750 train_time:93068ms step_avg:96.34ms
step:967/1750 train_time:93169ms step_avg:96.35ms
step:968/1750 train_time:93270ms step_avg:96.35ms
step:969/1750 train_time:93370ms step_avg:96.36ms
step:970/1750 train_time:93470ms step_avg:96.36ms
step:971/1750 train_time:93571ms step_avg:96.37ms
step:972/1750 train_time:93670ms step_avg:96.37ms
step:973/1750 train_time:93770ms step_avg:96.37ms
step:974/1750 train_time:93868ms step_avg:96.37ms
step:975/1750 train_time:93967ms step_avg:96.38ms
step:976/1750 train_time:94067ms step_avg:96.38ms
step:977/1750 train_time:94166ms step_avg:96.38ms
step:978/1750 train_time:94266ms step_avg:96.39ms
step:979/1750 train_time:94366ms step_avg:96.39ms
step:980/1750 train_time:94466ms step_avg:96.39ms
step:981/1750 train_time:94565ms step_avg:96.40ms
step:982/1750 train_time:94665ms step_avg:96.40ms
step:983/1750 train_time:94764ms step_avg:96.40ms
step:984/1750 train_time:94864ms step_avg:96.41ms
step:985/1750 train_time:94962ms step_avg:96.41ms
step:986/1750 train_time:95061ms step_avg:96.41ms
step:987/1750 train_time:95161ms step_avg:96.41ms
step:988/1750 train_time:95260ms step_avg:96.42ms
step:989/1750 train_time:95359ms step_avg:96.42ms
step:990/1750 train_time:95460ms step_avg:96.42ms
step:991/1750 train_time:95560ms step_avg:96.43ms
step:992/1750 train_time:95660ms step_avg:96.43ms
step:993/1750 train_time:95759ms step_avg:96.43ms
step:994/1750 train_time:95858ms step_avg:96.44ms
step:995/1750 train_time:95958ms step_avg:96.44ms
step:996/1750 train_time:96058ms step_avg:96.44ms
step:997/1750 train_time:96157ms step_avg:96.45ms
step:998/1750 train_time:96257ms step_avg:96.45ms
step:999/1750 train_time:96357ms step_avg:96.45ms
step:1000/1750 train_time:96457ms step_avg:96.46ms
step:1000/1750 val_loss:3.5080 train_time:96545ms step_avg:96.55ms
step:1001/1750 train_time:96568ms step_avg:96.47ms
step:1002/1750 train_time:96666ms step_avg:96.47ms
step:1003/1750 train_time:96767ms step_avg:96.48ms
step:1004/1750 train_time:96866ms step_avg:96.48ms
step:1005/1750 train_time:96966ms step_avg:96.48ms
step:1006/1750 train_time:97064ms step_avg:96.49ms
step:1007/1750 train_time:97162ms step_avg:96.49ms
step:1008/1750 train_time:97260ms step_avg:96.49ms
step:1009/1750 train_time:97358ms step_avg:96.49ms
step:1010/1750 train_time:97458ms step_avg:96.49ms
step:1011/1750 train_time:97559ms step_avg:96.50ms
step:1012/1750 train_time:97662ms step_avg:96.50ms
step:1013/1750 train_time:97763ms step_avg:96.51ms
step:1014/1750 train_time:97862ms step_avg:96.51ms
step:1015/1750 train_time:97961ms step_avg:96.51ms
step:1016/1750 train_time:98059ms step_avg:96.51ms
step:1017/1750 train_time:98158ms step_avg:96.52ms
step:1018/1750 train_time:98256ms step_avg:96.52ms
step:1019/1750 train_time:98355ms step_avg:96.52ms
step:1020/1750 train_time:98454ms step_avg:96.52ms
step:1021/1750 train_time:98555ms step_avg:96.53ms
step:1022/1750 train_time:98656ms step_avg:96.53ms
step:1023/1750 train_time:98757ms step_avg:96.54ms
step:1024/1750 train_time:98858ms step_avg:96.54ms
step:1025/1750 train_time:98958ms step_avg:96.54ms
step:1026/1750 train_time:99058ms step_avg:96.55ms
step:1027/1750 train_time:99156ms step_avg:96.55ms
step:1028/1750 train_time:99255ms step_avg:96.55ms
step:1029/1750 train_time:99355ms step_avg:96.55ms
step:1030/1750 train_time:99454ms step_avg:96.56ms
step:1031/1750 train_time:99554ms step_avg:96.56ms
step:1032/1750 train_time:99654ms step_avg:96.56ms
step:1033/1750 train_time:99754ms step_avg:96.57ms
step:1034/1750 train_time:99855ms step_avg:96.57ms
step:1035/1750 train_time:99955ms step_avg:96.57ms
step:1036/1750 train_time:100055ms step_avg:96.58ms
step:1037/1750 train_time:100155ms step_avg:96.58ms
step:1038/1750 train_time:100254ms step_avg:96.58ms
step:1039/1750 train_time:100352ms step_avg:96.59ms
step:1040/1750 train_time:100452ms step_avg:96.59ms
step:1041/1750 train_time:100553ms step_avg:96.59ms
step:1042/1750 train_time:100906ms step_avg:96.84ms
step:1043/1750 train_time:101005ms step_avg:96.84ms
step:1044/1750 train_time:101103ms step_avg:96.84ms
step:1045/1750 train_time:101202ms step_avg:96.84ms
step:1046/1750 train_time:101301ms step_avg:96.85ms
step:1047/1750 train_time:101399ms step_avg:96.85ms
step:1048/1750 train_time:101497ms step_avg:96.85ms
step:1049/1750 train_time:101595ms step_avg:96.85ms
step:1050/1750 train_time:101694ms step_avg:96.85ms
step:1051/1750 train_time:101798ms step_avg:96.86ms
step:1052/1750 train_time:101899ms step_avg:96.86ms
step:1053/1750 train_time:101998ms step_avg:96.86ms
step:1054/1750 train_time:102097ms step_avg:96.87ms
step:1055/1750 train_time:102197ms step_avg:96.87ms
step:1056/1750 train_time:102296ms step_avg:96.87ms
step:1057/1750 train_time:102395ms step_avg:96.87ms
step:1058/1750 train_time:102495ms step_avg:96.88ms
step:1059/1750 train_time:102594ms step_avg:96.88ms
step:1060/1750 train_time:102967ms step_avg:97.14ms
step:1061/1750 train_time:103065ms step_avg:97.14ms
step:1062/1750 train_time:103164ms step_avg:97.14ms
step:1063/1750 train_time:103263ms step_avg:97.14ms
step:1064/1750 train_time:103362ms step_avg:97.14ms
step:1065/1750 train_time:103460ms step_avg:97.15ms
step:1066/1750 train_time:103558ms step_avg:97.15ms
step:1067/1750 train_time:103657ms step_avg:97.15ms
step:1068/1750 train_time:103756ms step_avg:97.15ms
step:1069/1750 train_time:103856ms step_avg:97.15ms
step:1070/1750 train_time:103959ms step_avg:97.16ms
step:1071/1750 train_time:104058ms step_avg:97.16ms
step:1072/1750 train_time:104157ms step_avg:97.16ms
step:1073/1750 train_time:104256ms step_avg:97.16ms
step:1074/1750 train_time:104356ms step_avg:97.17ms
step:1075/1750 train_time:104455ms step_avg:97.17ms
step:1076/1750 train_time:104555ms step_avg:97.17ms
step:1077/1750 train_time:104942ms step_avg:97.44ms
step:1078/1750 train_time:105039ms step_avg:97.44ms
step:1079/1750 train_time:105137ms step_avg:97.44ms
step:1080/1750 train_time:105235ms step_avg:97.44ms
step:1081/1750 train_time:105334ms step_avg:97.44ms
step:1082/1750 train_time:105433ms step_avg:97.44ms
step:1083/1750 train_time:105530ms step_avg:97.44ms
step:1084/1750 train_time:105630ms step_avg:97.44ms
step:1085/1750 train_time:105728ms step_avg:97.45ms
step:1086/1750 train_time:105832ms step_avg:97.45ms
step:1087/1750 train_time:105936ms step_avg:97.46ms
step:1088/1750 train_time:106037ms step_avg:97.46ms
step:1089/1750 train_time:106136ms step_avg:97.46ms
step:1090/1750 train_time:106236ms step_avg:97.46ms
step:1091/1750 train_time:106335ms step_avg:97.47ms
step:1092/1750 train_time:106434ms step_avg:97.47ms
step:1093/1750 train_time:106532ms step_avg:97.47ms
step:1094/1750 train_time:106632ms step_avg:97.47ms
step:1095/1750 train_time:106732ms step_avg:97.47ms
step:1096/1750 train_time:106834ms step_avg:97.48ms
step:1097/1750 train_time:106935ms step_avg:97.48ms
step:1098/1750 train_time:107037ms step_avg:97.48ms
step:1099/1750 train_time:107136ms step_avg:97.49ms
step:1100/1750 train_time:107236ms step_avg:97.49ms
step:1101/1750 train_time:107335ms step_avg:97.49ms
step:1102/1750 train_time:107434ms step_avg:97.49ms
step:1103/1750 train_time:107533ms step_avg:97.49ms
step:1104/1750 train_time:107633ms step_avg:97.49ms
step:1105/1750 train_time:107732ms step_avg:97.50ms
step:1106/1750 train_time:107834ms step_avg:97.50ms
step:1107/1750 train_time:107935ms step_avg:97.50ms
step:1108/1750 train_time:108035ms step_avg:97.50ms
step:1109/1750 train_time:108136ms step_avg:97.51ms
step:1110/1750 train_time:108236ms step_avg:97.51ms
step:1111/1750 train_time:108335ms step_avg:97.51ms
step:1112/1750 train_time:108436ms step_avg:97.51ms
step:1113/1750 train_time:108536ms step_avg:97.52ms
step:1114/1750 train_time:108635ms step_avg:97.52ms
step:1115/1750 train_time:108734ms step_avg:97.52ms
step:1116/1750 train_time:108835ms step_avg:97.52ms
step:1117/1750 train_time:108935ms step_avg:97.52ms
step:1118/1750 train_time:109034ms step_avg:97.53ms
step:1119/1750 train_time:109135ms step_avg:97.53ms
step:1120/1750 train_time:109236ms step_avg:97.53ms
step:1121/1750 train_time:109336ms step_avg:97.53ms
step:1122/1750 train_time:109435ms step_avg:97.54ms
step:1123/1750 train_time:109535ms step_avg:97.54ms
step:1124/1750 train_time:109634ms step_avg:97.54ms
step:1125/1750 train_time:109734ms step_avg:97.54ms
step:1125/1750 val_loss:3.4573 train_time:109823ms step_avg:97.62ms
step:1126/1750 train_time:109844ms step_avg:97.55ms
step:1127/1750 train_time:109939ms step_avg:97.55ms
step:1128/1750 train_time:110043ms step_avg:97.56ms
step:1129/1750 train_time:110143ms step_avg:97.56ms
step:1130/1750 train_time:110243ms step_avg:97.56ms
step:1131/1750 train_time:110342ms step_avg:97.56ms
step:1132/1750 train_time:110440ms step_avg:97.56ms
step:1133/1750 train_time:110540ms step_avg:97.56ms
step:1134/1750 train_time:110639ms step_avg:97.56ms
step:1135/1750 train_time:110738ms step_avg:97.57ms
step:1136/1750 train_time:110840ms step_avg:97.57ms
step:1137/1750 train_time:110943ms step_avg:97.58ms
step:1138/1750 train_time:111044ms step_avg:97.58ms
step:1139/1750 train_time:111144ms step_avg:97.58ms
step:1140/1750 train_time:111244ms step_avg:97.58ms
step:1141/1750 train_time:111344ms step_avg:97.58ms
step:1142/1750 train_time:111442ms step_avg:97.58ms
step:1143/1750 train_time:111540ms step_avg:97.59ms
step:1144/1750 train_time:111640ms step_avg:97.59ms
step:1145/1750 train_time:111740ms step_avg:97.59ms
step:1146/1750 train_time:111840ms step_avg:97.59ms
step:1147/1750 train_time:111942ms step_avg:97.60ms
step:1148/1750 train_time:112042ms step_avg:97.60ms
step:1149/1750 train_time:112142ms step_avg:97.60ms
step:1150/1750 train_time:112242ms step_avg:97.60ms
step:1151/1750 train_time:112342ms step_avg:97.60ms
step:1152/1750 train_time:112441ms step_avg:97.61ms
step:1153/1750 train_time:112540ms step_avg:97.61ms
step:1154/1750 train_time:112640ms step_avg:97.61ms
step:1155/1750 train_time:112740ms step_avg:97.61ms
step:1156/1750 train_time:112841ms step_avg:97.61ms
step:1157/1750 train_time:112941ms step_avg:97.62ms
step:1158/1750 train_time:113297ms step_avg:97.84ms
step:1159/1750 train_time:113395ms step_avg:97.84ms
step:1160/1750 train_time:113493ms step_avg:97.84ms
step:1161/1750 train_time:113592ms step_avg:97.84ms
step:1162/1750 train_time:113689ms step_avg:97.84ms
step:1163/1750 train_time:113790ms step_avg:97.84ms
step:1164/1750 train_time:113889ms step_avg:97.84ms
step:1165/1750 train_time:113987ms step_avg:97.84ms
step:1166/1750 train_time:114085ms step_avg:97.84ms
step:1167/1750 train_time:114439ms step_avg:98.06ms
step:1168/1750 train_time:114537ms step_avg:98.06ms
step:1169/1750 train_time:114637ms step_avg:98.06ms
step:1170/1750 train_time:114736ms step_avg:98.07ms
step:1171/1750 train_time:114836ms step_avg:98.07ms
step:1172/1750 train_time:114936ms step_avg:98.07ms
step:1173/1750 train_time:115036ms step_avg:98.07ms
step:1174/1750 train_time:115136ms step_avg:98.07ms
step:1175/1750 train_time:115235ms step_avg:98.07ms
step:1176/1750 train_time:115339ms step_avg:98.08ms
step:1177/1750 train_time:115444ms step_avg:98.08ms
step:1178/1750 train_time:115546ms step_avg:98.09ms
step:1179/1750 train_time:115649ms step_avg:98.09ms
step:1180/1750 train_time:115750ms step_avg:98.09ms
step:1181/1750 train_time:115851ms step_avg:98.10ms
step:1182/1750 train_time:115951ms step_avg:98.10ms
step:1183/1750 train_time:116051ms step_avg:98.10ms
step:1184/1750 train_time:116153ms step_avg:98.10ms
step:1185/1750 train_time:116254ms step_avg:98.10ms
step:1186/1750 train_time:116354ms step_avg:98.11ms
step:1187/1750 train_time:116454ms step_avg:98.11ms
step:1188/1750 train_time:116555ms step_avg:98.11ms
step:1189/1750 train_time:116656ms step_avg:98.11ms
step:1190/1750 train_time:116757ms step_avg:98.11ms
step:1191/1750 train_time:116858ms step_avg:98.12ms
step:1192/1750 train_time:116960ms step_avg:98.12ms
step:1193/1750 train_time:117061ms step_avg:98.12ms
step:1194/1750 train_time:117161ms step_avg:98.12ms
step:1195/1750 train_time:117261ms step_avg:98.13ms
step:1196/1750 train_time:117362ms step_avg:98.13ms
step:1197/1750 train_time:117464ms step_avg:98.13ms
step:1198/1750 train_time:117566ms step_avg:98.13ms
step:1199/1750 train_time:117666ms step_avg:98.14ms
step:1200/1750 train_time:117769ms step_avg:98.14ms
step:1201/1750 train_time:117870ms step_avg:98.14ms
step:1202/1750 train_time:117972ms step_avg:98.15ms
step:1203/1750 train_time:118072ms step_avg:98.15ms
step:1204/1750 train_time:118172ms step_avg:98.15ms
step:1205/1750 train_time:118273ms step_avg:98.15ms
step:1206/1750 train_time:118373ms step_avg:98.15ms
step:1207/1750 train_time:118474ms step_avg:98.16ms
step:1208/1750 train_time:118574ms step_avg:98.16ms
step:1209/1750 train_time:118675ms step_avg:98.16ms
step:1210/1750 train_time:118776ms step_avg:98.16ms
step:1211/1750 train_time:118877ms step_avg:98.16ms
step:1212/1750 train_time:118978ms step_avg:98.17ms
step:1213/1750 train_time:119079ms step_avg:98.17ms
step:1214/1750 train_time:119179ms step_avg:98.17ms
step:1215/1750 train_time:119280ms step_avg:98.17ms
step:1216/1750 train_time:119383ms step_avg:98.18ms
step:1217/1750 train_time:119485ms step_avg:98.18ms
step:1218/1750 train_time:119586ms step_avg:98.18ms
step:1219/1750 train_time:119995ms step_avg:98.44ms
step:1220/1750 train_time:120093ms step_avg:98.44ms
step:1221/1750 train_time:120192ms step_avg:98.44ms
step:1222/1750 train_time:120292ms step_avg:98.44ms
step:1223/1750 train_time:120392ms step_avg:98.44ms
step:1224/1750 train_time:120491ms step_avg:98.44ms
step:1225/1750 train_time:120590ms step_avg:98.44ms
step:1226/1750 train_time:120689ms step_avg:98.44ms
step:1227/1750 train_time:120788ms step_avg:98.44ms
step:1228/1750 train_time:120890ms step_avg:98.44ms
step:1229/1750 train_time:120994ms step_avg:98.45ms
step:1230/1750 train_time:121095ms step_avg:98.45ms
step:1231/1750 train_time:121195ms step_avg:98.45ms
step:1232/1750 train_time:121295ms step_avg:98.45ms
step:1233/1750 train_time:121395ms step_avg:98.45ms
step:1234/1750 train_time:121496ms step_avg:98.46ms
step:1235/1750 train_time:121596ms step_avg:98.46ms
step:1236/1750 train_time:121696ms step_avg:98.46ms
step:1237/1750 train_time:121797ms step_avg:98.46ms
step:1238/1750 train_time:122196ms step_avg:98.70ms
step:1239/1750 train_time:122295ms step_avg:98.70ms
step:1240/1750 train_time:122394ms step_avg:98.70ms
step:1241/1750 train_time:122494ms step_avg:98.71ms
step:1242/1750 train_time:122594ms step_avg:98.71ms
step:1243/1750 train_time:122694ms step_avg:98.71ms
step:1244/1750 train_time:122793ms step_avg:98.71ms
step:1245/1750 train_time:122892ms step_avg:98.71ms
step:1246/1750 train_time:122992ms step_avg:98.71ms
step:1247/1750 train_time:123094ms step_avg:98.71ms
step:1248/1750 train_time:123198ms step_avg:98.72ms
step:1249/1750 train_time:123300ms step_avg:98.72ms
step:1250/1750 train_time:123400ms step_avg:98.72ms
step:1250/1750 val_loss:3.4118 train_time:123489ms step_avg:98.79ms
step:1251/1750 train_time:123511ms step_avg:98.73ms
step:1252/1750 train_time:123613ms step_avg:98.73ms
step:1253/1750 train_time:123713ms step_avg:98.73ms
step:1254/1750 train_time:123814ms step_avg:98.74ms
step:1255/1750 train_time:123915ms step_avg:98.74ms
step:1256/1750 train_time:124015ms step_avg:98.74ms
step:1257/1750 train_time:124115ms step_avg:98.74ms
step:1258/1750 train_time:124215ms step_avg:98.74ms
step:1259/1750 train_time:124315ms step_avg:98.74ms
step:1260/1750 train_time:124417ms step_avg:98.74ms
step:1261/1750 train_time:124521ms step_avg:98.75ms
step:1262/1750 train_time:124622ms step_avg:98.75ms
step:1263/1750 train_time:124721ms step_avg:98.75ms
step:1264/1750 train_time:124821ms step_avg:98.75ms
step:1265/1750 train_time:124920ms step_avg:98.75ms
step:1266/1750 train_time:125020ms step_avg:98.75ms
step:1267/1750 train_time:125120ms step_avg:98.75ms
step:1268/1750 train_time:125220ms step_avg:98.75ms
step:1269/1750 train_time:125320ms step_avg:98.75ms
step:1270/1750 train_time:125420ms step_avg:98.76ms
step:1271/1750 train_time:125522ms step_avg:98.76ms
step:1272/1750 train_time:125622ms step_avg:98.76ms
step:1273/1750 train_time:125722ms step_avg:98.76ms
step:1274/1750 train_time:125823ms step_avg:98.76ms
step:1275/1750 train_time:125923ms step_avg:98.76ms
step:1276/1750 train_time:126025ms step_avg:98.77ms
step:1277/1750 train_time:126125ms step_avg:98.77ms
step:1278/1750 train_time:126228ms step_avg:98.77ms
step:1279/1750 train_time:126329ms step_avg:98.77ms
step:1280/1750 train_time:126430ms step_avg:98.77ms
step:1281/1750 train_time:126533ms step_avg:98.78ms
step:1282/1750 train_time:126633ms step_avg:98.78ms
step:1283/1750 train_time:126735ms step_avg:98.78ms
step:1284/1750 train_time:126836ms step_avg:98.78ms
step:1285/1750 train_time:126937ms step_avg:98.78ms
step:1286/1750 train_time:127037ms step_avg:98.78ms
step:1287/1750 train_time:127138ms step_avg:98.79ms
step:1288/1750 train_time:127238ms step_avg:98.79ms
step:1289/1750 train_time:127339ms step_avg:98.79ms
step:1290/1750 train_time:127439ms step_avg:98.79ms
step:1291/1750 train_time:127538ms step_avg:98.79ms
step:1292/1750 train_time:127639ms step_avg:98.79ms
step:1293/1750 train_time:127740ms step_avg:98.79ms
step:1294/1750 train_time:127844ms step_avg:98.80ms
step:1295/1750 train_time:127945ms step_avg:98.80ms
step:1296/1750 train_time:128047ms step_avg:98.80ms
step:1297/1750 train_time:128149ms step_avg:98.80ms
step:1298/1750 train_time:128250ms step_avg:98.81ms
step:1299/1750 train_time:128350ms step_avg:98.81ms
step:1300/1750 train_time:128451ms step_avg:98.81ms
step:1301/1750 train_time:128553ms step_avg:98.81ms
step:1302/1750 train_time:128654ms step_avg:98.81ms
step:1303/1750 train_time:128757ms step_avg:98.82ms
step:1304/1750 train_time:128858ms step_avg:98.82ms
step:1305/1750 train_time:128958ms step_avg:98.82ms
step:1306/1750 train_time:129059ms step_avg:98.82ms
step:1307/1750 train_time:129158ms step_avg:98.82ms
step:1308/1750 train_time:129258ms step_avg:98.82ms
step:1309/1750 train_time:129358ms step_avg:98.82ms
step:1310/1750 train_time:129458ms step_avg:98.82ms
step:1311/1750 train_time:129559ms step_avg:98.82ms
step:1312/1750 train_time:129659ms step_avg:98.83ms
step:1313/1750 train_time:129761ms step_avg:98.83ms
step:1314/1750 train_time:129861ms step_avg:98.83ms
step:1315/1750 train_time:129962ms step_avg:98.83ms
step:1316/1750 train_time:130062ms step_avg:98.83ms
step:1317/1750 train_time:130162ms step_avg:98.83ms
step:1318/1750 train_time:130262ms step_avg:98.83ms
step:1319/1750 train_time:130362ms step_avg:98.83ms
step:1320/1750 train_time:130464ms step_avg:98.84ms
step:1321/1750 train_time:130564ms step_avg:98.84ms
step:1322/1750 train_time:130667ms step_avg:98.84ms
step:1323/1750 train_time:130768ms step_avg:98.84ms
step:1324/1750 train_time:130868ms step_avg:98.84ms
step:1325/1750 train_time:130971ms step_avg:98.85ms
step:1326/1750 train_time:131073ms step_avg:98.85ms
step:1327/1750 train_time:131174ms step_avg:98.85ms
step:1328/1750 train_time:131275ms step_avg:98.85ms
step:1329/1750 train_time:131377ms step_avg:98.85ms
step:1330/1750 train_time:131478ms step_avg:98.86ms
step:1331/1750 train_time:131579ms step_avg:98.86ms
step:1332/1750 train_time:131678ms step_avg:98.86ms
step:1333/1750 train_time:131779ms step_avg:98.86ms
step:1334/1750 train_time:131879ms step_avg:98.86ms
step:1335/1750 train_time:131980ms step_avg:98.86ms
step:1336/1750 train_time:132081ms step_avg:98.86ms
step:1337/1750 train_time:132181ms step_avg:98.86ms
step:1338/1750 train_time:132282ms step_avg:98.87ms
step:1339/1750 train_time:132383ms step_avg:98.87ms
step:1340/1750 train_time:132484ms step_avg:98.87ms
step:1341/1750 train_time:132584ms step_avg:98.87ms
step:1342/1750 train_time:132686ms step_avg:98.87ms
step:1343/1750 train_time:132787ms step_avg:98.87ms
step:1344/1750 train_time:132888ms step_avg:98.88ms
step:1345/1750 train_time:132989ms step_avg:98.88ms
step:1346/1750 train_time:133091ms step_avg:98.88ms
step:1347/1750 train_time:133193ms step_avg:98.88ms
step:1348/1750 train_time:133295ms step_avg:98.88ms
step:1349/1750 train_time:133395ms step_avg:98.88ms
step:1350/1750 train_time:133497ms step_avg:98.89ms
step:1351/1750 train_time:133598ms step_avg:98.89ms
step:1352/1750 train_time:133699ms step_avg:98.89ms
step:1353/1750 train_time:133799ms step_avg:98.89ms
step:1354/1750 train_time:133899ms step_avg:98.89ms
step:1355/1750 train_time:133999ms step_avg:98.89ms
step:1356/1750 train_time:134099ms step_avg:98.89ms
step:1357/1750 train_time:134199ms step_avg:98.89ms
step:1358/1750 train_time:134299ms step_avg:98.89ms
step:1359/1750 train_time:134400ms step_avg:98.90ms
step:1360/1750 train_time:134501ms step_avg:98.90ms
step:1361/1750 train_time:134601ms step_avg:98.90ms
step:1362/1750 train_time:134701ms step_avg:98.90ms
step:1363/1750 train_time:134802ms step_avg:98.90ms
step:1364/1750 train_time:134903ms step_avg:98.90ms
step:1365/1750 train_time:135004ms step_avg:98.90ms
step:1366/1750 train_time:135104ms step_avg:98.90ms
step:1367/1750 train_time:135204ms step_avg:98.91ms
step:1368/1750 train_time:135306ms step_avg:98.91ms
step:1369/1750 train_time:135408ms step_avg:98.91ms
step:1370/1750 train_time:135509ms step_avg:98.91ms
step:1371/1750 train_time:135610ms step_avg:98.91ms
step:1372/1750 train_time:135711ms step_avg:98.91ms
step:1373/1750 train_time:135812ms step_avg:98.92ms
step:1374/1750 train_time:135914ms step_avg:98.92ms
step:1375/1750 train_time:136017ms step_avg:98.92ms
step:1375/1750 val_loss:3.3719 train_time:136106ms step_avg:98.99ms
step:1376/1750 train_time:136128ms step_avg:98.93ms
step:1377/1750 train_time:136227ms step_avg:98.93ms
step:1378/1750 train_time:136331ms step_avg:98.93ms
step:1379/1750 train_time:136430ms step_avg:98.93ms
step:1380/1750 train_time:136532ms step_avg:98.94ms
step:1381/1750 train_time:136631ms step_avg:98.94ms
step:1382/1750 train_time:136730ms step_avg:98.94ms
step:1383/1750 train_time:136830ms step_avg:98.94ms
step:1384/1750 train_time:136930ms step_avg:98.94ms
step:1385/1750 train_time:137030ms step_avg:98.94ms
step:1386/1750 train_time:137134ms step_avg:98.94ms
step:1387/1750 train_time:137238ms step_avg:98.95ms
step:1388/1750 train_time:137339ms step_avg:98.95ms
step:1389/1750 train_time:137441ms step_avg:98.95ms
step:1390/1750 train_time:137542ms step_avg:98.95ms
step:1391/1750 train_time:137643ms step_avg:98.95ms
step:1392/1750 train_time:137744ms step_avg:98.95ms
step:1393/1750 train_time:137846ms step_avg:98.96ms
step:1394/1750 train_time:137946ms step_avg:98.96ms
step:1395/1750 train_time:138048ms step_avg:98.96ms
step:1396/1750 train_time:138149ms step_avg:98.96ms
step:1397/1750 train_time:138250ms step_avg:98.96ms
step:1398/1750 train_time:138351ms step_avg:98.96ms
step:1399/1750 train_time:138452ms step_avg:98.96ms
step:1400/1750 train_time:138553ms step_avg:98.97ms
step:1401/1750 train_time:138656ms step_avg:98.97ms
step:1402/1750 train_time:138758ms step_avg:98.97ms
step:1403/1750 train_time:138860ms step_avg:98.97ms
step:1404/1750 train_time:138961ms step_avg:98.97ms
step:1405/1750 train_time:139062ms step_avg:98.98ms
step:1406/1750 train_time:139165ms step_avg:98.98ms
step:1407/1750 train_time:139267ms step_avg:98.98ms
step:1408/1750 train_time:139368ms step_avg:98.98ms
step:1409/1750 train_time:139470ms step_avg:98.99ms
step:1410/1750 train_time:139571ms step_avg:98.99ms
step:1411/1750 train_time:139671ms step_avg:98.99ms
step:1412/1750 train_time:139773ms step_avg:98.99ms
step:1413/1750 train_time:139874ms step_avg:98.99ms
step:1414/1750 train_time:139975ms step_avg:98.99ms
step:1415/1750 train_time:140077ms step_avg:98.99ms
step:1416/1750 train_time:140178ms step_avg:99.00ms
step:1417/1750 train_time:140280ms step_avg:99.00ms
step:1418/1750 train_time:140381ms step_avg:99.00ms
step:1419/1750 train_time:140482ms step_avg:99.00ms
step:1420/1750 train_time:140584ms step_avg:99.00ms
step:1421/1750 train_time:140687ms step_avg:99.01ms
step:1422/1750 train_time:140787ms step_avg:99.01ms
step:1423/1750 train_time:140888ms step_avg:99.01ms
step:1424/1750 train_time:140988ms step_avg:99.01ms
step:1425/1750 train_time:141088ms step_avg:99.01ms
step:1426/1750 train_time:141189ms step_avg:99.01ms
step:1427/1750 train_time:141290ms step_avg:99.01ms
step:1428/1750 train_time:141392ms step_avg:99.01ms
step:1429/1750 train_time:141494ms step_avg:99.02ms
step:1430/1750 train_time:141596ms step_avg:99.02ms
step:1431/1750 train_time:141698ms step_avg:99.02ms
step:1432/1750 train_time:141799ms step_avg:99.02ms
step:1433/1750 train_time:141901ms step_avg:99.02ms
step:1434/1750 train_time:142003ms step_avg:99.03ms
step:1435/1750 train_time:142107ms step_avg:99.03ms
step:1436/1750 train_time:142210ms step_avg:99.03ms
step:1437/1750 train_time:142313ms step_avg:99.03ms
step:1438/1750 train_time:142413ms step_avg:99.04ms
step:1439/1750 train_time:142516ms step_avg:99.04ms
step:1440/1750 train_time:142620ms step_avg:99.04ms
step:1441/1750 train_time:142722ms step_avg:99.04ms
step:1442/1750 train_time:142823ms step_avg:99.05ms
step:1443/1750 train_time:142925ms step_avg:99.05ms
step:1444/1750 train_time:143026ms step_avg:99.05ms
step:1445/1750 train_time:143128ms step_avg:99.05ms
step:1446/1750 train_time:143230ms step_avg:99.05ms
step:1447/1750 train_time:143332ms step_avg:99.05ms
step:1448/1750 train_time:143434ms step_avg:99.06ms
step:1449/1750 train_time:143536ms step_avg:99.06ms
step:1450/1750 train_time:143639ms step_avg:99.06ms
step:1451/1750 train_time:143740ms step_avg:99.06ms
step:1452/1750 train_time:143842ms step_avg:99.06ms
step:1453/1750 train_time:143944ms step_avg:99.07ms
step:1454/1750 train_time:144048ms step_avg:99.07ms
step:1455/1750 train_time:144149ms step_avg:99.07ms
step:1456/1750 train_time:144250ms step_avg:99.07ms
step:1457/1750 train_time:144352ms step_avg:99.07ms
step:1458/1750 train_time:144453ms step_avg:99.08ms
step:1459/1750 train_time:144555ms step_avg:99.08ms
step:1460/1750 train_time:144657ms step_avg:99.08ms
step:1461/1750 train_time:144761ms step_avg:99.08ms
step:1462/1750 train_time:144863ms step_avg:99.09ms
step:1463/1750 train_time:144965ms step_avg:99.09ms
step:1464/1750 train_time:145066ms step_avg:99.09ms
step:1465/1750 train_time:145168ms step_avg:99.09ms
step:1466/1750 train_time:145269ms step_avg:99.09ms
step:1467/1750 train_time:145371ms step_avg:99.09ms
step:1468/1750 train_time:145473ms step_avg:99.10ms
step:1469/1750 train_time:145575ms step_avg:99.10ms
step:1470/1750 train_time:145677ms step_avg:99.10ms
step:1471/1750 train_time:145779ms step_avg:99.10ms
step:1472/1750 train_time:145881ms step_avg:99.10ms
step:1473/1750 train_time:145983ms step_avg:99.11ms
step:1474/1750 train_time:146085ms step_avg:99.11ms
step:1475/1750 train_time:146188ms step_avg:99.11ms
step:1476/1750 train_time:146290ms step_avg:99.11ms
step:1477/1750 train_time:146391ms step_avg:99.11ms
step:1478/1750 train_time:146492ms step_avg:99.11ms
step:1479/1750 train_time:146592ms step_avg:99.12ms
step:1480/1750 train_time:146694ms step_avg:99.12ms
step:1481/1750 train_time:146795ms step_avg:99.12ms
step:1482/1750 train_time:146900ms step_avg:99.12ms
step:1483/1750 train_time:147002ms step_avg:99.12ms
step:1484/1750 train_time:147105ms step_avg:99.13ms
step:1485/1750 train_time:147208ms step_avg:99.13ms
step:1486/1750 train_time:147309ms step_avg:99.13ms
step:1487/1750 train_time:147411ms step_avg:99.13ms
step:1488/1750 train_time:147513ms step_avg:99.14ms
step:1489/1750 train_time:147615ms step_avg:99.14ms
step:1490/1750 train_time:147717ms step_avg:99.14ms
step:1491/1750 train_time:147819ms step_avg:99.14ms
step:1492/1750 train_time:147920ms step_avg:99.14ms
step:1493/1750 train_time:148022ms step_avg:99.14ms
step:1494/1750 train_time:148124ms step_avg:99.15ms
step:1495/1750 train_time:148226ms step_avg:99.15ms
step:1496/1750 train_time:148328ms step_avg:99.15ms
step:1497/1750 train_time:148430ms step_avg:99.15ms
step:1498/1750 train_time:148532ms step_avg:99.15ms
step:1499/1750 train_time:148631ms step_avg:99.15ms
step:1500/1750 train_time:148732ms step_avg:99.15ms
step:1500/1750 val_loss:3.3364 train_time:148822ms step_avg:99.21ms
step:1501/1750 train_time:148844ms step_avg:99.16ms
step:1502/1750 train_time:148943ms step_avg:99.16ms
step:1503/1750 train_time:149044ms step_avg:99.16ms
step:1504/1750 train_time:149144ms step_avg:99.17ms
step:1505/1750 train_time:149245ms step_avg:99.17ms
step:1506/1750 train_time:149345ms step_avg:99.17ms
step:1507/1750 train_time:149446ms step_avg:99.17ms
step:1508/1750 train_time:149546ms step_avg:99.17ms
step:1509/1750 train_time:149648ms step_avg:99.17ms
step:1510/1750 train_time:149752ms step_avg:99.17ms
step:1511/1750 train_time:149857ms step_avg:99.18ms
step:1512/1750 train_time:149960ms step_avg:99.18ms
step:1513/1750 train_time:150061ms step_avg:99.18ms
step:1514/1750 train_time:150162ms step_avg:99.18ms
step:1515/1750 train_time:150267ms step_avg:99.19ms
step:1516/1750 train_time:150368ms step_avg:99.19ms
step:1517/1750 train_time:150468ms step_avg:99.19ms
step:1518/1750 train_time:150568ms step_avg:99.19ms
step:1519/1750 train_time:150670ms step_avg:99.19ms
step:1520/1750 train_time:150773ms step_avg:99.19ms
step:1521/1750 train_time:150875ms step_avg:99.19ms
step:1522/1750 train_time:150978ms step_avg:99.20ms
step:1523/1750 train_time:151080ms step_avg:99.20ms
step:1524/1750 train_time:151183ms step_avg:99.20ms
step:1525/1750 train_time:151286ms step_avg:99.20ms
step:1526/1750 train_time:151389ms step_avg:99.21ms
step:1527/1750 train_time:151491ms step_avg:99.21ms
step:1528/1750 train_time:151595ms step_avg:99.21ms
step:1529/1750 train_time:151697ms step_avg:99.21ms
step:1530/1750 train_time:151799ms step_avg:99.22ms
step:1531/1750 train_time:151900ms step_avg:99.22ms
step:1532/1750 train_time:152002ms step_avg:99.22ms
step:1533/1750 train_time:152103ms step_avg:99.22ms
step:1534/1750 train_time:152205ms step_avg:99.22ms
step:1535/1750 train_time:152307ms step_avg:99.22ms
step:1536/1750 train_time:152408ms step_avg:99.22ms
step:1537/1750 train_time:152509ms step_avg:99.23ms
step:1538/1750 train_time:152611ms step_avg:99.23ms
step:1539/1750 train_time:152714ms step_avg:99.23ms
step:1540/1750 train_time:152817ms step_avg:99.23ms
step:1541/1750 train_time:152921ms step_avg:99.24ms
step:1542/1750 train_time:153025ms step_avg:99.24ms
step:1543/1750 train_time:153126ms step_avg:99.24ms
step:1544/1750 train_time:153228ms step_avg:99.24ms
step:1545/1750 train_time:153330ms step_avg:99.24ms
step:1546/1750 train_time:153431ms step_avg:99.24ms
step:1547/1750 train_time:153534ms step_avg:99.25ms
step:1548/1750 train_time:153636ms step_avg:99.25ms
step:1549/1750 train_time:153737ms step_avg:99.25ms
step:1550/1750 train_time:153839ms step_avg:99.25ms
step:1551/1750 train_time:153942ms step_avg:99.25ms
step:1552/1750 train_time:154044ms step_avg:99.26ms
step:1553/1750 train_time:154145ms step_avg:99.26ms
step:1554/1750 train_time:154246ms step_avg:99.26ms
step:1555/1750 train_time:154347ms step_avg:99.26ms
step:1556/1750 train_time:154449ms step_avg:99.26ms
step:1557/1750 train_time:154553ms step_avg:99.26ms
step:1558/1750 train_time:154654ms step_avg:99.26ms
step:1559/1750 train_time:154757ms step_avg:99.27ms
step:1560/1750 train_time:154860ms step_avg:99.27ms
step:1561/1750 train_time:154962ms step_avg:99.27ms
step:1562/1750 train_time:155064ms step_avg:99.27ms
step:1563/1750 train_time:155170ms step_avg:99.28ms
step:1564/1750 train_time:155271ms step_avg:99.28ms
step:1565/1750 train_time:155372ms step_avg:99.28ms
step:1566/1750 train_time:155474ms step_avg:99.28ms
step:1567/1750 train_time:155575ms step_avg:99.28ms
step:1568/1750 train_time:155676ms step_avg:99.28ms
step:1569/1750 train_time:155777ms step_avg:99.28ms
step:1570/1750 train_time:155881ms step_avg:99.29ms
step:1571/1750 train_time:155982ms step_avg:99.29ms
step:1572/1750 train_time:156084ms step_avg:99.29ms
step:1573/1750 train_time:156186ms step_avg:99.29ms
step:1574/1750 train_time:156289ms step_avg:99.29ms
step:1575/1750 train_time:156390ms step_avg:99.30ms
step:1576/1750 train_time:156493ms step_avg:99.30ms
step:1577/1750 train_time:156597ms step_avg:99.30ms
step:1578/1750 train_time:156698ms step_avg:99.30ms
step:1579/1750 train_time:156800ms step_avg:99.30ms
step:1580/1750 train_time:156903ms step_avg:99.31ms
step:1581/1750 train_time:157005ms step_avg:99.31ms
step:1582/1750 train_time:157107ms step_avg:99.31ms
step:1583/1750 train_time:157211ms step_avg:99.31ms
step:1584/1750 train_time:157313ms step_avg:99.31ms
step:1585/1750 train_time:157415ms step_avg:99.32ms
step:1586/1750 train_time:157519ms step_avg:99.32ms
step:1587/1750 train_time:157620ms step_avg:99.32ms
step:1588/1750 train_time:157722ms step_avg:99.32ms
step:1589/1750 train_time:157824ms step_avg:99.32ms
step:1590/1750 train_time:157925ms step_avg:99.32ms
step:1591/1750 train_time:158027ms step_avg:99.33ms
step:1592/1750 train_time:158129ms step_avg:99.33ms
step:1593/1750 train_time:158231ms step_avg:99.33ms
step:1594/1750 train_time:158337ms step_avg:99.33ms
step:1595/1750 train_time:158438ms step_avg:99.33ms
step:1596/1750 train_time:158540ms step_avg:99.34ms
step:1597/1750 train_time:158641ms step_avg:99.34ms
step:1598/1750 train_time:158743ms step_avg:99.34ms
step:1599/1750 train_time:158844ms step_avg:99.34ms
step:1600/1750 train_time:158945ms step_avg:99.34ms
step:1601/1750 train_time:159047ms step_avg:99.34ms
step:1602/1750 train_time:159150ms step_avg:99.34ms
step:1603/1750 train_time:159251ms step_avg:99.35ms
step:1604/1750 train_time:159353ms step_avg:99.35ms
step:1605/1750 train_time:159456ms step_avg:99.35ms
step:1606/1750 train_time:159558ms step_avg:99.35ms
step:1607/1750 train_time:159660ms step_avg:99.35ms
step:1608/1750 train_time:159762ms step_avg:99.35ms
step:1609/1750 train_time:159864ms step_avg:99.36ms
step:1610/1750 train_time:159967ms step_avg:99.36ms
step:1611/1750 train_time:160070ms step_avg:99.36ms
step:1612/1750 train_time:160172ms step_avg:99.36ms
step:1613/1750 train_time:160273ms step_avg:99.36ms
step:1614/1750 train_time:160375ms step_avg:99.36ms
step:1615/1750 train_time:160476ms step_avg:99.37ms
step:1616/1750 train_time:160579ms step_avg:99.37ms
step:1617/1750 train_time:160681ms step_avg:99.37ms
step:1618/1750 train_time:160782ms step_avg:99.37ms
step:1619/1750 train_time:160884ms step_avg:99.37ms
step:1620/1750 train_time:160987ms step_avg:99.37ms
step:1621/1750 train_time:161088ms step_avg:99.38ms
step:1622/1750 train_time:161191ms step_avg:99.38ms
step:1623/1750 train_time:161293ms step_avg:99.38ms
step:1624/1750 train_time:161395ms step_avg:99.38ms
step:1625/1750 train_time:161499ms step_avg:99.38ms
step:1625/1750 val_loss:3.3062 train_time:161590ms step_avg:99.44ms
step:1626/1750 train_time:161611ms step_avg:99.39ms
step:1627/1750 train_time:161714ms step_avg:99.39ms
step:1628/1750 train_time:161817ms step_avg:99.40ms
step:1629/1750 train_time:161918ms step_avg:99.40ms
step:1630/1750 train_time:162018ms step_avg:99.40ms
step:1631/1750 train_time:162120ms step_avg:99.40ms
step:1632/1750 train_time:162221ms step_avg:99.40ms
step:1633/1750 train_time:162322ms step_avg:99.40ms
step:1634/1750 train_time:162425ms step_avg:99.40ms
step:1635/1750 train_time:162528ms step_avg:99.41ms
step:1636/1750 train_time:162631ms step_avg:99.41ms
step:1637/1750 train_time:162734ms step_avg:99.41ms
step:1638/1750 train_time:162837ms step_avg:99.41ms
step:1639/1750 train_time:162938ms step_avg:99.41ms
step:1640/1750 train_time:163039ms step_avg:99.41ms
step:1641/1750 train_time:163140ms step_avg:99.42ms
step:1642/1750 train_time:163241ms step_avg:99.42ms
step:1643/1750 train_time:163342ms step_avg:99.42ms
step:1644/1750 train_time:163444ms step_avg:99.42ms
step:1645/1750 train_time:163547ms step_avg:99.42ms
step:1646/1750 train_time:163650ms step_avg:99.42ms
step:1647/1750 train_time:163754ms step_avg:99.43ms
step:1648/1750 train_time:163858ms step_avg:99.43ms
step:1649/1750 train_time:163959ms step_avg:99.43ms
step:1650/1750 train_time:164060ms step_avg:99.43ms
step:1651/1750 train_time:164162ms step_avg:99.43ms
step:1652/1750 train_time:164264ms step_avg:99.43ms
step:1653/1750 train_time:164366ms step_avg:99.43ms
step:1654/1750 train_time:164467ms step_avg:99.44ms
step:1655/1750 train_time:164569ms step_avg:99.44ms
step:1656/1750 train_time:164673ms step_avg:99.44ms
step:1657/1750 train_time:164775ms step_avg:99.44ms
step:1658/1750 train_time:164877ms step_avg:99.44ms
step:1659/1750 train_time:164981ms step_avg:99.45ms
step:1660/1750 train_time:165082ms step_avg:99.45ms
step:1661/1750 train_time:165185ms step_avg:99.45ms
step:1662/1750 train_time:165289ms step_avg:99.45ms
step:1663/1750 train_time:165390ms step_avg:99.45ms
step:1664/1750 train_time:165492ms step_avg:99.45ms
step:1665/1750 train_time:165596ms step_avg:99.46ms
step:1666/1750 train_time:165698ms step_avg:99.46ms
step:1667/1750 train_time:165800ms step_avg:99.46ms
step:1668/1750 train_time:165905ms step_avg:99.46ms
step:1669/1750 train_time:166008ms step_avg:99.47ms
step:1670/1750 train_time:166110ms step_avg:99.47ms
step:1671/1750 train_time:166212ms step_avg:99.47ms
step:1672/1750 train_time:166313ms step_avg:99.47ms
step:1673/1750 train_time:166414ms step_avg:99.47ms
step:1674/1750 train_time:166514ms step_avg:99.47ms
step:1675/1750 train_time:166616ms step_avg:99.47ms
step:1676/1750 train_time:166719ms step_avg:99.47ms
step:1677/1750 train_time:166821ms step_avg:99.48ms
step:1678/1750 train_time:166924ms step_avg:99.48ms
step:1679/1750 train_time:167026ms step_avg:99.48ms
step:1680/1750 train_time:167128ms step_avg:99.48ms
step:1681/1750 train_time:167232ms step_avg:99.48ms
step:1682/1750 train_time:167336ms step_avg:99.49ms
step:1683/1750 train_time:167436ms step_avg:99.49ms
step:1684/1750 train_time:167538ms step_avg:99.49ms
step:1685/1750 train_time:167640ms step_avg:99.49ms
step:1686/1750 train_time:167741ms step_avg:99.49ms
step:1687/1750 train_time:167844ms step_avg:99.49ms
step:1688/1750 train_time:167947ms step_avg:99.49ms
step:1689/1750 train_time:168050ms step_avg:99.50ms
step:1690/1750 train_time:168153ms step_avg:99.50ms
step:1691/1750 train_time:168255ms step_avg:99.50ms
step:1692/1750 train_time:168357ms step_avg:99.50ms
step:1693/1750 train_time:168460ms step_avg:99.50ms
step:1694/1750 train_time:168564ms step_avg:99.51ms
step:1695/1750 train_time:168667ms step_avg:99.51ms
step:1696/1750 train_time:168771ms step_avg:99.51ms
step:1697/1750 train_time:168875ms step_avg:99.51ms
step:1698/1750 train_time:168977ms step_avg:99.52ms
step:1699/1750 train_time:169080ms step_avg:99.52ms
step:1700/1750 train_time:169184ms step_avg:99.52ms
step:1701/1750 train_time:169287ms step_avg:99.52ms
step:1702/1750 train_time:169392ms step_avg:99.53ms
step:1703/1750 train_time:169493ms step_avg:99.53ms
step:1704/1750 train_time:169595ms step_avg:99.53ms
step:1705/1750 train_time:169697ms step_avg:99.53ms
step:1706/1750 train_time:169801ms step_avg:99.53ms
step:1707/1750 train_time:169904ms step_avg:99.53ms
step:1708/1750 train_time:170008ms step_avg:99.54ms
step:1709/1750 train_time:170110ms step_avg:99.54ms
step:1710/1750 train_time:170212ms step_avg:99.54ms
step:1711/1750 train_time:170316ms step_avg:99.54ms
step:1712/1750 train_time:170417ms step_avg:99.54ms
step:1713/1750 train_time:170521ms step_avg:99.54ms
step:1714/1750 train_time:170623ms step_avg:99.55ms
step:1715/1750 train_time:170729ms step_avg:99.55ms
step:1716/1750 train_time:170831ms step_avg:99.55ms
step:1717/1750 train_time:170934ms step_avg:99.55ms
step:1718/1750 train_time:171036ms step_avg:99.56ms
step:1719/1750 train_time:171140ms step_avg:99.56ms
step:1720/1750 train_time:171243ms step_avg:99.56ms
step:1721/1750 train_time:171346ms step_avg:99.56ms
step:1722/1750 train_time:171450ms step_avg:99.56ms
step:1723/1750 train_time:171553ms step_avg:99.57ms
step:1724/1750 train_time:171656ms step_avg:99.57ms
step:1725/1750 train_time:171759ms step_avg:99.57ms
step:1726/1750 train_time:171861ms step_avg:99.57ms
step:1727/1750 train_time:171965ms step_avg:99.57ms
step:1728/1750 train_time:172070ms step_avg:99.58ms
step:1729/1750 train_time:172172ms step_avg:99.58ms
step:1730/1750 train_time:172274ms step_avg:99.58ms
step:1731/1750 train_time:172377ms step_avg:99.58ms
step:1732/1750 train_time:172480ms step_avg:99.58ms
step:1733/1750 train_time:172582ms step_avg:99.59ms
step:1734/1750 train_time:172685ms step_avg:99.59ms
step:1735/1750 train_time:172788ms step_avg:99.59ms
step:1736/1750 train_time:172891ms step_avg:99.59ms
step:1737/1750 train_time:172995ms step_avg:99.59ms
step:1738/1750 train_time:173097ms step_avg:99.60ms
step:1739/1750 train_time:173199ms step_avg:99.60ms
step:1740/1750 train_time:173302ms step_avg:99.60ms
step:1741/1750 train_time:173409ms step_avg:99.60ms
step:1742/1750 train_time:173512ms step_avg:99.61ms
step:1743/1750 train_time:173616ms step_avg:99.61ms
step:1744/1750 train_time:173718ms step_avg:99.61ms
step:1745/1750 train_time:173820ms step_avg:99.61ms
step:1746/1750 train_time:173924ms step_avg:99.61ms
step:1747/1750 train_time:174027ms step_avg:99.62ms
step:1748/1750 train_time:174132ms step_avg:99.62ms
step:1749/1750 train_time:174235ms step_avg:99.62ms
step:1750/1750 train_time:174337ms step_avg:99.62ms
step:1750/1750 val_loss:3.2830 train_time:174429ms step_avg:99.67ms
peak memory allocated: 33278 MiB reserved: 48754 MiB
