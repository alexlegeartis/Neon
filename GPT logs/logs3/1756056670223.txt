import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X
'''
import torch.utils.dlpack as thd
from cupyx.scipy.sparse.linalg import svds as cupyx_svds

def several_sv_svds_approximation(W_torch, k, num_iter=50):
    """SVD approximation using the top k singular values and corresponding vectors."""
    # Store original device and dtype
    original_device = W_torch.device
    original_dtype = W_torch.dtype
    
    W = cp.from_dlpack(thd.to_dlpack(W_torch)).astype(cp.float32)
    U, S, Vt = cupyx_svds(W, k=min([k, W.shape[0] - 1, W.shape[1] - 1]), maxiter=num_iter, which='LM')

    # Convert back to torch tensors and ensure they're on the correct device
    approx_torch_U = thd.from_dlpack(U.toDlpack()).to(device=original_device, dtype=original_dtype)
    approx_torch_S = thd.from_dlpack(S.toDlpack()).to(device=original_device, dtype=original_dtype)
    approx_torch_Vt = thd.from_dlpack(Vt.toDlpack()).to(device=original_device, dtype=original_dtype)
    
    return approx_torch_U, approx_torch_S, approx_torch_Vt

class NormNeon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    # v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    u, s, vt = several_sv_svds_approximation(grad.bfloat16(), 5)
                    p.add_(other=u@vt, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()
'''
class FastNormMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, sgd_coeff=0):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, sgd_coeff=sgd_coeff)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        self.sgd_coeff = sgd_coeff

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            sgd_coeff = group["sgd_coeff"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    # assume: grad is a Tensor, momentum_buffer exists, momentum, sgd_coeff, eps, eff_lr are defined
                    with torch.no_grad():
                        # inplace blend with momentum (keeps original semantic: grad becomes the lerped tensor)
                        grad.lerp_(momentum_buffer, momentum)

                        # compute zeropower part from the (post-lerp) grad, in bfloat16 to match the original code
                        if sgd_coeff != 1:
                            # create update_part in bfloat16 (may allocate once per call; unavoidable if zeropower expects bfloat16)
                            update_part = zeropower_via_newtonschulz5(grad.to(torch.bfloat16), 5)

                            # upcast update_part to grad dtype to do mixed-dtype blending without extra temporaries later
                            if update_part.dtype != grad.dtype:
                                update_part = update_part.to(grad.dtype)

                        # compute inverse norm scalar efficiently (no large temp tensors)
                        # - view(-1) is cheap (no copy)
                        # - dot(view, view) is a single reduction
                        norm_sq = grad.view(-1).dot(grad.view(-1))
                        inv_norm = (norm_sq + eps).rsqrt()  # stable rsqrt on the scalar

                        # normalize grad in-place (so we avoid allocating g_normalized)
                        grad.mul_(inv_norm)   # now grad == g_normalized (in-place)

                        # build final update (reuse update_part buffer if available)
                        if sgd_coeff != 1:
                            # update_part := (1 - sgd_coeff) * update_part + sgd_coeff * grad
                            # do it in-place on update_part to avoid creating another temp
                            update_part.mul_(1 - sgd_coeff)    # in-place scale
                            update_part.lerp_(grad, sgd_coeff) # in-place linear interpolation
                            update = update_part
                        else:
                            # if sgd_coeff == 1 we can use grad directly (already normalized in-place)
                            update = grad

                        # apply update (same as original)
                        p.add_(other=update, alpha=-eff_lr)
                    '''
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    eps = 1e-12
                    g_normalized = grad / (grad.norm() + eps)
                    if sgd_coeff != 1:
                        update_part = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                        update = (1 - sgd_coeff) * update_part + sgd_coeff * g_normalized
                    else:
                        update = sgd_coeff * g_normalized
                    p.add_(other=update, alpha=-eff_lr)
                    '''
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class NormMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, sgd_coeff=0):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, sgd_coeff=sgd_coeff)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        self.sgd_coeff = sgd_coeff

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            sgd_coeff = group["sgd_coeff"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    eps = 1e-12
                    g_normalized = grad / (grad.norm() + eps)
                    if sgd_coeff != 1:
                        update_part = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                        update = (1 - sgd_coeff) * update_part + sgd_coeff * g_normalized
                    else:
                        update = sgd_coeff * g_normalized
                    p.add_(other=update, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()
class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)
                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_reduce_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 27.5
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos : pos + max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()
    starts = []
    for i in range(1, len(boundary_positions)):
        end = boundary_positions[i].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == dist.get_world_size():
                return starts, end - pos
            start = end
    assert False # increase max_batch_span if necessary

def distributed_data_generator(filename_pattern: str, batch_size: int, align_to_bos: bool):
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    max_batch_span = 2 * batch_size if align_to_bos else batch_size # provide buffer to handle samples up to length local_batch_size
    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            batch_span = batch_size
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1750 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
from datetime import datetime
if master_process:
    # run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_id = str(int(time.time() * 1000))
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0)
optimizer2 = NormMuon(hidden_matrix_params, lr=0.4, momentum=0.9, weight_decay=0.0, sgd_coeff=0.5)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.11 | packaged by conda-forge | (main, Jun  4 2025, 14:45:31) [GCC 13.3.0]
Running PyTorch 2.9.0.dev20250824+cu126 compiled for CUDA 12.6
Sun Aug 24 17:31:10 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:04:00.0 Off |                    0 |
| N/A   39C    P0            121W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:23:00.0 Off |                    0 |
| N/A   33C    P0            114W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:43:00.0 Off |                    0 |
| N/A   37C    P0            124W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   32C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:84:00.0 Off |                    0 |
| N/A   38C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:A3:00.0 Off |                    0 |
| N/A   33C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:C3:00.0 Off |                    0 |
| N/A   39C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E4:00.0 Off |                    0 |
| N/A   32C    P0            113W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.06ms
step:1/1750 train_time:141ms step_avg:141.11ms
step:2/1750 train_time:163ms step_avg:81.31ms
step:3/1750 train_time:243ms step_avg:81.02ms
step:4/1750 train_time:334ms step_avg:83.60ms
step:5/1750 train_time:427ms step_avg:85.34ms
step:6/1750 train_time:519ms step_avg:86.46ms
step:7/1750 train_time:611ms step_avg:87.22ms
step:8/1750 train_time:703ms step_avg:87.82ms
step:9/1750 train_time:795ms step_avg:88.30ms
step:10/1750 train_time:888ms step_avg:88.77ms
step:11/1750 train_time:979ms step_avg:89.04ms
step:12/1750 train_time:1074ms step_avg:89.51ms
step:13/1750 train_time:1171ms step_avg:90.10ms
step:14/1750 train_time:1265ms step_avg:90.35ms
step:15/1750 train_time:1358ms step_avg:90.53ms
step:16/1750 train_time:1452ms step_avg:90.72ms
step:17/1750 train_time:1544ms step_avg:90.85ms
step:18/1750 train_time:1637ms step_avg:90.93ms
step:19/1750 train_time:1729ms step_avg:90.99ms
step:20/1750 train_time:1821ms step_avg:91.04ms
step:21/1750 train_time:1913ms step_avg:91.10ms
step:22/1750 train_time:2007ms step_avg:91.24ms
step:23/1750 train_time:2101ms step_avg:91.37ms
step:24/1750 train_time:2195ms step_avg:91.46ms
step:25/1750 train_time:2288ms step_avg:91.54ms
step:26/1750 train_time:2382ms step_avg:91.60ms
step:27/1750 train_time:2475ms step_avg:91.65ms
step:28/1750 train_time:2567ms step_avg:91.68ms
step:29/1750 train_time:2660ms step_avg:91.71ms
step:30/1750 train_time:2752ms step_avg:91.74ms
step:31/1750 train_time:2845ms step_avg:91.76ms
step:32/1750 train_time:2937ms step_avg:91.80ms
step:33/1750 train_time:3030ms step_avg:91.81ms
step:34/1750 train_time:3122ms step_avg:91.83ms
step:35/1750 train_time:3216ms step_avg:91.89ms
step:36/1750 train_time:3309ms step_avg:91.92ms
step:37/1750 train_time:3403ms step_avg:91.98ms
step:38/1750 train_time:3496ms step_avg:92.00ms
step:39/1750 train_time:3589ms step_avg:92.03ms
step:40/1750 train_time:3681ms step_avg:92.02ms
step:41/1750 train_time:3774ms step_avg:92.05ms
step:42/1750 train_time:3866ms step_avg:92.04ms
step:43/1750 train_time:3959ms step_avg:92.06ms
step:44/1750 train_time:4052ms step_avg:92.10ms
step:45/1750 train_time:4145ms step_avg:92.10ms
step:46/1750 train_time:4239ms step_avg:92.14ms
step:47/1750 train_time:4333ms step_avg:92.19ms
step:48/1750 train_time:4426ms step_avg:92.22ms
step:49/1750 train_time:4519ms step_avg:92.23ms
step:50/1750 train_time:4612ms step_avg:92.24ms
step:51/1750 train_time:4705ms step_avg:92.25ms
step:52/1750 train_time:4797ms step_avg:92.25ms
step:53/1750 train_time:4890ms step_avg:92.27ms
step:54/1750 train_time:4982ms step_avg:92.26ms
step:55/1750 train_time:5075ms step_avg:92.28ms
step:56/1750 train_time:5169ms step_avg:92.30ms
step:57/1750 train_time:5262ms step_avg:92.31ms
step:58/1750 train_time:5354ms step_avg:92.32ms
step:59/1750 train_time:5447ms step_avg:92.33ms
step:60/1750 train_time:5540ms step_avg:92.34ms
step:61/1750 train_time:5633ms step_avg:92.35ms
step:62/1750 train_time:5726ms step_avg:92.36ms
step:63/1750 train_time:5819ms step_avg:92.36ms
step:64/1750 train_time:5912ms step_avg:92.37ms
step:65/1750 train_time:6004ms step_avg:92.37ms
step:66/1750 train_time:6098ms step_avg:92.39ms
step:67/1750 train_time:6192ms step_avg:92.41ms
step:68/1750 train_time:6285ms step_avg:92.42ms
step:69/1750 train_time:6378ms step_avg:92.43ms
step:70/1750 train_time:6471ms step_avg:92.44ms
step:71/1750 train_time:6564ms step_avg:92.45ms
step:72/1750 train_time:6657ms step_avg:92.45ms
step:73/1750 train_time:6749ms step_avg:92.45ms
step:74/1750 train_time:6843ms step_avg:92.47ms
step:75/1750 train_time:6936ms step_avg:92.48ms
step:76/1750 train_time:7029ms step_avg:92.48ms
step:77/1750 train_time:7122ms step_avg:92.49ms
step:78/1750 train_time:7215ms step_avg:92.50ms
step:79/1750 train_time:7308ms step_avg:92.51ms
step:80/1750 train_time:7402ms step_avg:92.52ms
step:81/1750 train_time:7495ms step_avg:92.53ms
step:82/1750 train_time:7588ms step_avg:92.54ms
step:83/1750 train_time:7681ms step_avg:92.54ms
step:84/1750 train_time:7774ms step_avg:92.55ms
step:85/1750 train_time:7868ms step_avg:92.56ms
step:86/1750 train_time:7961ms step_avg:92.57ms
step:87/1750 train_time:8054ms step_avg:92.57ms
step:88/1750 train_time:8147ms step_avg:92.58ms
step:89/1750 train_time:8240ms step_avg:92.59ms
step:90/1750 train_time:8334ms step_avg:92.60ms
step:91/1750 train_time:8427ms step_avg:92.60ms
step:92/1750 train_time:8520ms step_avg:92.61ms
step:93/1750 train_time:8613ms step_avg:92.61ms
step:94/1750 train_time:8707ms step_avg:92.62ms
step:95/1750 train_time:8799ms step_avg:92.63ms
step:96/1750 train_time:8893ms step_avg:92.63ms
step:97/1750 train_time:8986ms step_avg:92.64ms
step:98/1750 train_time:9079ms step_avg:92.65ms
step:99/1750 train_time:9173ms step_avg:92.66ms
step:100/1750 train_time:9266ms step_avg:92.66ms
step:101/1750 train_time:9360ms step_avg:92.67ms
step:102/1750 train_time:9454ms step_avg:92.68ms
step:103/1750 train_time:9546ms step_avg:92.68ms
step:104/1750 train_time:9640ms step_avg:92.69ms
step:105/1750 train_time:9733ms step_avg:92.70ms
step:106/1750 train_time:9827ms step_avg:92.71ms
step:107/1750 train_time:9919ms step_avg:92.70ms
step:108/1750 train_time:10012ms step_avg:92.70ms
step:109/1750 train_time:10105ms step_avg:92.71ms
step:110/1750 train_time:10199ms step_avg:92.71ms
step:111/1750 train_time:10292ms step_avg:92.72ms
step:112/1750 train_time:10385ms step_avg:92.73ms
step:113/1750 train_time:10479ms step_avg:92.73ms
step:114/1750 train_time:10572ms step_avg:92.74ms
step:115/1750 train_time:10665ms step_avg:92.74ms
step:116/1750 train_time:10760ms step_avg:92.75ms
step:117/1750 train_time:10852ms step_avg:92.76ms
step:118/1750 train_time:10945ms step_avg:92.75ms
step:119/1750 train_time:11039ms step_avg:92.76ms
step:120/1750 train_time:11132ms step_avg:92.76ms
step:121/1750 train_time:11224ms step_avg:92.76ms
step:122/1750 train_time:11317ms step_avg:92.76ms
step:123/1750 train_time:11410ms step_avg:92.77ms
step:124/1750 train_time:11503ms step_avg:92.77ms
step:125/1750 train_time:11597ms step_avg:92.77ms
step:125/1750 val_loss:4.8336 train_time:11680ms step_avg:93.44ms
step:126/1750 train_time:11703ms step_avg:92.88ms
step:127/1750 train_time:11793ms step_avg:92.86ms
step:128/1750 train_time:11892ms step_avg:92.90ms
step:129/1750 train_time:11986ms step_avg:92.92ms
step:130/1750 train_time:12079ms step_avg:92.92ms
step:131/1750 train_time:12171ms step_avg:92.91ms
step:132/1750 train_time:12264ms step_avg:92.91ms
step:133/1750 train_time:12357ms step_avg:92.91ms
step:134/1750 train_time:12449ms step_avg:92.90ms
step:135/1750 train_time:12542ms step_avg:92.90ms
step:136/1750 train_time:12635ms step_avg:92.91ms
step:137/1750 train_time:12730ms step_avg:92.92ms
step:138/1750 train_time:12826ms step_avg:92.95ms
step:139/1750 train_time:12921ms step_avg:92.96ms
step:140/1750 train_time:13014ms step_avg:92.96ms
step:141/1750 train_time:13108ms step_avg:92.97ms
step:142/1750 train_time:13202ms step_avg:92.97ms
step:143/1750 train_time:13295ms step_avg:92.98ms
step:144/1750 train_time:13388ms step_avg:92.97ms
step:145/1750 train_time:13481ms step_avg:92.97ms
step:146/1750 train_time:13574ms step_avg:92.97ms
step:147/1750 train_time:13667ms step_avg:92.97ms
step:148/1750 train_time:13762ms step_avg:92.98ms
step:149/1750 train_time:13857ms step_avg:93.00ms
step:150/1750 train_time:13951ms step_avg:93.01ms
step:151/1750 train_time:14046ms step_avg:93.02ms
step:152/1750 train_time:14141ms step_avg:93.03ms
step:153/1750 train_time:14234ms step_avg:93.03ms
step:154/1750 train_time:14327ms step_avg:93.03ms
step:155/1750 train_time:14420ms step_avg:93.03ms
step:156/1750 train_time:14513ms step_avg:93.03ms
step:157/1750 train_time:14606ms step_avg:93.03ms
step:158/1750 train_time:14699ms step_avg:93.03ms
step:159/1750 train_time:14794ms step_avg:93.04ms
step:160/1750 train_time:14888ms step_avg:93.05ms
step:161/1750 train_time:14982ms step_avg:93.06ms
step:162/1750 train_time:15076ms step_avg:93.06ms
step:163/1750 train_time:15170ms step_avg:93.07ms
step:164/1750 train_time:15264ms step_avg:93.07ms
step:165/1750 train_time:15357ms step_avg:93.07ms
step:166/1750 train_time:15450ms step_avg:93.07ms
step:167/1750 train_time:15543ms step_avg:93.07ms
step:168/1750 train_time:15636ms step_avg:93.07ms
step:169/1750 train_time:15730ms step_avg:93.07ms
step:170/1750 train_time:15823ms step_avg:93.08ms
step:171/1750 train_time:15918ms step_avg:93.09ms
step:172/1750 train_time:16011ms step_avg:93.09ms
step:173/1750 train_time:16104ms step_avg:93.09ms
step:174/1750 train_time:16197ms step_avg:93.09ms
step:175/1750 train_time:16291ms step_avg:93.09ms
step:176/1750 train_time:16385ms step_avg:93.09ms
step:177/1750 train_time:16478ms step_avg:93.10ms
step:178/1750 train_time:16571ms step_avg:93.10ms
step:179/1750 train_time:16666ms step_avg:93.11ms
step:180/1750 train_time:16760ms step_avg:93.11ms
step:181/1750 train_time:16854ms step_avg:93.11ms
step:182/1750 train_time:16948ms step_avg:93.12ms
step:183/1750 train_time:17042ms step_avg:93.13ms
step:184/1750 train_time:17135ms step_avg:93.12ms
step:185/1750 train_time:17228ms step_avg:93.13ms
step:186/1750 train_time:17322ms step_avg:93.13ms
step:187/1750 train_time:17415ms step_avg:93.13ms
step:188/1750 train_time:17508ms step_avg:93.13ms
step:189/1750 train_time:17602ms step_avg:93.13ms
step:190/1750 train_time:17695ms step_avg:93.13ms
step:191/1750 train_time:17789ms step_avg:93.14ms
step:192/1750 train_time:17884ms step_avg:93.14ms
step:193/1750 train_time:17977ms step_avg:93.15ms
step:194/1750 train_time:18071ms step_avg:93.15ms
step:195/1750 train_time:18165ms step_avg:93.15ms
step:196/1750 train_time:18258ms step_avg:93.15ms
step:197/1750 train_time:18351ms step_avg:93.15ms
step:198/1750 train_time:18444ms step_avg:93.15ms
step:199/1750 train_time:18538ms step_avg:93.15ms
step:200/1750 train_time:18632ms step_avg:93.16ms
step:201/1750 train_time:18725ms step_avg:93.16ms
step:202/1750 train_time:18819ms step_avg:93.16ms
step:203/1750 train_time:18913ms step_avg:93.17ms
step:204/1750 train_time:19007ms step_avg:93.17ms
step:205/1750 train_time:19100ms step_avg:93.17ms
step:206/1750 train_time:19194ms step_avg:93.17ms
step:207/1750 train_time:19288ms step_avg:93.18ms
step:208/1750 train_time:19381ms step_avg:93.18ms
step:209/1750 train_time:19474ms step_avg:93.18ms
step:210/1750 train_time:19568ms step_avg:93.18ms
step:211/1750 train_time:19661ms step_avg:93.18ms
step:212/1750 train_time:19755ms step_avg:93.19ms
step:213/1750 train_time:19849ms step_avg:93.19ms
step:214/1750 train_time:19943ms step_avg:93.19ms
step:215/1750 train_time:20037ms step_avg:93.19ms
step:216/1750 train_time:20130ms step_avg:93.19ms
step:217/1750 train_time:20224ms step_avg:93.20ms
step:218/1750 train_time:20317ms step_avg:93.20ms
step:219/1750 train_time:20410ms step_avg:93.20ms
step:220/1750 train_time:20503ms step_avg:93.20ms
step:221/1750 train_time:20596ms step_avg:93.20ms
step:222/1750 train_time:20690ms step_avg:93.20ms
step:223/1750 train_time:20784ms step_avg:93.20ms
step:224/1750 train_time:20877ms step_avg:93.20ms
step:225/1750 train_time:20971ms step_avg:93.21ms
step:226/1750 train_time:21065ms step_avg:93.21ms
step:227/1750 train_time:21159ms step_avg:93.21ms
step:228/1750 train_time:21253ms step_avg:93.21ms
step:229/1750 train_time:21346ms step_avg:93.21ms
step:230/1750 train_time:21439ms step_avg:93.21ms
step:231/1750 train_time:21533ms step_avg:93.22ms
step:232/1750 train_time:21626ms step_avg:93.21ms
step:233/1750 train_time:21720ms step_avg:93.22ms
step:234/1750 train_time:21814ms step_avg:93.22ms
step:235/1750 train_time:21907ms step_avg:93.22ms
step:236/1750 train_time:22001ms step_avg:93.22ms
step:237/1750 train_time:22094ms step_avg:93.22ms
step:238/1750 train_time:22187ms step_avg:93.22ms
step:239/1750 train_time:22281ms step_avg:93.22ms
step:240/1750 train_time:22374ms step_avg:93.23ms
step:241/1750 train_time:22468ms step_avg:93.23ms
step:242/1750 train_time:22561ms step_avg:93.23ms
step:243/1750 train_time:22654ms step_avg:93.23ms
step:244/1750 train_time:22748ms step_avg:93.23ms
step:245/1750 train_time:22842ms step_avg:93.23ms
step:246/1750 train_time:22935ms step_avg:93.23ms
step:247/1750 train_time:23029ms step_avg:93.24ms
step:248/1750 train_time:23124ms step_avg:93.24ms
step:249/1750 train_time:23218ms step_avg:93.24ms
step:250/1750 train_time:23311ms step_avg:93.24ms
step:250/1750 val_loss:4.1921 train_time:23395ms step_avg:93.58ms
step:251/1750 train_time:23419ms step_avg:93.30ms
step:252/1750 train_time:23504ms step_avg:93.27ms
step:253/1750 train_time:23600ms step_avg:93.28ms
step:254/1750 train_time:23695ms step_avg:93.29ms
step:255/1750 train_time:23787ms step_avg:93.28ms
step:256/1750 train_time:23881ms step_avg:93.28ms
step:257/1750 train_time:23974ms step_avg:93.28ms
step:258/1750 train_time:24067ms step_avg:93.28ms
step:259/1750 train_time:24160ms step_avg:93.28ms
step:260/1750 train_time:24252ms step_avg:93.28ms
step:261/1750 train_time:24346ms step_avg:93.28ms
step:262/1750 train_time:24441ms step_avg:93.29ms
step:263/1750 train_time:24537ms step_avg:93.29ms
step:264/1750 train_time:24631ms step_avg:93.30ms
step:265/1750 train_time:24725ms step_avg:93.30ms
step:266/1750 train_time:24819ms step_avg:93.30ms
step:267/1750 train_time:24912ms step_avg:93.30ms
step:268/1750 train_time:25006ms step_avg:93.31ms
step:269/1750 train_time:25100ms step_avg:93.31ms
step:270/1750 train_time:25193ms step_avg:93.31ms
step:271/1750 train_time:25287ms step_avg:93.31ms
step:272/1750 train_time:25382ms step_avg:93.31ms
step:273/1750 train_time:25476ms step_avg:93.32ms
step:274/1750 train_time:25571ms step_avg:93.32ms
step:275/1750 train_time:25665ms step_avg:93.33ms
step:276/1750 train_time:25760ms step_avg:93.33ms
step:277/1750 train_time:25854ms step_avg:93.34ms
step:278/1750 train_time:25948ms step_avg:93.34ms
step:279/1750 train_time:26042ms step_avg:93.34ms
step:280/1750 train_time:26136ms step_avg:93.34ms
step:281/1750 train_time:26229ms step_avg:93.34ms
step:282/1750 train_time:26323ms step_avg:93.34ms
step:283/1750 train_time:26417ms step_avg:93.34ms
step:284/1750 train_time:26511ms step_avg:93.35ms
step:285/1750 train_time:26606ms step_avg:93.36ms
step:286/1750 train_time:26702ms step_avg:93.36ms
step:287/1750 train_time:26797ms step_avg:93.37ms
step:288/1750 train_time:26891ms step_avg:93.37ms
step:289/1750 train_time:26985ms step_avg:93.37ms
step:290/1750 train_time:27078ms step_avg:93.37ms
step:291/1750 train_time:27171ms step_avg:93.37ms
step:292/1750 train_time:27265ms step_avg:93.37ms
step:293/1750 train_time:27358ms step_avg:93.37ms
step:294/1750 train_time:27452ms step_avg:93.37ms
step:295/1750 train_time:27547ms step_avg:93.38ms
step:296/1750 train_time:27642ms step_avg:93.38ms
step:297/1750 train_time:27736ms step_avg:93.39ms
step:298/1750 train_time:27830ms step_avg:93.39ms
step:299/1750 train_time:27925ms step_avg:93.39ms
step:300/1750 train_time:28019ms step_avg:93.40ms
step:301/1750 train_time:28113ms step_avg:93.40ms
step:302/1750 train_time:28207ms step_avg:93.40ms
step:303/1750 train_time:28301ms step_avg:93.40ms
step:304/1750 train_time:28395ms step_avg:93.40ms
step:305/1750 train_time:28489ms step_avg:93.41ms
step:306/1750 train_time:28583ms step_avg:93.41ms
step:307/1750 train_time:28678ms step_avg:93.41ms
step:308/1750 train_time:28772ms step_avg:93.41ms
step:309/1750 train_time:28866ms step_avg:93.42ms
step:310/1750 train_time:28960ms step_avg:93.42ms
step:311/1750 train_time:29054ms step_avg:93.42ms
step:312/1750 train_time:29148ms step_avg:93.42ms
step:313/1750 train_time:29241ms step_avg:93.42ms
step:314/1750 train_time:29335ms step_avg:93.42ms
step:315/1750 train_time:29429ms step_avg:93.43ms
step:316/1750 train_time:29522ms step_avg:93.43ms
step:317/1750 train_time:29616ms step_avg:93.43ms
step:318/1750 train_time:29710ms step_avg:93.43ms
step:319/1750 train_time:29804ms step_avg:93.43ms
step:320/1750 train_time:29898ms step_avg:93.43ms
step:321/1750 train_time:29992ms step_avg:93.43ms
step:322/1750 train_time:30086ms step_avg:93.43ms
step:323/1750 train_time:30180ms step_avg:93.44ms
step:324/1750 train_time:30274ms step_avg:93.44ms
step:325/1750 train_time:30369ms step_avg:93.44ms
step:326/1750 train_time:30463ms step_avg:93.45ms
step:327/1750 train_time:30558ms step_avg:93.45ms
step:328/1750 train_time:30651ms step_avg:93.45ms
step:329/1750 train_time:30745ms step_avg:93.45ms
step:330/1750 train_time:30839ms step_avg:93.45ms
step:331/1750 train_time:30933ms step_avg:93.45ms
step:332/1750 train_time:31027ms step_avg:93.45ms
step:333/1750 train_time:31121ms step_avg:93.46ms
step:334/1750 train_time:31215ms step_avg:93.46ms
step:335/1750 train_time:31309ms step_avg:93.46ms
step:336/1750 train_time:31403ms step_avg:93.46ms
step:337/1750 train_time:31497ms step_avg:93.46ms
step:338/1750 train_time:31590ms step_avg:93.46ms
step:339/1750 train_time:31684ms step_avg:93.46ms
step:340/1750 train_time:31779ms step_avg:93.47ms
step:341/1750 train_time:31873ms step_avg:93.47ms
step:342/1750 train_time:31967ms step_avg:93.47ms
step:343/1750 train_time:32062ms step_avg:93.47ms
step:344/1750 train_time:32156ms step_avg:93.48ms
step:345/1750 train_time:32249ms step_avg:93.48ms
step:346/1750 train_time:32343ms step_avg:93.48ms
step:347/1750 train_time:32436ms step_avg:93.48ms
step:348/1750 train_time:32530ms step_avg:93.48ms
step:349/1750 train_time:32624ms step_avg:93.48ms
step:350/1750 train_time:32718ms step_avg:93.48ms
step:351/1750 train_time:32812ms step_avg:93.48ms
step:352/1750 train_time:32905ms step_avg:93.48ms
step:353/1750 train_time:33000ms step_avg:93.48ms
step:354/1750 train_time:33094ms step_avg:93.49ms
step:355/1750 train_time:33188ms step_avg:93.49ms
step:356/1750 train_time:33282ms step_avg:93.49ms
step:357/1750 train_time:33375ms step_avg:93.49ms
step:358/1750 train_time:33469ms step_avg:93.49ms
step:359/1750 train_time:33563ms step_avg:93.49ms
step:360/1750 train_time:33658ms step_avg:93.49ms
step:361/1750 train_time:33752ms step_avg:93.49ms
step:362/1750 train_time:33846ms step_avg:93.50ms
step:363/1750 train_time:33940ms step_avg:93.50ms
step:364/1750 train_time:34034ms step_avg:93.50ms
step:365/1750 train_time:34129ms step_avg:93.50ms
step:366/1750 train_time:34223ms step_avg:93.50ms
step:367/1750 train_time:34316ms step_avg:93.51ms
step:368/1750 train_time:34411ms step_avg:93.51ms
step:369/1750 train_time:34504ms step_avg:93.51ms
step:370/1750 train_time:34599ms step_avg:93.51ms
step:371/1750 train_time:34692ms step_avg:93.51ms
step:372/1750 train_time:34787ms step_avg:93.51ms
step:373/1750 train_time:34881ms step_avg:93.51ms
step:374/1750 train_time:34975ms step_avg:93.52ms
step:375/1750 train_time:35069ms step_avg:93.52ms
step:375/1750 val_loss:3.9706 train_time:35153ms step_avg:93.74ms
step:376/1750 train_time:35175ms step_avg:93.55ms
step:377/1750 train_time:35266ms step_avg:93.54ms
step:378/1750 train_time:35363ms step_avg:93.55ms
step:379/1750 train_time:35458ms step_avg:93.56ms
step:380/1750 train_time:35551ms step_avg:93.55ms
step:381/1750 train_time:35643ms step_avg:93.55ms
step:382/1750 train_time:35737ms step_avg:93.55ms
step:383/1750 train_time:35830ms step_avg:93.55ms
step:384/1750 train_time:35923ms step_avg:93.55ms
step:385/1750 train_time:36017ms step_avg:93.55ms
step:386/1750 train_time:36111ms step_avg:93.55ms
step:387/1750 train_time:36208ms step_avg:93.56ms
step:388/1750 train_time:36303ms step_avg:93.56ms
step:389/1750 train_time:36398ms step_avg:93.57ms
step:390/1750 train_time:36493ms step_avg:93.57ms
step:391/1750 train_time:36589ms step_avg:93.58ms
step:392/1750 train_time:36684ms step_avg:93.58ms
step:393/1750 train_time:36780ms step_avg:93.59ms
step:394/1750 train_time:36874ms step_avg:93.59ms
step:395/1750 train_time:36970ms step_avg:93.59ms
step:396/1750 train_time:37064ms step_avg:93.60ms
step:397/1750 train_time:37161ms step_avg:93.61ms
step:398/1750 train_time:37258ms step_avg:93.61ms
step:399/1750 train_time:37355ms step_avg:93.62ms
step:400/1750 train_time:37451ms step_avg:93.63ms
step:401/1750 train_time:37547ms step_avg:93.63ms
step:402/1750 train_time:37643ms step_avg:93.64ms
step:403/1750 train_time:37738ms step_avg:93.64ms
step:404/1750 train_time:37834ms step_avg:93.65ms
step:405/1750 train_time:37930ms step_avg:93.65ms
step:406/1750 train_time:38025ms step_avg:93.66ms
step:407/1750 train_time:38120ms step_avg:93.66ms
step:408/1750 train_time:38217ms step_avg:93.67ms
step:409/1750 train_time:38313ms step_avg:93.67ms
step:410/1750 train_time:38409ms step_avg:93.68ms
step:411/1750 train_time:38505ms step_avg:93.69ms
step:412/1750 train_time:38600ms step_avg:93.69ms
step:413/1750 train_time:38696ms step_avg:93.70ms
step:414/1750 train_time:38792ms step_avg:93.70ms
step:415/1750 train_time:38888ms step_avg:93.71ms
step:416/1750 train_time:38983ms step_avg:93.71ms
step:417/1750 train_time:39078ms step_avg:93.71ms
step:418/1750 train_time:39174ms step_avg:93.72ms
step:419/1750 train_time:39271ms step_avg:93.73ms
step:420/1750 train_time:39367ms step_avg:93.73ms
step:421/1750 train_time:39464ms step_avg:93.74ms
step:422/1750 train_time:39559ms step_avg:93.74ms
step:423/1750 train_time:39656ms step_avg:93.75ms
step:424/1750 train_time:39753ms step_avg:93.76ms
step:425/1750 train_time:39849ms step_avg:93.76ms
step:426/1750 train_time:39945ms step_avg:93.77ms
step:427/1750 train_time:40040ms step_avg:93.77ms
step:428/1750 train_time:40136ms step_avg:93.78ms
step:429/1750 train_time:40232ms step_avg:93.78ms
step:430/1750 train_time:40328ms step_avg:93.79ms
step:431/1750 train_time:40425ms step_avg:93.79ms
step:432/1750 train_time:40520ms step_avg:93.80ms
step:433/1750 train_time:40616ms step_avg:93.80ms
step:434/1750 train_time:40712ms step_avg:93.81ms
step:435/1750 train_time:40807ms step_avg:93.81ms
step:436/1750 train_time:40903ms step_avg:93.81ms
step:437/1750 train_time:40998ms step_avg:93.82ms
step:438/1750 train_time:41094ms step_avg:93.82ms
step:439/1750 train_time:41189ms step_avg:93.83ms
step:440/1750 train_time:41285ms step_avg:93.83ms
step:441/1750 train_time:41381ms step_avg:93.83ms
step:442/1750 train_time:41477ms step_avg:93.84ms
step:443/1750 train_time:41573ms step_avg:93.84ms
step:444/1750 train_time:41668ms step_avg:93.85ms
step:445/1750 train_time:41764ms step_avg:93.85ms
step:446/1750 train_time:41861ms step_avg:93.86ms
step:447/1750 train_time:41957ms step_avg:93.86ms
step:448/1750 train_time:42053ms step_avg:93.87ms
step:449/1750 train_time:42148ms step_avg:93.87ms
step:450/1750 train_time:42245ms step_avg:93.88ms
step:451/1750 train_time:42341ms step_avg:93.88ms
step:452/1750 train_time:42437ms step_avg:93.89ms
step:453/1750 train_time:42533ms step_avg:93.89ms
step:454/1750 train_time:42628ms step_avg:93.90ms
step:455/1750 train_time:42725ms step_avg:93.90ms
step:456/1750 train_time:42821ms step_avg:93.91ms
step:457/1750 train_time:42918ms step_avg:93.91ms
step:458/1750 train_time:43014ms step_avg:93.92ms
step:459/1750 train_time:43109ms step_avg:93.92ms
step:460/1750 train_time:43205ms step_avg:93.92ms
step:461/1750 train_time:43301ms step_avg:93.93ms
step:462/1750 train_time:43397ms step_avg:93.93ms
step:463/1750 train_time:43494ms step_avg:93.94ms
step:464/1750 train_time:43590ms step_avg:93.94ms
step:465/1750 train_time:43685ms step_avg:93.95ms
step:466/1750 train_time:43781ms step_avg:93.95ms
step:467/1750 train_time:43877ms step_avg:93.96ms
step:468/1750 train_time:43973ms step_avg:93.96ms
step:469/1750 train_time:44068ms step_avg:93.96ms
step:470/1750 train_time:44164ms step_avg:93.97ms
step:471/1750 train_time:44260ms step_avg:93.97ms
step:472/1750 train_time:44356ms step_avg:93.98ms
step:473/1750 train_time:44452ms step_avg:93.98ms
step:474/1750 train_time:44548ms step_avg:93.98ms
step:475/1750 train_time:44644ms step_avg:93.99ms
step:476/1750 train_time:44740ms step_avg:93.99ms
step:477/1750 train_time:44836ms step_avg:94.00ms
step:478/1750 train_time:44931ms step_avg:94.00ms
step:479/1750 train_time:45027ms step_avg:94.00ms
step:480/1750 train_time:45123ms step_avg:94.01ms
step:481/1750 train_time:45219ms step_avg:94.01ms
step:482/1750 train_time:45315ms step_avg:94.01ms
step:483/1750 train_time:45411ms step_avg:94.02ms
step:484/1750 train_time:45507ms step_avg:94.02ms
step:485/1750 train_time:45603ms step_avg:94.03ms
step:486/1750 train_time:45700ms step_avg:94.03ms
step:487/1750 train_time:45796ms step_avg:94.04ms
step:488/1750 train_time:45892ms step_avg:94.04ms
step:489/1750 train_time:45988ms step_avg:94.04ms
step:490/1750 train_time:46083ms step_avg:94.05ms
step:491/1750 train_time:46178ms step_avg:94.05ms
step:492/1750 train_time:46275ms step_avg:94.05ms
step:493/1750 train_time:46370ms step_avg:94.06ms
step:494/1750 train_time:46466ms step_avg:94.06ms
step:495/1750 train_time:46562ms step_avg:94.06ms
step:496/1750 train_time:46658ms step_avg:94.07ms
step:497/1750 train_time:46754ms step_avg:94.07ms
step:498/1750 train_time:46850ms step_avg:94.08ms
step:499/1750 train_time:46946ms step_avg:94.08ms
step:500/1750 train_time:47041ms step_avg:94.08ms
step:500/1750 val_loss:3.8238 train_time:47128ms step_avg:94.26ms
step:501/1750 train_time:47151ms step_avg:94.11ms
step:502/1750 train_time:47239ms step_avg:94.10ms
step:503/1750 train_time:47341ms step_avg:94.12ms
step:504/1750 train_time:47438ms step_avg:94.12ms
step:505/1750 train_time:47533ms step_avg:94.13ms
step:506/1750 train_time:47628ms step_avg:94.13ms
step:507/1750 train_time:47723ms step_avg:94.13ms
step:508/1750 train_time:47818ms step_avg:94.13ms
step:509/1750 train_time:47913ms step_avg:94.13ms
step:510/1750 train_time:48008ms step_avg:94.13ms
step:511/1750 train_time:48104ms step_avg:94.14ms
step:512/1750 train_time:48201ms step_avg:94.14ms
step:513/1750 train_time:48300ms step_avg:94.15ms
step:514/1750 train_time:48398ms step_avg:94.16ms
step:515/1750 train_time:48494ms step_avg:94.16ms
step:516/1750 train_time:48590ms step_avg:94.17ms
step:517/1750 train_time:48685ms step_avg:94.17ms
step:518/1750 train_time:48780ms step_avg:94.17ms
step:519/1750 train_time:48876ms step_avg:94.17ms
step:520/1750 train_time:48971ms step_avg:94.17ms
step:521/1750 train_time:49067ms step_avg:94.18ms
step:522/1750 train_time:49163ms step_avg:94.18ms
step:523/1750 train_time:49261ms step_avg:94.19ms
step:524/1750 train_time:49358ms step_avg:94.20ms
step:525/1750 train_time:49455ms step_avg:94.20ms
step:526/1750 train_time:49552ms step_avg:94.20ms
step:527/1750 train_time:49649ms step_avg:94.21ms
step:528/1750 train_time:49744ms step_avg:94.21ms
step:529/1750 train_time:49840ms step_avg:94.22ms
step:530/1750 train_time:49936ms step_avg:94.22ms
step:531/1750 train_time:50031ms step_avg:94.22ms
step:532/1750 train_time:50129ms step_avg:94.23ms
step:533/1750 train_time:50226ms step_avg:94.23ms
step:534/1750 train_time:50323ms step_avg:94.24ms
step:535/1750 train_time:50420ms step_avg:94.24ms
step:536/1750 train_time:50517ms step_avg:94.25ms
step:537/1750 train_time:50613ms step_avg:94.25ms
step:538/1750 train_time:50710ms step_avg:94.26ms
step:539/1750 train_time:50806ms step_avg:94.26ms
step:540/1750 train_time:50901ms step_avg:94.26ms
step:541/1750 train_time:50999ms step_avg:94.27ms
step:542/1750 train_time:51096ms step_avg:94.27ms
step:543/1750 train_time:51192ms step_avg:94.28ms
step:544/1750 train_time:51289ms step_avg:94.28ms
step:545/1750 train_time:51387ms step_avg:94.29ms
step:546/1750 train_time:51484ms step_avg:94.29ms
step:547/1750 train_time:51581ms step_avg:94.30ms
step:548/1750 train_time:51678ms step_avg:94.30ms
step:549/1750 train_time:51775ms step_avg:94.31ms
step:550/1750 train_time:51870ms step_avg:94.31ms
step:551/1750 train_time:51965ms step_avg:94.31ms
step:552/1750 train_time:52061ms step_avg:94.31ms
step:553/1750 train_time:52159ms step_avg:94.32ms
step:554/1750 train_time:52256ms step_avg:94.32ms
step:555/1750 train_time:52353ms step_avg:94.33ms
step:556/1750 train_time:52449ms step_avg:94.33ms
step:557/1750 train_time:52546ms step_avg:94.34ms
step:558/1750 train_time:52642ms step_avg:94.34ms
step:559/1750 train_time:52738ms step_avg:94.34ms
step:560/1750 train_time:52834ms step_avg:94.35ms
step:561/1750 train_time:52930ms step_avg:94.35ms
step:562/1750 train_time:53026ms step_avg:94.35ms
step:563/1750 train_time:53122ms step_avg:94.35ms
step:564/1750 train_time:53218ms step_avg:94.36ms
step:565/1750 train_time:53315ms step_avg:94.36ms
step:566/1750 train_time:53412ms step_avg:94.37ms
step:567/1750 train_time:53508ms step_avg:94.37ms
step:568/1750 train_time:53604ms step_avg:94.37ms
step:569/1750 train_time:53700ms step_avg:94.38ms
step:570/1750 train_time:53796ms step_avg:94.38ms
step:571/1750 train_time:53891ms step_avg:94.38ms
step:572/1750 train_time:53987ms step_avg:94.38ms
step:573/1750 train_time:54083ms step_avg:94.39ms
step:574/1750 train_time:54179ms step_avg:94.39ms
step:575/1750 train_time:54276ms step_avg:94.39ms
step:576/1750 train_time:54372ms step_avg:94.40ms
step:577/1750 train_time:54470ms step_avg:94.40ms
step:578/1750 train_time:54566ms step_avg:94.40ms
step:579/1750 train_time:54662ms step_avg:94.41ms
step:580/1750 train_time:54758ms step_avg:94.41ms
step:581/1750 train_time:54855ms step_avg:94.41ms
step:582/1750 train_time:54951ms step_avg:94.42ms
step:583/1750 train_time:55046ms step_avg:94.42ms
step:584/1750 train_time:55142ms step_avg:94.42ms
step:585/1750 train_time:55239ms step_avg:94.43ms
step:586/1750 train_time:55336ms step_avg:94.43ms
step:587/1750 train_time:55434ms step_avg:94.44ms
step:588/1750 train_time:55531ms step_avg:94.44ms
step:589/1750 train_time:55628ms step_avg:94.44ms
step:590/1750 train_time:55725ms step_avg:94.45ms
step:591/1750 train_time:55821ms step_avg:94.45ms
step:592/1750 train_time:55917ms step_avg:94.45ms
step:593/1750 train_time:56013ms step_avg:94.46ms
step:594/1750 train_time:56109ms step_avg:94.46ms
step:595/1750 train_time:56205ms step_avg:94.46ms
step:596/1750 train_time:56301ms step_avg:94.46ms
step:597/1750 train_time:56399ms step_avg:94.47ms
step:598/1750 train_time:56495ms step_avg:94.47ms
step:599/1750 train_time:56592ms step_avg:94.48ms
step:600/1750 train_time:56688ms step_avg:94.48ms
step:601/1750 train_time:56784ms step_avg:94.48ms
step:602/1750 train_time:56880ms step_avg:94.49ms
step:603/1750 train_time:56976ms step_avg:94.49ms
step:604/1750 train_time:57072ms step_avg:94.49ms
step:605/1750 train_time:57169ms step_avg:94.49ms
step:606/1750 train_time:57265ms step_avg:94.50ms
step:607/1750 train_time:57361ms step_avg:94.50ms
step:608/1750 train_time:57457ms step_avg:94.50ms
step:609/1750 train_time:57554ms step_avg:94.51ms
step:610/1750 train_time:57651ms step_avg:94.51ms
step:611/1750 train_time:57748ms step_avg:94.51ms
step:612/1750 train_time:57845ms step_avg:94.52ms
step:613/1750 train_time:57942ms step_avg:94.52ms
step:614/1750 train_time:58039ms step_avg:94.53ms
step:615/1750 train_time:58136ms step_avg:94.53ms
step:616/1750 train_time:58232ms step_avg:94.53ms
step:617/1750 train_time:58328ms step_avg:94.54ms
step:618/1750 train_time:58424ms step_avg:94.54ms
step:619/1750 train_time:58521ms step_avg:94.54ms
step:620/1750 train_time:58619ms step_avg:94.55ms
step:621/1750 train_time:58716ms step_avg:94.55ms
step:622/1750 train_time:58813ms step_avg:94.55ms
step:623/1750 train_time:58909ms step_avg:94.56ms
step:624/1750 train_time:59006ms step_avg:94.56ms
step:625/1750 train_time:59102ms step_avg:94.56ms
step:625/1750 val_loss:3.7180 train_time:59187ms step_avg:94.70ms
step:626/1750 train_time:59209ms step_avg:94.58ms
step:627/1750 train_time:59304ms step_avg:94.58ms
step:628/1750 train_time:59403ms step_avg:94.59ms
step:629/1750 train_time:59500ms step_avg:94.59ms
step:630/1750 train_time:59596ms step_avg:94.60ms
step:631/1750 train_time:59691ms step_avg:94.60ms
step:632/1750 train_time:59787ms step_avg:94.60ms
step:633/1750 train_time:59882ms step_avg:94.60ms
step:634/1750 train_time:59978ms step_avg:94.60ms
step:635/1750 train_time:60073ms step_avg:94.60ms
step:636/1750 train_time:60169ms step_avg:94.61ms
step:637/1750 train_time:60267ms step_avg:94.61ms
step:638/1750 train_time:60365ms step_avg:94.62ms
step:639/1750 train_time:60461ms step_avg:94.62ms
step:640/1750 train_time:60558ms step_avg:94.62ms
step:641/1750 train_time:60654ms step_avg:94.62ms
step:642/1750 train_time:60750ms step_avg:94.63ms
step:643/1750 train_time:60846ms step_avg:94.63ms
step:644/1750 train_time:60941ms step_avg:94.63ms
step:645/1750 train_time:61037ms step_avg:94.63ms
step:646/1750 train_time:61135ms step_avg:94.64ms
step:647/1750 train_time:61232ms step_avg:94.64ms
step:648/1750 train_time:61329ms step_avg:94.64ms
step:649/1750 train_time:61426ms step_avg:94.65ms
step:650/1750 train_time:61522ms step_avg:94.65ms
step:651/1750 train_time:61620ms step_avg:94.65ms
step:652/1750 train_time:61718ms step_avg:94.66ms
step:653/1750 train_time:61815ms step_avg:94.66ms
step:654/1750 train_time:61914ms step_avg:94.67ms
step:655/1750 train_time:62011ms step_avg:94.67ms
step:656/1750 train_time:62110ms step_avg:94.68ms
step:657/1750 train_time:62209ms step_avg:94.69ms
step:658/1750 train_time:62307ms step_avg:94.69ms
step:659/1750 train_time:62405ms step_avg:94.70ms
step:660/1750 train_time:62503ms step_avg:94.70ms
step:661/1750 train_time:62602ms step_avg:94.71ms
step:662/1750 train_time:62700ms step_avg:94.71ms
step:663/1750 train_time:62798ms step_avg:94.72ms
step:664/1750 train_time:62895ms step_avg:94.72ms
step:665/1750 train_time:62992ms step_avg:94.73ms
step:666/1750 train_time:63090ms step_avg:94.73ms
step:667/1750 train_time:63188ms step_avg:94.73ms
step:668/1750 train_time:63285ms step_avg:94.74ms
step:669/1750 train_time:63383ms step_avg:94.74ms
step:670/1750 train_time:63481ms step_avg:94.75ms
step:671/1750 train_time:63578ms step_avg:94.75ms
step:672/1750 train_time:63676ms step_avg:94.76ms
step:673/1750 train_time:63774ms step_avg:94.76ms
step:674/1750 train_time:63872ms step_avg:94.77ms
step:675/1750 train_time:63969ms step_avg:94.77ms
step:676/1750 train_time:64067ms step_avg:94.77ms
step:677/1750 train_time:64164ms step_avg:94.78ms
step:678/1750 train_time:64262ms step_avg:94.78ms
step:679/1750 train_time:64359ms step_avg:94.79ms
step:680/1750 train_time:64458ms step_avg:94.79ms
step:681/1750 train_time:64557ms step_avg:94.80ms
step:682/1750 train_time:64655ms step_avg:94.80ms
step:683/1750 train_time:64753ms step_avg:94.81ms
step:684/1750 train_time:64851ms step_avg:94.81ms
step:685/1750 train_time:64949ms step_avg:94.82ms
step:686/1750 train_time:65046ms step_avg:94.82ms
step:687/1750 train_time:65144ms step_avg:94.82ms
step:688/1750 train_time:65242ms step_avg:94.83ms
step:689/1750 train_time:65339ms step_avg:94.83ms
step:690/1750 train_time:65437ms step_avg:94.84ms
step:691/1750 train_time:65535ms step_avg:94.84ms
step:692/1750 train_time:65632ms step_avg:94.84ms
step:693/1750 train_time:65730ms step_avg:94.85ms
step:694/1750 train_time:65827ms step_avg:94.85ms
step:695/1750 train_time:65924ms step_avg:94.86ms
step:696/1750 train_time:66023ms step_avg:94.86ms
step:697/1750 train_time:66120ms step_avg:94.86ms
step:698/1750 train_time:66218ms step_avg:94.87ms
step:699/1750 train_time:66317ms step_avg:94.87ms
step:700/1750 train_time:66414ms step_avg:94.88ms
step:701/1750 train_time:66512ms step_avg:94.88ms
step:702/1750 train_time:66610ms step_avg:94.89ms
step:703/1750 train_time:66707ms step_avg:94.89ms
step:704/1750 train_time:66805ms step_avg:94.89ms
step:705/1750 train_time:66902ms step_avg:94.90ms
step:706/1750 train_time:67000ms step_avg:94.90ms
step:707/1750 train_time:67097ms step_avg:94.90ms
step:708/1750 train_time:67195ms step_avg:94.91ms
step:709/1750 train_time:67293ms step_avg:94.91ms
step:710/1750 train_time:67390ms step_avg:94.92ms
step:711/1750 train_time:67488ms step_avg:94.92ms
step:712/1750 train_time:67586ms step_avg:94.92ms
step:713/1750 train_time:67683ms step_avg:94.93ms
step:714/1750 train_time:67782ms step_avg:94.93ms
step:715/1750 train_time:67879ms step_avg:94.94ms
step:716/1750 train_time:67978ms step_avg:94.94ms
step:717/1750 train_time:68076ms step_avg:94.94ms
step:718/1750 train_time:68173ms step_avg:94.95ms
step:719/1750 train_time:68271ms step_avg:94.95ms
step:720/1750 train_time:68370ms step_avg:94.96ms
step:721/1750 train_time:68468ms step_avg:94.96ms
step:722/1750 train_time:68565ms step_avg:94.97ms
step:723/1750 train_time:68662ms step_avg:94.97ms
step:724/1750 train_time:68761ms step_avg:94.97ms
step:725/1750 train_time:68858ms step_avg:94.98ms
step:726/1750 train_time:68957ms step_avg:94.98ms
step:727/1750 train_time:69055ms step_avg:94.99ms
step:728/1750 train_time:69152ms step_avg:94.99ms
step:729/1750 train_time:69249ms step_avg:94.99ms
step:730/1750 train_time:69348ms step_avg:95.00ms
step:731/1750 train_time:69445ms step_avg:95.00ms
step:732/1750 train_time:69542ms step_avg:95.00ms
step:733/1750 train_time:69639ms step_avg:95.01ms
step:734/1750 train_time:69737ms step_avg:95.01ms
step:735/1750 train_time:69835ms step_avg:95.01ms
step:736/1750 train_time:69933ms step_avg:95.02ms
step:737/1750 train_time:70030ms step_avg:95.02ms
step:738/1750 train_time:70128ms step_avg:95.02ms
step:739/1750 train_time:70225ms step_avg:95.03ms
step:740/1750 train_time:70324ms step_avg:95.03ms
step:741/1750 train_time:70422ms step_avg:95.04ms
step:742/1750 train_time:70520ms step_avg:95.04ms
step:743/1750 train_time:70618ms step_avg:95.04ms
step:744/1750 train_time:70717ms step_avg:95.05ms
step:745/1750 train_time:70816ms step_avg:95.05ms
step:746/1750 train_time:70913ms step_avg:95.06ms
step:747/1750 train_time:71011ms step_avg:95.06ms
step:748/1750 train_time:71109ms step_avg:95.07ms
step:749/1750 train_time:71207ms step_avg:95.07ms
step:750/1750 train_time:71305ms step_avg:95.07ms
step:750/1750 val_loss:3.6567 train_time:71391ms step_avg:95.19ms
step:751/1750 train_time:71414ms step_avg:95.09ms
step:752/1750 train_time:71507ms step_avg:95.09ms
step:753/1750 train_time:71607ms step_avg:95.10ms
step:754/1750 train_time:71708ms step_avg:95.10ms
step:755/1750 train_time:71806ms step_avg:95.11ms
step:756/1750 train_time:71903ms step_avg:95.11ms
step:757/1750 train_time:72000ms step_avg:95.11ms
step:758/1750 train_time:72097ms step_avg:95.12ms
step:759/1750 train_time:72194ms step_avg:95.12ms
step:760/1750 train_time:72291ms step_avg:95.12ms
step:761/1750 train_time:72390ms step_avg:95.13ms
step:762/1750 train_time:72490ms step_avg:95.13ms
step:763/1750 train_time:72589ms step_avg:95.14ms
step:764/1750 train_time:72686ms step_avg:95.14ms
step:765/1750 train_time:72784ms step_avg:95.14ms
step:766/1750 train_time:72883ms step_avg:95.15ms
step:767/1750 train_time:72981ms step_avg:95.15ms
step:768/1750 train_time:73078ms step_avg:95.15ms
step:769/1750 train_time:73175ms step_avg:95.16ms
step:770/1750 train_time:73272ms step_avg:95.16ms
step:771/1750 train_time:73369ms step_avg:95.16ms
step:772/1750 train_time:73468ms step_avg:95.17ms
step:773/1750 train_time:73567ms step_avg:95.17ms
step:774/1750 train_time:73666ms step_avg:95.18ms
step:775/1750 train_time:73765ms step_avg:95.18ms
step:776/1750 train_time:73862ms step_avg:95.18ms
step:777/1750 train_time:73960ms step_avg:95.19ms
step:778/1750 train_time:74057ms step_avg:95.19ms
step:779/1750 train_time:74154ms step_avg:95.19ms
step:780/1750 train_time:74252ms step_avg:95.19ms
step:781/1750 train_time:74349ms step_avg:95.20ms
step:782/1750 train_time:74447ms step_avg:95.20ms
step:783/1750 train_time:74547ms step_avg:95.21ms
step:784/1750 train_time:74647ms step_avg:95.21ms
step:785/1750 train_time:74746ms step_avg:95.22ms
step:786/1750 train_time:74844ms step_avg:95.22ms
step:787/1750 train_time:74943ms step_avg:95.23ms
step:788/1750 train_time:75041ms step_avg:95.23ms
step:789/1750 train_time:75140ms step_avg:95.23ms
step:790/1750 train_time:75237ms step_avg:95.24ms
step:791/1750 train_time:75335ms step_avg:95.24ms
step:792/1750 train_time:75433ms step_avg:95.24ms
step:793/1750 train_time:75532ms step_avg:95.25ms
step:794/1750 train_time:75630ms step_avg:95.25ms
step:795/1750 train_time:75729ms step_avg:95.26ms
step:796/1750 train_time:75827ms step_avg:95.26ms
step:797/1750 train_time:75925ms step_avg:95.26ms
step:798/1750 train_time:76025ms step_avg:95.27ms
step:799/1750 train_time:76125ms step_avg:95.28ms
step:800/1750 train_time:76223ms step_avg:95.28ms
step:801/1750 train_time:76322ms step_avg:95.28ms
step:802/1750 train_time:76421ms step_avg:95.29ms
step:803/1750 train_time:76520ms step_avg:95.29ms
step:804/1750 train_time:76618ms step_avg:95.30ms
step:805/1750 train_time:76717ms step_avg:95.30ms
step:806/1750 train_time:76816ms step_avg:95.30ms
step:807/1750 train_time:76915ms step_avg:95.31ms
step:808/1750 train_time:77015ms step_avg:95.32ms
step:809/1750 train_time:77114ms step_avg:95.32ms
step:810/1750 train_time:77212ms step_avg:95.32ms
step:811/1750 train_time:77310ms step_avg:95.33ms
step:812/1750 train_time:77409ms step_avg:95.33ms
step:813/1750 train_time:77507ms step_avg:95.33ms
step:814/1750 train_time:77604ms step_avg:95.34ms
step:815/1750 train_time:77703ms step_avg:95.34ms
step:816/1750 train_time:77801ms step_avg:95.34ms
step:817/1750 train_time:77898ms step_avg:95.35ms
step:818/1750 train_time:77997ms step_avg:95.35ms
step:819/1750 train_time:78096ms step_avg:95.36ms
step:820/1750 train_time:78194ms step_avg:95.36ms
step:821/1750 train_time:78292ms step_avg:95.36ms
step:822/1750 train_time:78390ms step_avg:95.37ms
step:823/1750 train_time:78489ms step_avg:95.37ms
step:824/1750 train_time:78587ms step_avg:95.37ms
step:825/1750 train_time:78686ms step_avg:95.38ms
step:826/1750 train_time:78784ms step_avg:95.38ms
step:827/1750 train_time:78883ms step_avg:95.38ms
step:828/1750 train_time:78982ms step_avg:95.39ms
step:829/1750 train_time:79081ms step_avg:95.39ms
step:830/1750 train_time:79179ms step_avg:95.40ms
step:831/1750 train_time:79277ms step_avg:95.40ms
step:832/1750 train_time:79376ms step_avg:95.40ms
step:833/1750 train_time:79474ms step_avg:95.41ms
step:834/1750 train_time:79573ms step_avg:95.41ms
step:835/1750 train_time:79671ms step_avg:95.41ms
step:836/1750 train_time:79770ms step_avg:95.42ms
step:837/1750 train_time:79868ms step_avg:95.42ms
step:838/1750 train_time:79967ms step_avg:95.43ms
step:839/1750 train_time:80065ms step_avg:95.43ms
step:840/1750 train_time:80164ms step_avg:95.43ms
step:841/1750 train_time:80263ms step_avg:95.44ms
step:842/1750 train_time:80362ms step_avg:95.44ms
step:843/1750 train_time:80461ms step_avg:95.45ms
step:844/1750 train_time:80560ms step_avg:95.45ms
step:845/1750 train_time:80658ms step_avg:95.45ms
step:846/1750 train_time:80757ms step_avg:95.46ms
step:847/1750 train_time:80856ms step_avg:95.46ms
step:848/1750 train_time:80955ms step_avg:95.47ms
step:849/1750 train_time:81053ms step_avg:95.47ms
step:850/1750 train_time:81152ms step_avg:95.47ms
step:851/1750 train_time:81250ms step_avg:95.48ms
step:852/1750 train_time:81349ms step_avg:95.48ms
step:853/1750 train_time:81448ms step_avg:95.48ms
step:854/1750 train_time:81547ms step_avg:95.49ms
step:855/1750 train_time:81645ms step_avg:95.49ms
step:856/1750 train_time:81744ms step_avg:95.49ms
step:857/1750 train_time:81843ms step_avg:95.50ms
step:858/1750 train_time:81942ms step_avg:95.50ms
step:859/1750 train_time:82041ms step_avg:95.51ms
step:860/1750 train_time:82140ms step_avg:95.51ms
step:861/1750 train_time:82237ms step_avg:95.51ms
step:862/1750 train_time:82335ms step_avg:95.52ms
step:863/1750 train_time:82433ms step_avg:95.52ms
step:864/1750 train_time:82531ms step_avg:95.52ms
step:865/1750 train_time:82629ms step_avg:95.52ms
step:866/1750 train_time:82727ms step_avg:95.53ms
step:867/1750 train_time:82825ms step_avg:95.53ms
step:868/1750 train_time:82923ms step_avg:95.53ms
step:869/1750 train_time:83022ms step_avg:95.54ms
step:870/1750 train_time:83120ms step_avg:95.54ms
step:871/1750 train_time:83219ms step_avg:95.54ms
step:872/1750 train_time:83318ms step_avg:95.55ms
step:873/1750 train_time:83416ms step_avg:95.55ms
step:874/1750 train_time:83513ms step_avg:95.55ms
step:875/1750 train_time:83612ms step_avg:95.56ms
step:875/1750 val_loss:3.5954 train_time:83698ms step_avg:95.66ms
step:876/1750 train_time:83720ms step_avg:95.57ms
step:877/1750 train_time:83815ms step_avg:95.57ms
step:878/1750 train_time:83916ms step_avg:95.58ms
step:879/1750 train_time:84015ms step_avg:95.58ms
step:880/1750 train_time:84113ms step_avg:95.58ms
step:881/1750 train_time:84212ms step_avg:95.59ms
step:882/1750 train_time:84310ms step_avg:95.59ms
step:883/1750 train_time:84407ms step_avg:95.59ms
step:884/1750 train_time:84504ms step_avg:95.59ms
step:885/1750 train_time:84601ms step_avg:95.59ms
step:886/1750 train_time:84700ms step_avg:95.60ms
step:887/1750 train_time:84800ms step_avg:95.60ms
step:888/1750 train_time:84899ms step_avg:95.61ms
step:889/1750 train_time:84997ms step_avg:95.61ms
step:890/1750 train_time:85096ms step_avg:95.61ms
step:891/1750 train_time:85194ms step_avg:95.62ms
step:892/1750 train_time:85292ms step_avg:95.62ms
step:893/1750 train_time:85389ms step_avg:95.62ms
step:894/1750 train_time:85486ms step_avg:95.62ms
step:895/1750 train_time:85583ms step_avg:95.62ms
step:896/1750 train_time:85681ms step_avg:95.63ms
step:897/1750 train_time:85779ms step_avg:95.63ms
step:898/1750 train_time:85878ms step_avg:95.63ms
step:899/1750 train_time:85976ms step_avg:95.64ms
step:900/1750 train_time:86076ms step_avg:95.64ms
step:901/1750 train_time:86175ms step_avg:95.64ms
step:902/1750 train_time:86273ms step_avg:95.65ms
step:903/1750 train_time:86370ms step_avg:95.65ms
step:904/1750 train_time:86468ms step_avg:95.65ms
step:905/1750 train_time:86566ms step_avg:95.65ms
step:906/1750 train_time:86664ms step_avg:95.66ms
step:907/1750 train_time:86763ms step_avg:95.66ms
step:908/1750 train_time:86862ms step_avg:95.66ms
step:909/1750 train_time:86961ms step_avg:95.67ms
step:910/1750 train_time:87060ms step_avg:95.67ms
step:911/1750 train_time:87159ms step_avg:95.67ms
step:912/1750 train_time:87259ms step_avg:95.68ms
step:913/1750 train_time:87359ms step_avg:95.68ms
step:914/1750 train_time:87458ms step_avg:95.69ms
step:915/1750 train_time:87558ms step_avg:95.69ms
step:916/1750 train_time:87657ms step_avg:95.70ms
step:917/1750 train_time:87757ms step_avg:95.70ms
step:918/1750 train_time:87856ms step_avg:95.70ms
step:919/1750 train_time:87956ms step_avg:95.71ms
step:920/1750 train_time:88057ms step_avg:95.71ms
step:921/1750 train_time:88157ms step_avg:95.72ms
step:922/1750 train_time:88256ms step_avg:95.72ms
step:923/1750 train_time:88356ms step_avg:95.73ms
step:924/1750 train_time:88456ms step_avg:95.73ms
step:925/1750 train_time:88556ms step_avg:95.74ms
step:926/1750 train_time:88657ms step_avg:95.74ms
step:927/1750 train_time:88756ms step_avg:95.75ms
step:928/1750 train_time:88855ms step_avg:95.75ms
step:929/1750 train_time:88955ms step_avg:95.75ms
step:930/1750 train_time:89055ms step_avg:95.76ms
step:931/1750 train_time:89154ms step_avg:95.76ms
step:932/1750 train_time:89254ms step_avg:95.77ms
step:933/1750 train_time:89354ms step_avg:95.77ms
step:934/1750 train_time:89453ms step_avg:95.77ms
step:935/1750 train_time:89553ms step_avg:95.78ms
step:936/1750 train_time:89652ms step_avg:95.78ms
step:937/1750 train_time:89754ms step_avg:95.79ms
step:938/1750 train_time:89854ms step_avg:95.79ms
step:939/1750 train_time:89954ms step_avg:95.80ms
step:940/1750 train_time:90055ms step_avg:95.80ms
step:941/1750 train_time:90155ms step_avg:95.81ms
step:942/1750 train_time:90255ms step_avg:95.81ms
step:943/1750 train_time:90356ms step_avg:95.82ms
step:944/1750 train_time:90456ms step_avg:95.82ms
step:945/1750 train_time:90555ms step_avg:95.83ms
step:946/1750 train_time:90655ms step_avg:95.83ms
step:947/1750 train_time:90755ms step_avg:95.83ms
step:948/1750 train_time:90854ms step_avg:95.84ms
step:949/1750 train_time:90954ms step_avg:95.84ms
step:950/1750 train_time:91054ms step_avg:95.85ms
step:951/1750 train_time:91153ms step_avg:95.85ms
step:952/1750 train_time:91254ms step_avg:95.85ms
step:953/1750 train_time:91353ms step_avg:95.86ms
step:954/1750 train_time:91453ms step_avg:95.86ms
step:955/1750 train_time:91553ms step_avg:95.87ms
step:956/1750 train_time:91652ms step_avg:95.87ms
step:957/1750 train_time:91751ms step_avg:95.87ms
step:958/1750 train_time:91851ms step_avg:95.88ms
step:959/1750 train_time:91951ms step_avg:95.88ms
step:960/1750 train_time:92053ms step_avg:95.89ms
step:961/1750 train_time:92152ms step_avg:95.89ms
step:962/1750 train_time:92252ms step_avg:95.90ms
step:963/1750 train_time:92352ms step_avg:95.90ms
step:964/1750 train_time:92452ms step_avg:95.90ms
step:965/1750 train_time:92552ms step_avg:95.91ms
step:966/1750 train_time:92651ms step_avg:95.91ms
step:967/1750 train_time:92751ms step_avg:95.92ms
step:968/1750 train_time:92851ms step_avg:95.92ms
step:969/1750 train_time:92950ms step_avg:95.92ms
step:970/1750 train_time:93049ms step_avg:95.93ms
step:971/1750 train_time:93148ms step_avg:95.93ms
step:972/1750 train_time:93248ms step_avg:95.93ms
step:973/1750 train_time:93348ms step_avg:95.94ms
step:974/1750 train_time:93448ms step_avg:95.94ms
step:975/1750 train_time:93547ms step_avg:95.95ms
step:976/1750 train_time:93647ms step_avg:95.95ms
step:977/1750 train_time:93746ms step_avg:95.95ms
step:978/1750 train_time:93845ms step_avg:95.96ms
step:979/1750 train_time:93946ms step_avg:95.96ms
step:980/1750 train_time:94045ms step_avg:95.96ms
step:981/1750 train_time:94145ms step_avg:95.97ms
step:982/1750 train_time:94245ms step_avg:95.97ms
step:983/1750 train_time:94344ms step_avg:95.98ms
step:984/1750 train_time:94443ms step_avg:95.98ms
step:985/1750 train_time:94541ms step_avg:95.98ms
step:986/1750 train_time:94640ms step_avg:95.98ms
step:987/1750 train_time:94740ms step_avg:95.99ms
step:988/1750 train_time:94840ms step_avg:95.99ms
step:989/1750 train_time:94940ms step_avg:96.00ms
step:990/1750 train_time:95041ms step_avg:96.00ms
step:991/1750 train_time:95141ms step_avg:96.00ms
step:992/1750 train_time:95240ms step_avg:96.01ms
step:993/1750 train_time:95339ms step_avg:96.01ms
step:994/1750 train_time:95438ms step_avg:96.01ms
step:995/1750 train_time:95538ms step_avg:96.02ms
step:996/1750 train_time:95637ms step_avg:96.02ms
step:997/1750 train_time:95737ms step_avg:96.03ms
step:998/1750 train_time:95838ms step_avg:96.03ms
step:999/1750 train_time:95938ms step_avg:96.03ms
step:1000/1750 train_time:96037ms step_avg:96.04ms
step:1000/1750 val_loss:3.5530 train_time:96127ms step_avg:96.13ms
step:1001/1750 train_time:96149ms step_avg:96.05ms
step:1002/1750 train_time:96245ms step_avg:96.05ms
step:1003/1750 train_time:96345ms step_avg:96.06ms
step:1004/1750 train_time:96446ms step_avg:96.06ms
step:1005/1750 train_time:96545ms step_avg:96.06ms
step:1006/1750 train_time:96644ms step_avg:96.07ms
step:1007/1750 train_time:96743ms step_avg:96.07ms
step:1008/1750 train_time:96842ms step_avg:96.07ms
step:1009/1750 train_time:96941ms step_avg:96.08ms
step:1010/1750 train_time:97042ms step_avg:96.08ms
step:1011/1750 train_time:97144ms step_avg:96.09ms
step:1012/1750 train_time:97246ms step_avg:96.09ms
step:1013/1750 train_time:97347ms step_avg:96.10ms
step:1014/1750 train_time:97448ms step_avg:96.10ms
step:1015/1750 train_time:97548ms step_avg:96.11ms
step:1016/1750 train_time:97646ms step_avg:96.11ms
step:1017/1750 train_time:97745ms step_avg:96.11ms
step:1018/1750 train_time:97844ms step_avg:96.11ms
step:1019/1750 train_time:97943ms step_avg:96.12ms
step:1020/1750 train_time:98042ms step_avg:96.12ms
step:1021/1750 train_time:98142ms step_avg:96.12ms
step:1022/1750 train_time:98242ms step_avg:96.13ms
step:1023/1750 train_time:98343ms step_avg:96.13ms
step:1024/1750 train_time:98445ms step_avg:96.14ms
step:1025/1750 train_time:98545ms step_avg:96.14ms
step:1026/1750 train_time:98644ms step_avg:96.14ms
step:1027/1750 train_time:98743ms step_avg:96.15ms
step:1028/1750 train_time:98842ms step_avg:96.15ms
step:1029/1750 train_time:98941ms step_avg:96.15ms
step:1030/1750 train_time:99040ms step_avg:96.16ms
step:1031/1750 train_time:99140ms step_avg:96.16ms
step:1032/1750 train_time:99239ms step_avg:96.16ms
step:1033/1750 train_time:99339ms step_avg:96.17ms
step:1034/1750 train_time:99438ms step_avg:96.17ms
step:1035/1750 train_time:99539ms step_avg:96.17ms
step:1036/1750 train_time:99640ms step_avg:96.18ms
step:1037/1750 train_time:99738ms step_avg:96.18ms
step:1038/1750 train_time:99837ms step_avg:96.18ms
step:1039/1750 train_time:99937ms step_avg:96.19ms
step:1040/1750 train_time:100036ms step_avg:96.19ms
step:1041/1750 train_time:100135ms step_avg:96.19ms
step:1042/1750 train_time:100234ms step_avg:96.19ms
step:1043/1750 train_time:100333ms step_avg:96.20ms
step:1044/1750 train_time:100433ms step_avg:96.20ms
step:1045/1750 train_time:100533ms step_avg:96.20ms
step:1046/1750 train_time:100633ms step_avg:96.21ms
step:1047/1750 train_time:100732ms step_avg:96.21ms
step:1048/1750 train_time:100831ms step_avg:96.21ms
step:1049/1750 train_time:100930ms step_avg:96.22ms
step:1050/1750 train_time:101030ms step_avg:96.22ms
step:1051/1750 train_time:101131ms step_avg:96.22ms
step:1052/1750 train_time:101230ms step_avg:96.23ms
step:1053/1750 train_time:101330ms step_avg:96.23ms
step:1054/1750 train_time:101429ms step_avg:96.23ms
step:1055/1750 train_time:101529ms step_avg:96.24ms
step:1056/1750 train_time:101630ms step_avg:96.24ms
step:1057/1750 train_time:101729ms step_avg:96.24ms
step:1058/1750 train_time:101829ms step_avg:96.25ms
step:1059/1750 train_time:101929ms step_avg:96.25ms
step:1060/1750 train_time:102293ms step_avg:96.50ms
step:1061/1750 train_time:102391ms step_avg:96.50ms
step:1062/1750 train_time:102490ms step_avg:96.51ms
step:1063/1750 train_time:102588ms step_avg:96.51ms
step:1064/1750 train_time:102687ms step_avg:96.51ms
step:1065/1750 train_time:102785ms step_avg:96.51ms
step:1066/1750 train_time:102883ms step_avg:96.51ms
step:1067/1750 train_time:102982ms step_avg:96.52ms
step:1068/1750 train_time:103080ms step_avg:96.52ms
step:1069/1750 train_time:103182ms step_avg:96.52ms
step:1070/1750 train_time:103287ms step_avg:96.53ms
step:1071/1750 train_time:103390ms step_avg:96.54ms
step:1072/1750 train_time:103491ms step_avg:96.54ms
step:1073/1750 train_time:103590ms step_avg:96.54ms
step:1074/1750 train_time:103688ms step_avg:96.54ms
step:1075/1750 train_time:103787ms step_avg:96.55ms
step:1076/1750 train_time:103886ms step_avg:96.55ms
step:1077/1750 train_time:103984ms step_avg:96.55ms
step:1078/1750 train_time:104083ms step_avg:96.55ms
step:1079/1750 train_time:104184ms step_avg:96.56ms
step:1080/1750 train_time:104566ms step_avg:96.82ms
step:1081/1750 train_time:104664ms step_avg:96.82ms
step:1082/1750 train_time:104763ms step_avg:96.82ms
step:1083/1750 train_time:104861ms step_avg:96.82ms
step:1084/1750 train_time:104959ms step_avg:96.83ms
step:1085/1750 train_time:105058ms step_avg:96.83ms
step:1086/1750 train_time:105156ms step_avg:96.83ms
step:1087/1750 train_time:105254ms step_avg:96.83ms
step:1088/1750 train_time:105353ms step_avg:96.83ms
step:1089/1750 train_time:105454ms step_avg:96.84ms
step:1090/1750 train_time:105560ms step_avg:96.84ms
step:1091/1750 train_time:105661ms step_avg:96.85ms
step:1092/1750 train_time:105761ms step_avg:96.85ms
step:1093/1750 train_time:105860ms step_avg:96.85ms
step:1094/1750 train_time:105958ms step_avg:96.85ms
step:1095/1750 train_time:106057ms step_avg:96.86ms
step:1096/1750 train_time:106155ms step_avg:96.86ms
step:1097/1750 train_time:106254ms step_avg:96.86ms
step:1098/1750 train_time:106353ms step_avg:96.86ms
step:1099/1750 train_time:106453ms step_avg:96.86ms
step:1100/1750 train_time:106553ms step_avg:96.87ms
step:1101/1750 train_time:106652ms step_avg:96.87ms
step:1102/1750 train_time:106753ms step_avg:96.87ms
step:1103/1750 train_time:106853ms step_avg:96.87ms
step:1104/1750 train_time:106952ms step_avg:96.88ms
step:1105/1750 train_time:107052ms step_avg:96.88ms
step:1106/1750 train_time:107151ms step_avg:96.88ms
step:1107/1750 train_time:107543ms step_avg:97.15ms
step:1108/1750 train_time:107640ms step_avg:97.15ms
step:1109/1750 train_time:107738ms step_avg:97.15ms
step:1110/1750 train_time:107838ms step_avg:97.15ms
step:1111/1750 train_time:107936ms step_avg:97.15ms
step:1112/1750 train_time:108035ms step_avg:97.15ms
step:1113/1750 train_time:108134ms step_avg:97.16ms
step:1114/1750 train_time:108232ms step_avg:97.16ms
step:1115/1750 train_time:108330ms step_avg:97.16ms
step:1116/1750 train_time:108431ms step_avg:97.16ms
step:1117/1750 train_time:108537ms step_avg:97.17ms
step:1118/1750 train_time:108637ms step_avg:97.17ms
step:1119/1750 train_time:108737ms step_avg:97.17ms
step:1120/1750 train_time:108836ms step_avg:97.17ms
step:1121/1750 train_time:108935ms step_avg:97.18ms
step:1122/1750 train_time:109033ms step_avg:97.18ms
step:1123/1750 train_time:109132ms step_avg:97.18ms
step:1124/1750 train_time:109230ms step_avg:97.18ms
step:1125/1750 train_time:109331ms step_avg:97.18ms
step:1125/1750 val_loss:3.4979 train_time:109420ms step_avg:97.26ms
step:1126/1750 train_time:109443ms step_avg:97.20ms
step:1127/1750 train_time:109539ms step_avg:97.19ms
step:1128/1750 train_time:109640ms step_avg:97.20ms
step:1129/1750 train_time:109741ms step_avg:97.20ms
step:1130/1750 train_time:109841ms step_avg:97.20ms
step:1131/1750 train_time:109941ms step_avg:97.21ms
step:1132/1750 train_time:110041ms step_avg:97.21ms
step:1133/1750 train_time:110139ms step_avg:97.21ms
step:1134/1750 train_time:110238ms step_avg:97.21ms
step:1135/1750 train_time:110337ms step_avg:97.21ms
step:1136/1750 train_time:110439ms step_avg:97.22ms
step:1137/1750 train_time:110540ms step_avg:97.22ms
step:1138/1750 train_time:110642ms step_avg:97.23ms
step:1139/1750 train_time:110743ms step_avg:97.23ms
step:1140/1750 train_time:110843ms step_avg:97.23ms
step:1141/1750 train_time:110944ms step_avg:97.23ms
step:1142/1750 train_time:111043ms step_avg:97.24ms
step:1143/1750 train_time:111142ms step_avg:97.24ms
step:1144/1750 train_time:111242ms step_avg:97.24ms
step:1145/1750 train_time:111342ms step_avg:97.24ms
step:1146/1750 train_time:111441ms step_avg:97.24ms
step:1147/1750 train_time:111542ms step_avg:97.25ms
step:1148/1750 train_time:111643ms step_avg:97.25ms
step:1149/1750 train_time:111743ms step_avg:97.25ms
step:1150/1750 train_time:111844ms step_avg:97.26ms
step:1151/1750 train_time:111944ms step_avg:97.26ms
step:1152/1750 train_time:112044ms step_avg:97.26ms
step:1153/1750 train_time:112143ms step_avg:97.26ms
step:1154/1750 train_time:112243ms step_avg:97.26ms
step:1155/1750 train_time:112343ms step_avg:97.27ms
step:1156/1750 train_time:112443ms step_avg:97.27ms
step:1157/1750 train_time:112543ms step_avg:97.27ms
step:1158/1750 train_time:112902ms step_avg:97.50ms
step:1159/1750 train_time:112999ms step_avg:97.50ms
step:1160/1750 train_time:113098ms step_avg:97.50ms
step:1161/1750 train_time:113196ms step_avg:97.50ms
step:1162/1750 train_time:113295ms step_avg:97.50ms
step:1163/1750 train_time:113394ms step_avg:97.50ms
step:1164/1750 train_time:113493ms step_avg:97.50ms
step:1165/1750 train_time:113591ms step_avg:97.50ms
step:1166/1750 train_time:113689ms step_avg:97.50ms
step:1167/1750 train_time:114060ms step_avg:97.74ms
step:1168/1750 train_time:114157ms step_avg:97.74ms
step:1169/1750 train_time:114257ms step_avg:97.74ms
step:1170/1750 train_time:114356ms step_avg:97.74ms
step:1171/1750 train_time:114456ms step_avg:97.74ms
step:1172/1750 train_time:114555ms step_avg:97.74ms
step:1173/1750 train_time:114655ms step_avg:97.74ms
step:1174/1750 train_time:114755ms step_avg:97.75ms
step:1175/1750 train_time:114854ms step_avg:97.75ms
step:1176/1750 train_time:114955ms step_avg:97.75ms
step:1177/1750 train_time:115061ms step_avg:97.76ms
step:1178/1750 train_time:115164ms step_avg:97.76ms
step:1179/1750 train_time:115268ms step_avg:97.77ms
step:1180/1750 train_time:115367ms step_avg:97.77ms
step:1181/1750 train_time:115468ms step_avg:97.77ms
step:1182/1750 train_time:115568ms step_avg:97.77ms
step:1183/1750 train_time:115668ms step_avg:97.78ms
step:1184/1750 train_time:115771ms step_avg:97.78ms
step:1185/1750 train_time:115871ms step_avg:97.78ms
step:1186/1750 train_time:115974ms step_avg:97.79ms
step:1187/1750 train_time:116076ms step_avg:97.79ms
step:1188/1750 train_time:116177ms step_avg:97.79ms
step:1189/1750 train_time:116277ms step_avg:97.79ms
step:1190/1750 train_time:116376ms step_avg:97.80ms
step:1191/1750 train_time:116478ms step_avg:97.80ms
step:1192/1750 train_time:116579ms step_avg:97.80ms
step:1193/1750 train_time:116681ms step_avg:97.80ms
step:1194/1750 train_time:116782ms step_avg:97.81ms
step:1195/1750 train_time:116883ms step_avg:97.81ms
step:1196/1750 train_time:116985ms step_avg:97.81ms
step:1197/1750 train_time:117087ms step_avg:97.82ms
step:1198/1750 train_time:117188ms step_avg:97.82ms
step:1199/1750 train_time:117288ms step_avg:97.82ms
step:1200/1750 train_time:117388ms step_avg:97.82ms
step:1201/1750 train_time:117488ms step_avg:97.83ms
step:1202/1750 train_time:117589ms step_avg:97.83ms
step:1203/1750 train_time:117690ms step_avg:97.83ms
step:1204/1750 train_time:117792ms step_avg:97.83ms
step:1205/1750 train_time:117892ms step_avg:97.84ms
step:1206/1750 train_time:117993ms step_avg:97.84ms
step:1207/1750 train_time:118094ms step_avg:97.84ms
step:1208/1750 train_time:118195ms step_avg:97.84ms
step:1209/1750 train_time:118295ms step_avg:97.85ms
step:1210/1750 train_time:118396ms step_avg:97.85ms
step:1211/1750 train_time:118882ms step_avg:98.17ms
step:1212/1750 train_time:118944ms step_avg:98.14ms
step:1213/1750 train_time:119042ms step_avg:98.14ms
step:1214/1750 train_time:119142ms step_avg:98.14ms
step:1215/1750 train_time:119241ms step_avg:98.14ms
step:1216/1750 train_time:119344ms step_avg:98.14ms
step:1217/1750 train_time:119444ms step_avg:98.15ms
step:1218/1750 train_time:119544ms step_avg:98.15ms
step:1219/1750 train_time:119644ms step_avg:98.15ms
step:1220/1750 train_time:119744ms step_avg:98.15ms
step:1221/1750 train_time:119852ms step_avg:98.16ms
step:1222/1750 train_time:119954ms step_avg:98.16ms
step:1223/1750 train_time:120055ms step_avg:98.16ms
step:1224/1750 train_time:120155ms step_avg:98.17ms
step:1225/1750 train_time:120255ms step_avg:98.17ms
step:1226/1750 train_time:120651ms step_avg:98.41ms
step:1227/1750 train_time:120750ms step_avg:98.41ms
step:1228/1750 train_time:120848ms step_avg:98.41ms
step:1229/1750 train_time:120948ms step_avg:98.41ms
step:1230/1750 train_time:121047ms step_avg:98.41ms
step:1231/1750 train_time:121147ms step_avg:98.41ms
step:1232/1750 train_time:121247ms step_avg:98.41ms
step:1233/1750 train_time:121346ms step_avg:98.42ms
step:1234/1750 train_time:121447ms step_avg:98.42ms
step:1235/1750 train_time:121551ms step_avg:98.42ms
step:1236/1750 train_time:121658ms step_avg:98.43ms
step:1237/1750 train_time:121759ms step_avg:98.43ms
step:1238/1750 train_time:121859ms step_avg:98.43ms
step:1239/1750 train_time:121960ms step_avg:98.43ms
step:1240/1750 train_time:122060ms step_avg:98.44ms
step:1241/1750 train_time:122162ms step_avg:98.44ms
step:1242/1750 train_time:122264ms step_avg:98.44ms
step:1243/1750 train_time:122364ms step_avg:98.44ms
step:1244/1750 train_time:122465ms step_avg:98.44ms
step:1245/1750 train_time:122568ms step_avg:98.45ms
step:1246/1750 train_time:122670ms step_avg:98.45ms
step:1247/1750 train_time:122770ms step_avg:98.45ms
step:1248/1750 train_time:122871ms step_avg:98.45ms
step:1249/1750 train_time:122972ms step_avg:98.46ms
step:1250/1750 train_time:123073ms step_avg:98.46ms
step:1250/1750 val_loss:3.4516 train_time:123162ms step_avg:98.53ms
step:1251/1750 train_time:123184ms step_avg:98.47ms
step:1252/1750 train_time:123284ms step_avg:98.47ms
step:1253/1750 train_time:123387ms step_avg:98.47ms
step:1254/1750 train_time:123488ms step_avg:98.48ms
step:1255/1750 train_time:123588ms step_avg:98.48ms
step:1256/1750 train_time:123689ms step_avg:98.48ms
step:1257/1750 train_time:123789ms step_avg:98.48ms
step:1258/1750 train_time:123888ms step_avg:98.48ms
step:1259/1750 train_time:123989ms step_avg:98.48ms
step:1260/1750 train_time:124090ms step_avg:98.48ms
step:1261/1750 train_time:124194ms step_avg:98.49ms
step:1262/1750 train_time:124298ms step_avg:98.49ms
step:1263/1750 train_time:124399ms step_avg:98.49ms
step:1264/1750 train_time:124499ms step_avg:98.50ms
step:1265/1750 train_time:124599ms step_avg:98.50ms
step:1266/1750 train_time:124698ms step_avg:98.50ms
step:1267/1750 train_time:124798ms step_avg:98.50ms
step:1268/1750 train_time:124898ms step_avg:98.50ms
step:1269/1750 train_time:124998ms step_avg:98.50ms
step:1270/1750 train_time:125099ms step_avg:98.50ms
step:1271/1750 train_time:125200ms step_avg:98.51ms
step:1272/1750 train_time:125301ms step_avg:98.51ms
step:1273/1750 train_time:125401ms step_avg:98.51ms
step:1274/1750 train_time:125501ms step_avg:98.51ms
step:1275/1750 train_time:125602ms step_avg:98.51ms
step:1276/1750 train_time:125703ms step_avg:98.51ms
step:1277/1750 train_time:125803ms step_avg:98.51ms
step:1278/1750 train_time:125903ms step_avg:98.52ms
step:1279/1750 train_time:126004ms step_avg:98.52ms
step:1280/1750 train_time:126104ms step_avg:98.52ms
step:1281/1750 train_time:126206ms step_avg:98.52ms
step:1282/1750 train_time:126306ms step_avg:98.52ms
step:1283/1750 train_time:126406ms step_avg:98.52ms
step:1284/1750 train_time:126507ms step_avg:98.53ms
step:1285/1750 train_time:126607ms step_avg:98.53ms
step:1286/1750 train_time:126707ms step_avg:98.53ms
step:1287/1750 train_time:126808ms step_avg:98.53ms
step:1288/1750 train_time:126908ms step_avg:98.53ms
step:1289/1750 train_time:127009ms step_avg:98.53ms
step:1290/1750 train_time:127110ms step_avg:98.54ms
step:1291/1750 train_time:127211ms step_avg:98.54ms
step:1292/1750 train_time:127312ms step_avg:98.54ms
step:1293/1750 train_time:127414ms step_avg:98.54ms
step:1294/1750 train_time:127515ms step_avg:98.54ms
step:1295/1750 train_time:127618ms step_avg:98.55ms
step:1296/1750 train_time:127718ms step_avg:98.55ms
step:1297/1750 train_time:127819ms step_avg:98.55ms
step:1298/1750 train_time:127920ms step_avg:98.55ms
step:1299/1750 train_time:128020ms step_avg:98.55ms
step:1300/1750 train_time:128122ms step_avg:98.56ms
step:1301/1750 train_time:128223ms step_avg:98.56ms
step:1302/1750 train_time:128324ms step_avg:98.56ms
step:1303/1750 train_time:128426ms step_avg:98.56ms
step:1304/1750 train_time:128527ms step_avg:98.56ms
step:1305/1750 train_time:128628ms step_avg:98.57ms
step:1306/1750 train_time:128729ms step_avg:98.57ms
step:1307/1750 train_time:128830ms step_avg:98.57ms
step:1308/1750 train_time:128931ms step_avg:98.57ms
step:1309/1750 train_time:129032ms step_avg:98.57ms
step:1310/1750 train_time:129133ms step_avg:98.58ms
step:1311/1750 train_time:129237ms step_avg:98.58ms
step:1312/1750 train_time:129338ms step_avg:98.58ms
step:1313/1750 train_time:129440ms step_avg:98.58ms
step:1314/1750 train_time:129540ms step_avg:98.58ms
step:1315/1750 train_time:129641ms step_avg:98.59ms
step:1316/1750 train_time:129741ms step_avg:98.59ms
step:1317/1750 train_time:129843ms step_avg:98.59ms
step:1318/1750 train_time:129943ms step_avg:98.59ms
step:1319/1750 train_time:130045ms step_avg:98.59ms
step:1320/1750 train_time:130148ms step_avg:98.60ms
step:1321/1750 train_time:130249ms step_avg:98.60ms
step:1322/1750 train_time:130350ms step_avg:98.60ms
step:1323/1750 train_time:130451ms step_avg:98.60ms
step:1324/1750 train_time:130552ms step_avg:98.60ms
step:1325/1750 train_time:130653ms step_avg:98.61ms
step:1326/1750 train_time:130756ms step_avg:98.61ms
step:1327/1750 train_time:130858ms step_avg:98.61ms
step:1328/1750 train_time:130959ms step_avg:98.61ms
step:1329/1750 train_time:131059ms step_avg:98.61ms
step:1330/1750 train_time:131160ms step_avg:98.62ms
step:1331/1750 train_time:131262ms step_avg:98.62ms
step:1332/1750 train_time:131364ms step_avg:98.62ms
step:1333/1750 train_time:131465ms step_avg:98.62ms
step:1334/1750 train_time:131565ms step_avg:98.62ms
step:1335/1750 train_time:131665ms step_avg:98.63ms
step:1336/1750 train_time:131765ms step_avg:98.63ms
step:1337/1750 train_time:131867ms step_avg:98.63ms
step:1338/1750 train_time:131968ms step_avg:98.63ms
step:1339/1750 train_time:132069ms step_avg:98.63ms
step:1340/1750 train_time:132171ms step_avg:98.63ms
step:1341/1750 train_time:132272ms step_avg:98.64ms
step:1342/1750 train_time:132374ms step_avg:98.64ms
step:1343/1750 train_time:132475ms step_avg:98.64ms
step:1344/1750 train_time:132575ms step_avg:98.64ms
step:1345/1750 train_time:132677ms step_avg:98.64ms
step:1346/1750 train_time:132779ms step_avg:98.65ms
step:1347/1750 train_time:132880ms step_avg:98.65ms
step:1348/1750 train_time:132981ms step_avg:98.65ms
step:1349/1750 train_time:133082ms step_avg:98.65ms
step:1350/1750 train_time:133184ms step_avg:98.65ms
step:1351/1750 train_time:133285ms step_avg:98.66ms
step:1352/1750 train_time:133385ms step_avg:98.66ms
step:1353/1750 train_time:133486ms step_avg:98.66ms
step:1354/1750 train_time:133586ms step_avg:98.66ms
step:1355/1750 train_time:133688ms step_avg:98.66ms
step:1356/1750 train_time:133789ms step_avg:98.66ms
step:1357/1750 train_time:133890ms step_avg:98.67ms
step:1358/1750 train_time:133992ms step_avg:98.67ms
step:1359/1750 train_time:134093ms step_avg:98.67ms
step:1360/1750 train_time:134194ms step_avg:98.67ms
step:1361/1750 train_time:134296ms step_avg:98.67ms
step:1362/1750 train_time:134397ms step_avg:98.68ms
step:1363/1750 train_time:134498ms step_avg:98.68ms
step:1364/1750 train_time:134600ms step_avg:98.68ms
step:1365/1750 train_time:134702ms step_avg:98.68ms
step:1366/1750 train_time:134803ms step_avg:98.68ms
step:1367/1750 train_time:134903ms step_avg:98.69ms
step:1368/1750 train_time:135006ms step_avg:98.69ms
step:1369/1750 train_time:135106ms step_avg:98.69ms
step:1370/1750 train_time:135206ms step_avg:98.69ms
step:1371/1750 train_time:135306ms step_avg:98.69ms
step:1372/1750 train_time:135407ms step_avg:98.69ms
step:1373/1750 train_time:135509ms step_avg:98.70ms
step:1374/1750 train_time:135611ms step_avg:98.70ms
step:1375/1750 train_time:135713ms step_avg:98.70ms
step:1375/1750 val_loss:3.4096 train_time:135803ms step_avg:98.77ms
step:1376/1750 train_time:135825ms step_avg:98.71ms
step:1377/1750 train_time:135924ms step_avg:98.71ms
step:1378/1750 train_time:136023ms step_avg:98.71ms
step:1379/1750 train_time:136124ms step_avg:98.71ms
step:1380/1750 train_time:136226ms step_avg:98.71ms
step:1381/1750 train_time:136326ms step_avg:98.72ms
step:1382/1750 train_time:136426ms step_avg:98.72ms
step:1383/1750 train_time:136527ms step_avg:98.72ms
step:1384/1750 train_time:136628ms step_avg:98.72ms
step:1385/1750 train_time:136730ms step_avg:98.72ms
step:1386/1750 train_time:136835ms step_avg:98.73ms
step:1387/1750 train_time:136936ms step_avg:98.73ms
step:1388/1750 train_time:137037ms step_avg:98.73ms
step:1389/1750 train_time:137138ms step_avg:98.73ms
step:1390/1750 train_time:137238ms step_avg:98.73ms
step:1391/1750 train_time:137340ms step_avg:98.73ms
step:1392/1750 train_time:137439ms step_avg:98.74ms
step:1393/1750 train_time:137541ms step_avg:98.74ms
step:1394/1750 train_time:137642ms step_avg:98.74ms
step:1395/1750 train_time:137744ms step_avg:98.74ms
step:1396/1750 train_time:137845ms step_avg:98.74ms
step:1397/1750 train_time:137948ms step_avg:98.75ms
step:1398/1750 train_time:138050ms step_avg:98.75ms
step:1399/1750 train_time:138152ms step_avg:98.75ms
step:1400/1750 train_time:138253ms step_avg:98.75ms
step:1401/1750 train_time:138354ms step_avg:98.75ms
step:1402/1750 train_time:138455ms step_avg:98.76ms
step:1403/1750 train_time:138556ms step_avg:98.76ms
step:1404/1750 train_time:138657ms step_avg:98.76ms
step:1405/1750 train_time:138758ms step_avg:98.76ms
step:1406/1750 train_time:138860ms step_avg:98.76ms
step:1407/1750 train_time:138961ms step_avg:98.76ms
step:1408/1750 train_time:139061ms step_avg:98.76ms
step:1409/1750 train_time:139164ms step_avg:98.77ms
step:1410/1750 train_time:139265ms step_avg:98.77ms
step:1411/1750 train_time:139366ms step_avg:98.77ms
step:1412/1750 train_time:139469ms step_avg:98.77ms
step:1413/1750 train_time:139570ms step_avg:98.78ms
step:1414/1750 train_time:139672ms step_avg:98.78ms
step:1415/1750 train_time:139773ms step_avg:98.78ms
step:1416/1750 train_time:139873ms step_avg:98.78ms
step:1417/1750 train_time:139975ms step_avg:98.78ms
step:1418/1750 train_time:140076ms step_avg:98.78ms
step:1419/1750 train_time:140177ms step_avg:98.79ms
step:1420/1750 train_time:140279ms step_avg:98.79ms
step:1421/1750 train_time:140380ms step_avg:98.79ms
step:1422/1750 train_time:140479ms step_avg:98.79ms
step:1423/1750 train_time:140581ms step_avg:98.79ms
step:1424/1750 train_time:140683ms step_avg:98.79ms
step:1425/1750 train_time:140783ms step_avg:98.80ms
step:1426/1750 train_time:140884ms step_avg:98.80ms
step:1427/1750 train_time:140986ms step_avg:98.80ms
step:1428/1750 train_time:141088ms step_avg:98.80ms
step:1429/1750 train_time:141192ms step_avg:98.80ms
step:1430/1750 train_time:141295ms step_avg:98.81ms
step:1431/1750 train_time:141396ms step_avg:98.81ms
step:1432/1750 train_time:141498ms step_avg:98.81ms
step:1433/1750 train_time:141600ms step_avg:98.81ms
step:1434/1750 train_time:141700ms step_avg:98.81ms
step:1435/1750 train_time:141803ms step_avg:98.82ms
step:1436/1750 train_time:141905ms step_avg:98.82ms
step:1437/1750 train_time:142009ms step_avg:98.82ms
step:1438/1750 train_time:142109ms step_avg:98.82ms
step:1439/1750 train_time:142213ms step_avg:98.83ms
step:1440/1750 train_time:142317ms step_avg:98.83ms
step:1441/1750 train_time:142419ms step_avg:98.83ms
step:1442/1750 train_time:142520ms step_avg:98.83ms
step:1443/1750 train_time:142621ms step_avg:98.84ms
step:1444/1750 train_time:142722ms step_avg:98.84ms
step:1445/1750 train_time:142824ms step_avg:98.84ms
step:1446/1750 train_time:142925ms step_avg:98.84ms
step:1447/1750 train_time:143027ms step_avg:98.84ms
step:1448/1750 train_time:143131ms step_avg:98.85ms
step:1449/1750 train_time:143233ms step_avg:98.85ms
step:1450/1750 train_time:143336ms step_avg:98.85ms
step:1451/1750 train_time:143437ms step_avg:98.85ms
step:1452/1750 train_time:143540ms step_avg:98.86ms
step:1453/1750 train_time:143641ms step_avg:98.86ms
step:1454/1750 train_time:143745ms step_avg:98.86ms
step:1455/1750 train_time:143846ms step_avg:98.86ms
step:1456/1750 train_time:143947ms step_avg:98.86ms
step:1457/1750 train_time:144049ms step_avg:98.87ms
step:1458/1750 train_time:144152ms step_avg:98.87ms
step:1459/1750 train_time:144256ms step_avg:98.87ms
step:1460/1750 train_time:144357ms step_avg:98.87ms
step:1461/1750 train_time:144460ms step_avg:98.88ms
step:1462/1750 train_time:144561ms step_avg:98.88ms
step:1463/1750 train_time:144662ms step_avg:98.88ms
step:1464/1750 train_time:144764ms step_avg:98.88ms
step:1465/1750 train_time:144865ms step_avg:98.88ms
step:1466/1750 train_time:144966ms step_avg:98.89ms
step:1467/1750 train_time:145069ms step_avg:98.89ms
step:1468/1750 train_time:145171ms step_avg:98.89ms
step:1469/1750 train_time:145273ms step_avg:98.89ms
step:1470/1750 train_time:145376ms step_avg:98.90ms
step:1471/1750 train_time:145479ms step_avg:98.90ms
step:1472/1750 train_time:145580ms step_avg:98.90ms
step:1473/1750 train_time:145681ms step_avg:98.90ms
step:1474/1750 train_time:145782ms step_avg:98.90ms
step:1475/1750 train_time:145883ms step_avg:98.90ms
step:1476/1750 train_time:145986ms step_avg:98.91ms
step:1477/1750 train_time:146089ms step_avg:98.91ms
step:1478/1750 train_time:146192ms step_avg:98.91ms
step:1479/1750 train_time:146293ms step_avg:98.91ms
step:1480/1750 train_time:146396ms step_avg:98.92ms
step:1481/1750 train_time:146496ms step_avg:98.92ms
step:1482/1750 train_time:146599ms step_avg:98.92ms
step:1483/1750 train_time:146700ms step_avg:98.92ms
step:1484/1750 train_time:146803ms step_avg:98.92ms
step:1485/1750 train_time:146906ms step_avg:98.93ms
step:1486/1750 train_time:147008ms step_avg:98.93ms
step:1487/1750 train_time:147109ms step_avg:98.93ms
step:1488/1750 train_time:147213ms step_avg:98.93ms
step:1489/1750 train_time:147315ms step_avg:98.94ms
step:1490/1750 train_time:147417ms step_avg:98.94ms
step:1491/1750 train_time:147519ms step_avg:98.94ms
step:1492/1750 train_time:147620ms step_avg:98.94ms
step:1493/1750 train_time:147721ms step_avg:98.94ms
step:1494/1750 train_time:147823ms step_avg:98.94ms
step:1495/1750 train_time:147924ms step_avg:98.95ms
step:1496/1750 train_time:148027ms step_avg:98.95ms
step:1497/1750 train_time:148128ms step_avg:98.95ms
step:1498/1750 train_time:148231ms step_avg:98.95ms
step:1499/1750 train_time:148332ms step_avg:98.95ms
step:1500/1750 train_time:148435ms step_avg:98.96ms
step:1500/1750 val_loss:3.3720 train_time:148525ms step_avg:99.02ms
step:1501/1750 train_time:148547ms step_avg:98.97ms
step:1502/1750 train_time:148646ms step_avg:98.97ms
step:1503/1750 train_time:148748ms step_avg:98.97ms
step:1504/1750 train_time:148849ms step_avg:98.97ms
step:1505/1750 train_time:148951ms step_avg:98.97ms
step:1506/1750 train_time:149051ms step_avg:98.97ms
step:1507/1750 train_time:149152ms step_avg:98.97ms
step:1508/1750 train_time:149253ms step_avg:98.97ms
step:1509/1750 train_time:149356ms step_avg:98.98ms
step:1510/1750 train_time:149458ms step_avg:98.98ms
step:1511/1750 train_time:149562ms step_avg:98.98ms
step:1512/1750 train_time:149666ms step_avg:98.99ms
step:1513/1750 train_time:149768ms step_avg:98.99ms
step:1514/1750 train_time:149869ms step_avg:98.99ms
step:1515/1750 train_time:149973ms step_avg:98.99ms
step:1516/1750 train_time:150075ms step_avg:98.99ms
step:1517/1750 train_time:150175ms step_avg:98.99ms
step:1518/1750 train_time:150276ms step_avg:99.00ms
step:1519/1750 train_time:150378ms step_avg:99.00ms
step:1520/1750 train_time:150479ms step_avg:99.00ms
step:1521/1750 train_time:150581ms step_avg:99.00ms
step:1522/1750 train_time:150682ms step_avg:99.00ms
step:1523/1750 train_time:150784ms step_avg:99.00ms
step:1524/1750 train_time:150886ms step_avg:99.01ms
step:1525/1750 train_time:150990ms step_avg:99.01ms
step:1526/1750 train_time:151093ms step_avg:99.01ms
step:1527/1750 train_time:151194ms step_avg:99.01ms
step:1528/1750 train_time:151299ms step_avg:99.02ms
step:1529/1750 train_time:151400ms step_avg:99.02ms
step:1530/1750 train_time:151503ms step_avg:99.02ms
step:1531/1750 train_time:151604ms step_avg:99.02ms
step:1532/1750 train_time:151706ms step_avg:99.02ms
step:1533/1750 train_time:151807ms step_avg:99.03ms
step:1534/1750 train_time:151909ms step_avg:99.03ms
step:1535/1750 train_time:152012ms step_avg:99.03ms
step:1536/1750 train_time:152114ms step_avg:99.03ms
step:1537/1750 train_time:152215ms step_avg:99.03ms
step:1538/1750 train_time:152317ms step_avg:99.04ms
step:1539/1750 train_time:152419ms step_avg:99.04ms
step:1540/1750 train_time:152521ms step_avg:99.04ms
step:1541/1750 train_time:152624ms step_avg:99.04ms
step:1542/1750 train_time:152727ms step_avg:99.04ms
step:1543/1750 train_time:152829ms step_avg:99.05ms
step:1544/1750 train_time:152931ms step_avg:99.05ms
step:1545/1750 train_time:153033ms step_avg:99.05ms
step:1546/1750 train_time:153134ms step_avg:99.05ms
step:1547/1750 train_time:153237ms step_avg:99.05ms
step:1548/1750 train_time:153338ms step_avg:99.06ms
step:1549/1750 train_time:153440ms step_avg:99.06ms
step:1550/1750 train_time:153541ms step_avg:99.06ms
step:1551/1750 train_time:153643ms step_avg:99.06ms
step:1552/1750 train_time:153745ms step_avg:99.06ms
step:1553/1750 train_time:153848ms step_avg:99.06ms
step:1554/1750 train_time:153949ms step_avg:99.07ms
step:1555/1750 train_time:154051ms step_avg:99.07ms
step:1556/1750 train_time:154153ms step_avg:99.07ms
step:1557/1750 train_time:154256ms step_avg:99.07ms
step:1558/1750 train_time:154358ms step_avg:99.07ms
step:1559/1750 train_time:154461ms step_avg:99.08ms
step:1560/1750 train_time:154562ms step_avg:99.08ms
step:1561/1750 train_time:154664ms step_avg:99.08ms
step:1562/1750 train_time:154766ms step_avg:99.08ms
step:1563/1750 train_time:154871ms step_avg:99.09ms
step:1564/1750 train_time:154972ms step_avg:99.09ms
step:1565/1750 train_time:155073ms step_avg:99.09ms
step:1566/1750 train_time:155174ms step_avg:99.09ms
step:1567/1750 train_time:155275ms step_avg:99.09ms
step:1568/1750 train_time:155377ms step_avg:99.09ms
step:1569/1750 train_time:155479ms step_avg:99.09ms
step:1570/1750 train_time:155583ms step_avg:99.10ms
step:1571/1750 train_time:155685ms step_avg:99.10ms
step:1572/1750 train_time:155786ms step_avg:99.10ms
step:1573/1750 train_time:155888ms step_avg:99.10ms
step:1574/1750 train_time:155990ms step_avg:99.10ms
step:1575/1750 train_time:156091ms step_avg:99.11ms
step:1576/1750 train_time:156194ms step_avg:99.11ms
step:1577/1750 train_time:156298ms step_avg:99.11ms
step:1578/1750 train_time:156399ms step_avg:99.11ms
step:1579/1750 train_time:156501ms step_avg:99.11ms
step:1580/1750 train_time:156603ms step_avg:99.12ms
step:1581/1750 train_time:156705ms step_avg:99.12ms
step:1582/1750 train_time:156806ms step_avg:99.12ms
step:1583/1750 train_time:156911ms step_avg:99.12ms
step:1584/1750 train_time:157013ms step_avg:99.12ms
step:1585/1750 train_time:157115ms step_avg:99.13ms
step:1586/1750 train_time:157218ms step_avg:99.13ms
step:1587/1750 train_time:157320ms step_avg:99.13ms
step:1588/1750 train_time:157421ms step_avg:99.13ms
step:1589/1750 train_time:157523ms step_avg:99.13ms
step:1590/1750 train_time:157624ms step_avg:99.13ms
step:1591/1750 train_time:157726ms step_avg:99.14ms
step:1592/1750 train_time:157828ms step_avg:99.14ms
step:1593/1750 train_time:157931ms step_avg:99.14ms
step:1594/1750 train_time:158036ms step_avg:99.14ms
step:1595/1750 train_time:158138ms step_avg:99.15ms
step:1596/1750 train_time:158240ms step_avg:99.15ms
step:1597/1750 train_time:158341ms step_avg:99.15ms
step:1598/1750 train_time:158443ms step_avg:99.15ms
step:1599/1750 train_time:158544ms step_avg:99.15ms
step:1600/1750 train_time:158645ms step_avg:99.15ms
step:1601/1750 train_time:158747ms step_avg:99.16ms
step:1602/1750 train_time:158850ms step_avg:99.16ms
step:1603/1750 train_time:158952ms step_avg:99.16ms
step:1604/1750 train_time:159054ms step_avg:99.16ms
step:1605/1750 train_time:159156ms step_avg:99.16ms
step:1606/1750 train_time:159259ms step_avg:99.16ms
step:1607/1750 train_time:159360ms step_avg:99.17ms
step:1608/1750 train_time:159461ms step_avg:99.17ms
step:1609/1750 train_time:159562ms step_avg:99.17ms
step:1610/1750 train_time:159665ms step_avg:99.17ms
step:1611/1750 train_time:159768ms step_avg:99.17ms
step:1612/1750 train_time:159872ms step_avg:99.18ms
step:1613/1750 train_time:159973ms step_avg:99.18ms
step:1614/1750 train_time:160074ms step_avg:99.18ms
step:1615/1750 train_time:160177ms step_avg:99.18ms
step:1616/1750 train_time:160278ms step_avg:99.18ms
step:1617/1750 train_time:160380ms step_avg:99.18ms
step:1618/1750 train_time:160482ms step_avg:99.19ms
step:1619/1750 train_time:160583ms step_avg:99.19ms
step:1620/1750 train_time:160686ms step_avg:99.19ms
step:1621/1750 train_time:160787ms step_avg:99.19ms
step:1622/1750 train_time:160890ms step_avg:99.19ms
step:1623/1750 train_time:160993ms step_avg:99.19ms
step:1624/1750 train_time:161095ms step_avg:99.20ms
step:1625/1750 train_time:161199ms step_avg:99.20ms
step:1625/1750 val_loss:3.3409 train_time:161290ms step_avg:99.26ms
step:1626/1750 train_time:161313ms step_avg:99.21ms
step:1627/1750 train_time:161414ms step_avg:99.21ms
step:1628/1750 train_time:161515ms step_avg:99.21ms
step:1629/1750 train_time:161616ms step_avg:99.21ms
step:1630/1750 train_time:161717ms step_avg:99.21ms
step:1631/1750 train_time:161819ms step_avg:99.21ms
step:1632/1750 train_time:161919ms step_avg:99.22ms
step:1633/1750 train_time:162021ms step_avg:99.22ms
step:1634/1750 train_time:162125ms step_avg:99.22ms
step:1635/1750 train_time:162226ms step_avg:99.22ms
step:1636/1750 train_time:162330ms step_avg:99.22ms
step:1637/1750 train_time:162433ms step_avg:99.23ms
step:1638/1750 train_time:162535ms step_avg:99.23ms
step:1639/1750 train_time:162637ms step_avg:99.23ms
step:1640/1750 train_time:162738ms step_avg:99.23ms
step:1641/1750 train_time:162839ms step_avg:99.23ms
step:1642/1750 train_time:162940ms step_avg:99.23ms
step:1643/1750 train_time:163041ms step_avg:99.23ms
step:1644/1750 train_time:163142ms step_avg:99.23ms
step:1645/1750 train_time:163244ms step_avg:99.24ms
step:1646/1750 train_time:163347ms step_avg:99.24ms
step:1647/1750 train_time:163452ms step_avg:99.24ms
step:1648/1750 train_time:163556ms step_avg:99.24ms
step:1649/1750 train_time:163657ms step_avg:99.25ms
step:1650/1750 train_time:163758ms step_avg:99.25ms
step:1651/1750 train_time:163859ms step_avg:99.25ms
step:1652/1750 train_time:163961ms step_avg:99.25ms
step:1653/1750 train_time:164063ms step_avg:99.25ms
step:1654/1750 train_time:164164ms step_avg:99.25ms
step:1655/1750 train_time:164268ms step_avg:99.26ms
step:1656/1750 train_time:164370ms step_avg:99.26ms
step:1657/1750 train_time:164473ms step_avg:99.26ms
step:1658/1750 train_time:164576ms step_avg:99.26ms
step:1659/1750 train_time:164679ms step_avg:99.26ms
step:1660/1750 train_time:164780ms step_avg:99.26ms
step:1661/1750 train_time:164883ms step_avg:99.27ms
step:1662/1750 train_time:164986ms step_avg:99.27ms
step:1663/1750 train_time:165089ms step_avg:99.27ms
step:1664/1750 train_time:165192ms step_avg:99.27ms
step:1665/1750 train_time:165298ms step_avg:99.28ms
step:1666/1750 train_time:165400ms step_avg:99.28ms
step:1667/1750 train_time:165501ms step_avg:99.28ms
step:1668/1750 train_time:165605ms step_avg:99.28ms
step:1669/1750 train_time:165709ms step_avg:99.29ms
step:1670/1750 train_time:165811ms step_avg:99.29ms
step:1671/1750 train_time:165913ms step_avg:99.29ms
step:1672/1750 train_time:166014ms step_avg:99.29ms
step:1673/1750 train_time:166116ms step_avg:99.29ms
step:1674/1750 train_time:166217ms step_avg:99.29ms
step:1675/1750 train_time:166321ms step_avg:99.30ms
step:1676/1750 train_time:166422ms step_avg:99.30ms
step:1677/1750 train_time:166522ms step_avg:99.30ms
step:1678/1750 train_time:166625ms step_avg:99.30ms
step:1679/1750 train_time:166728ms step_avg:99.30ms
step:1680/1750 train_time:166831ms step_avg:99.30ms
step:1681/1750 train_time:166933ms step_avg:99.31ms
step:1682/1750 train_time:167038ms step_avg:99.31ms
step:1683/1750 train_time:167140ms step_avg:99.31ms
step:1684/1750 train_time:167242ms step_avg:99.31ms
step:1685/1750 train_time:167344ms step_avg:99.31ms
step:1686/1750 train_time:167445ms step_avg:99.32ms
step:1687/1750 train_time:167547ms step_avg:99.32ms
step:1688/1750 train_time:167650ms step_avg:99.32ms
step:1689/1750 train_time:167753ms step_avg:99.32ms
step:1690/1750 train_time:167856ms step_avg:99.32ms
step:1691/1750 train_time:167958ms step_avg:99.32ms
step:1692/1750 train_time:168061ms step_avg:99.33ms
step:1693/1750 train_time:168164ms step_avg:99.33ms
step:1694/1750 train_time:168267ms step_avg:99.33ms
step:1695/1750 train_time:168370ms step_avg:99.33ms
step:1696/1750 train_time:168472ms step_avg:99.34ms
step:1697/1750 train_time:168576ms step_avg:99.34ms
step:1698/1750 train_time:168678ms step_avg:99.34ms
step:1699/1750 train_time:168780ms step_avg:99.34ms
step:1700/1750 train_time:168884ms step_avg:99.34ms
step:1701/1750 train_time:168986ms step_avg:99.35ms
step:1702/1750 train_time:169092ms step_avg:99.35ms
step:1703/1750 train_time:169195ms step_avg:99.35ms
step:1704/1750 train_time:169297ms step_avg:99.35ms
step:1705/1750 train_time:169398ms step_avg:99.35ms
step:1706/1750 train_time:169501ms step_avg:99.36ms
step:1707/1750 train_time:169604ms step_avg:99.36ms
step:1708/1750 train_time:169707ms step_avg:99.36ms
step:1709/1750 train_time:169811ms step_avg:99.36ms
step:1710/1750 train_time:169913ms step_avg:99.36ms
step:1711/1750 train_time:170018ms step_avg:99.37ms
step:1712/1750 train_time:170121ms step_avg:99.37ms
step:1713/1750 train_time:170224ms step_avg:99.37ms
step:1714/1750 train_time:170326ms step_avg:99.37ms
step:1715/1750 train_time:170431ms step_avg:99.38ms
step:1716/1750 train_time:170534ms step_avg:99.38ms
step:1717/1750 train_time:170637ms step_avg:99.38ms
step:1718/1750 train_time:170738ms step_avg:99.38ms
step:1719/1750 train_time:170844ms step_avg:99.39ms
step:1720/1750 train_time:170946ms step_avg:99.39ms
step:1721/1750 train_time:171050ms step_avg:99.39ms
step:1722/1750 train_time:171153ms step_avg:99.39ms
step:1723/1750 train_time:171255ms step_avg:99.39ms
step:1724/1750 train_time:171358ms step_avg:99.40ms
step:1725/1750 train_time:171461ms step_avg:99.40ms
step:1726/1750 train_time:171563ms step_avg:99.40ms
step:1727/1750 train_time:171668ms step_avg:99.40ms
step:1728/1750 train_time:171771ms step_avg:99.40ms
step:1729/1750 train_time:171874ms step_avg:99.41ms
step:1730/1750 train_time:171977ms step_avg:99.41ms
step:1731/1750 train_time:172080ms step_avg:99.41ms
step:1732/1750 train_time:172182ms step_avg:99.41ms
step:1733/1750 train_time:172285ms step_avg:99.41ms
step:1734/1750 train_time:172390ms step_avg:99.42ms
step:1735/1750 train_time:172493ms step_avg:99.42ms
step:1736/1750 train_time:172595ms step_avg:99.42ms
step:1737/1750 train_time:172700ms step_avg:99.42ms
step:1738/1750 train_time:172802ms step_avg:99.43ms
step:1739/1750 train_time:172903ms step_avg:99.43ms
step:1740/1750 train_time:173005ms step_avg:99.43ms
step:1741/1750 train_time:173110ms step_avg:99.43ms
step:1742/1750 train_time:173213ms step_avg:99.43ms
step:1743/1750 train_time:173316ms step_avg:99.44ms
step:1744/1750 train_time:173419ms step_avg:99.44ms
step:1745/1750 train_time:173520ms step_avg:99.44ms
step:1746/1750 train_time:173623ms step_avg:99.44ms
step:1747/1750 train_time:173728ms step_avg:99.44ms
step:1748/1750 train_time:173831ms step_avg:99.45ms
step:1749/1750 train_time:173934ms step_avg:99.45ms
step:1750/1750 train_time:174036ms step_avg:99.45ms
step:1750/1750 val_loss:3.3165 train_time:174126ms step_avg:99.50ms
peak memory allocated: 33278 MiB reserved: 48954 MiB
