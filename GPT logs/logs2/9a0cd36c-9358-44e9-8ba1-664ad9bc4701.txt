import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


class NormMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, sgd_coeff=0):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, sgd_coeff=sgd_coeff)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        self.sgd_coeff = sgd_coeff

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            sgd_coeff = group["sgd_coeff"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    eps = 1e-12
                    g_normalized = grad / (grad.norm() + eps)
                    if sgd_coeff != 1:
                        update_part = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                        update = (1 - sgd_coeff) * update_part + sgd_coeff * g_normalized
                    else:
                        update = sgd_coeff * g_normalized
                    p.add_(other=update, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()
class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)
                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_reduce_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 27.5
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos : pos + max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()
    starts = []
    for i in range(1, len(boundary_positions)):
        end = boundary_positions[i].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == dist.get_world_size():
                return starts, end - pos
            start = end
    assert False # increase max_batch_span if necessary

def distributed_data_generator(filename_pattern: str, batch_size: int, align_to_bos: bool):
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    max_batch_span = 2 * batch_size if align_to_bos else batch_size # provide buffer to handle samples up to length local_batch_size
    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            batch_span = batch_size
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1750 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0)
optimizer2 = NormMuon(hidden_matrix_params, lr=0.1, momentum=0.95, weight_decay=0.0, sgd_coeff=0.5)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.11 | packaged by conda-forge | (main, Jun  4 2025, 14:45:31) [GCC 13.3.0]
Running PyTorch 2.9.0.dev20250824+cu126 compiled for CUDA 12.6
Sun Aug 24 16:41:51 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:04:00.0 Off |                    0 |
| N/A   39C    P0            121W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:23:00.0 Off |                    0 |
| N/A   32C    P0            115W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:43:00.0 Off |                    0 |
| N/A   37C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   32C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:84:00.0 Off |                    0 |
| N/A   37C    P0            121W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:A3:00.0 Off |                    0 |
| N/A   32C    P0            121W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:C3:00.0 Off |                    0 |
| N/A   38C    P0            121W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E4:00.0 Off |                    0 |
| N/A   31C    P0            113W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.05ms
step:1/1750 train_time:144ms step_avg:143.91ms
step:2/1750 train_time:164ms step_avg:82.20ms
step:3/1750 train_time:246ms step_avg:81.87ms
step:4/1750 train_time:337ms step_avg:84.28ms
step:5/1750 train_time:429ms step_avg:85.88ms
step:6/1750 train_time:522ms step_avg:86.97ms
step:7/1750 train_time:614ms step_avg:87.69ms
step:8/1750 train_time:707ms step_avg:88.32ms
step:9/1750 train_time:799ms step_avg:88.75ms
step:10/1750 train_time:891ms step_avg:89.08ms
step:11/1750 train_time:984ms step_avg:89.42ms
step:12/1750 train_time:1079ms step_avg:89.90ms
step:13/1750 train_time:1176ms step_avg:90.45ms
step:14/1750 train_time:1272ms step_avg:90.84ms
step:15/1750 train_time:1365ms step_avg:90.98ms
step:16/1750 train_time:1457ms step_avg:91.08ms
step:17/1750 train_time:1550ms step_avg:91.18ms
step:18/1750 train_time:1643ms step_avg:91.28ms
step:19/1750 train_time:1737ms step_avg:91.40ms
step:20/1750 train_time:1829ms step_avg:91.45ms
step:21/1750 train_time:1922ms step_avg:91.52ms
step:22/1750 train_time:2016ms step_avg:91.63ms
step:23/1750 train_time:2110ms step_avg:91.75ms
step:24/1750 train_time:2205ms step_avg:91.86ms
step:25/1750 train_time:2299ms step_avg:91.95ms
step:26/1750 train_time:2393ms step_avg:92.05ms
step:27/1750 train_time:2486ms step_avg:92.09ms
step:28/1750 train_time:2579ms step_avg:92.12ms
step:29/1750 train_time:2672ms step_avg:92.13ms
step:30/1750 train_time:2764ms step_avg:92.15ms
step:31/1750 train_time:2857ms step_avg:92.16ms
step:32/1750 train_time:2951ms step_avg:92.21ms
step:33/1750 train_time:3044ms step_avg:92.25ms
step:34/1750 train_time:3137ms step_avg:92.28ms
step:35/1750 train_time:3232ms step_avg:92.33ms
step:36/1750 train_time:3326ms step_avg:92.38ms
step:37/1750 train_time:3419ms step_avg:92.40ms
step:38/1750 train_time:3513ms step_avg:92.44ms
step:39/1750 train_time:3606ms step_avg:92.45ms
step:40/1750 train_time:3699ms step_avg:92.46ms
step:41/1750 train_time:3792ms step_avg:92.48ms
step:42/1750 train_time:3884ms step_avg:92.48ms
step:43/1750 train_time:3978ms step_avg:92.50ms
step:44/1750 train_time:4070ms step_avg:92.50ms
step:45/1750 train_time:4163ms step_avg:92.51ms
step:46/1750 train_time:4257ms step_avg:92.53ms
step:47/1750 train_time:4350ms step_avg:92.56ms
step:48/1750 train_time:4445ms step_avg:92.60ms
step:49/1750 train_time:4538ms step_avg:92.60ms
step:50/1750 train_time:4630ms step_avg:92.61ms
step:51/1750 train_time:4724ms step_avg:92.63ms
step:52/1750 train_time:4817ms step_avg:92.64ms
step:53/1750 train_time:4909ms step_avg:92.63ms
step:54/1750 train_time:5003ms step_avg:92.64ms
step:55/1750 train_time:5096ms step_avg:92.65ms
step:56/1750 train_time:5189ms step_avg:92.66ms
step:57/1750 train_time:5283ms step_avg:92.68ms
step:58/1750 train_time:5376ms step_avg:92.69ms
step:59/1750 train_time:5469ms step_avg:92.70ms
step:60/1750 train_time:5563ms step_avg:92.72ms
step:61/1750 train_time:5656ms step_avg:92.73ms
step:62/1750 train_time:5749ms step_avg:92.73ms
step:63/1750 train_time:5842ms step_avg:92.74ms
step:64/1750 train_time:5936ms step_avg:92.75ms
step:65/1750 train_time:6028ms step_avg:92.74ms
step:66/1750 train_time:6122ms step_avg:92.76ms
step:67/1750 train_time:6215ms step_avg:92.76ms
step:68/1750 train_time:6308ms step_avg:92.77ms
step:69/1750 train_time:6402ms step_avg:92.79ms
step:70/1750 train_time:6496ms step_avg:92.80ms
step:71/1750 train_time:6590ms step_avg:92.81ms
step:72/1750 train_time:6683ms step_avg:92.82ms
step:73/1750 train_time:6776ms step_avg:92.82ms
step:74/1750 train_time:6869ms step_avg:92.83ms
step:75/1750 train_time:6963ms step_avg:92.84ms
step:76/1750 train_time:7056ms step_avg:92.84ms
step:77/1750 train_time:7148ms step_avg:92.83ms
step:78/1750 train_time:7241ms step_avg:92.84ms
step:79/1750 train_time:7335ms step_avg:92.85ms
step:80/1750 train_time:7428ms step_avg:92.85ms
step:81/1750 train_time:7522ms step_avg:92.86ms
step:82/1750 train_time:7614ms step_avg:92.86ms
step:83/1750 train_time:7707ms step_avg:92.86ms
step:84/1750 train_time:7800ms step_avg:92.86ms
step:85/1750 train_time:7894ms step_avg:92.87ms
step:86/1750 train_time:7987ms step_avg:92.87ms
step:87/1750 train_time:8080ms step_avg:92.87ms
step:88/1750 train_time:8174ms step_avg:92.88ms
step:89/1750 train_time:8267ms step_avg:92.88ms
step:90/1750 train_time:8360ms step_avg:92.89ms
step:91/1750 train_time:8454ms step_avg:92.90ms
step:92/1750 train_time:8547ms step_avg:92.90ms
step:93/1750 train_time:8640ms step_avg:92.90ms
step:94/1750 train_time:8733ms step_avg:92.90ms
step:95/1750 train_time:8826ms step_avg:92.90ms
step:96/1750 train_time:8919ms step_avg:92.90ms
step:97/1750 train_time:9012ms step_avg:92.91ms
step:98/1750 train_time:9106ms step_avg:92.91ms
step:99/1750 train_time:9198ms step_avg:92.91ms
step:100/1750 train_time:9292ms step_avg:92.92ms
step:101/1750 train_time:9385ms step_avg:92.92ms
step:102/1750 train_time:9478ms step_avg:92.93ms
step:103/1750 train_time:9572ms step_avg:92.93ms
step:104/1750 train_time:9666ms step_avg:92.94ms
step:105/1750 train_time:9759ms step_avg:92.95ms
step:106/1750 train_time:9853ms step_avg:92.95ms
step:107/1750 train_time:9945ms step_avg:92.95ms
step:108/1750 train_time:10038ms step_avg:92.95ms
step:109/1750 train_time:10131ms step_avg:92.95ms
step:110/1750 train_time:10225ms step_avg:92.96ms
step:111/1750 train_time:10319ms step_avg:92.96ms
step:112/1750 train_time:10412ms step_avg:92.97ms
step:113/1750 train_time:10506ms step_avg:92.97ms
step:114/1750 train_time:10599ms step_avg:92.97ms
step:115/1750 train_time:10693ms step_avg:92.98ms
step:116/1750 train_time:10786ms step_avg:92.98ms
step:117/1750 train_time:10879ms step_avg:92.98ms
step:118/1750 train_time:10972ms step_avg:92.98ms
step:119/1750 train_time:11065ms step_avg:92.98ms
step:120/1750 train_time:11158ms step_avg:92.98ms
step:121/1750 train_time:11252ms step_avg:92.99ms
step:122/1750 train_time:11345ms step_avg:92.99ms
step:123/1750 train_time:11438ms step_avg:92.99ms
step:124/1750 train_time:11532ms step_avg:93.00ms
step:125/1750 train_time:11626ms step_avg:93.01ms
step:125/1750 val_loss:4.6662 train_time:11709ms step_avg:93.67ms
step:126/1750 train_time:11731ms step_avg:93.10ms
step:127/1750 train_time:11821ms step_avg:93.08ms
step:128/1750 train_time:11924ms step_avg:93.16ms
step:129/1750 train_time:12019ms step_avg:93.17ms
step:130/1750 train_time:12112ms step_avg:93.17ms
step:131/1750 train_time:12205ms step_avg:93.17ms
step:132/1750 train_time:12297ms step_avg:93.16ms
step:133/1750 train_time:12390ms step_avg:93.16ms
step:134/1750 train_time:12482ms step_avg:93.15ms
step:135/1750 train_time:12575ms step_avg:93.15ms
step:136/1750 train_time:12668ms step_avg:93.15ms
step:137/1750 train_time:12762ms step_avg:93.15ms
step:138/1750 train_time:12859ms step_avg:93.18ms
step:139/1750 train_time:12955ms step_avg:93.20ms
step:140/1750 train_time:13049ms step_avg:93.21ms
step:141/1750 train_time:13143ms step_avg:93.21ms
step:142/1750 train_time:13237ms step_avg:93.21ms
step:143/1750 train_time:13330ms step_avg:93.22ms
step:144/1750 train_time:13423ms step_avg:93.22ms
step:145/1750 train_time:13516ms step_avg:93.21ms
step:146/1750 train_time:13609ms step_avg:93.22ms
step:147/1750 train_time:13702ms step_avg:93.21ms
step:148/1750 train_time:13796ms step_avg:93.22ms
step:149/1750 train_time:13891ms step_avg:93.23ms
step:150/1750 train_time:13986ms step_avg:93.24ms
step:151/1750 train_time:14080ms step_avg:93.24ms
step:152/1750 train_time:14173ms step_avg:93.25ms
step:153/1750 train_time:14267ms step_avg:93.25ms
step:154/1750 train_time:14361ms step_avg:93.25ms
step:155/1750 train_time:14454ms step_avg:93.25ms
step:156/1750 train_time:14547ms step_avg:93.25ms
step:157/1750 train_time:14640ms step_avg:93.25ms
step:158/1750 train_time:14733ms step_avg:93.25ms
step:159/1750 train_time:14827ms step_avg:93.25ms
step:160/1750 train_time:14921ms step_avg:93.26ms
step:161/1750 train_time:15016ms step_avg:93.26ms
step:162/1750 train_time:15110ms step_avg:93.27ms
step:163/1750 train_time:15203ms step_avg:93.27ms
step:164/1750 train_time:15297ms step_avg:93.27ms
step:165/1750 train_time:15391ms step_avg:93.28ms
step:166/1750 train_time:15485ms step_avg:93.28ms
step:167/1750 train_time:15578ms step_avg:93.28ms
step:168/1750 train_time:15671ms step_avg:93.28ms
step:169/1750 train_time:15764ms step_avg:93.28ms
step:170/1750 train_time:15858ms step_avg:93.28ms
step:171/1750 train_time:15952ms step_avg:93.29ms
step:172/1750 train_time:16047ms step_avg:93.29ms
step:173/1750 train_time:16140ms step_avg:93.30ms
step:174/1750 train_time:16233ms step_avg:93.29ms
step:175/1750 train_time:16327ms step_avg:93.30ms
step:176/1750 train_time:16422ms step_avg:93.31ms
step:177/1750 train_time:16515ms step_avg:93.31ms
step:178/1750 train_time:16608ms step_avg:93.31ms
step:179/1750 train_time:16701ms step_avg:93.30ms
step:180/1750 train_time:16795ms step_avg:93.31ms
step:181/1750 train_time:16889ms step_avg:93.31ms
step:182/1750 train_time:16984ms step_avg:93.32ms
step:183/1750 train_time:17077ms step_avg:93.32ms
step:184/1750 train_time:17171ms step_avg:93.32ms
step:185/1750 train_time:17265ms step_avg:93.32ms
step:186/1750 train_time:17358ms step_avg:93.33ms
step:187/1750 train_time:17452ms step_avg:93.33ms
step:188/1750 train_time:17546ms step_avg:93.33ms
step:189/1750 train_time:17638ms step_avg:93.32ms
step:190/1750 train_time:17731ms step_avg:93.32ms
step:191/1750 train_time:17824ms step_avg:93.32ms
step:192/1750 train_time:17918ms step_avg:93.32ms
step:193/1750 train_time:18012ms step_avg:93.32ms
step:194/1750 train_time:18105ms step_avg:93.32ms
step:195/1750 train_time:18199ms step_avg:93.33ms
step:196/1750 train_time:18293ms step_avg:93.33ms
step:197/1750 train_time:18386ms step_avg:93.33ms
step:198/1750 train_time:18480ms step_avg:93.33ms
step:199/1750 train_time:18573ms step_avg:93.33ms
step:200/1750 train_time:18667ms step_avg:93.33ms
step:201/1750 train_time:18759ms step_avg:93.33ms
step:202/1750 train_time:18854ms step_avg:93.34ms
step:203/1750 train_time:18948ms step_avg:93.34ms
step:204/1750 train_time:19041ms step_avg:93.34ms
step:205/1750 train_time:19134ms step_avg:93.34ms
step:206/1750 train_time:19227ms step_avg:93.34ms
step:207/1750 train_time:19321ms step_avg:93.34ms
step:208/1750 train_time:19416ms step_avg:93.35ms
step:209/1750 train_time:19510ms step_avg:93.35ms
step:210/1750 train_time:19603ms step_avg:93.35ms
step:211/1750 train_time:19696ms step_avg:93.35ms
step:212/1750 train_time:19790ms step_avg:93.35ms
step:213/1750 train_time:19883ms step_avg:93.35ms
step:214/1750 train_time:19978ms step_avg:93.35ms
step:215/1750 train_time:20071ms step_avg:93.35ms
step:216/1750 train_time:20164ms step_avg:93.35ms
step:217/1750 train_time:20259ms step_avg:93.36ms
step:218/1750 train_time:20353ms step_avg:93.36ms
step:219/1750 train_time:20448ms step_avg:93.37ms
step:220/1750 train_time:20541ms step_avg:93.37ms
step:221/1750 train_time:20634ms step_avg:93.37ms
step:222/1750 train_time:20728ms step_avg:93.37ms
step:223/1750 train_time:20822ms step_avg:93.37ms
step:224/1750 train_time:20916ms step_avg:93.38ms
step:225/1750 train_time:21009ms step_avg:93.37ms
step:226/1750 train_time:21102ms step_avg:93.37ms
step:227/1750 train_time:21195ms step_avg:93.37ms
step:228/1750 train_time:21289ms step_avg:93.37ms
step:229/1750 train_time:21383ms step_avg:93.38ms
step:230/1750 train_time:21478ms step_avg:93.38ms
step:231/1750 train_time:21571ms step_avg:93.38ms
step:232/1750 train_time:21664ms step_avg:93.38ms
step:233/1750 train_time:21758ms step_avg:93.38ms
step:234/1750 train_time:21852ms step_avg:93.38ms
step:235/1750 train_time:21945ms step_avg:93.38ms
step:236/1750 train_time:22039ms step_avg:93.39ms
step:237/1750 train_time:22133ms step_avg:93.39ms
step:238/1750 train_time:22227ms step_avg:93.39ms
step:239/1750 train_time:22320ms step_avg:93.39ms
step:240/1750 train_time:22414ms step_avg:93.39ms
step:241/1750 train_time:22508ms step_avg:93.40ms
step:242/1750 train_time:22602ms step_avg:93.40ms
step:243/1750 train_time:22695ms step_avg:93.40ms
step:244/1750 train_time:22789ms step_avg:93.40ms
step:245/1750 train_time:22883ms step_avg:93.40ms
step:246/1750 train_time:22977ms step_avg:93.40ms
step:247/1750 train_time:23070ms step_avg:93.40ms
step:248/1750 train_time:23163ms step_avg:93.40ms
step:249/1750 train_time:23258ms step_avg:93.40ms
step:250/1750 train_time:23351ms step_avg:93.41ms
step:250/1750 val_loss:4.1164 train_time:23435ms step_avg:93.74ms
step:251/1750 train_time:23457ms step_avg:93.45ms
step:252/1750 train_time:23547ms step_avg:93.44ms
step:253/1750 train_time:23646ms step_avg:93.46ms
step:254/1750 train_time:23739ms step_avg:93.46ms
step:255/1750 train_time:23832ms step_avg:93.46ms
step:256/1750 train_time:23924ms step_avg:93.45ms
step:257/1750 train_time:24017ms step_avg:93.45ms
step:258/1750 train_time:24110ms step_avg:93.45ms
step:259/1750 train_time:24203ms step_avg:93.45ms
step:260/1750 train_time:24295ms step_avg:93.44ms
step:261/1750 train_time:24388ms step_avg:93.44ms
step:262/1750 train_time:24485ms step_avg:93.45ms
step:263/1750 train_time:24581ms step_avg:93.46ms
step:264/1750 train_time:24676ms step_avg:93.47ms
step:265/1750 train_time:24769ms step_avg:93.47ms
step:266/1750 train_time:24863ms step_avg:93.47ms
step:267/1750 train_time:24956ms step_avg:93.47ms
step:268/1750 train_time:25050ms step_avg:93.47ms
step:269/1750 train_time:25143ms step_avg:93.47ms
step:270/1750 train_time:25236ms step_avg:93.47ms
step:271/1750 train_time:25329ms step_avg:93.47ms
step:272/1750 train_time:25423ms step_avg:93.47ms
step:273/1750 train_time:25518ms step_avg:93.47ms
step:274/1750 train_time:25613ms step_avg:93.48ms
step:275/1750 train_time:25707ms step_avg:93.48ms
step:276/1750 train_time:25801ms step_avg:93.48ms
step:277/1750 train_time:25895ms step_avg:93.48ms
step:278/1750 train_time:25989ms step_avg:93.49ms
step:279/1750 train_time:26083ms step_avg:93.49ms
step:280/1750 train_time:26176ms step_avg:93.49ms
step:281/1750 train_time:26270ms step_avg:93.49ms
step:282/1750 train_time:26363ms step_avg:93.49ms
step:283/1750 train_time:26457ms step_avg:93.49ms
step:284/1750 train_time:26552ms step_avg:93.49ms
step:285/1750 train_time:26646ms step_avg:93.50ms
step:286/1750 train_time:26740ms step_avg:93.50ms
step:287/1750 train_time:26834ms step_avg:93.50ms
step:288/1750 train_time:26928ms step_avg:93.50ms
step:289/1750 train_time:27022ms step_avg:93.50ms
step:290/1750 train_time:27116ms step_avg:93.50ms
step:291/1750 train_time:27209ms step_avg:93.50ms
step:292/1750 train_time:27303ms step_avg:93.50ms
step:293/1750 train_time:27397ms step_avg:93.51ms
step:294/1750 train_time:27492ms step_avg:93.51ms
step:295/1750 train_time:27587ms step_avg:93.51ms
step:296/1750 train_time:27680ms step_avg:93.51ms
step:297/1750 train_time:27774ms step_avg:93.52ms
step:298/1750 train_time:27868ms step_avg:93.52ms
step:299/1750 train_time:27962ms step_avg:93.52ms
step:300/1750 train_time:28056ms step_avg:93.52ms
step:301/1750 train_time:28150ms step_avg:93.52ms
step:302/1750 train_time:28244ms step_avg:93.52ms
step:303/1750 train_time:28338ms step_avg:93.52ms
step:304/1750 train_time:28432ms step_avg:93.53ms
step:305/1750 train_time:28526ms step_avg:93.53ms
step:306/1750 train_time:28621ms step_avg:93.53ms
step:307/1750 train_time:28715ms step_avg:93.53ms
step:308/1750 train_time:28809ms step_avg:93.53ms
step:309/1750 train_time:28902ms step_avg:93.54ms
step:310/1750 train_time:28996ms step_avg:93.54ms
step:311/1750 train_time:29090ms step_avg:93.54ms
step:312/1750 train_time:29184ms step_avg:93.54ms
step:313/1750 train_time:29278ms step_avg:93.54ms
step:314/1750 train_time:29371ms step_avg:93.54ms
step:315/1750 train_time:29465ms step_avg:93.54ms
step:316/1750 train_time:29559ms step_avg:93.54ms
step:317/1750 train_time:29653ms step_avg:93.54ms
step:318/1750 train_time:29747ms step_avg:93.54ms
step:319/1750 train_time:29841ms step_avg:93.55ms
step:320/1750 train_time:29935ms step_avg:93.55ms
step:321/1750 train_time:30030ms step_avg:93.55ms
step:322/1750 train_time:30123ms step_avg:93.55ms
step:323/1750 train_time:30217ms step_avg:93.55ms
step:324/1750 train_time:30311ms step_avg:93.55ms
step:325/1750 train_time:30405ms step_avg:93.55ms
step:326/1750 train_time:30499ms step_avg:93.55ms
step:327/1750 train_time:30594ms step_avg:93.56ms
step:328/1750 train_time:30688ms step_avg:93.56ms
step:329/1750 train_time:30781ms step_avg:93.56ms
step:330/1750 train_time:30875ms step_avg:93.56ms
step:331/1750 train_time:30969ms step_avg:93.56ms
step:332/1750 train_time:31063ms step_avg:93.56ms
step:333/1750 train_time:31157ms step_avg:93.56ms
step:334/1750 train_time:31251ms step_avg:93.57ms
step:335/1750 train_time:31345ms step_avg:93.57ms
step:336/1750 train_time:31439ms step_avg:93.57ms
step:337/1750 train_time:31533ms step_avg:93.57ms
step:338/1750 train_time:31628ms step_avg:93.57ms
step:339/1750 train_time:31722ms step_avg:93.57ms
step:340/1750 train_time:31815ms step_avg:93.57ms
step:341/1750 train_time:31909ms step_avg:93.58ms
step:342/1750 train_time:32003ms step_avg:93.58ms
step:343/1750 train_time:32097ms step_avg:93.58ms
step:344/1750 train_time:32192ms step_avg:93.58ms
step:345/1750 train_time:32286ms step_avg:93.58ms
step:346/1750 train_time:32380ms step_avg:93.58ms
step:347/1750 train_time:32474ms step_avg:93.58ms
step:348/1750 train_time:32567ms step_avg:93.58ms
step:349/1750 train_time:32662ms step_avg:93.59ms
step:350/1750 train_time:32756ms step_avg:93.59ms
step:351/1750 train_time:32850ms step_avg:93.59ms
step:352/1750 train_time:32944ms step_avg:93.59ms
step:353/1750 train_time:33037ms step_avg:93.59ms
step:354/1750 train_time:33131ms step_avg:93.59ms
step:355/1750 train_time:33225ms step_avg:93.59ms
step:356/1750 train_time:33319ms step_avg:93.59ms
step:357/1750 train_time:33413ms step_avg:93.59ms
step:358/1750 train_time:33507ms step_avg:93.59ms
step:359/1750 train_time:33601ms step_avg:93.60ms
step:360/1750 train_time:33695ms step_avg:93.60ms
step:361/1750 train_time:33789ms step_avg:93.60ms
step:362/1750 train_time:33883ms step_avg:93.60ms
step:363/1750 train_time:33977ms step_avg:93.60ms
step:364/1750 train_time:34071ms step_avg:93.60ms
step:365/1750 train_time:34166ms step_avg:93.61ms
step:366/1750 train_time:34260ms step_avg:93.61ms
step:367/1750 train_time:34354ms step_avg:93.61ms
step:368/1750 train_time:34448ms step_avg:93.61ms
step:369/1750 train_time:34541ms step_avg:93.61ms
step:370/1750 train_time:34635ms step_avg:93.61ms
step:371/1750 train_time:34729ms step_avg:93.61ms
step:372/1750 train_time:34823ms step_avg:93.61ms
step:373/1750 train_time:34917ms step_avg:93.61ms
step:374/1750 train_time:35011ms step_avg:93.61ms
step:375/1750 train_time:35105ms step_avg:93.61ms
step:375/1750 val_loss:3.9172 train_time:35188ms step_avg:93.83ms
step:376/1750 train_time:35209ms step_avg:93.64ms
step:377/1750 train_time:35298ms step_avg:93.63ms
step:378/1750 train_time:35395ms step_avg:93.64ms
step:379/1750 train_time:35491ms step_avg:93.64ms
step:380/1750 train_time:35584ms step_avg:93.64ms
step:381/1750 train_time:35678ms step_avg:93.64ms
step:382/1750 train_time:35771ms step_avg:93.64ms
step:383/1750 train_time:35864ms step_avg:93.64ms
step:384/1750 train_time:35957ms step_avg:93.64ms
step:385/1750 train_time:36051ms step_avg:93.64ms
step:386/1750 train_time:36145ms step_avg:93.64ms
step:387/1750 train_time:36241ms step_avg:93.64ms
step:388/1750 train_time:36337ms step_avg:93.65ms
step:389/1750 train_time:36433ms step_avg:93.66ms
step:390/1750 train_time:36528ms step_avg:93.66ms
step:391/1750 train_time:36625ms step_avg:93.67ms
step:392/1750 train_time:36719ms step_avg:93.67ms
step:393/1750 train_time:36815ms step_avg:93.68ms
step:394/1750 train_time:36910ms step_avg:93.68ms
step:395/1750 train_time:37005ms step_avg:93.68ms
step:396/1750 train_time:37100ms step_avg:93.69ms
step:397/1750 train_time:37197ms step_avg:93.69ms
step:398/1750 train_time:37293ms step_avg:93.70ms
step:399/1750 train_time:37390ms step_avg:93.71ms
step:400/1750 train_time:37486ms step_avg:93.71ms
step:401/1750 train_time:37582ms step_avg:93.72ms
step:402/1750 train_time:37677ms step_avg:93.72ms
step:403/1750 train_time:37773ms step_avg:93.73ms
step:404/1750 train_time:37869ms step_avg:93.73ms
step:405/1750 train_time:37964ms step_avg:93.74ms
step:406/1750 train_time:38060ms step_avg:93.74ms
step:407/1750 train_time:38156ms step_avg:93.75ms
step:408/1750 train_time:38252ms step_avg:93.75ms
step:409/1750 train_time:38348ms step_avg:93.76ms
step:410/1750 train_time:38444ms step_avg:93.77ms
step:411/1750 train_time:38540ms step_avg:93.77ms
step:412/1750 train_time:38636ms step_avg:93.78ms
step:413/1750 train_time:38732ms step_avg:93.78ms
step:414/1750 train_time:38828ms step_avg:93.79ms
step:415/1750 train_time:38923ms step_avg:93.79ms
step:416/1750 train_time:39019ms step_avg:93.80ms
step:417/1750 train_time:39114ms step_avg:93.80ms
step:418/1750 train_time:39210ms step_avg:93.80ms
step:419/1750 train_time:39307ms step_avg:93.81ms
step:420/1750 train_time:39403ms step_avg:93.82ms
step:421/1750 train_time:39499ms step_avg:93.82ms
step:422/1750 train_time:39595ms step_avg:93.83ms
step:423/1750 train_time:39691ms step_avg:93.83ms
step:424/1750 train_time:39787ms step_avg:93.84ms
step:425/1750 train_time:39882ms step_avg:93.84ms
step:426/1750 train_time:39977ms step_avg:93.84ms
step:427/1750 train_time:40073ms step_avg:93.85ms
step:428/1750 train_time:40169ms step_avg:93.85ms
step:429/1750 train_time:40265ms step_avg:93.86ms
step:430/1750 train_time:40361ms step_avg:93.86ms
step:431/1750 train_time:40457ms step_avg:93.87ms
step:432/1750 train_time:40553ms step_avg:93.87ms
step:433/1750 train_time:40649ms step_avg:93.88ms
step:434/1750 train_time:40744ms step_avg:93.88ms
step:435/1750 train_time:40841ms step_avg:93.89ms
step:436/1750 train_time:40936ms step_avg:93.89ms
step:437/1750 train_time:41032ms step_avg:93.90ms
step:438/1750 train_time:41128ms step_avg:93.90ms
step:439/1750 train_time:41224ms step_avg:93.90ms
step:440/1750 train_time:41319ms step_avg:93.91ms
step:441/1750 train_time:41415ms step_avg:93.91ms
step:442/1750 train_time:41512ms step_avg:93.92ms
step:443/1750 train_time:41607ms step_avg:93.92ms
step:444/1750 train_time:41704ms step_avg:93.93ms
step:445/1750 train_time:41799ms step_avg:93.93ms
step:446/1750 train_time:41894ms step_avg:93.93ms
step:447/1750 train_time:41991ms step_avg:93.94ms
step:448/1750 train_time:42086ms step_avg:93.94ms
step:449/1750 train_time:42182ms step_avg:93.95ms
step:450/1750 train_time:42277ms step_avg:93.95ms
step:451/1750 train_time:42373ms step_avg:93.95ms
step:452/1750 train_time:42470ms step_avg:93.96ms
step:453/1750 train_time:42566ms step_avg:93.96ms
step:454/1750 train_time:42662ms step_avg:93.97ms
step:455/1750 train_time:42758ms step_avg:93.97ms
step:456/1750 train_time:42854ms step_avg:93.98ms
step:457/1750 train_time:42951ms step_avg:93.98ms
step:458/1750 train_time:43046ms step_avg:93.99ms
step:459/1750 train_time:43141ms step_avg:93.99ms
step:460/1750 train_time:43236ms step_avg:93.99ms
step:461/1750 train_time:43332ms step_avg:94.00ms
step:462/1750 train_time:43429ms step_avg:94.00ms
step:463/1750 train_time:43524ms step_avg:94.00ms
step:464/1750 train_time:43621ms step_avg:94.01ms
step:465/1750 train_time:43717ms step_avg:94.01ms
step:466/1750 train_time:43813ms step_avg:94.02ms
step:467/1750 train_time:43908ms step_avg:94.02ms
step:468/1750 train_time:44004ms step_avg:94.02ms
step:469/1750 train_time:44099ms step_avg:94.03ms
step:470/1750 train_time:44195ms step_avg:94.03ms
step:471/1750 train_time:44291ms step_avg:94.04ms
step:472/1750 train_time:44387ms step_avg:94.04ms
step:473/1750 train_time:44482ms step_avg:94.04ms
step:474/1750 train_time:44578ms step_avg:94.05ms
step:475/1750 train_time:44674ms step_avg:94.05ms
step:476/1750 train_time:44771ms step_avg:94.06ms
step:477/1750 train_time:44867ms step_avg:94.06ms
step:478/1750 train_time:44962ms step_avg:94.06ms
step:479/1750 train_time:45058ms step_avg:94.07ms
step:480/1750 train_time:45153ms step_avg:94.07ms
step:481/1750 train_time:45249ms step_avg:94.07ms
step:482/1750 train_time:45344ms step_avg:94.08ms
step:483/1750 train_time:45440ms step_avg:94.08ms
step:484/1750 train_time:45536ms step_avg:94.08ms
step:485/1750 train_time:45633ms step_avg:94.09ms
step:486/1750 train_time:45729ms step_avg:94.09ms
step:487/1750 train_time:45825ms step_avg:94.10ms
step:488/1750 train_time:45921ms step_avg:94.10ms
step:489/1750 train_time:46017ms step_avg:94.10ms
step:490/1750 train_time:46113ms step_avg:94.11ms
step:491/1750 train_time:46209ms step_avg:94.11ms
step:492/1750 train_time:46304ms step_avg:94.11ms
step:493/1750 train_time:46400ms step_avg:94.12ms
step:494/1750 train_time:46496ms step_avg:94.12ms
step:495/1750 train_time:46592ms step_avg:94.12ms
step:496/1750 train_time:46687ms step_avg:94.13ms
step:497/1750 train_time:46783ms step_avg:94.13ms
step:498/1750 train_time:46878ms step_avg:94.13ms
step:499/1750 train_time:46974ms step_avg:94.14ms
step:500/1750 train_time:47071ms step_avg:94.14ms
step:500/1750 val_loss:3.7633 train_time:47155ms step_avg:94.31ms
step:501/1750 train_time:47177ms step_avg:94.16ms
step:502/1750 train_time:47275ms step_avg:94.17ms
step:503/1750 train_time:47374ms step_avg:94.18ms
step:504/1750 train_time:47470ms step_avg:94.19ms
step:505/1750 train_time:47566ms step_avg:94.19ms
step:506/1750 train_time:47661ms step_avg:94.19ms
step:507/1750 train_time:47755ms step_avg:94.19ms
step:508/1750 train_time:47850ms step_avg:94.19ms
step:509/1750 train_time:47944ms step_avg:94.19ms
step:510/1750 train_time:48039ms step_avg:94.19ms
step:511/1750 train_time:48135ms step_avg:94.20ms
step:512/1750 train_time:48233ms step_avg:94.21ms
step:513/1750 train_time:48331ms step_avg:94.21ms
step:514/1750 train_time:48429ms step_avg:94.22ms
step:515/1750 train_time:48526ms step_avg:94.22ms
step:516/1750 train_time:48622ms step_avg:94.23ms
step:517/1750 train_time:48717ms step_avg:94.23ms
step:518/1750 train_time:48812ms step_avg:94.23ms
step:519/1750 train_time:48908ms step_avg:94.23ms
step:520/1750 train_time:49003ms step_avg:94.24ms
step:521/1750 train_time:49099ms step_avg:94.24ms
step:522/1750 train_time:49196ms step_avg:94.24ms
step:523/1750 train_time:49294ms step_avg:94.25ms
step:524/1750 train_time:49391ms step_avg:94.26ms
step:525/1750 train_time:49489ms step_avg:94.26ms
step:526/1750 train_time:49586ms step_avg:94.27ms
step:527/1750 train_time:49683ms step_avg:94.27ms
step:528/1750 train_time:49779ms step_avg:94.28ms
step:529/1750 train_time:49875ms step_avg:94.28ms
step:530/1750 train_time:49970ms step_avg:94.28ms
step:531/1750 train_time:50065ms step_avg:94.29ms
step:532/1750 train_time:50161ms step_avg:94.29ms
step:533/1750 train_time:50259ms step_avg:94.30ms
step:534/1750 train_time:50357ms step_avg:94.30ms
step:535/1750 train_time:50454ms step_avg:94.31ms
step:536/1750 train_time:50551ms step_avg:94.31ms
step:537/1750 train_time:50647ms step_avg:94.32ms
step:538/1750 train_time:50743ms step_avg:94.32ms
step:539/1750 train_time:50839ms step_avg:94.32ms
step:540/1750 train_time:50935ms step_avg:94.32ms
step:541/1750 train_time:51030ms step_avg:94.33ms
step:542/1750 train_time:51127ms step_avg:94.33ms
step:543/1750 train_time:51223ms step_avg:94.33ms
step:544/1750 train_time:51319ms step_avg:94.34ms
step:545/1750 train_time:51416ms step_avg:94.34ms
step:546/1750 train_time:51512ms step_avg:94.34ms
step:547/1750 train_time:51608ms step_avg:94.35ms
step:548/1750 train_time:51704ms step_avg:94.35ms
step:549/1750 train_time:51800ms step_avg:94.35ms
step:550/1750 train_time:51897ms step_avg:94.36ms
step:551/1750 train_time:51993ms step_avg:94.36ms
step:552/1750 train_time:52088ms step_avg:94.36ms
step:553/1750 train_time:52184ms step_avg:94.37ms
step:554/1750 train_time:52281ms step_avg:94.37ms
step:555/1750 train_time:52377ms step_avg:94.37ms
step:556/1750 train_time:52474ms step_avg:94.38ms
step:557/1750 train_time:52570ms step_avg:94.38ms
step:558/1750 train_time:52667ms step_avg:94.38ms
step:559/1750 train_time:52762ms step_avg:94.39ms
step:560/1750 train_time:52858ms step_avg:94.39ms
step:561/1750 train_time:52954ms step_avg:94.39ms
step:562/1750 train_time:53050ms step_avg:94.40ms
step:563/1750 train_time:53147ms step_avg:94.40ms
step:564/1750 train_time:53242ms step_avg:94.40ms
step:565/1750 train_time:53339ms step_avg:94.41ms
step:566/1750 train_time:53435ms step_avg:94.41ms
step:567/1750 train_time:53531ms step_avg:94.41ms
step:568/1750 train_time:53628ms step_avg:94.42ms
step:569/1750 train_time:53724ms step_avg:94.42ms
step:570/1750 train_time:53821ms step_avg:94.42ms
step:571/1750 train_time:53917ms step_avg:94.43ms
step:572/1750 train_time:54014ms step_avg:94.43ms
step:573/1750 train_time:54111ms step_avg:94.43ms
step:574/1750 train_time:54207ms step_avg:94.44ms
step:575/1750 train_time:54303ms step_avg:94.44ms
step:576/1750 train_time:54399ms step_avg:94.44ms
step:577/1750 train_time:54495ms step_avg:94.45ms
step:578/1750 train_time:54591ms step_avg:94.45ms
step:579/1750 train_time:54689ms step_avg:94.45ms
step:580/1750 train_time:54785ms step_avg:94.46ms
step:581/1750 train_time:54882ms step_avg:94.46ms
step:582/1750 train_time:54978ms step_avg:94.46ms
step:583/1750 train_time:55074ms step_avg:94.47ms
step:584/1750 train_time:55171ms step_avg:94.47ms
step:585/1750 train_time:55267ms step_avg:94.47ms
step:586/1750 train_time:55363ms step_avg:94.48ms
step:587/1750 train_time:55459ms step_avg:94.48ms
step:588/1750 train_time:55556ms step_avg:94.48ms
step:589/1750 train_time:55652ms step_avg:94.49ms
step:590/1750 train_time:55748ms step_avg:94.49ms
step:591/1750 train_time:55845ms step_avg:94.49ms
step:592/1750 train_time:55941ms step_avg:94.49ms
step:593/1750 train_time:56038ms step_avg:94.50ms
step:594/1750 train_time:56134ms step_avg:94.50ms
step:595/1750 train_time:56230ms step_avg:94.50ms
step:596/1750 train_time:56326ms step_avg:94.51ms
step:597/1750 train_time:56421ms step_avg:94.51ms
step:598/1750 train_time:56517ms step_avg:94.51ms
step:599/1750 train_time:56614ms step_avg:94.51ms
step:600/1750 train_time:56710ms step_avg:94.52ms
step:601/1750 train_time:56807ms step_avg:94.52ms
step:602/1750 train_time:56903ms step_avg:94.52ms
step:603/1750 train_time:56999ms step_avg:94.53ms
step:604/1750 train_time:57095ms step_avg:94.53ms
step:605/1750 train_time:57191ms step_avg:94.53ms
step:606/1750 train_time:57287ms step_avg:94.53ms
step:607/1750 train_time:57384ms step_avg:94.54ms
step:608/1750 train_time:57481ms step_avg:94.54ms
step:609/1750 train_time:57578ms step_avg:94.54ms
step:610/1750 train_time:57675ms step_avg:94.55ms
step:611/1750 train_time:57771ms step_avg:94.55ms
step:612/1750 train_time:57867ms step_avg:94.55ms
step:613/1750 train_time:57964ms step_avg:94.56ms
step:614/1750 train_time:58060ms step_avg:94.56ms
step:615/1750 train_time:58155ms step_avg:94.56ms
step:616/1750 train_time:58251ms step_avg:94.56ms
step:617/1750 train_time:58348ms step_avg:94.57ms
step:618/1750 train_time:58445ms step_avg:94.57ms
step:619/1750 train_time:58542ms step_avg:94.58ms
step:620/1750 train_time:58638ms step_avg:94.58ms
step:621/1750 train_time:58735ms step_avg:94.58ms
step:622/1750 train_time:58831ms step_avg:94.58ms
step:623/1750 train_time:58928ms step_avg:94.59ms
step:624/1750 train_time:59024ms step_avg:94.59ms
step:625/1750 train_time:59120ms step_avg:94.59ms
step:625/1750 val_loss:3.6746 train_time:59205ms step_avg:94.73ms
step:626/1750 train_time:59226ms step_avg:94.61ms
step:627/1750 train_time:59319ms step_avg:94.61ms
step:628/1750 train_time:59416ms step_avg:94.61ms
step:629/1750 train_time:59512ms step_avg:94.61ms
step:630/1750 train_time:59607ms step_avg:94.61ms
step:631/1750 train_time:59703ms step_avg:94.62ms
step:632/1750 train_time:59798ms step_avg:94.62ms
step:633/1750 train_time:59893ms step_avg:94.62ms
step:634/1750 train_time:59988ms step_avg:94.62ms
step:635/1750 train_time:60084ms step_avg:94.62ms
step:636/1750 train_time:60181ms step_avg:94.62ms
step:637/1750 train_time:60280ms step_avg:94.63ms
step:638/1750 train_time:60377ms step_avg:94.63ms
step:639/1750 train_time:60474ms step_avg:94.64ms
step:640/1750 train_time:60570ms step_avg:94.64ms
step:641/1750 train_time:60665ms step_avg:94.64ms
step:642/1750 train_time:60761ms step_avg:94.64ms
step:643/1750 train_time:60856ms step_avg:94.64ms
step:644/1750 train_time:60953ms step_avg:94.65ms
step:645/1750 train_time:61049ms step_avg:94.65ms
step:646/1750 train_time:61145ms step_avg:94.65ms
step:647/1750 train_time:61241ms step_avg:94.65ms
step:648/1750 train_time:61339ms step_avg:94.66ms
step:649/1750 train_time:61435ms step_avg:94.66ms
step:650/1750 train_time:61532ms step_avg:94.67ms
step:651/1750 train_time:61630ms step_avg:94.67ms
step:652/1750 train_time:61728ms step_avg:94.67ms
step:653/1750 train_time:61825ms step_avg:94.68ms
step:654/1750 train_time:61922ms step_avg:94.68ms
step:655/1750 train_time:62021ms step_avg:94.69ms
step:656/1750 train_time:62118ms step_avg:94.69ms
step:657/1750 train_time:62216ms step_avg:94.70ms
step:658/1750 train_time:62314ms step_avg:94.70ms
step:659/1750 train_time:62412ms step_avg:94.71ms
step:660/1750 train_time:62509ms step_avg:94.71ms
step:661/1750 train_time:62607ms step_avg:94.72ms
step:662/1750 train_time:62704ms step_avg:94.72ms
step:663/1750 train_time:62802ms step_avg:94.72ms
step:664/1750 train_time:62900ms step_avg:94.73ms
step:665/1750 train_time:62999ms step_avg:94.73ms
step:666/1750 train_time:63096ms step_avg:94.74ms
step:667/1750 train_time:63194ms step_avg:94.74ms
step:668/1750 train_time:63291ms step_avg:94.75ms
step:669/1750 train_time:63390ms step_avg:94.75ms
step:670/1750 train_time:63487ms step_avg:94.76ms
step:671/1750 train_time:63585ms step_avg:94.76ms
step:672/1750 train_time:63682ms step_avg:94.76ms
step:673/1750 train_time:63780ms step_avg:94.77ms
step:674/1750 train_time:63878ms step_avg:94.77ms
step:675/1750 train_time:63975ms step_avg:94.78ms
step:676/1750 train_time:64073ms step_avg:94.78ms
step:677/1750 train_time:64170ms step_avg:94.79ms
step:678/1750 train_time:64267ms step_avg:94.79ms
step:679/1750 train_time:64365ms step_avg:94.79ms
step:680/1750 train_time:64463ms step_avg:94.80ms
step:681/1750 train_time:64561ms step_avg:94.80ms
step:682/1750 train_time:64659ms step_avg:94.81ms
step:683/1750 train_time:64756ms step_avg:94.81ms
step:684/1750 train_time:64854ms step_avg:94.82ms
step:685/1750 train_time:64952ms step_avg:94.82ms
step:686/1750 train_time:65050ms step_avg:94.83ms
step:687/1750 train_time:65147ms step_avg:94.83ms
step:688/1750 train_time:65244ms step_avg:94.83ms
step:689/1750 train_time:65342ms step_avg:94.84ms
step:690/1750 train_time:65440ms step_avg:94.84ms
step:691/1750 train_time:65538ms step_avg:94.84ms
step:692/1750 train_time:65636ms step_avg:94.85ms
step:693/1750 train_time:65734ms step_avg:94.85ms
step:694/1750 train_time:65831ms step_avg:94.86ms
step:695/1750 train_time:65930ms step_avg:94.86ms
step:696/1750 train_time:66029ms step_avg:94.87ms
step:697/1750 train_time:66126ms step_avg:94.87ms
step:698/1750 train_time:66222ms step_avg:94.87ms
step:699/1750 train_time:66321ms step_avg:94.88ms
step:700/1750 train_time:66419ms step_avg:94.88ms
step:701/1750 train_time:66516ms step_avg:94.89ms
step:702/1750 train_time:66613ms step_avg:94.89ms
step:703/1750 train_time:66711ms step_avg:94.89ms
step:704/1750 train_time:66809ms step_avg:94.90ms
step:705/1750 train_time:66907ms step_avg:94.90ms
step:706/1750 train_time:67004ms step_avg:94.91ms
step:707/1750 train_time:67103ms step_avg:94.91ms
step:708/1750 train_time:67200ms step_avg:94.92ms
step:709/1750 train_time:67299ms step_avg:94.92ms
step:710/1750 train_time:67397ms step_avg:94.92ms
step:711/1750 train_time:67494ms step_avg:94.93ms
step:712/1750 train_time:67592ms step_avg:94.93ms
step:713/1750 train_time:67689ms step_avg:94.94ms
step:714/1750 train_time:67787ms step_avg:94.94ms
step:715/1750 train_time:67884ms step_avg:94.94ms
step:716/1750 train_time:67982ms step_avg:94.95ms
step:717/1750 train_time:68080ms step_avg:94.95ms
step:718/1750 train_time:68178ms step_avg:94.96ms
step:719/1750 train_time:68275ms step_avg:94.96ms
step:720/1750 train_time:68372ms step_avg:94.96ms
step:721/1750 train_time:68470ms step_avg:94.97ms
step:722/1750 train_time:68568ms step_avg:94.97ms
step:723/1750 train_time:68665ms step_avg:94.97ms
step:724/1750 train_time:68763ms step_avg:94.98ms
step:725/1750 train_time:68861ms step_avg:94.98ms
step:726/1750 train_time:68959ms step_avg:94.98ms
step:727/1750 train_time:69056ms step_avg:94.99ms
step:728/1750 train_time:69153ms step_avg:94.99ms
step:729/1750 train_time:69250ms step_avg:94.99ms
step:730/1750 train_time:69348ms step_avg:95.00ms
step:731/1750 train_time:69446ms step_avg:95.00ms
step:732/1750 train_time:69544ms step_avg:95.01ms
step:733/1750 train_time:69642ms step_avg:95.01ms
step:734/1750 train_time:69740ms step_avg:95.01ms
step:735/1750 train_time:69838ms step_avg:95.02ms
step:736/1750 train_time:69935ms step_avg:95.02ms
step:737/1750 train_time:70032ms step_avg:95.02ms
step:738/1750 train_time:70130ms step_avg:95.03ms
step:739/1750 train_time:70227ms step_avg:95.03ms
step:740/1750 train_time:70325ms step_avg:95.03ms
step:741/1750 train_time:70423ms step_avg:95.04ms
step:742/1750 train_time:70520ms step_avg:95.04ms
step:743/1750 train_time:70619ms step_avg:95.05ms
step:744/1750 train_time:70717ms step_avg:95.05ms
step:745/1750 train_time:70815ms step_avg:95.05ms
step:746/1750 train_time:70912ms step_avg:95.06ms
step:747/1750 train_time:71010ms step_avg:95.06ms
step:748/1750 train_time:71107ms step_avg:95.06ms
step:749/1750 train_time:71205ms step_avg:95.07ms
step:750/1750 train_time:71302ms step_avg:95.07ms
step:750/1750 val_loss:3.6102 train_time:71390ms step_avg:95.19ms
step:751/1750 train_time:71411ms step_avg:95.09ms
step:752/1750 train_time:71508ms step_avg:95.09ms
step:753/1750 train_time:71610ms step_avg:95.10ms
step:754/1750 train_time:71708ms step_avg:95.10ms
step:755/1750 train_time:71806ms step_avg:95.11ms
step:756/1750 train_time:71903ms step_avg:95.11ms
step:757/1750 train_time:72000ms step_avg:95.11ms
step:758/1750 train_time:72097ms step_avg:95.11ms
step:759/1750 train_time:72193ms step_avg:95.12ms
step:760/1750 train_time:72290ms step_avg:95.12ms
step:761/1750 train_time:72389ms step_avg:95.12ms
step:762/1750 train_time:72488ms step_avg:95.13ms
step:763/1750 train_time:72588ms step_avg:95.13ms
step:764/1750 train_time:72687ms step_avg:95.14ms
step:765/1750 train_time:72785ms step_avg:95.14ms
step:766/1750 train_time:72882ms step_avg:95.15ms
step:767/1750 train_time:72979ms step_avg:95.15ms
step:768/1750 train_time:73076ms step_avg:95.15ms
step:769/1750 train_time:73173ms step_avg:95.15ms
step:770/1750 train_time:73270ms step_avg:95.16ms
step:771/1750 train_time:73368ms step_avg:95.16ms
step:772/1750 train_time:73467ms step_avg:95.16ms
step:773/1750 train_time:73567ms step_avg:95.17ms
step:774/1750 train_time:73665ms step_avg:95.17ms
step:775/1750 train_time:73764ms step_avg:95.18ms
step:776/1750 train_time:73861ms step_avg:95.18ms
step:777/1750 train_time:73958ms step_avg:95.18ms
step:778/1750 train_time:74055ms step_avg:95.19ms
step:779/1750 train_time:74152ms step_avg:95.19ms
step:780/1750 train_time:74249ms step_avg:95.19ms
step:781/1750 train_time:74347ms step_avg:95.20ms
step:782/1750 train_time:74446ms step_avg:95.20ms
step:783/1750 train_time:74545ms step_avg:95.20ms
step:784/1750 train_time:74643ms step_avg:95.21ms
step:785/1750 train_time:74742ms step_avg:95.21ms
step:786/1750 train_time:74840ms step_avg:95.22ms
step:787/1750 train_time:74937ms step_avg:95.22ms
step:788/1750 train_time:75035ms step_avg:95.22ms
step:789/1750 train_time:75133ms step_avg:95.23ms
step:790/1750 train_time:75230ms step_avg:95.23ms
step:791/1750 train_time:75328ms step_avg:95.23ms
step:792/1750 train_time:75426ms step_avg:95.23ms
step:793/1750 train_time:75523ms step_avg:95.24ms
step:794/1750 train_time:75620ms step_avg:95.24ms
step:795/1750 train_time:75718ms step_avg:95.24ms
step:796/1750 train_time:75818ms step_avg:95.25ms
step:797/1750 train_time:75916ms step_avg:95.25ms
step:798/1750 train_time:76013ms step_avg:95.25ms
step:799/1750 train_time:76111ms step_avg:95.26ms
step:800/1750 train_time:76208ms step_avg:95.26ms
step:801/1750 train_time:76305ms step_avg:95.26ms
step:802/1750 train_time:76403ms step_avg:95.27ms
step:803/1750 train_time:76501ms step_avg:95.27ms
step:804/1750 train_time:76599ms step_avg:95.27ms
step:805/1750 train_time:76697ms step_avg:95.28ms
step:806/1750 train_time:76797ms step_avg:95.28ms
step:807/1750 train_time:76895ms step_avg:95.28ms
step:808/1750 train_time:76993ms step_avg:95.29ms
step:809/1750 train_time:77090ms step_avg:95.29ms
step:810/1750 train_time:77187ms step_avg:95.29ms
step:811/1750 train_time:77285ms step_avg:95.30ms
step:812/1750 train_time:77382ms step_avg:95.30ms
step:813/1750 train_time:77479ms step_avg:95.30ms
step:814/1750 train_time:77577ms step_avg:95.30ms
step:815/1750 train_time:77675ms step_avg:95.31ms
step:816/1750 train_time:77773ms step_avg:95.31ms
step:817/1750 train_time:77871ms step_avg:95.31ms
step:818/1750 train_time:77970ms step_avg:95.32ms
step:819/1750 train_time:78068ms step_avg:95.32ms
step:820/1750 train_time:78166ms step_avg:95.32ms
step:821/1750 train_time:78264ms step_avg:95.33ms
step:822/1750 train_time:78362ms step_avg:95.33ms
step:823/1750 train_time:78459ms step_avg:95.33ms
step:824/1750 train_time:78558ms step_avg:95.34ms
step:825/1750 train_time:78656ms step_avg:95.34ms
step:826/1750 train_time:78755ms step_avg:95.34ms
step:827/1750 train_time:78853ms step_avg:95.35ms
step:828/1750 train_time:78950ms step_avg:95.35ms
step:829/1750 train_time:79048ms step_avg:95.35ms
step:830/1750 train_time:79146ms step_avg:95.36ms
step:831/1750 train_time:79245ms step_avg:95.36ms
step:832/1750 train_time:79343ms step_avg:95.36ms
step:833/1750 train_time:79441ms step_avg:95.37ms
step:834/1750 train_time:79539ms step_avg:95.37ms
step:835/1750 train_time:79637ms step_avg:95.37ms
step:836/1750 train_time:79736ms step_avg:95.38ms
step:837/1750 train_time:79834ms step_avg:95.38ms
step:838/1750 train_time:79933ms step_avg:95.39ms
step:839/1750 train_time:80030ms step_avg:95.39ms
step:840/1750 train_time:80128ms step_avg:95.39ms
step:841/1750 train_time:80226ms step_avg:95.39ms
step:842/1750 train_time:80325ms step_avg:95.40ms
step:843/1750 train_time:80423ms step_avg:95.40ms
step:844/1750 train_time:80520ms step_avg:95.40ms
step:845/1750 train_time:80618ms step_avg:95.41ms
step:846/1750 train_time:80717ms step_avg:95.41ms
step:847/1750 train_time:80815ms step_avg:95.41ms
step:848/1750 train_time:80913ms step_avg:95.42ms
step:849/1750 train_time:81011ms step_avg:95.42ms
step:850/1750 train_time:81109ms step_avg:95.42ms
step:851/1750 train_time:81207ms step_avg:95.43ms
step:852/1750 train_time:81306ms step_avg:95.43ms
step:853/1750 train_time:81404ms step_avg:95.43ms
step:854/1750 train_time:81501ms step_avg:95.43ms
step:855/1750 train_time:81599ms step_avg:95.44ms
step:856/1750 train_time:81698ms step_avg:95.44ms
step:857/1750 train_time:81795ms step_avg:95.44ms
step:858/1750 train_time:81893ms step_avg:95.45ms
step:859/1750 train_time:81990ms step_avg:95.45ms
step:860/1750 train_time:82088ms step_avg:95.45ms
step:861/1750 train_time:82186ms step_avg:95.45ms
step:862/1750 train_time:82283ms step_avg:95.46ms
step:863/1750 train_time:82381ms step_avg:95.46ms
step:864/1750 train_time:82479ms step_avg:95.46ms
step:865/1750 train_time:82576ms step_avg:95.46ms
step:866/1750 train_time:82674ms step_avg:95.47ms
step:867/1750 train_time:82771ms step_avg:95.47ms
step:868/1750 train_time:82869ms step_avg:95.47ms
step:869/1750 train_time:82967ms step_avg:95.47ms
step:870/1750 train_time:83065ms step_avg:95.48ms
step:871/1750 train_time:83165ms step_avg:95.48ms
step:872/1750 train_time:83262ms step_avg:95.48ms
step:873/1750 train_time:83359ms step_avg:95.49ms
step:874/1750 train_time:83457ms step_avg:95.49ms
step:875/1750 train_time:83553ms step_avg:95.49ms
step:875/1750 val_loss:3.5608 train_time:83640ms step_avg:95.59ms
step:876/1750 train_time:83661ms step_avg:95.50ms
step:877/1750 train_time:83760ms step_avg:95.51ms
step:878/1750 train_time:83859ms step_avg:95.51ms
step:879/1750 train_time:83957ms step_avg:95.51ms
step:880/1750 train_time:84054ms step_avg:95.52ms
step:881/1750 train_time:84151ms step_avg:95.52ms
step:882/1750 train_time:84250ms step_avg:95.52ms
step:883/1750 train_time:84347ms step_avg:95.52ms
step:884/1750 train_time:84443ms step_avg:95.52ms
step:885/1750 train_time:84540ms step_avg:95.53ms
step:886/1750 train_time:84638ms step_avg:95.53ms
step:887/1750 train_time:84739ms step_avg:95.53ms
step:888/1750 train_time:84837ms step_avg:95.54ms
step:889/1750 train_time:84935ms step_avg:95.54ms
step:890/1750 train_time:85034ms step_avg:95.54ms
step:891/1750 train_time:85131ms step_avg:95.55ms
step:892/1750 train_time:85229ms step_avg:95.55ms
step:893/1750 train_time:85325ms step_avg:95.55ms
step:894/1750 train_time:85423ms step_avg:95.55ms
step:895/1750 train_time:85520ms step_avg:95.55ms
step:896/1750 train_time:85617ms step_avg:95.56ms
step:897/1750 train_time:85716ms step_avg:95.56ms
step:898/1750 train_time:85815ms step_avg:95.56ms
step:899/1750 train_time:85914ms step_avg:95.57ms
step:900/1750 train_time:86013ms step_avg:95.57ms
step:901/1750 train_time:86112ms step_avg:95.57ms
step:902/1750 train_time:86210ms step_avg:95.58ms
step:903/1750 train_time:86309ms step_avg:95.58ms
step:904/1750 train_time:86407ms step_avg:95.58ms
step:905/1750 train_time:86505ms step_avg:95.59ms
step:906/1750 train_time:86602ms step_avg:95.59ms
step:907/1750 train_time:86700ms step_avg:95.59ms
step:908/1750 train_time:86799ms step_avg:95.59ms
step:909/1750 train_time:86898ms step_avg:95.60ms
step:910/1750 train_time:86997ms step_avg:95.60ms
step:911/1750 train_time:87096ms step_avg:95.60ms
step:912/1750 train_time:87195ms step_avg:95.61ms
step:913/1750 train_time:87295ms step_avg:95.61ms
step:914/1750 train_time:87394ms step_avg:95.62ms
step:915/1750 train_time:87494ms step_avg:95.62ms
step:916/1750 train_time:87593ms step_avg:95.63ms
step:917/1750 train_time:87694ms step_avg:95.63ms
step:918/1750 train_time:87794ms step_avg:95.64ms
step:919/1750 train_time:87894ms step_avg:95.64ms
step:920/1750 train_time:87994ms step_avg:95.65ms
step:921/1750 train_time:88094ms step_avg:95.65ms
step:922/1750 train_time:88193ms step_avg:95.65ms
step:923/1750 train_time:88293ms step_avg:95.66ms
step:924/1750 train_time:88393ms step_avg:95.66ms
step:925/1750 train_time:88491ms step_avg:95.67ms
step:926/1750 train_time:88592ms step_avg:95.67ms
step:927/1750 train_time:88692ms step_avg:95.68ms
step:928/1750 train_time:88792ms step_avg:95.68ms
step:929/1750 train_time:88892ms step_avg:95.69ms
step:930/1750 train_time:88993ms step_avg:95.69ms
step:931/1750 train_time:89092ms step_avg:95.69ms
step:932/1750 train_time:89192ms step_avg:95.70ms
step:933/1750 train_time:89291ms step_avg:95.70ms
step:934/1750 train_time:89390ms step_avg:95.71ms
step:935/1750 train_time:89490ms step_avg:95.71ms
step:936/1750 train_time:89590ms step_avg:95.72ms
step:937/1750 train_time:89690ms step_avg:95.72ms
step:938/1750 train_time:89790ms step_avg:95.73ms
step:939/1750 train_time:89890ms step_avg:95.73ms
step:940/1750 train_time:89990ms step_avg:95.73ms
step:941/1750 train_time:90090ms step_avg:95.74ms
step:942/1750 train_time:90190ms step_avg:95.74ms
step:943/1750 train_time:90290ms step_avg:95.75ms
step:944/1750 train_time:90389ms step_avg:95.75ms
step:945/1750 train_time:90489ms step_avg:95.76ms
step:946/1750 train_time:90588ms step_avg:95.76ms
step:947/1750 train_time:90687ms step_avg:95.76ms
step:948/1750 train_time:90786ms step_avg:95.77ms
step:949/1750 train_time:90885ms step_avg:95.77ms
step:950/1750 train_time:90985ms step_avg:95.77ms
step:951/1750 train_time:91084ms step_avg:95.78ms
step:952/1750 train_time:91184ms step_avg:95.78ms
step:953/1750 train_time:91284ms step_avg:95.79ms
step:954/1750 train_time:91383ms step_avg:95.79ms
step:955/1750 train_time:91482ms step_avg:95.79ms
step:956/1750 train_time:91582ms step_avg:95.80ms
step:957/1750 train_time:91681ms step_avg:95.80ms
step:958/1750 train_time:91779ms step_avg:95.80ms
step:959/1750 train_time:91878ms step_avg:95.81ms
step:960/1750 train_time:91979ms step_avg:95.81ms
step:961/1750 train_time:92077ms step_avg:95.81ms
step:962/1750 train_time:92177ms step_avg:95.82ms
step:963/1750 train_time:92276ms step_avg:95.82ms
step:964/1750 train_time:92374ms step_avg:95.82ms
step:965/1750 train_time:92475ms step_avg:95.83ms
step:966/1750 train_time:92574ms step_avg:95.83ms
step:967/1750 train_time:92674ms step_avg:95.84ms
step:968/1750 train_time:92775ms step_avg:95.84ms
step:969/1750 train_time:92875ms step_avg:95.85ms
step:970/1750 train_time:92976ms step_avg:95.85ms
step:971/1750 train_time:93074ms step_avg:95.85ms
step:972/1750 train_time:93174ms step_avg:95.86ms
step:973/1750 train_time:93273ms step_avg:95.86ms
step:974/1750 train_time:93372ms step_avg:95.86ms
step:975/1750 train_time:93472ms step_avg:95.87ms
step:976/1750 train_time:93572ms step_avg:95.87ms
step:977/1750 train_time:93672ms step_avg:95.88ms
step:978/1750 train_time:93774ms step_avg:95.88ms
step:979/1750 train_time:93873ms step_avg:95.89ms
step:980/1750 train_time:93973ms step_avg:95.89ms
step:981/1750 train_time:94073ms step_avg:95.89ms
step:982/1750 train_time:94172ms step_avg:95.90ms
step:983/1750 train_time:94272ms step_avg:95.90ms
step:984/1750 train_time:94372ms step_avg:95.91ms
step:985/1750 train_time:94471ms step_avg:95.91ms
step:986/1750 train_time:94571ms step_avg:95.91ms
step:987/1750 train_time:94671ms step_avg:95.92ms
step:988/1750 train_time:94771ms step_avg:95.92ms
step:989/1750 train_time:94871ms step_avg:95.93ms
step:990/1750 train_time:94971ms step_avg:95.93ms
step:991/1750 train_time:95071ms step_avg:95.93ms
step:992/1750 train_time:95170ms step_avg:95.94ms
step:993/1750 train_time:95269ms step_avg:95.94ms
step:994/1750 train_time:95368ms step_avg:95.94ms
step:995/1750 train_time:95468ms step_avg:95.95ms
step:996/1750 train_time:95567ms step_avg:95.95ms
step:997/1750 train_time:95665ms step_avg:95.95ms
step:998/1750 train_time:95764ms step_avg:95.96ms
step:999/1750 train_time:95864ms step_avg:95.96ms
step:1000/1750 train_time:95964ms step_avg:95.96ms
step:1000/1750 val_loss:3.5193 train_time:96052ms step_avg:96.05ms
step:1001/1750 train_time:96074ms step_avg:95.98ms
step:1002/1750 train_time:96177ms step_avg:95.98ms
step:1003/1750 train_time:96276ms step_avg:95.99ms
step:1004/1750 train_time:96376ms step_avg:95.99ms
step:1005/1750 train_time:96474ms step_avg:95.99ms
step:1006/1750 train_time:96573ms step_avg:96.00ms
step:1007/1750 train_time:96671ms step_avg:96.00ms
step:1008/1750 train_time:96770ms step_avg:96.00ms
step:1009/1750 train_time:96868ms step_avg:96.00ms
step:1010/1750 train_time:96966ms step_avg:96.01ms
step:1011/1750 train_time:97067ms step_avg:96.01ms
step:1012/1750 train_time:97169ms step_avg:96.02ms
step:1013/1750 train_time:97271ms step_avg:96.02ms
step:1014/1750 train_time:97371ms step_avg:96.03ms
step:1015/1750 train_time:97471ms step_avg:96.03ms
step:1016/1750 train_time:97570ms step_avg:96.03ms
step:1017/1750 train_time:97669ms step_avg:96.04ms
step:1018/1750 train_time:97767ms step_avg:96.04ms
step:1019/1750 train_time:97865ms step_avg:96.04ms
step:1020/1750 train_time:97964ms step_avg:96.04ms
step:1021/1750 train_time:98063ms step_avg:96.05ms
step:1022/1750 train_time:98162ms step_avg:96.05ms
step:1023/1750 train_time:98263ms step_avg:96.05ms
step:1024/1750 train_time:98363ms step_avg:96.06ms
step:1025/1750 train_time:98463ms step_avg:96.06ms
step:1026/1750 train_time:98563ms step_avg:96.06ms
step:1027/1750 train_time:98662ms step_avg:96.07ms
step:1028/1750 train_time:98761ms step_avg:96.07ms
step:1029/1750 train_time:98861ms step_avg:96.07ms
step:1030/1750 train_time:98960ms step_avg:96.08ms
step:1031/1750 train_time:99060ms step_avg:96.08ms
step:1032/1750 train_time:99160ms step_avg:96.09ms
step:1033/1750 train_time:99259ms step_avg:96.09ms
step:1034/1750 train_time:99359ms step_avg:96.09ms
step:1035/1750 train_time:99458ms step_avg:96.10ms
step:1036/1750 train_time:99558ms step_avg:96.10ms
step:1037/1750 train_time:99657ms step_avg:96.10ms
step:1038/1750 train_time:99757ms step_avg:96.10ms
step:1039/1750 train_time:99856ms step_avg:96.11ms
step:1040/1750 train_time:99955ms step_avg:96.11ms
step:1041/1750 train_time:100055ms step_avg:96.11ms
step:1042/1750 train_time:100154ms step_avg:96.12ms
step:1043/1750 train_time:100254ms step_avg:96.12ms
step:1044/1750 train_time:100354ms step_avg:96.12ms
step:1045/1750 train_time:100453ms step_avg:96.13ms
step:1046/1750 train_time:100554ms step_avg:96.13ms
step:1047/1750 train_time:100653ms step_avg:96.13ms
step:1048/1750 train_time:100753ms step_avg:96.14ms
step:1049/1750 train_time:101113ms step_avg:96.39ms
step:1050/1750 train_time:101210ms step_avg:96.39ms
step:1051/1750 train_time:101308ms step_avg:96.39ms
step:1052/1750 train_time:101407ms step_avg:96.39ms
step:1053/1750 train_time:101505ms step_avg:96.40ms
step:1054/1750 train_time:101602ms step_avg:96.40ms
step:1055/1750 train_time:101701ms step_avg:96.40ms
step:1056/1750 train_time:101799ms step_avg:96.40ms
step:1057/1750 train_time:101897ms step_avg:96.40ms
step:1058/1750 train_time:101998ms step_avg:96.41ms
step:1059/1750 train_time:102104ms step_avg:96.42ms
step:1060/1750 train_time:102205ms step_avg:96.42ms
step:1061/1750 train_time:102304ms step_avg:96.42ms
step:1062/1750 train_time:102402ms step_avg:96.42ms
step:1063/1750 train_time:102765ms step_avg:96.67ms
step:1064/1750 train_time:102863ms step_avg:96.68ms
step:1065/1750 train_time:102962ms step_avg:96.68ms
step:1066/1750 train_time:103060ms step_avg:96.68ms
step:1067/1750 train_time:103420ms step_avg:96.93ms
step:1068/1750 train_time:103517ms step_avg:96.93ms
step:1069/1750 train_time:103615ms step_avg:96.93ms
step:1070/1750 train_time:103715ms step_avg:96.93ms
step:1071/1750 train_time:103812ms step_avg:96.93ms
step:1072/1750 train_time:104178ms step_avg:97.18ms
step:1073/1750 train_time:104274ms step_avg:97.18ms
step:1074/1750 train_time:104373ms step_avg:97.18ms
step:1075/1750 train_time:104471ms step_avg:97.18ms
step:1076/1750 train_time:104570ms step_avg:97.18ms
step:1077/1750 train_time:104668ms step_avg:97.18ms
step:1078/1750 train_time:105054ms step_avg:97.45ms
step:1079/1750 train_time:105152ms step_avg:97.45ms
step:1080/1750 train_time:105251ms step_avg:97.45ms
step:1081/1750 train_time:105349ms step_avg:97.46ms
step:1082/1750 train_time:105447ms step_avg:97.46ms
step:1083/1750 train_time:105545ms step_avg:97.46ms
step:1084/1750 train_time:105643ms step_avg:97.46ms
step:1085/1750 train_time:105741ms step_avg:97.46ms
step:1086/1750 train_time:105839ms step_avg:97.46ms
step:1087/1750 train_time:105939ms step_avg:97.46ms
step:1088/1750 train_time:106043ms step_avg:97.47ms
step:1089/1750 train_time:106143ms step_avg:97.47ms
step:1090/1750 train_time:106244ms step_avg:97.47ms
step:1091/1750 train_time:106343ms step_avg:97.47ms
step:1092/1750 train_time:106443ms step_avg:97.47ms
step:1093/1750 train_time:106541ms step_avg:97.48ms
step:1094/1750 train_time:106639ms step_avg:97.48ms
step:1095/1750 train_time:106737ms step_avg:97.48ms
step:1096/1750 train_time:106835ms step_avg:97.48ms
step:1097/1750 train_time:106935ms step_avg:97.48ms
step:1098/1750 train_time:107035ms step_avg:97.48ms
step:1099/1750 train_time:107135ms step_avg:97.48ms
step:1100/1750 train_time:107234ms step_avg:97.49ms
step:1101/1750 train_time:107334ms step_avg:97.49ms
step:1102/1750 train_time:107433ms step_avg:97.49ms
step:1103/1750 train_time:107533ms step_avg:97.49ms
step:1104/1750 train_time:107633ms step_avg:97.49ms
step:1105/1750 train_time:107732ms step_avg:97.50ms
step:1106/1750 train_time:107831ms step_avg:97.50ms
step:1107/1750 train_time:107931ms step_avg:97.50ms
step:1108/1750 train_time:108031ms step_avg:97.50ms
step:1109/1750 train_time:108131ms step_avg:97.50ms
step:1110/1750 train_time:108232ms step_avg:97.51ms
step:1111/1750 train_time:108331ms step_avg:97.51ms
step:1112/1750 train_time:108432ms step_avg:97.51ms
step:1113/1750 train_time:108533ms step_avg:97.51ms
step:1114/1750 train_time:108633ms step_avg:97.52ms
step:1115/1750 train_time:108732ms step_avg:97.52ms
step:1116/1750 train_time:108832ms step_avg:97.52ms
step:1117/1750 train_time:108931ms step_avg:97.52ms
step:1118/1750 train_time:109031ms step_avg:97.52ms
step:1119/1750 train_time:109131ms step_avg:97.53ms
step:1120/1750 train_time:109231ms step_avg:97.53ms
step:1121/1750 train_time:109331ms step_avg:97.53ms
step:1122/1750 train_time:109430ms step_avg:97.53ms
step:1123/1750 train_time:109530ms step_avg:97.53ms
step:1124/1750 train_time:109630ms step_avg:97.54ms
step:1125/1750 train_time:109730ms step_avg:97.54ms
step:1125/1750 val_loss:3.4656 train_time:109818ms step_avg:97.62ms
step:1126/1750 train_time:109840ms step_avg:97.55ms
step:1127/1750 train_time:109939ms step_avg:97.55ms
step:1128/1750 train_time:110040ms step_avg:97.55ms
step:1129/1750 train_time:110141ms step_avg:97.56ms
step:1130/1750 train_time:110240ms step_avg:97.56ms
step:1131/1750 train_time:110339ms step_avg:97.56ms
step:1132/1750 train_time:110437ms step_avg:97.56ms
step:1133/1750 train_time:110535ms step_avg:97.56ms
step:1134/1750 train_time:110634ms step_avg:97.56ms
step:1135/1750 train_time:110732ms step_avg:97.56ms
step:1136/1750 train_time:110834ms step_avg:97.56ms
step:1137/1750 train_time:110934ms step_avg:97.57ms
step:1138/1750 train_time:111035ms step_avg:97.57ms
step:1139/1750 train_time:111134ms step_avg:97.57ms
step:1140/1750 train_time:111234ms step_avg:97.57ms
step:1141/1750 train_time:111334ms step_avg:97.58ms
step:1142/1750 train_time:111433ms step_avg:97.58ms
step:1143/1750 train_time:111532ms step_avg:97.58ms
step:1144/1750 train_time:111631ms step_avg:97.58ms
step:1145/1750 train_time:111730ms step_avg:97.58ms
step:1146/1750 train_time:111830ms step_avg:97.58ms
step:1147/1750 train_time:111930ms step_avg:97.58ms
step:1148/1750 train_time:112030ms step_avg:97.59ms
step:1149/1750 train_time:112132ms step_avg:97.59ms
step:1150/1750 train_time:112233ms step_avg:97.59ms
step:1151/1750 train_time:112333ms step_avg:97.60ms
step:1152/1750 train_time:112433ms step_avg:97.60ms
step:1153/1750 train_time:112532ms step_avg:97.60ms
step:1154/1750 train_time:112632ms step_avg:97.60ms
step:1155/1750 train_time:112731ms step_avg:97.60ms
step:1156/1750 train_time:112830ms step_avg:97.60ms
step:1157/1750 train_time:112930ms step_avg:97.61ms
step:1158/1750 train_time:113030ms step_avg:97.61ms
step:1159/1750 train_time:113131ms step_avg:97.61ms
step:1160/1750 train_time:113231ms step_avg:97.61ms
step:1161/1750 train_time:113332ms step_avg:97.62ms
step:1162/1750 train_time:113432ms step_avg:97.62ms
step:1163/1750 train_time:113533ms step_avg:97.62ms
step:1164/1750 train_time:113634ms step_avg:97.62ms
step:1165/1750 train_time:113733ms step_avg:97.62ms
step:1166/1750 train_time:113832ms step_avg:97.63ms
step:1167/1750 train_time:113932ms step_avg:97.63ms
step:1168/1750 train_time:114033ms step_avg:97.63ms
step:1169/1750 train_time:114133ms step_avg:97.63ms
step:1170/1750 train_time:114234ms step_avg:97.64ms
step:1171/1750 train_time:114335ms step_avg:97.64ms
step:1172/1750 train_time:114436ms step_avg:97.64ms
step:1173/1750 train_time:114536ms step_avg:97.64ms
step:1174/1750 train_time:114637ms step_avg:97.65ms
step:1175/1750 train_time:114738ms step_avg:97.65ms
step:1176/1750 train_time:114839ms step_avg:97.65ms
step:1177/1750 train_time:114941ms step_avg:97.66ms
step:1178/1750 train_time:115041ms step_avg:97.66ms
step:1179/1750 train_time:115143ms step_avg:97.66ms
step:1180/1750 train_time:115244ms step_avg:97.66ms
step:1181/1750 train_time:115344ms step_avg:97.67ms
step:1182/1750 train_time:115446ms step_avg:97.67ms
step:1183/1750 train_time:115546ms step_avg:97.67ms
step:1184/1750 train_time:115647ms step_avg:97.67ms
step:1185/1750 train_time:115748ms step_avg:97.68ms
step:1186/1750 train_time:115849ms step_avg:97.68ms
step:1187/1750 train_time:115951ms step_avg:97.68ms
step:1188/1750 train_time:116051ms step_avg:97.69ms
step:1189/1750 train_time:116153ms step_avg:97.69ms
step:1190/1750 train_time:116253ms step_avg:97.69ms
step:1191/1750 train_time:116354ms step_avg:97.69ms
step:1192/1750 train_time:116455ms step_avg:97.70ms
step:1193/1750 train_time:116555ms step_avg:97.70ms
step:1194/1750 train_time:116655ms step_avg:97.70ms
step:1195/1750 train_time:116755ms step_avg:97.70ms
step:1196/1750 train_time:116856ms step_avg:97.71ms
step:1197/1750 train_time:116957ms step_avg:97.71ms
step:1198/1750 train_time:117058ms step_avg:97.71ms
step:1199/1750 train_time:117160ms step_avg:97.71ms
step:1200/1750 train_time:117261ms step_avg:97.72ms
step:1201/1750 train_time:117363ms step_avg:97.72ms
step:1202/1750 train_time:117464ms step_avg:97.72ms
step:1203/1750 train_time:117564ms step_avg:97.73ms
step:1204/1750 train_time:117664ms step_avg:97.73ms
step:1205/1750 train_time:117765ms step_avg:97.73ms
step:1206/1750 train_time:117865ms step_avg:97.73ms
step:1207/1750 train_time:117966ms step_avg:97.73ms
step:1208/1750 train_time:118066ms step_avg:97.74ms
step:1209/1750 train_time:118168ms step_avg:97.74ms
step:1210/1750 train_time:118270ms step_avg:97.74ms
step:1211/1750 train_time:118372ms step_avg:97.75ms
step:1212/1750 train_time:118473ms step_avg:97.75ms
step:1213/1750 train_time:118574ms step_avg:97.75ms
step:1214/1750 train_time:118674ms step_avg:97.75ms
step:1215/1750 train_time:118775ms step_avg:97.76ms
step:1216/1750 train_time:118876ms step_avg:97.76ms
step:1217/1750 train_time:118977ms step_avg:97.76ms
step:1218/1750 train_time:119078ms step_avg:97.76ms
step:1219/1750 train_time:119179ms step_avg:97.77ms
step:1220/1750 train_time:119280ms step_avg:97.77ms
step:1221/1750 train_time:119382ms step_avg:97.77ms
step:1222/1750 train_time:119483ms step_avg:97.78ms
step:1223/1750 train_time:119913ms step_avg:98.05ms
step:1224/1750 train_time:119975ms step_avg:98.02ms
step:1225/1750 train_time:120074ms step_avg:98.02ms
step:1226/1750 train_time:120174ms step_avg:98.02ms
step:1227/1750 train_time:120273ms step_avg:98.02ms
step:1228/1750 train_time:120372ms step_avg:98.02ms
step:1229/1750 train_time:120472ms step_avg:98.02ms
step:1230/1750 train_time:120571ms step_avg:98.03ms
step:1231/1750 train_time:120672ms step_avg:98.03ms
step:1232/1750 train_time:120773ms step_avg:98.03ms
step:1233/1750 train_time:120878ms step_avg:98.04ms
step:1234/1750 train_time:120982ms step_avg:98.04ms
step:1235/1750 train_time:121386ms step_avg:98.29ms
step:1236/1750 train_time:121485ms step_avg:98.29ms
step:1237/1750 train_time:121584ms step_avg:98.29ms
step:1238/1750 train_time:121684ms step_avg:98.29ms
step:1239/1750 train_time:122084ms step_avg:98.53ms
step:1240/1750 train_time:122182ms step_avg:98.53ms
step:1241/1750 train_time:122283ms step_avg:98.54ms
step:1242/1750 train_time:122383ms step_avg:98.54ms
step:1243/1750 train_time:122482ms step_avg:98.54ms
step:1244/1750 train_time:122581ms step_avg:98.54ms
step:1245/1750 train_time:122680ms step_avg:98.54ms
step:1246/1750 train_time:122780ms step_avg:98.54ms
step:1247/1750 train_time:122879ms step_avg:98.54ms
step:1248/1750 train_time:122983ms step_avg:98.54ms
step:1249/1750 train_time:123090ms step_avg:98.55ms
step:1250/1750 train_time:123192ms step_avg:98.55ms
step:1250/1750 val_loss:3.4187 train_time:123281ms step_avg:98.63ms
step:1251/1750 train_time:123303ms step_avg:98.56ms
step:1252/1750 train_time:123406ms step_avg:98.57ms
step:1253/1750 train_time:123508ms step_avg:98.57ms
step:1254/1750 train_time:123608ms step_avg:98.57ms
step:1255/1750 train_time:123708ms step_avg:98.57ms
step:1256/1750 train_time:123808ms step_avg:98.57ms
step:1257/1750 train_time:123907ms step_avg:98.57ms
step:1258/1750 train_time:124006ms step_avg:98.57ms
step:1259/1750 train_time:124105ms step_avg:98.57ms
step:1260/1750 train_time:124205ms step_avg:98.58ms
step:1261/1750 train_time:124307ms step_avg:98.58ms
step:1262/1750 train_time:124410ms step_avg:98.58ms
step:1263/1750 train_time:124512ms step_avg:98.58ms
step:1264/1750 train_time:124612ms step_avg:98.59ms
step:1265/1750 train_time:124713ms step_avg:98.59ms
step:1266/1750 train_time:124812ms step_avg:98.59ms
step:1267/1750 train_time:124913ms step_avg:98.59ms
step:1268/1750 train_time:125013ms step_avg:98.59ms
step:1269/1750 train_time:125113ms step_avg:98.59ms
step:1270/1750 train_time:125213ms step_avg:98.59ms
step:1271/1750 train_time:125316ms step_avg:98.60ms
step:1272/1750 train_time:125417ms step_avg:98.60ms
step:1273/1750 train_time:125517ms step_avg:98.60ms
step:1274/1750 train_time:125618ms step_avg:98.60ms
step:1275/1750 train_time:125718ms step_avg:98.60ms
step:1276/1750 train_time:125819ms step_avg:98.60ms
step:1277/1750 train_time:125920ms step_avg:98.61ms
step:1278/1750 train_time:126021ms step_avg:98.61ms
step:1279/1750 train_time:126121ms step_avg:98.61ms
step:1280/1750 train_time:126222ms step_avg:98.61ms
step:1281/1750 train_time:126324ms step_avg:98.61ms
step:1282/1750 train_time:126424ms step_avg:98.61ms
step:1283/1750 train_time:126525ms step_avg:98.62ms
step:1284/1750 train_time:126626ms step_avg:98.62ms
step:1285/1750 train_time:126725ms step_avg:98.62ms
step:1286/1750 train_time:126825ms step_avg:98.62ms
step:1287/1750 train_time:126925ms step_avg:98.62ms
step:1288/1750 train_time:127025ms step_avg:98.62ms
step:1289/1750 train_time:127126ms step_avg:98.62ms
step:1290/1750 train_time:127227ms step_avg:98.63ms
step:1291/1750 train_time:127326ms step_avg:98.63ms
step:1292/1750 train_time:127426ms step_avg:98.63ms
step:1293/1750 train_time:127527ms step_avg:98.63ms
step:1294/1750 train_time:127628ms step_avg:98.63ms
step:1295/1750 train_time:127729ms step_avg:98.63ms
step:1296/1750 train_time:127829ms step_avg:98.63ms
step:1297/1750 train_time:127930ms step_avg:98.64ms
step:1298/1750 train_time:128032ms step_avg:98.64ms
step:1299/1750 train_time:128133ms step_avg:98.64ms
step:1300/1750 train_time:128234ms step_avg:98.64ms
step:1301/1750 train_time:128335ms step_avg:98.64ms
step:1302/1750 train_time:128436ms step_avg:98.65ms
step:1303/1750 train_time:128537ms step_avg:98.65ms
step:1304/1750 train_time:128637ms step_avg:98.65ms
step:1305/1750 train_time:128738ms step_avg:98.65ms
step:1306/1750 train_time:128839ms step_avg:98.65ms
step:1307/1750 train_time:128940ms step_avg:98.65ms
step:1308/1750 train_time:129041ms step_avg:98.66ms
step:1309/1750 train_time:129142ms step_avg:98.66ms
step:1310/1750 train_time:129243ms step_avg:98.66ms
step:1311/1750 train_time:129344ms step_avg:98.66ms
step:1312/1750 train_time:129445ms step_avg:98.66ms
step:1313/1750 train_time:129546ms step_avg:98.66ms
step:1314/1750 train_time:129647ms step_avg:98.67ms
step:1315/1750 train_time:129747ms step_avg:98.67ms
step:1316/1750 train_time:129847ms step_avg:98.67ms
step:1317/1750 train_time:129948ms step_avg:98.67ms
step:1318/1750 train_time:130048ms step_avg:98.67ms
step:1319/1750 train_time:130148ms step_avg:98.67ms
step:1320/1750 train_time:130251ms step_avg:98.67ms
step:1321/1750 train_time:130353ms step_avg:98.68ms
step:1322/1750 train_time:130455ms step_avg:98.68ms
step:1323/1750 train_time:130556ms step_avg:98.68ms
step:1324/1750 train_time:130656ms step_avg:98.68ms
step:1325/1750 train_time:130757ms step_avg:98.68ms
step:1326/1750 train_time:130859ms step_avg:98.69ms
step:1327/1750 train_time:130959ms step_avg:98.69ms
step:1328/1750 train_time:131060ms step_avg:98.69ms
step:1329/1750 train_time:131161ms step_avg:98.69ms
step:1330/1750 train_time:131263ms step_avg:98.69ms
step:1331/1750 train_time:131365ms step_avg:98.70ms
step:1332/1750 train_time:131466ms step_avg:98.70ms
step:1333/1750 train_time:131567ms step_avg:98.70ms
step:1334/1750 train_time:131668ms step_avg:98.70ms
step:1335/1750 train_time:131768ms step_avg:98.70ms
step:1336/1750 train_time:131869ms step_avg:98.70ms
step:1337/1750 train_time:131971ms step_avg:98.71ms
step:1338/1750 train_time:132072ms step_avg:98.71ms
step:1339/1750 train_time:132174ms step_avg:98.71ms
step:1340/1750 train_time:132275ms step_avg:98.71ms
step:1341/1750 train_time:132377ms step_avg:98.71ms
step:1342/1750 train_time:132477ms step_avg:98.72ms
step:1343/1750 train_time:132578ms step_avg:98.72ms
step:1344/1750 train_time:132680ms step_avg:98.72ms
step:1345/1750 train_time:132781ms step_avg:98.72ms
step:1346/1750 train_time:132883ms step_avg:98.72ms
step:1347/1750 train_time:132983ms step_avg:98.73ms
step:1348/1750 train_time:133085ms step_avg:98.73ms
step:1349/1750 train_time:133185ms step_avg:98.73ms
step:1350/1750 train_time:133287ms step_avg:98.73ms
step:1351/1750 train_time:133386ms step_avg:98.73ms
step:1352/1750 train_time:133487ms step_avg:98.73ms
step:1353/1750 train_time:133588ms step_avg:98.73ms
step:1354/1750 train_time:133689ms step_avg:98.74ms
step:1355/1750 train_time:133789ms step_avg:98.74ms
step:1356/1750 train_time:133891ms step_avg:98.74ms
step:1357/1750 train_time:133991ms step_avg:98.74ms
step:1358/1750 train_time:134093ms step_avg:98.74ms
step:1359/1750 train_time:134194ms step_avg:98.74ms
step:1360/1750 train_time:134295ms step_avg:98.75ms
step:1361/1750 train_time:134396ms step_avg:98.75ms
step:1362/1750 train_time:134497ms step_avg:98.75ms
step:1363/1750 train_time:134599ms step_avg:98.75ms
step:1364/1750 train_time:134700ms step_avg:98.75ms
step:1365/1750 train_time:134802ms step_avg:98.76ms
step:1366/1750 train_time:134902ms step_avg:98.76ms
step:1367/1750 train_time:135003ms step_avg:98.76ms
step:1368/1750 train_time:135105ms step_avg:98.76ms
step:1369/1750 train_time:135205ms step_avg:98.76ms
step:1370/1750 train_time:135305ms step_avg:98.76ms
step:1371/1750 train_time:135407ms step_avg:98.77ms
step:1372/1750 train_time:135508ms step_avg:98.77ms
step:1373/1750 train_time:135607ms step_avg:98.77ms
step:1374/1750 train_time:135708ms step_avg:98.77ms
step:1375/1750 train_time:135808ms step_avg:98.77ms
step:1375/1750 val_loss:3.3784 train_time:135898ms step_avg:98.83ms
step:1376/1750 train_time:135919ms step_avg:98.78ms
step:1377/1750 train_time:136021ms step_avg:98.78ms
step:1378/1750 train_time:136121ms step_avg:98.78ms
step:1379/1750 train_time:136221ms step_avg:98.78ms
step:1380/1750 train_time:136322ms step_avg:98.78ms
step:1381/1750 train_time:136421ms step_avg:98.78ms
step:1382/1750 train_time:136520ms step_avg:98.78ms
step:1383/1750 train_time:136620ms step_avg:98.79ms
step:1384/1750 train_time:136719ms step_avg:98.79ms
step:1385/1750 train_time:136819ms step_avg:98.79ms
step:1386/1750 train_time:136923ms step_avg:98.79ms
step:1387/1750 train_time:137027ms step_avg:98.79ms
step:1388/1750 train_time:137128ms step_avg:98.80ms
step:1389/1750 train_time:137230ms step_avg:98.80ms
step:1390/1750 train_time:137329ms step_avg:98.80ms
step:1391/1750 train_time:137429ms step_avg:98.80ms
step:1392/1750 train_time:137530ms step_avg:98.80ms
step:1393/1750 train_time:137630ms step_avg:98.80ms
step:1394/1750 train_time:137729ms step_avg:98.80ms
step:1395/1750 train_time:137831ms step_avg:98.80ms
step:1396/1750 train_time:137932ms step_avg:98.81ms
step:1397/1750 train_time:138036ms step_avg:98.81ms
step:1398/1750 train_time:138138ms step_avg:98.81ms
step:1399/1750 train_time:138239ms step_avg:98.81ms
step:1400/1750 train_time:138339ms step_avg:98.81ms
step:1401/1750 train_time:138440ms step_avg:98.82ms
step:1402/1750 train_time:138541ms step_avg:98.82ms
step:1403/1750 train_time:138642ms step_avg:98.82ms
step:1404/1750 train_time:138743ms step_avg:98.82ms
step:1405/1750 train_time:138845ms step_avg:98.82ms
step:1406/1750 train_time:138947ms step_avg:98.82ms
step:1407/1750 train_time:139049ms step_avg:98.83ms
step:1408/1750 train_time:139149ms step_avg:98.83ms
step:1409/1750 train_time:139252ms step_avg:98.83ms
step:1410/1750 train_time:139351ms step_avg:98.83ms
step:1411/1750 train_time:139452ms step_avg:98.83ms
step:1412/1750 train_time:139555ms step_avg:98.84ms
step:1413/1750 train_time:139656ms step_avg:98.84ms
step:1414/1750 train_time:139757ms step_avg:98.84ms
step:1415/1750 train_time:139858ms step_avg:98.84ms
step:1416/1750 train_time:139960ms step_avg:98.84ms
step:1417/1750 train_time:140060ms step_avg:98.84ms
step:1418/1750 train_time:140160ms step_avg:98.84ms
step:1419/1750 train_time:140260ms step_avg:98.84ms
step:1420/1750 train_time:140361ms step_avg:98.85ms
step:1421/1750 train_time:140462ms step_avg:98.85ms
step:1422/1750 train_time:140563ms step_avg:98.85ms
step:1423/1750 train_time:140664ms step_avg:98.85ms
step:1424/1750 train_time:140766ms step_avg:98.85ms
step:1425/1750 train_time:140865ms step_avg:98.85ms
step:1426/1750 train_time:140967ms step_avg:98.85ms
step:1427/1750 train_time:141068ms step_avg:98.86ms
step:1428/1750 train_time:141170ms step_avg:98.86ms
step:1429/1750 train_time:141270ms step_avg:98.86ms
step:1430/1750 train_time:141372ms step_avg:98.86ms
step:1431/1750 train_time:141474ms step_avg:98.86ms
step:1432/1750 train_time:141576ms step_avg:98.87ms
step:1433/1750 train_time:141678ms step_avg:98.87ms
step:1434/1750 train_time:141780ms step_avg:98.87ms
step:1435/1750 train_time:141882ms step_avg:98.87ms
step:1436/1750 train_time:141985ms step_avg:98.88ms
step:1437/1750 train_time:142087ms step_avg:98.88ms
step:1438/1750 train_time:142188ms step_avg:98.88ms
step:1439/1750 train_time:142290ms step_avg:98.88ms
step:1440/1750 train_time:142393ms step_avg:98.88ms
step:1441/1750 train_time:142495ms step_avg:98.89ms
step:1442/1750 train_time:142596ms step_avg:98.89ms
step:1443/1750 train_time:142698ms step_avg:98.89ms
step:1444/1750 train_time:142800ms step_avg:98.89ms
step:1445/1750 train_time:142901ms step_avg:98.89ms
step:1446/1750 train_time:143002ms step_avg:98.89ms
step:1447/1750 train_time:143104ms step_avg:98.90ms
step:1448/1750 train_time:143207ms step_avg:98.90ms
step:1449/1750 train_time:143308ms step_avg:98.90ms
step:1450/1750 train_time:143409ms step_avg:98.90ms
step:1451/1750 train_time:143511ms step_avg:98.90ms
step:1452/1750 train_time:143613ms step_avg:98.91ms
step:1453/1750 train_time:143714ms step_avg:98.91ms
step:1454/1750 train_time:143819ms step_avg:98.91ms
step:1455/1750 train_time:143921ms step_avg:98.91ms
step:1456/1750 train_time:144021ms step_avg:98.92ms
step:1457/1750 train_time:144123ms step_avg:98.92ms
step:1458/1750 train_time:144224ms step_avg:98.92ms
step:1459/1750 train_time:144326ms step_avg:98.92ms
step:1460/1750 train_time:144427ms step_avg:98.92ms
step:1461/1750 train_time:144528ms step_avg:98.92ms
step:1462/1750 train_time:144629ms step_avg:98.93ms
step:1463/1750 train_time:144731ms step_avg:98.93ms
step:1464/1750 train_time:144833ms step_avg:98.93ms
step:1465/1750 train_time:144935ms step_avg:98.93ms
step:1466/1750 train_time:145039ms step_avg:98.93ms
step:1467/1750 train_time:145140ms step_avg:98.94ms
step:1468/1750 train_time:145243ms step_avg:98.94ms
step:1469/1750 train_time:145345ms step_avg:98.94ms
step:1470/1750 train_time:145446ms step_avg:98.94ms
step:1471/1750 train_time:145547ms step_avg:98.94ms
step:1472/1750 train_time:145648ms step_avg:98.95ms
step:1473/1750 train_time:145749ms step_avg:98.95ms
step:1474/1750 train_time:145851ms step_avg:98.95ms
step:1475/1750 train_time:145953ms step_avg:98.95ms
step:1476/1750 train_time:146056ms step_avg:98.95ms
step:1477/1750 train_time:146159ms step_avg:98.96ms
step:1478/1750 train_time:146261ms step_avg:98.96ms
step:1479/1750 train_time:146363ms step_avg:98.96ms
step:1480/1750 train_time:146465ms step_avg:98.96ms
step:1481/1750 train_time:146566ms step_avg:98.96ms
step:1482/1750 train_time:146670ms step_avg:98.97ms
step:1483/1750 train_time:146770ms step_avg:98.97ms
step:1484/1750 train_time:146873ms step_avg:98.97ms
step:1485/1750 train_time:146976ms step_avg:98.97ms
step:1486/1750 train_time:147077ms step_avg:98.98ms
step:1487/1750 train_time:147179ms step_avg:98.98ms
step:1488/1750 train_time:147282ms step_avg:98.98ms
step:1489/1750 train_time:147383ms step_avg:98.98ms
step:1490/1750 train_time:147485ms step_avg:98.98ms
step:1491/1750 train_time:147586ms step_avg:98.98ms
step:1492/1750 train_time:147688ms step_avg:98.99ms
step:1493/1750 train_time:147790ms step_avg:98.99ms
step:1494/1750 train_time:147892ms step_avg:98.99ms
step:1495/1750 train_time:147993ms step_avg:98.99ms
step:1496/1750 train_time:148094ms step_avg:98.99ms
step:1497/1750 train_time:148196ms step_avg:99.00ms
step:1498/1750 train_time:148300ms step_avg:99.00ms
step:1499/1750 train_time:148401ms step_avg:99.00ms
step:1500/1750 train_time:148502ms step_avg:99.00ms
step:1500/1750 val_loss:3.3416 train_time:148592ms step_avg:99.06ms
step:1501/1750 train_time:148613ms step_avg:99.01ms
step:1502/1750 train_time:148715ms step_avg:99.01ms
step:1503/1750 train_time:148817ms step_avg:99.01ms
step:1504/1750 train_time:148917ms step_avg:99.01ms
step:1505/1750 train_time:149018ms step_avg:99.02ms
step:1506/1750 train_time:149118ms step_avg:99.02ms
step:1507/1750 train_time:149219ms step_avg:99.02ms
step:1508/1750 train_time:149318ms step_avg:99.02ms
step:1509/1750 train_time:149419ms step_avg:99.02ms
step:1510/1750 train_time:149522ms step_avg:99.02ms
step:1511/1750 train_time:149627ms step_avg:99.02ms
step:1512/1750 train_time:149731ms step_avg:99.03ms
step:1513/1750 train_time:149833ms step_avg:99.03ms
step:1514/1750 train_time:149935ms step_avg:99.03ms
step:1515/1750 train_time:150039ms step_avg:99.04ms
step:1516/1750 train_time:150139ms step_avg:99.04ms
step:1517/1750 train_time:150239ms step_avg:99.04ms
step:1518/1750 train_time:150339ms step_avg:99.04ms
step:1519/1750 train_time:150442ms step_avg:99.04ms
step:1520/1750 train_time:150544ms step_avg:99.04ms
step:1521/1750 train_time:150647ms step_avg:99.04ms
step:1522/1750 train_time:150748ms step_avg:99.05ms
step:1523/1750 train_time:150850ms step_avg:99.05ms
step:1524/1750 train_time:150954ms step_avg:99.05ms
step:1525/1750 train_time:151058ms step_avg:99.05ms
step:1526/1750 train_time:151159ms step_avg:99.06ms
step:1527/1750 train_time:151260ms step_avg:99.06ms
step:1528/1750 train_time:151364ms step_avg:99.06ms
step:1529/1750 train_time:151465ms step_avg:99.06ms
step:1530/1750 train_time:151567ms step_avg:99.06ms
step:1531/1750 train_time:151668ms step_avg:99.06ms
step:1532/1750 train_time:151769ms step_avg:99.07ms
step:1533/1750 train_time:151870ms step_avg:99.07ms
step:1534/1750 train_time:151973ms step_avg:99.07ms
step:1535/1750 train_time:152076ms step_avg:99.07ms
step:1536/1750 train_time:152177ms step_avg:99.07ms
step:1537/1750 train_time:152278ms step_avg:99.08ms
step:1538/1750 train_time:152379ms step_avg:99.08ms
step:1539/1750 train_time:152482ms step_avg:99.08ms
step:1540/1750 train_time:152584ms step_avg:99.08ms
step:1541/1750 train_time:152686ms step_avg:99.08ms
step:1542/1750 train_time:152790ms step_avg:99.09ms
step:1543/1750 train_time:152892ms step_avg:99.09ms
step:1544/1750 train_time:152995ms step_avg:99.09ms
step:1545/1750 train_time:153097ms step_avg:99.09ms
step:1546/1750 train_time:153198ms step_avg:99.09ms
step:1547/1750 train_time:153300ms step_avg:99.09ms
step:1548/1750 train_time:153401ms step_avg:99.10ms
step:1549/1750 train_time:153503ms step_avg:99.10ms
step:1550/1750 train_time:153604ms step_avg:99.10ms
step:1551/1750 train_time:153707ms step_avg:99.10ms
step:1552/1750 train_time:153808ms step_avg:99.10ms
step:1553/1750 train_time:153910ms step_avg:99.11ms
step:1554/1750 train_time:154011ms step_avg:99.11ms
step:1555/1750 train_time:154113ms step_avg:99.11ms
step:1556/1750 train_time:154214ms step_avg:99.11ms
step:1557/1750 train_time:154317ms step_avg:99.11ms
step:1558/1750 train_time:154420ms step_avg:99.11ms
step:1559/1750 train_time:154522ms step_avg:99.12ms
step:1560/1750 train_time:154623ms step_avg:99.12ms
step:1561/1750 train_time:154724ms step_avg:99.12ms
step:1562/1750 train_time:154827ms step_avg:99.12ms
step:1563/1750 train_time:154931ms step_avg:99.12ms
step:1564/1750 train_time:155032ms step_avg:99.13ms
step:1565/1750 train_time:155133ms step_avg:99.13ms
step:1566/1750 train_time:155235ms step_avg:99.13ms
step:1567/1750 train_time:155337ms step_avg:99.13ms
step:1568/1750 train_time:155438ms step_avg:99.13ms
step:1569/1750 train_time:155541ms step_avg:99.13ms
step:1570/1750 train_time:155643ms step_avg:99.14ms
step:1571/1750 train_time:155744ms step_avg:99.14ms
step:1572/1750 train_time:155846ms step_avg:99.14ms
step:1573/1750 train_time:155948ms step_avg:99.14ms
step:1574/1750 train_time:156051ms step_avg:99.14ms
step:1575/1750 train_time:156152ms step_avg:99.14ms
step:1576/1750 train_time:156255ms step_avg:99.15ms
step:1577/1750 train_time:156358ms step_avg:99.15ms
step:1578/1750 train_time:156460ms step_avg:99.15ms
step:1579/1750 train_time:156561ms step_avg:99.15ms
step:1580/1750 train_time:156663ms step_avg:99.15ms
step:1581/1750 train_time:156765ms step_avg:99.16ms
step:1582/1750 train_time:156866ms step_avg:99.16ms
step:1583/1750 train_time:156970ms step_avg:99.16ms
step:1584/1750 train_time:157073ms step_avg:99.16ms
step:1585/1750 train_time:157175ms step_avg:99.16ms
step:1586/1750 train_time:157277ms step_avg:99.17ms
step:1587/1750 train_time:157379ms step_avg:99.17ms
step:1588/1750 train_time:157480ms step_avg:99.17ms
step:1589/1750 train_time:157581ms step_avg:99.17ms
step:1590/1750 train_time:157683ms step_avg:99.17ms
step:1591/1750 train_time:157784ms step_avg:99.17ms
step:1592/1750 train_time:157886ms step_avg:99.17ms
step:1593/1750 train_time:157988ms step_avg:99.18ms
step:1594/1750 train_time:158094ms step_avg:99.18ms
step:1595/1750 train_time:158195ms step_avg:99.18ms
step:1596/1750 train_time:158296ms step_avg:99.18ms
step:1597/1750 train_time:158398ms step_avg:99.18ms
step:1598/1750 train_time:158500ms step_avg:99.19ms
step:1599/1750 train_time:158602ms step_avg:99.19ms
step:1600/1750 train_time:158703ms step_avg:99.19ms
step:1601/1750 train_time:158805ms step_avg:99.19ms
step:1602/1750 train_time:158907ms step_avg:99.19ms
step:1603/1750 train_time:159009ms step_avg:99.19ms
step:1604/1750 train_time:159110ms step_avg:99.20ms
step:1605/1750 train_time:159213ms step_avg:99.20ms
step:1606/1750 train_time:159316ms step_avg:99.20ms
step:1607/1750 train_time:159417ms step_avg:99.20ms
step:1608/1750 train_time:159519ms step_avg:99.20ms
step:1609/1750 train_time:159621ms step_avg:99.21ms
step:1610/1750 train_time:159723ms step_avg:99.21ms
step:1611/1750 train_time:159826ms step_avg:99.21ms
step:1612/1750 train_time:159928ms step_avg:99.21ms
step:1613/1750 train_time:160029ms step_avg:99.21ms
step:1614/1750 train_time:160131ms step_avg:99.21ms
step:1615/1750 train_time:160231ms step_avg:99.21ms
step:1616/1750 train_time:160332ms step_avg:99.22ms
step:1617/1750 train_time:160435ms step_avg:99.22ms
step:1618/1750 train_time:160538ms step_avg:99.22ms
step:1619/1750 train_time:160640ms step_avg:99.22ms
step:1620/1750 train_time:160742ms step_avg:99.22ms
step:1621/1750 train_time:160842ms step_avg:99.22ms
step:1622/1750 train_time:160944ms step_avg:99.23ms
step:1623/1750 train_time:161048ms step_avg:99.23ms
step:1624/1750 train_time:161151ms step_avg:99.23ms
step:1625/1750 train_time:161252ms step_avg:99.23ms
step:1625/1750 val_loss:3.3118 train_time:161343ms step_avg:99.29ms
step:1626/1750 train_time:161364ms step_avg:99.24ms
step:1627/1750 train_time:161463ms step_avg:99.24ms
step:1628/1750 train_time:161565ms step_avg:99.24ms
step:1629/1750 train_time:161667ms step_avg:99.24ms
step:1630/1750 train_time:161768ms step_avg:99.24ms
step:1631/1750 train_time:161868ms step_avg:99.24ms
step:1632/1750 train_time:161969ms step_avg:99.25ms
step:1633/1750 train_time:162069ms step_avg:99.25ms
step:1634/1750 train_time:162174ms step_avg:99.25ms
step:1635/1750 train_time:162275ms step_avg:99.25ms
step:1636/1750 train_time:162379ms step_avg:99.25ms
step:1637/1750 train_time:162481ms step_avg:99.26ms
step:1638/1750 train_time:162583ms step_avg:99.26ms
step:1639/1750 train_time:162684ms step_avg:99.26ms
step:1640/1750 train_time:162785ms step_avg:99.26ms
step:1641/1750 train_time:162885ms step_avg:99.26ms
step:1642/1750 train_time:162986ms step_avg:99.26ms
step:1643/1750 train_time:163087ms step_avg:99.26ms
step:1644/1750 train_time:163189ms step_avg:99.26ms
step:1645/1750 train_time:163291ms step_avg:99.26ms
step:1646/1750 train_time:163394ms step_avg:99.27ms
step:1647/1750 train_time:163499ms step_avg:99.27ms
step:1648/1750 train_time:163602ms step_avg:99.27ms
step:1649/1750 train_time:163704ms step_avg:99.27ms
step:1650/1750 train_time:163805ms step_avg:99.28ms
step:1651/1750 train_time:163906ms step_avg:99.28ms
step:1652/1750 train_time:164008ms step_avg:99.28ms
step:1653/1750 train_time:164109ms step_avg:99.28ms
step:1654/1750 train_time:164212ms step_avg:99.28ms
step:1655/1750 train_time:164313ms step_avg:99.28ms
step:1656/1750 train_time:164417ms step_avg:99.29ms
step:1657/1750 train_time:164518ms step_avg:99.29ms
step:1658/1750 train_time:164620ms step_avg:99.29ms
step:1659/1750 train_time:164724ms step_avg:99.29ms
step:1660/1750 train_time:164825ms step_avg:99.29ms
step:1661/1750 train_time:164927ms step_avg:99.29ms
step:1662/1750 train_time:165030ms step_avg:99.30ms
step:1663/1750 train_time:165133ms step_avg:99.30ms
step:1664/1750 train_time:165235ms step_avg:99.30ms
step:1665/1750 train_time:165339ms step_avg:99.30ms
step:1666/1750 train_time:165442ms step_avg:99.30ms
step:1667/1750 train_time:165543ms step_avg:99.31ms
step:1668/1750 train_time:165646ms step_avg:99.31ms
step:1669/1750 train_time:165748ms step_avg:99.31ms
step:1670/1750 train_time:165849ms step_avg:99.31ms
step:1671/1750 train_time:165950ms step_avg:99.31ms
step:1672/1750 train_time:166052ms step_avg:99.31ms
step:1673/1750 train_time:166153ms step_avg:99.31ms
step:1674/1750 train_time:166254ms step_avg:99.32ms
step:1675/1750 train_time:166356ms step_avg:99.32ms
step:1676/1750 train_time:166460ms step_avg:99.32ms
step:1677/1750 train_time:166562ms step_avg:99.32ms
step:1678/1750 train_time:166665ms step_avg:99.32ms
step:1679/1750 train_time:166766ms step_avg:99.32ms
step:1680/1750 train_time:166867ms step_avg:99.33ms
step:1681/1750 train_time:166969ms step_avg:99.33ms
step:1682/1750 train_time:167072ms step_avg:99.33ms
step:1683/1750 train_time:167174ms step_avg:99.33ms
step:1684/1750 train_time:167276ms step_avg:99.33ms
step:1685/1750 train_time:167378ms step_avg:99.33ms
step:1686/1750 train_time:167480ms step_avg:99.34ms
step:1687/1750 train_time:167583ms step_avg:99.34ms
step:1688/1750 train_time:167685ms step_avg:99.34ms
step:1689/1750 train_time:167788ms step_avg:99.34ms
step:1690/1750 train_time:167890ms step_avg:99.34ms
step:1691/1750 train_time:167992ms step_avg:99.34ms
step:1692/1750 train_time:168094ms step_avg:99.35ms
step:1693/1750 train_time:168196ms step_avg:99.35ms
step:1694/1750 train_time:168300ms step_avg:99.35ms
step:1695/1750 train_time:168404ms step_avg:99.35ms
step:1696/1750 train_time:168508ms step_avg:99.36ms
step:1697/1750 train_time:168612ms step_avg:99.36ms
step:1698/1750 train_time:168715ms step_avg:99.36ms
step:1699/1750 train_time:168816ms step_avg:99.36ms
step:1700/1750 train_time:168920ms step_avg:99.36ms
step:1701/1750 train_time:169024ms step_avg:99.37ms
step:1702/1750 train_time:169128ms step_avg:99.37ms
step:1703/1750 train_time:169230ms step_avg:99.37ms
step:1704/1750 train_time:169332ms step_avg:99.37ms
step:1705/1750 train_time:169434ms step_avg:99.37ms
step:1706/1750 train_time:169537ms step_avg:99.38ms
step:1707/1750 train_time:169641ms step_avg:99.38ms
step:1708/1750 train_time:169744ms step_avg:99.38ms
step:1709/1750 train_time:169846ms step_avg:99.38ms
step:1710/1750 train_time:169948ms step_avg:99.38ms
step:1711/1750 train_time:170052ms step_avg:99.39ms
step:1712/1750 train_time:170153ms step_avg:99.39ms
step:1713/1750 train_time:170257ms step_avg:99.39ms
step:1714/1750 train_time:170359ms step_avg:99.39ms
step:1715/1750 train_time:170464ms step_avg:99.40ms
step:1716/1750 train_time:170566ms step_avg:99.40ms
step:1717/1750 train_time:170668ms step_avg:99.40ms
step:1718/1750 train_time:170770ms step_avg:99.40ms
step:1719/1750 train_time:170875ms step_avg:99.40ms
step:1720/1750 train_time:170977ms step_avg:99.40ms
step:1721/1750 train_time:171080ms step_avg:99.41ms
step:1722/1750 train_time:171184ms step_avg:99.41ms
step:1723/1750 train_time:171285ms step_avg:99.41ms
step:1724/1750 train_time:171388ms step_avg:99.41ms
step:1725/1750 train_time:171492ms step_avg:99.42ms
step:1726/1750 train_time:171593ms step_avg:99.42ms
step:1727/1750 train_time:171696ms step_avg:99.42ms
step:1728/1750 train_time:171799ms step_avg:99.42ms
step:1729/1750 train_time:171903ms step_avg:99.42ms
step:1730/1750 train_time:172005ms step_avg:99.42ms
step:1731/1750 train_time:172109ms step_avg:99.43ms
step:1732/1750 train_time:172211ms step_avg:99.43ms
step:1733/1750 train_time:172315ms step_avg:99.43ms
step:1734/1750 train_time:172419ms step_avg:99.43ms
step:1735/1750 train_time:172522ms step_avg:99.44ms
step:1736/1750 train_time:172625ms step_avg:99.44ms
step:1737/1750 train_time:172728ms step_avg:99.44ms
step:1738/1750 train_time:172830ms step_avg:99.44ms
step:1739/1750 train_time:172932ms step_avg:99.44ms
step:1740/1750 train_time:173034ms step_avg:99.44ms
step:1741/1750 train_time:173140ms step_avg:99.45ms
step:1742/1750 train_time:173245ms step_avg:99.45ms
step:1743/1750 train_time:173347ms step_avg:99.45ms
step:1744/1750 train_time:173450ms step_avg:99.46ms
step:1745/1750 train_time:173552ms step_avg:99.46ms
step:1746/1750 train_time:173654ms step_avg:99.46ms
step:1747/1750 train_time:173756ms step_avg:99.46ms
step:1748/1750 train_time:173860ms step_avg:99.46ms
step:1749/1750 train_time:173962ms step_avg:99.46ms
step:1750/1750 train_time:174065ms step_avg:99.47ms
step:1750/1750 val_loss:3.2874 train_time:174156ms step_avg:99.52ms
peak memory allocated: 33278 MiB reserved: 48274 MiB
