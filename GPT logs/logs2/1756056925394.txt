import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X
'''
import torch.utils.dlpack as thd
from cupyx.scipy.sparse.linalg import svds as cupyx_svds

def several_sv_svds_approximation(W_torch, k, num_iter=50):
    """SVD approximation using the top k singular values and corresponding vectors."""
    # Store original device and dtype
    original_device = W_torch.device
    original_dtype = W_torch.dtype
    
    W = cp.from_dlpack(thd.to_dlpack(W_torch)).astype(cp.float32)
    U, S, Vt = cupyx_svds(W, k=min([k, W.shape[0] - 1, W.shape[1] - 1]), maxiter=num_iter, which='LM')

    # Convert back to torch tensors and ensure they're on the correct device
    approx_torch_U = thd.from_dlpack(U.toDlpack()).to(device=original_device, dtype=original_dtype)
    approx_torch_S = thd.from_dlpack(S.toDlpack()).to(device=original_device, dtype=original_dtype)
    approx_torch_Vt = thd.from_dlpack(Vt.toDlpack()).to(device=original_device, dtype=original_dtype)
    
    return approx_torch_U, approx_torch_S, approx_torch_Vt

class NormNeon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    # v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    u, s, vt = several_sv_svds_approximation(grad.bfloat16(), 5)
                    p.add_(other=u@vt, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()
'''
class FastNormMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, sgd_coeff=0):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, sgd_coeff=sgd_coeff)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        self.sgd_coeff = sgd_coeff

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            sgd_coeff = group["sgd_coeff"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    # assume: grad is a Tensor, momentum_buffer exists, momentum, sgd_coeff, eps, eff_lr are defined
                    with torch.no_grad():
                        # inplace blend with momentum (keeps original semantic: grad becomes the lerped tensor)
                        grad.lerp_(momentum_buffer, momentum)

                        # compute zeropower part from the (post-lerp) grad, in bfloat16 to match the original code
                        if sgd_coeff != 1:
                            # create update_part in bfloat16 (may allocate once per call; unavoidable if zeropower expects bfloat16)
                            update_part = zeropower_via_newtonschulz5(grad.to(torch.bfloat16), 5)

                            # upcast update_part to grad dtype to do mixed-dtype blending without extra temporaries later
                            if update_part.dtype != grad.dtype:
                                update_part = update_part.to(grad.dtype)

                        # compute inverse norm scalar efficiently (no large temp tensors)
                        # - view(-1) is cheap (no copy)
                        # - dot(view, view) is a single reduction
                        norm_sq = grad.view(-1).dot(grad.view(-1))
                        inv_norm = (norm_sq + eps).rsqrt()  # stable rsqrt on the scalar

                        # normalize grad in-place (so we avoid allocating g_normalized)
                        grad.mul_(inv_norm)   # now grad == g_normalized (in-place)

                        # build final update (reuse update_part buffer if available)
                        if sgd_coeff != 1:
                            # update_part := (1 - sgd_coeff) * update_part + sgd_coeff * grad
                            # do it in-place on update_part to avoid creating another temp
                            update_part.mul_(1 - sgd_coeff)    # in-place scale
                            update_part.lerp_(grad, sgd_coeff) # in-place linear interpolation
                            update = update_part
                        else:
                            # if sgd_coeff == 1 we can use grad directly (already normalized in-place)
                            update = grad

                        # apply update (same as original)
                        p.add_(other=update, alpha=-eff_lr)
                    '''
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    eps = 1e-12
                    g_normalized = grad / (grad.norm() + eps)
                    if sgd_coeff != 1:
                        update_part = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                        update = (1 - sgd_coeff) * update_part + sgd_coeff * g_normalized
                    else:
                        update = sgd_coeff * g_normalized
                    p.add_(other=update, alpha=-eff_lr)
                    '''
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class NormMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, sgd_coeff=0):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, sgd_coeff=sgd_coeff)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        self.sgd_coeff = sgd_coeff

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            sgd_coeff = group["sgd_coeff"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    eps = 1e-12
                    g_normalized = grad / (grad.norm() + eps)
                    if sgd_coeff != 1:
                        update_part = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                        update = (1 - sgd_coeff) * update_part + sgd_coeff * g_normalized
                    else:
                        update = sgd_coeff * g_normalized
                    p.add_(other=update, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()
class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)
                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_reduce_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 27.5
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos : pos + max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()
    starts = []
    for i in range(1, len(boundary_positions)):
        end = boundary_positions[i].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == dist.get_world_size():
                return starts, end - pos
            start = end
    assert False # increase max_batch_span if necessary

def distributed_data_generator(filename_pattern: str, batch_size: int, align_to_bos: bool):
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    max_batch_span = 2 * batch_size if align_to_bos else batch_size # provide buffer to handle samples up to length local_batch_size
    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            batch_span = batch_size
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1750 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
from datetime import datetime
if master_process:
    # run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_id = str(int(time.time() * 1000))
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0)
optimizer2 = NormMuon(hidden_matrix_params, lr=0.04, momentum=0.95, weight_decay=0.0, sgd_coeff=0.5)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.11 | packaged by conda-forge | (main, Jun  4 2025, 14:45:31) [GCC 13.3.0]
Running PyTorch 2.9.0.dev20250824+cu126 compiled for CUDA 12.6
Sun Aug 24 17:35:25 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:04:00.0 Off |                    0 |
| N/A   42C    P0            122W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:23:00.0 Off |                    0 |
| N/A   34C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:43:00.0 Off |                    0 |
| N/A   39C    P0            124W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   33C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:84:00.0 Off |                    0 |
| N/A   40C    P0            123W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:A3:00.0 Off |                    0 |
| N/A   34C    P0            123W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:C3:00.0 Off |                    0 |
| N/A   41C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E4:00.0 Off |                    0 |
| N/A   33C    P0            114W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.06ms
step:1/1750 train_time:140ms step_avg:139.53ms
step:2/1750 train_time:161ms step_avg:80.35ms
step:3/1750 train_time:242ms step_avg:80.61ms
step:4/1750 train_time:333ms step_avg:83.33ms
step:5/1750 train_time:426ms step_avg:85.29ms
step:6/1750 train_time:518ms step_avg:86.42ms
step:7/1750 train_time:611ms step_avg:87.34ms
step:8/1750 train_time:704ms step_avg:88.00ms
step:9/1750 train_time:796ms step_avg:88.49ms
step:10/1750 train_time:889ms step_avg:88.89ms
step:11/1750 train_time:982ms step_avg:89.30ms
step:12/1750 train_time:1079ms step_avg:89.91ms
step:13/1750 train_time:1175ms step_avg:90.35ms
step:14/1750 train_time:1271ms step_avg:90.78ms
step:15/1750 train_time:1365ms step_avg:90.97ms
step:16/1750 train_time:1457ms step_avg:91.07ms
step:17/1750 train_time:1551ms step_avg:91.22ms
step:18/1750 train_time:1644ms step_avg:91.35ms
step:19/1750 train_time:1737ms step_avg:91.41ms
step:20/1750 train_time:1829ms step_avg:91.45ms
step:21/1750 train_time:1923ms step_avg:91.55ms
step:22/1750 train_time:2016ms step_avg:91.63ms
step:23/1750 train_time:2110ms step_avg:91.75ms
step:24/1750 train_time:2206ms step_avg:91.91ms
step:25/1750 train_time:2300ms step_avg:92.01ms
step:26/1750 train_time:2394ms step_avg:92.07ms
step:27/1750 train_time:2488ms step_avg:92.13ms
step:28/1750 train_time:2580ms step_avg:92.16ms
step:29/1750 train_time:2674ms step_avg:92.20ms
step:30/1750 train_time:2767ms step_avg:92.22ms
step:31/1750 train_time:2860ms step_avg:92.24ms
step:32/1750 train_time:2952ms step_avg:92.26ms
step:33/1750 train_time:3046ms step_avg:92.30ms
step:34/1750 train_time:3139ms step_avg:92.33ms
step:35/1750 train_time:3234ms step_avg:92.39ms
step:36/1750 train_time:3329ms step_avg:92.46ms
step:37/1750 train_time:3423ms step_avg:92.51ms
step:38/1750 train_time:3516ms step_avg:92.53ms
step:39/1750 train_time:3610ms step_avg:92.55ms
step:40/1750 train_time:3703ms step_avg:92.58ms
step:41/1750 train_time:3796ms step_avg:92.58ms
step:42/1750 train_time:3889ms step_avg:92.59ms
step:43/1750 train_time:3982ms step_avg:92.60ms
step:44/1750 train_time:4075ms step_avg:92.62ms
step:45/1750 train_time:4169ms step_avg:92.65ms
step:46/1750 train_time:4263ms step_avg:92.67ms
step:47/1750 train_time:4357ms step_avg:92.70ms
step:48/1750 train_time:4451ms step_avg:92.72ms
step:49/1750 train_time:4544ms step_avg:92.74ms
step:50/1750 train_time:4638ms step_avg:92.75ms
step:51/1750 train_time:4731ms step_avg:92.77ms
step:52/1750 train_time:4826ms step_avg:92.80ms
step:53/1750 train_time:4919ms step_avg:92.81ms
step:54/1750 train_time:5013ms step_avg:92.82ms
step:55/1750 train_time:5106ms step_avg:92.84ms
step:56/1750 train_time:5200ms step_avg:92.86ms
step:57/1750 train_time:5293ms step_avg:92.85ms
step:58/1750 train_time:5386ms step_avg:92.87ms
step:59/1750 train_time:5480ms step_avg:92.88ms
step:60/1750 train_time:5573ms step_avg:92.89ms
step:61/1750 train_time:5667ms step_avg:92.90ms
step:62/1750 train_time:5761ms step_avg:92.92ms
step:63/1750 train_time:5854ms step_avg:92.92ms
step:64/1750 train_time:5948ms step_avg:92.93ms
step:65/1750 train_time:6041ms step_avg:92.94ms
step:66/1750 train_time:6134ms step_avg:92.94ms
step:67/1750 train_time:6228ms step_avg:92.95ms
step:68/1750 train_time:6321ms step_avg:92.96ms
step:69/1750 train_time:6414ms step_avg:92.96ms
step:70/1750 train_time:6509ms step_avg:92.98ms
step:71/1750 train_time:6602ms step_avg:92.99ms
step:72/1750 train_time:6695ms step_avg:92.98ms
step:73/1750 train_time:6789ms step_avg:92.99ms
step:74/1750 train_time:6882ms step_avg:93.00ms
step:75/1750 train_time:6975ms step_avg:93.00ms
step:76/1750 train_time:7069ms step_avg:93.02ms
step:77/1750 train_time:7163ms step_avg:93.03ms
step:78/1750 train_time:7257ms step_avg:93.04ms
step:79/1750 train_time:7350ms step_avg:93.04ms
step:80/1750 train_time:7444ms step_avg:93.05ms
step:81/1750 train_time:7537ms step_avg:93.05ms
step:82/1750 train_time:7631ms step_avg:93.06ms
step:83/1750 train_time:7725ms step_avg:93.07ms
step:84/1750 train_time:7818ms step_avg:93.07ms
step:85/1750 train_time:7911ms step_avg:93.07ms
step:86/1750 train_time:8004ms step_avg:93.07ms
step:87/1750 train_time:8098ms step_avg:93.08ms
step:88/1750 train_time:8191ms step_avg:93.08ms
step:89/1750 train_time:8285ms step_avg:93.09ms
step:90/1750 train_time:8379ms step_avg:93.10ms
step:91/1750 train_time:8472ms step_avg:93.10ms
step:92/1750 train_time:8567ms step_avg:93.12ms
step:93/1750 train_time:8661ms step_avg:93.13ms
step:94/1750 train_time:8754ms step_avg:93.13ms
step:95/1750 train_time:8847ms step_avg:93.13ms
step:96/1750 train_time:8940ms step_avg:93.13ms
step:97/1750 train_time:9034ms step_avg:93.13ms
step:98/1750 train_time:9128ms step_avg:93.14ms
step:99/1750 train_time:9221ms step_avg:93.14ms
step:100/1750 train_time:9315ms step_avg:93.15ms
step:101/1750 train_time:9408ms step_avg:93.15ms
step:102/1750 train_time:9502ms step_avg:93.15ms
step:103/1750 train_time:9594ms step_avg:93.15ms
step:104/1750 train_time:9688ms step_avg:93.16ms
step:105/1750 train_time:9782ms step_avg:93.16ms
step:106/1750 train_time:9875ms step_avg:93.16ms
step:107/1750 train_time:9968ms step_avg:93.16ms
step:108/1750 train_time:10062ms step_avg:93.17ms
step:109/1750 train_time:10155ms step_avg:93.17ms
step:110/1750 train_time:10248ms step_avg:93.16ms
step:111/1750 train_time:10341ms step_avg:93.17ms
step:112/1750 train_time:10436ms step_avg:93.18ms
step:113/1750 train_time:10529ms step_avg:93.18ms
step:114/1750 train_time:10623ms step_avg:93.18ms
step:115/1750 train_time:10717ms step_avg:93.19ms
step:116/1750 train_time:10810ms step_avg:93.19ms
step:117/1750 train_time:10903ms step_avg:93.19ms
step:118/1750 train_time:10997ms step_avg:93.19ms
step:119/1750 train_time:11090ms step_avg:93.20ms
step:120/1750 train_time:11184ms step_avg:93.20ms
step:121/1750 train_time:11277ms step_avg:93.20ms
step:122/1750 train_time:11371ms step_avg:93.21ms
step:123/1750 train_time:11466ms step_avg:93.22ms
step:124/1750 train_time:11559ms step_avg:93.22ms
step:125/1750 train_time:11652ms step_avg:93.22ms
step:125/1750 val_loss:4.6934 train_time:11735ms step_avg:93.88ms
step:126/1750 train_time:11759ms step_avg:93.32ms
step:127/1750 train_time:11847ms step_avg:93.28ms
step:128/1750 train_time:11950ms step_avg:93.36ms
step:129/1750 train_time:12046ms step_avg:93.38ms
step:130/1750 train_time:12139ms step_avg:93.38ms
step:131/1750 train_time:12232ms step_avg:93.38ms
step:132/1750 train_time:12326ms step_avg:93.38ms
step:133/1750 train_time:12418ms step_avg:93.37ms
step:134/1750 train_time:12511ms step_avg:93.37ms
step:135/1750 train_time:12604ms step_avg:93.36ms
step:136/1750 train_time:12697ms step_avg:93.36ms
step:137/1750 train_time:12791ms step_avg:93.37ms
step:138/1750 train_time:12888ms step_avg:93.39ms
step:139/1750 train_time:12984ms step_avg:93.41ms
step:140/1750 train_time:13079ms step_avg:93.42ms
step:141/1750 train_time:13174ms step_avg:93.43ms
step:142/1750 train_time:13268ms step_avg:93.44ms
step:143/1750 train_time:13361ms step_avg:93.44ms
step:144/1750 train_time:13455ms step_avg:93.44ms
step:145/1750 train_time:13547ms step_avg:93.43ms
step:146/1750 train_time:13640ms step_avg:93.43ms
step:147/1750 train_time:13734ms step_avg:93.43ms
step:148/1750 train_time:13828ms step_avg:93.43ms
step:149/1750 train_time:13924ms step_avg:93.45ms
step:150/1750 train_time:14020ms step_avg:93.47ms
step:151/1750 train_time:14114ms step_avg:93.47ms
step:152/1750 train_time:14208ms step_avg:93.47ms
step:153/1750 train_time:14302ms step_avg:93.48ms
step:154/1750 train_time:14396ms step_avg:93.48ms
step:155/1750 train_time:14489ms step_avg:93.48ms
step:156/1750 train_time:14583ms step_avg:93.48ms
step:157/1750 train_time:14676ms step_avg:93.48ms
step:158/1750 train_time:14769ms step_avg:93.47ms
step:159/1750 train_time:14864ms step_avg:93.48ms
step:160/1750 train_time:14960ms step_avg:93.50ms
step:161/1750 train_time:15054ms step_avg:93.50ms
step:162/1750 train_time:15147ms step_avg:93.50ms
step:163/1750 train_time:15242ms step_avg:93.51ms
step:164/1750 train_time:15336ms step_avg:93.51ms
step:165/1750 train_time:15430ms step_avg:93.51ms
step:166/1750 train_time:15524ms step_avg:93.52ms
step:167/1750 train_time:15617ms step_avg:93.52ms
step:168/1750 train_time:15710ms step_avg:93.51ms
step:169/1750 train_time:15805ms step_avg:93.52ms
step:170/1750 train_time:15899ms step_avg:93.52ms
step:171/1750 train_time:15993ms step_avg:93.53ms
step:172/1750 train_time:16088ms step_avg:93.53ms
step:173/1750 train_time:16182ms step_avg:93.54ms
step:174/1750 train_time:16276ms step_avg:93.54ms
step:175/1750 train_time:16369ms step_avg:93.54ms
step:176/1750 train_time:16463ms step_avg:93.54ms
step:177/1750 train_time:16557ms step_avg:93.54ms
step:178/1750 train_time:16650ms step_avg:93.54ms
step:179/1750 train_time:16744ms step_avg:93.54ms
step:180/1750 train_time:16839ms step_avg:93.55ms
step:181/1750 train_time:16932ms step_avg:93.55ms
step:182/1750 train_time:17027ms step_avg:93.55ms
step:183/1750 train_time:17121ms step_avg:93.56ms
step:184/1750 train_time:17215ms step_avg:93.56ms
step:185/1750 train_time:17309ms step_avg:93.56ms
step:186/1750 train_time:17403ms step_avg:93.56ms
step:187/1750 train_time:17496ms step_avg:93.56ms
step:188/1750 train_time:17589ms step_avg:93.56ms
step:189/1750 train_time:17684ms step_avg:93.56ms
step:190/1750 train_time:17778ms step_avg:93.57ms
step:191/1750 train_time:17871ms step_avg:93.57ms
step:192/1750 train_time:17965ms step_avg:93.57ms
step:193/1750 train_time:18060ms step_avg:93.58ms
step:194/1750 train_time:18154ms step_avg:93.58ms
step:195/1750 train_time:18248ms step_avg:93.58ms
step:196/1750 train_time:18343ms step_avg:93.59ms
step:197/1750 train_time:18437ms step_avg:93.59ms
step:198/1750 train_time:18531ms step_avg:93.59ms
step:199/1750 train_time:18624ms step_avg:93.59ms
step:200/1750 train_time:18719ms step_avg:93.59ms
step:201/1750 train_time:18813ms step_avg:93.60ms
step:202/1750 train_time:18907ms step_avg:93.60ms
step:203/1750 train_time:19001ms step_avg:93.60ms
step:204/1750 train_time:19095ms step_avg:93.60ms
step:205/1750 train_time:19190ms step_avg:93.61ms
step:206/1750 train_time:19284ms step_avg:93.61ms
step:207/1750 train_time:19377ms step_avg:93.61ms
step:208/1750 train_time:19470ms step_avg:93.61ms
step:209/1750 train_time:19564ms step_avg:93.61ms
step:210/1750 train_time:19659ms step_avg:93.61ms
step:211/1750 train_time:19753ms step_avg:93.62ms
step:212/1750 train_time:19847ms step_avg:93.62ms
step:213/1750 train_time:19941ms step_avg:93.62ms
step:214/1750 train_time:20034ms step_avg:93.62ms
step:215/1750 train_time:20128ms step_avg:93.62ms
step:216/1750 train_time:20222ms step_avg:93.62ms
step:217/1750 train_time:20316ms step_avg:93.62ms
step:218/1750 train_time:20409ms step_avg:93.62ms
step:219/1750 train_time:20503ms step_avg:93.62ms
step:220/1750 train_time:20597ms step_avg:93.62ms
step:221/1750 train_time:20691ms step_avg:93.62ms
step:222/1750 train_time:20784ms step_avg:93.62ms
step:223/1750 train_time:20878ms step_avg:93.62ms
step:224/1750 train_time:20972ms step_avg:93.63ms
step:225/1750 train_time:21066ms step_avg:93.63ms
step:226/1750 train_time:21160ms step_avg:93.63ms
step:227/1750 train_time:21254ms step_avg:93.63ms
step:228/1750 train_time:21348ms step_avg:93.63ms
step:229/1750 train_time:21441ms step_avg:93.63ms
step:230/1750 train_time:21535ms step_avg:93.63ms
step:231/1750 train_time:21629ms step_avg:93.63ms
step:232/1750 train_time:21723ms step_avg:93.63ms
step:233/1750 train_time:21818ms step_avg:93.64ms
step:234/1750 train_time:21911ms step_avg:93.64ms
step:235/1750 train_time:22006ms step_avg:93.64ms
step:236/1750 train_time:22099ms step_avg:93.64ms
step:237/1750 train_time:22194ms step_avg:93.65ms
step:238/1750 train_time:22288ms step_avg:93.65ms
step:239/1750 train_time:22382ms step_avg:93.65ms
step:240/1750 train_time:22476ms step_avg:93.65ms
step:241/1750 train_time:22569ms step_avg:93.65ms
step:242/1750 train_time:22663ms step_avg:93.65ms
step:243/1750 train_time:22758ms step_avg:93.66ms
step:244/1750 train_time:22852ms step_avg:93.66ms
step:245/1750 train_time:22945ms step_avg:93.65ms
step:246/1750 train_time:23040ms step_avg:93.66ms
step:247/1750 train_time:23134ms step_avg:93.66ms
step:248/1750 train_time:23228ms step_avg:93.66ms
step:249/1750 train_time:23322ms step_avg:93.66ms
step:250/1750 train_time:23416ms step_avg:93.66ms
step:250/1750 val_loss:4.1180 train_time:23499ms step_avg:94.00ms
step:251/1750 train_time:23523ms step_avg:93.72ms
step:252/1750 train_time:23611ms step_avg:93.70ms
step:253/1750 train_time:23708ms step_avg:93.71ms
step:254/1750 train_time:23802ms step_avg:93.71ms
step:255/1750 train_time:23896ms step_avg:93.71ms
step:256/1750 train_time:23989ms step_avg:93.71ms
step:257/1750 train_time:24082ms step_avg:93.70ms
step:258/1750 train_time:24175ms step_avg:93.70ms
step:259/1750 train_time:24268ms step_avg:93.70ms
step:260/1750 train_time:24360ms step_avg:93.69ms
step:261/1750 train_time:24455ms step_avg:93.70ms
step:262/1750 train_time:24550ms step_avg:93.70ms
step:263/1750 train_time:24646ms step_avg:93.71ms
step:264/1750 train_time:24742ms step_avg:93.72ms
step:265/1750 train_time:24837ms step_avg:93.72ms
step:266/1750 train_time:24931ms step_avg:93.72ms
step:267/1750 train_time:25024ms step_avg:93.72ms
step:268/1750 train_time:25118ms step_avg:93.72ms
step:269/1750 train_time:25211ms step_avg:93.72ms
step:270/1750 train_time:25305ms step_avg:93.72ms
step:271/1750 train_time:25399ms step_avg:93.72ms
step:272/1750 train_time:25493ms step_avg:93.73ms
step:273/1750 train_time:25589ms step_avg:93.73ms
step:274/1750 train_time:25683ms step_avg:93.73ms
step:275/1750 train_time:25778ms step_avg:93.74ms
step:276/1750 train_time:25872ms step_avg:93.74ms
step:277/1750 train_time:25967ms step_avg:93.74ms
step:278/1750 train_time:26061ms step_avg:93.74ms
step:279/1750 train_time:26155ms step_avg:93.75ms
step:280/1750 train_time:26249ms step_avg:93.75ms
step:281/1750 train_time:26343ms step_avg:93.75ms
step:282/1750 train_time:26438ms step_avg:93.75ms
step:283/1750 train_time:26533ms step_avg:93.76ms
step:284/1750 train_time:26628ms step_avg:93.76ms
step:285/1750 train_time:26723ms step_avg:93.76ms
step:286/1750 train_time:26818ms step_avg:93.77ms
step:287/1750 train_time:26913ms step_avg:93.77ms
step:288/1750 train_time:27007ms step_avg:93.77ms
step:289/1750 train_time:27101ms step_avg:93.78ms
step:290/1750 train_time:27195ms step_avg:93.78ms
step:291/1750 train_time:27288ms step_avg:93.77ms
step:292/1750 train_time:27383ms step_avg:93.78ms
step:293/1750 train_time:27478ms step_avg:93.78ms
step:294/1750 train_time:27572ms step_avg:93.78ms
step:295/1750 train_time:27667ms step_avg:93.78ms
step:296/1750 train_time:27762ms step_avg:93.79ms
step:297/1750 train_time:27857ms step_avg:93.79ms
step:298/1750 train_time:27951ms step_avg:93.80ms
step:299/1750 train_time:28046ms step_avg:93.80ms
step:300/1750 train_time:28140ms step_avg:93.80ms
step:301/1750 train_time:28233ms step_avg:93.80ms
step:302/1750 train_time:28327ms step_avg:93.80ms
step:303/1750 train_time:28422ms step_avg:93.80ms
step:304/1750 train_time:28516ms step_avg:93.80ms
step:305/1750 train_time:28611ms step_avg:93.81ms
step:306/1750 train_time:28705ms step_avg:93.81ms
step:307/1750 train_time:28801ms step_avg:93.81ms
step:308/1750 train_time:28896ms step_avg:93.82ms
step:309/1750 train_time:28990ms step_avg:93.82ms
step:310/1750 train_time:29083ms step_avg:93.82ms
step:311/1750 train_time:29177ms step_avg:93.82ms
step:312/1750 train_time:29271ms step_avg:93.82ms
step:313/1750 train_time:29366ms step_avg:93.82ms
step:314/1750 train_time:29460ms step_avg:93.82ms
step:315/1750 train_time:29554ms step_avg:93.82ms
step:316/1750 train_time:29648ms step_avg:93.82ms
step:317/1750 train_time:29742ms step_avg:93.82ms
step:318/1750 train_time:29836ms step_avg:93.82ms
step:319/1750 train_time:29931ms step_avg:93.83ms
step:320/1750 train_time:30025ms step_avg:93.83ms
step:321/1750 train_time:30119ms step_avg:93.83ms
step:322/1750 train_time:30212ms step_avg:93.83ms
step:323/1750 train_time:30307ms step_avg:93.83ms
step:324/1750 train_time:30401ms step_avg:93.83ms
step:325/1750 train_time:30494ms step_avg:93.83ms
step:326/1750 train_time:30588ms step_avg:93.83ms
step:327/1750 train_time:30682ms step_avg:93.83ms
step:328/1750 train_time:30777ms step_avg:93.83ms
step:329/1750 train_time:30871ms step_avg:93.83ms
step:330/1750 train_time:30965ms step_avg:93.83ms
step:331/1750 train_time:31060ms step_avg:93.84ms
step:332/1750 train_time:31154ms step_avg:93.84ms
step:333/1750 train_time:31247ms step_avg:93.84ms
step:334/1750 train_time:31342ms step_avg:93.84ms
step:335/1750 train_time:31435ms step_avg:93.84ms
step:336/1750 train_time:31530ms step_avg:93.84ms
step:337/1750 train_time:31623ms step_avg:93.84ms
step:338/1750 train_time:31718ms step_avg:93.84ms
step:339/1750 train_time:31812ms step_avg:93.84ms
step:340/1750 train_time:31906ms step_avg:93.84ms
step:341/1750 train_time:32001ms step_avg:93.84ms
step:342/1750 train_time:32095ms step_avg:93.84ms
step:343/1750 train_time:32188ms step_avg:93.84ms
step:344/1750 train_time:32283ms step_avg:93.85ms
step:345/1750 train_time:32377ms step_avg:93.85ms
step:346/1750 train_time:32472ms step_avg:93.85ms
step:347/1750 train_time:32565ms step_avg:93.85ms
step:348/1750 train_time:32660ms step_avg:93.85ms
step:349/1750 train_time:32754ms step_avg:93.85ms
step:350/1750 train_time:32848ms step_avg:93.85ms
step:351/1750 train_time:32942ms step_avg:93.85ms
step:352/1750 train_time:33036ms step_avg:93.85ms
step:353/1750 train_time:33131ms step_avg:93.86ms
step:354/1750 train_time:33226ms step_avg:93.86ms
step:355/1750 train_time:33320ms step_avg:93.86ms
step:356/1750 train_time:33414ms step_avg:93.86ms
step:357/1750 train_time:33509ms step_avg:93.86ms
step:358/1750 train_time:33602ms step_avg:93.86ms
step:359/1750 train_time:33697ms step_avg:93.86ms
step:360/1750 train_time:33792ms step_avg:93.87ms
step:361/1750 train_time:33886ms step_avg:93.87ms
step:362/1750 train_time:33981ms step_avg:93.87ms
step:363/1750 train_time:34076ms step_avg:93.87ms
step:364/1750 train_time:34169ms step_avg:93.87ms
step:365/1750 train_time:34264ms step_avg:93.87ms
step:366/1750 train_time:34358ms step_avg:93.88ms
step:367/1750 train_time:34453ms step_avg:93.88ms
step:368/1750 train_time:34547ms step_avg:93.88ms
step:369/1750 train_time:34641ms step_avg:93.88ms
step:370/1750 train_time:34735ms step_avg:93.88ms
step:371/1750 train_time:34830ms step_avg:93.88ms
step:372/1750 train_time:34924ms step_avg:93.88ms
step:373/1750 train_time:35018ms step_avg:93.88ms
step:374/1750 train_time:35112ms step_avg:93.88ms
step:375/1750 train_time:35206ms step_avg:93.88ms
step:375/1750 val_loss:3.9061 train_time:35289ms step_avg:94.10ms
step:376/1750 train_time:35311ms step_avg:93.91ms
step:377/1750 train_time:35400ms step_avg:93.90ms
step:378/1750 train_time:35499ms step_avg:93.91ms
step:379/1750 train_time:35592ms step_avg:93.91ms
step:380/1750 train_time:35686ms step_avg:93.91ms
step:381/1750 train_time:35779ms step_avg:93.91ms
step:382/1750 train_time:35872ms step_avg:93.91ms
step:383/1750 train_time:35966ms step_avg:93.91ms
step:384/1750 train_time:36059ms step_avg:93.90ms
step:385/1750 train_time:36152ms step_avg:93.90ms
step:386/1750 train_time:36248ms step_avg:93.91ms
step:387/1750 train_time:36345ms step_avg:93.91ms
step:388/1750 train_time:36440ms step_avg:93.92ms
step:389/1750 train_time:36536ms step_avg:93.92ms
step:390/1750 train_time:36630ms step_avg:93.92ms
step:391/1750 train_time:36726ms step_avg:93.93ms
step:392/1750 train_time:36822ms step_avg:93.93ms
step:393/1750 train_time:36917ms step_avg:93.94ms
step:394/1750 train_time:37013ms step_avg:93.94ms
step:395/1750 train_time:37108ms step_avg:93.95ms
step:396/1750 train_time:37204ms step_avg:93.95ms
step:397/1750 train_time:37301ms step_avg:93.96ms
step:398/1750 train_time:37398ms step_avg:93.97ms
step:399/1750 train_time:37496ms step_avg:93.97ms
step:400/1750 train_time:37592ms step_avg:93.98ms
step:401/1750 train_time:37688ms step_avg:93.99ms
step:402/1750 train_time:37784ms step_avg:93.99ms
step:403/1750 train_time:37879ms step_avg:93.99ms
step:404/1750 train_time:37975ms step_avg:94.00ms
step:405/1750 train_time:38071ms step_avg:94.00ms
step:406/1750 train_time:38167ms step_avg:94.01ms
step:407/1750 train_time:38262ms step_avg:94.01ms
step:408/1750 train_time:38359ms step_avg:94.02ms
step:409/1750 train_time:38455ms step_avg:94.02ms
step:410/1750 train_time:38551ms step_avg:94.03ms
step:411/1750 train_time:38647ms step_avg:94.03ms
step:412/1750 train_time:38744ms step_avg:94.04ms
step:413/1750 train_time:38839ms step_avg:94.04ms
step:414/1750 train_time:38935ms step_avg:94.05ms
step:415/1750 train_time:39031ms step_avg:94.05ms
step:416/1750 train_time:39127ms step_avg:94.06ms
step:417/1750 train_time:39223ms step_avg:94.06ms
step:418/1750 train_time:39319ms step_avg:94.07ms
step:419/1750 train_time:39416ms step_avg:94.07ms
step:420/1750 train_time:39512ms step_avg:94.08ms
step:421/1750 train_time:39609ms step_avg:94.08ms
step:422/1750 train_time:39705ms step_avg:94.09ms
step:423/1750 train_time:39801ms step_avg:94.09ms
step:424/1750 train_time:39896ms step_avg:94.09ms
step:425/1750 train_time:39991ms step_avg:94.10ms
step:426/1750 train_time:40087ms step_avg:94.10ms
step:427/1750 train_time:40183ms step_avg:94.11ms
step:428/1750 train_time:40279ms step_avg:94.11ms
step:429/1750 train_time:40375ms step_avg:94.11ms
step:430/1750 train_time:40471ms step_avg:94.12ms
step:431/1750 train_time:40567ms step_avg:94.12ms
step:432/1750 train_time:40663ms step_avg:94.13ms
step:433/1750 train_time:40759ms step_avg:94.13ms
step:434/1750 train_time:40855ms step_avg:94.14ms
step:435/1750 train_time:40951ms step_avg:94.14ms
step:436/1750 train_time:41047ms step_avg:94.14ms
step:437/1750 train_time:41142ms step_avg:94.15ms
step:438/1750 train_time:41239ms step_avg:94.15ms
step:439/1750 train_time:41335ms step_avg:94.16ms
step:440/1750 train_time:41431ms step_avg:94.16ms
step:441/1750 train_time:41528ms step_avg:94.17ms
step:442/1750 train_time:41624ms step_avg:94.17ms
step:443/1750 train_time:41720ms step_avg:94.18ms
step:444/1750 train_time:41816ms step_avg:94.18ms
step:445/1750 train_time:41912ms step_avg:94.18ms
step:446/1750 train_time:42008ms step_avg:94.19ms
step:447/1750 train_time:42104ms step_avg:94.19ms
step:448/1750 train_time:42201ms step_avg:94.20ms
step:449/1750 train_time:42298ms step_avg:94.20ms
step:450/1750 train_time:42394ms step_avg:94.21ms
step:451/1750 train_time:42491ms step_avg:94.21ms
step:452/1750 train_time:42587ms step_avg:94.22ms
step:453/1750 train_time:42684ms step_avg:94.22ms
step:454/1750 train_time:42779ms step_avg:94.23ms
step:455/1750 train_time:42875ms step_avg:94.23ms
step:456/1750 train_time:42972ms step_avg:94.24ms
step:457/1750 train_time:43068ms step_avg:94.24ms
step:458/1750 train_time:43164ms step_avg:94.24ms
step:459/1750 train_time:43259ms step_avg:94.25ms
step:460/1750 train_time:43355ms step_avg:94.25ms
step:461/1750 train_time:43452ms step_avg:94.26ms
step:462/1750 train_time:43549ms step_avg:94.26ms
step:463/1750 train_time:43645ms step_avg:94.27ms
step:464/1750 train_time:43741ms step_avg:94.27ms
step:465/1750 train_time:43837ms step_avg:94.27ms
step:466/1750 train_time:43933ms step_avg:94.28ms
step:467/1750 train_time:44029ms step_avg:94.28ms
step:468/1750 train_time:44126ms step_avg:94.29ms
step:469/1750 train_time:44223ms step_avg:94.29ms
step:470/1750 train_time:44319ms step_avg:94.30ms
step:471/1750 train_time:44415ms step_avg:94.30ms
step:472/1750 train_time:44511ms step_avg:94.30ms
step:473/1750 train_time:44607ms step_avg:94.31ms
step:474/1750 train_time:44703ms step_avg:94.31ms
step:475/1750 train_time:44799ms step_avg:94.31ms
step:476/1750 train_time:44895ms step_avg:94.32ms
step:477/1750 train_time:44991ms step_avg:94.32ms
step:478/1750 train_time:45087ms step_avg:94.32ms
step:479/1750 train_time:45183ms step_avg:94.33ms
step:480/1750 train_time:45279ms step_avg:94.33ms
step:481/1750 train_time:45375ms step_avg:94.34ms
step:482/1750 train_time:45471ms step_avg:94.34ms
step:483/1750 train_time:45568ms step_avg:94.34ms
step:484/1750 train_time:45663ms step_avg:94.35ms
step:485/1750 train_time:45758ms step_avg:94.35ms
step:486/1750 train_time:45855ms step_avg:94.35ms
step:487/1750 train_time:45952ms step_avg:94.36ms
step:488/1750 train_time:46047ms step_avg:94.36ms
step:489/1750 train_time:46143ms step_avg:94.36ms
step:490/1750 train_time:46239ms step_avg:94.36ms
step:491/1750 train_time:46335ms step_avg:94.37ms
step:492/1750 train_time:46431ms step_avg:94.37ms
step:493/1750 train_time:46528ms step_avg:94.38ms
step:494/1750 train_time:46624ms step_avg:94.38ms
step:495/1750 train_time:46720ms step_avg:94.38ms
step:496/1750 train_time:46816ms step_avg:94.39ms
step:497/1750 train_time:46912ms step_avg:94.39ms
step:498/1750 train_time:47008ms step_avg:94.39ms
step:499/1750 train_time:47105ms step_avg:94.40ms
step:500/1750 train_time:47200ms step_avg:94.40ms
step:500/1750 val_loss:3.7517 train_time:47286ms step_avg:94.57ms
step:501/1750 train_time:47310ms step_avg:94.43ms
step:502/1750 train_time:47404ms step_avg:94.43ms
step:503/1750 train_time:47503ms step_avg:94.44ms
step:504/1750 train_time:47599ms step_avg:94.44ms
step:505/1750 train_time:47694ms step_avg:94.44ms
step:506/1750 train_time:47790ms step_avg:94.45ms
step:507/1750 train_time:47884ms step_avg:94.45ms
step:508/1750 train_time:47980ms step_avg:94.45ms
step:509/1750 train_time:48076ms step_avg:94.45ms
step:510/1750 train_time:48171ms step_avg:94.45ms
step:511/1750 train_time:48267ms step_avg:94.46ms
step:512/1750 train_time:48366ms step_avg:94.46ms
step:513/1750 train_time:48464ms step_avg:94.47ms
step:514/1750 train_time:48561ms step_avg:94.48ms
step:515/1750 train_time:48656ms step_avg:94.48ms
step:516/1750 train_time:48752ms step_avg:94.48ms
step:517/1750 train_time:48847ms step_avg:94.48ms
step:518/1750 train_time:48943ms step_avg:94.48ms
step:519/1750 train_time:49038ms step_avg:94.49ms
step:520/1750 train_time:49134ms step_avg:94.49ms
step:521/1750 train_time:49230ms step_avg:94.49ms
step:522/1750 train_time:49327ms step_avg:94.50ms
step:523/1750 train_time:49424ms step_avg:94.50ms
step:524/1750 train_time:49521ms step_avg:94.51ms
step:525/1750 train_time:49618ms step_avg:94.51ms
step:526/1750 train_time:49714ms step_avg:94.51ms
step:527/1750 train_time:49811ms step_avg:94.52ms
step:528/1750 train_time:49907ms step_avg:94.52ms
step:529/1750 train_time:50003ms step_avg:94.52ms
step:530/1750 train_time:50099ms step_avg:94.53ms
step:531/1750 train_time:50195ms step_avg:94.53ms
step:532/1750 train_time:50292ms step_avg:94.53ms
step:533/1750 train_time:50390ms step_avg:94.54ms
step:534/1750 train_time:50487ms step_avg:94.54ms
step:535/1750 train_time:50584ms step_avg:94.55ms
step:536/1750 train_time:50681ms step_avg:94.55ms
step:537/1750 train_time:50778ms step_avg:94.56ms
step:538/1750 train_time:50874ms step_avg:94.56ms
step:539/1750 train_time:50970ms step_avg:94.56ms
step:540/1750 train_time:51066ms step_avg:94.57ms
step:541/1750 train_time:51163ms step_avg:94.57ms
step:542/1750 train_time:51259ms step_avg:94.57ms
step:543/1750 train_time:51355ms step_avg:94.58ms
step:544/1750 train_time:51452ms step_avg:94.58ms
step:545/1750 train_time:51549ms step_avg:94.59ms
step:546/1750 train_time:51647ms step_avg:94.59ms
step:547/1750 train_time:51743ms step_avg:94.59ms
step:548/1750 train_time:51840ms step_avg:94.60ms
step:549/1750 train_time:51936ms step_avg:94.60ms
step:550/1750 train_time:52032ms step_avg:94.60ms
step:551/1750 train_time:52128ms step_avg:94.61ms
step:552/1750 train_time:52225ms step_avg:94.61ms
step:553/1750 train_time:52321ms step_avg:94.61ms
step:554/1750 train_time:52418ms step_avg:94.62ms
step:555/1750 train_time:52515ms step_avg:94.62ms
step:556/1750 train_time:52613ms step_avg:94.63ms
step:557/1750 train_time:52710ms step_avg:94.63ms
step:558/1750 train_time:52806ms step_avg:94.63ms
step:559/1750 train_time:52903ms step_avg:94.64ms
step:560/1750 train_time:52999ms step_avg:94.64ms
step:561/1750 train_time:53095ms step_avg:94.64ms
step:562/1750 train_time:53192ms step_avg:94.65ms
step:563/1750 train_time:53289ms step_avg:94.65ms
step:564/1750 train_time:53385ms step_avg:94.65ms
step:565/1750 train_time:53483ms step_avg:94.66ms
step:566/1750 train_time:53580ms step_avg:94.66ms
step:567/1750 train_time:53678ms step_avg:94.67ms
step:568/1750 train_time:53774ms step_avg:94.67ms
step:569/1750 train_time:53872ms step_avg:94.68ms
step:570/1750 train_time:53969ms step_avg:94.68ms
step:571/1750 train_time:54065ms step_avg:94.68ms
step:572/1750 train_time:54161ms step_avg:94.69ms
step:573/1750 train_time:54257ms step_avg:94.69ms
step:574/1750 train_time:54354ms step_avg:94.69ms
step:575/1750 train_time:54451ms step_avg:94.70ms
step:576/1750 train_time:54548ms step_avg:94.70ms
step:577/1750 train_time:54645ms step_avg:94.71ms
step:578/1750 train_time:54742ms step_avg:94.71ms
step:579/1750 train_time:54838ms step_avg:94.71ms
step:580/1750 train_time:54934ms step_avg:94.71ms
step:581/1750 train_time:55031ms step_avg:94.72ms
step:582/1750 train_time:55128ms step_avg:94.72ms
step:583/1750 train_time:55224ms step_avg:94.72ms
step:584/1750 train_time:55320ms step_avg:94.73ms
step:585/1750 train_time:55417ms step_avg:94.73ms
step:586/1750 train_time:55514ms step_avg:94.73ms
step:587/1750 train_time:55611ms step_avg:94.74ms
step:588/1750 train_time:55708ms step_avg:94.74ms
step:589/1750 train_time:55804ms step_avg:94.74ms
step:590/1750 train_time:55900ms step_avg:94.75ms
step:591/1750 train_time:55996ms step_avg:94.75ms
step:592/1750 train_time:56093ms step_avg:94.75ms
step:593/1750 train_time:56189ms step_avg:94.75ms
step:594/1750 train_time:56286ms step_avg:94.76ms
step:595/1750 train_time:56382ms step_avg:94.76ms
step:596/1750 train_time:56479ms step_avg:94.76ms
step:597/1750 train_time:56576ms step_avg:94.77ms
step:598/1750 train_time:56672ms step_avg:94.77ms
step:599/1750 train_time:56769ms step_avg:94.77ms
step:600/1750 train_time:56866ms step_avg:94.78ms
step:601/1750 train_time:56962ms step_avg:94.78ms
step:602/1750 train_time:57058ms step_avg:94.78ms
step:603/1750 train_time:57154ms step_avg:94.78ms
step:604/1750 train_time:57251ms step_avg:94.79ms
step:605/1750 train_time:57348ms step_avg:94.79ms
step:606/1750 train_time:57444ms step_avg:94.79ms
step:607/1750 train_time:57542ms step_avg:94.80ms
step:608/1750 train_time:57638ms step_avg:94.80ms
step:609/1750 train_time:57734ms step_avg:94.80ms
step:610/1750 train_time:57831ms step_avg:94.81ms
step:611/1750 train_time:57928ms step_avg:94.81ms
step:612/1750 train_time:58023ms step_avg:94.81ms
step:613/1750 train_time:58120ms step_avg:94.81ms
step:614/1750 train_time:58216ms step_avg:94.81ms
step:615/1750 train_time:58312ms step_avg:94.82ms
step:616/1750 train_time:58409ms step_avg:94.82ms
step:617/1750 train_time:58505ms step_avg:94.82ms
step:618/1750 train_time:58602ms step_avg:94.83ms
step:619/1750 train_time:58699ms step_avg:94.83ms
step:620/1750 train_time:58794ms step_avg:94.83ms
step:621/1750 train_time:58892ms step_avg:94.83ms
step:622/1750 train_time:58988ms step_avg:94.84ms
step:623/1750 train_time:59084ms step_avg:94.84ms
step:624/1750 train_time:59181ms step_avg:94.84ms
step:625/1750 train_time:59278ms step_avg:94.84ms
step:625/1750 val_loss:3.6616 train_time:59363ms step_avg:94.98ms
step:626/1750 train_time:59386ms step_avg:94.87ms
step:627/1750 train_time:59477ms step_avg:94.86ms
step:628/1750 train_time:59574ms step_avg:94.86ms
step:629/1750 train_time:59670ms step_avg:94.87ms
step:630/1750 train_time:59766ms step_avg:94.87ms
step:631/1750 train_time:59861ms step_avg:94.87ms
step:632/1750 train_time:59957ms step_avg:94.87ms
step:633/1750 train_time:60053ms step_avg:94.87ms
step:634/1750 train_time:60149ms step_avg:94.87ms
step:635/1750 train_time:60245ms step_avg:94.87ms
step:636/1750 train_time:60344ms step_avg:94.88ms
step:637/1750 train_time:60442ms step_avg:94.89ms
step:638/1750 train_time:60540ms step_avg:94.89ms
step:639/1750 train_time:60637ms step_avg:94.89ms
step:640/1750 train_time:60733ms step_avg:94.90ms
step:641/1750 train_time:60829ms step_avg:94.90ms
step:642/1750 train_time:60925ms step_avg:94.90ms
step:643/1750 train_time:61022ms step_avg:94.90ms
step:644/1750 train_time:61118ms step_avg:94.90ms
step:645/1750 train_time:61215ms step_avg:94.91ms
step:646/1750 train_time:61312ms step_avg:94.91ms
step:647/1750 train_time:61409ms step_avg:94.91ms
step:648/1750 train_time:61507ms step_avg:94.92ms
step:649/1750 train_time:61604ms step_avg:94.92ms
step:650/1750 train_time:61701ms step_avg:94.92ms
step:651/1750 train_time:61800ms step_avg:94.93ms
step:652/1750 train_time:61898ms step_avg:94.93ms
step:653/1750 train_time:61996ms step_avg:94.94ms
step:654/1750 train_time:62093ms step_avg:94.94ms
step:655/1750 train_time:62191ms step_avg:94.95ms
step:656/1750 train_time:62288ms step_avg:94.95ms
step:657/1750 train_time:62386ms step_avg:94.96ms
step:658/1750 train_time:62484ms step_avg:94.96ms
step:659/1750 train_time:62583ms step_avg:94.97ms
step:660/1750 train_time:62681ms step_avg:94.97ms
step:661/1750 train_time:62779ms step_avg:94.98ms
step:662/1750 train_time:62876ms step_avg:94.98ms
step:663/1750 train_time:62974ms step_avg:94.98ms
step:664/1750 train_time:63072ms step_avg:94.99ms
step:665/1750 train_time:63169ms step_avg:94.99ms
step:666/1750 train_time:63266ms step_avg:94.99ms
step:667/1750 train_time:63364ms step_avg:95.00ms
step:668/1750 train_time:63462ms step_avg:95.00ms
step:669/1750 train_time:63560ms step_avg:95.01ms
step:670/1750 train_time:63659ms step_avg:95.01ms
step:671/1750 train_time:63757ms step_avg:95.02ms
step:672/1750 train_time:63855ms step_avg:95.02ms
step:673/1750 train_time:63952ms step_avg:95.03ms
step:674/1750 train_time:64051ms step_avg:95.03ms
step:675/1750 train_time:64148ms step_avg:95.03ms
step:676/1750 train_time:64246ms step_avg:95.04ms
step:677/1750 train_time:64344ms step_avg:95.04ms
step:678/1750 train_time:64442ms step_avg:95.05ms
step:679/1750 train_time:64540ms step_avg:95.05ms
step:680/1750 train_time:64638ms step_avg:95.06ms
step:681/1750 train_time:64736ms step_avg:95.06ms
step:682/1750 train_time:64834ms step_avg:95.06ms
step:683/1750 train_time:64931ms step_avg:95.07ms
step:684/1750 train_time:65028ms step_avg:95.07ms
step:685/1750 train_time:65126ms step_avg:95.07ms
step:686/1750 train_time:65224ms step_avg:95.08ms
step:687/1750 train_time:65322ms step_avg:95.08ms
step:688/1750 train_time:65420ms step_avg:95.09ms
step:689/1750 train_time:65519ms step_avg:95.09ms
step:690/1750 train_time:65617ms step_avg:95.10ms
step:691/1750 train_time:65715ms step_avg:95.10ms
step:692/1750 train_time:65812ms step_avg:95.10ms
step:693/1750 train_time:65910ms step_avg:95.11ms
step:694/1750 train_time:66008ms step_avg:95.11ms
step:695/1750 train_time:66106ms step_avg:95.12ms
step:696/1750 train_time:66203ms step_avg:95.12ms
step:697/1750 train_time:66302ms step_avg:95.12ms
step:698/1750 train_time:66400ms step_avg:95.13ms
step:699/1750 train_time:66498ms step_avg:95.13ms
step:700/1750 train_time:66596ms step_avg:95.14ms
step:701/1750 train_time:66694ms step_avg:95.14ms
step:702/1750 train_time:66792ms step_avg:95.14ms
step:703/1750 train_time:66889ms step_avg:95.15ms
step:704/1750 train_time:66986ms step_avg:95.15ms
step:705/1750 train_time:67084ms step_avg:95.15ms
step:706/1750 train_time:67182ms step_avg:95.16ms
step:707/1750 train_time:67280ms step_avg:95.16ms
step:708/1750 train_time:67378ms step_avg:95.17ms
step:709/1750 train_time:67476ms step_avg:95.17ms
step:710/1750 train_time:67574ms step_avg:95.17ms
step:711/1750 train_time:67672ms step_avg:95.18ms
step:712/1750 train_time:67770ms step_avg:95.18ms
step:713/1750 train_time:67867ms step_avg:95.19ms
step:714/1750 train_time:67965ms step_avg:95.19ms
step:715/1750 train_time:68063ms step_avg:95.19ms
step:716/1750 train_time:68160ms step_avg:95.20ms
step:717/1750 train_time:68259ms step_avg:95.20ms
step:718/1750 train_time:68358ms step_avg:95.21ms
step:719/1750 train_time:68456ms step_avg:95.21ms
step:720/1750 train_time:68553ms step_avg:95.21ms
step:721/1750 train_time:68650ms step_avg:95.22ms
step:722/1750 train_time:68749ms step_avg:95.22ms
step:723/1750 train_time:68846ms step_avg:95.22ms
step:724/1750 train_time:68944ms step_avg:95.23ms
step:725/1750 train_time:69042ms step_avg:95.23ms
step:726/1750 train_time:69140ms step_avg:95.23ms
step:727/1750 train_time:69238ms step_avg:95.24ms
step:728/1750 train_time:69336ms step_avg:95.24ms
step:729/1750 train_time:69434ms step_avg:95.25ms
step:730/1750 train_time:69532ms step_avg:95.25ms
step:731/1750 train_time:69630ms step_avg:95.25ms
step:732/1750 train_time:69728ms step_avg:95.26ms
step:733/1750 train_time:69826ms step_avg:95.26ms
step:734/1750 train_time:69924ms step_avg:95.26ms
step:735/1750 train_time:70021ms step_avg:95.27ms
step:736/1750 train_time:70118ms step_avg:95.27ms
step:737/1750 train_time:70216ms step_avg:95.27ms
step:738/1750 train_time:70314ms step_avg:95.28ms
step:739/1750 train_time:70411ms step_avg:95.28ms
step:740/1750 train_time:70509ms step_avg:95.28ms
step:741/1750 train_time:70606ms step_avg:95.28ms
step:742/1750 train_time:70704ms step_avg:95.29ms
step:743/1750 train_time:70802ms step_avg:95.29ms
step:744/1750 train_time:70901ms step_avg:95.30ms
step:745/1750 train_time:70999ms step_avg:95.30ms
step:746/1750 train_time:71098ms step_avg:95.31ms
step:747/1750 train_time:71195ms step_avg:95.31ms
step:748/1750 train_time:71293ms step_avg:95.31ms
step:749/1750 train_time:71391ms step_avg:95.32ms
step:750/1750 train_time:71489ms step_avg:95.32ms
step:750/1750 val_loss:3.5998 train_time:71576ms step_avg:95.43ms
step:751/1750 train_time:71598ms step_avg:95.34ms
step:752/1750 train_time:71696ms step_avg:95.34ms
step:753/1750 train_time:71795ms step_avg:95.35ms
step:754/1750 train_time:71894ms step_avg:95.35ms
step:755/1750 train_time:71991ms step_avg:95.35ms
step:756/1750 train_time:72088ms step_avg:95.35ms
step:757/1750 train_time:72185ms step_avg:95.36ms
step:758/1750 train_time:72282ms step_avg:95.36ms
step:759/1750 train_time:72380ms step_avg:95.36ms
step:760/1750 train_time:72477ms step_avg:95.36ms
step:761/1750 train_time:72575ms step_avg:95.37ms
step:762/1750 train_time:72676ms step_avg:95.38ms
step:763/1750 train_time:72775ms step_avg:95.38ms
step:764/1750 train_time:72873ms step_avg:95.38ms
step:765/1750 train_time:72971ms step_avg:95.39ms
step:766/1750 train_time:73068ms step_avg:95.39ms
step:767/1750 train_time:73166ms step_avg:95.39ms
step:768/1750 train_time:73263ms step_avg:95.39ms
step:769/1750 train_time:73360ms step_avg:95.40ms
step:770/1750 train_time:73458ms step_avg:95.40ms
step:771/1750 train_time:73556ms step_avg:95.40ms
step:772/1750 train_time:73655ms step_avg:95.41ms
step:773/1750 train_time:73753ms step_avg:95.41ms
step:774/1750 train_time:73852ms step_avg:95.42ms
step:775/1750 train_time:73951ms step_avg:95.42ms
step:776/1750 train_time:74049ms step_avg:95.42ms
step:777/1750 train_time:74146ms step_avg:95.43ms
step:778/1750 train_time:74244ms step_avg:95.43ms
step:779/1750 train_time:74341ms step_avg:95.43ms
step:780/1750 train_time:74440ms step_avg:95.44ms
step:781/1750 train_time:74538ms step_avg:95.44ms
step:782/1750 train_time:74638ms step_avg:95.45ms
step:783/1750 train_time:74736ms step_avg:95.45ms
step:784/1750 train_time:74834ms step_avg:95.45ms
step:785/1750 train_time:74932ms step_avg:95.46ms
step:786/1750 train_time:75030ms step_avg:95.46ms
step:787/1750 train_time:75127ms step_avg:95.46ms
step:788/1750 train_time:75225ms step_avg:95.46ms
step:789/1750 train_time:75322ms step_avg:95.47ms
step:790/1750 train_time:75421ms step_avg:95.47ms
step:791/1750 train_time:75518ms step_avg:95.47ms
step:792/1750 train_time:75617ms step_avg:95.48ms
step:793/1750 train_time:75714ms step_avg:95.48ms
step:794/1750 train_time:75812ms step_avg:95.48ms
step:795/1750 train_time:75912ms step_avg:95.49ms
step:796/1750 train_time:76011ms step_avg:95.49ms
step:797/1750 train_time:76108ms step_avg:95.49ms
step:798/1750 train_time:76206ms step_avg:95.50ms
step:799/1750 train_time:76303ms step_avg:95.50ms
step:800/1750 train_time:76401ms step_avg:95.50ms
step:801/1750 train_time:76499ms step_avg:95.50ms
step:802/1750 train_time:76597ms step_avg:95.51ms
step:803/1750 train_time:76697ms step_avg:95.51ms
step:804/1750 train_time:76795ms step_avg:95.52ms
step:805/1750 train_time:76894ms step_avg:95.52ms
step:806/1750 train_time:76993ms step_avg:95.52ms
step:807/1750 train_time:77092ms step_avg:95.53ms
step:808/1750 train_time:77191ms step_avg:95.53ms
step:809/1750 train_time:77290ms step_avg:95.54ms
step:810/1750 train_time:77389ms step_avg:95.54ms
step:811/1750 train_time:77487ms step_avg:95.55ms
step:812/1750 train_time:77585ms step_avg:95.55ms
step:813/1750 train_time:77683ms step_avg:95.55ms
step:814/1750 train_time:77781ms step_avg:95.55ms
step:815/1750 train_time:77880ms step_avg:95.56ms
step:816/1750 train_time:77978ms step_avg:95.56ms
step:817/1750 train_time:78077ms step_avg:95.57ms
step:818/1750 train_time:78175ms step_avg:95.57ms
step:819/1750 train_time:78274ms step_avg:95.57ms
step:820/1750 train_time:78372ms step_avg:95.58ms
step:821/1750 train_time:78470ms step_avg:95.58ms
step:822/1750 train_time:78568ms step_avg:95.58ms
step:823/1750 train_time:78666ms step_avg:95.58ms
step:824/1750 train_time:78765ms step_avg:95.59ms
step:825/1750 train_time:78864ms step_avg:95.59ms
step:826/1750 train_time:78963ms step_avg:95.60ms
step:827/1750 train_time:79063ms step_avg:95.60ms
step:828/1750 train_time:79162ms step_avg:95.61ms
step:829/1750 train_time:79262ms step_avg:95.61ms
step:830/1750 train_time:79361ms step_avg:95.62ms
step:831/1750 train_time:79460ms step_avg:95.62ms
step:832/1750 train_time:79559ms step_avg:95.62ms
step:833/1750 train_time:79657ms step_avg:95.63ms
step:834/1750 train_time:79755ms step_avg:95.63ms
step:835/1750 train_time:79854ms step_avg:95.63ms
step:836/1750 train_time:79951ms step_avg:95.64ms
step:837/1750 train_time:80050ms step_avg:95.64ms
step:838/1750 train_time:80148ms step_avg:95.64ms
step:839/1750 train_time:80247ms step_avg:95.65ms
step:840/1750 train_time:80346ms step_avg:95.65ms
step:841/1750 train_time:80445ms step_avg:95.65ms
step:842/1750 train_time:80545ms step_avg:95.66ms
step:843/1750 train_time:80643ms step_avg:95.66ms
step:844/1750 train_time:80742ms step_avg:95.67ms
step:845/1750 train_time:80841ms step_avg:95.67ms
step:846/1750 train_time:80940ms step_avg:95.67ms
step:847/1750 train_time:81038ms step_avg:95.68ms
step:848/1750 train_time:81136ms step_avg:95.68ms
step:849/1750 train_time:81234ms step_avg:95.68ms
step:850/1750 train_time:81332ms step_avg:95.68ms
step:851/1750 train_time:81431ms step_avg:95.69ms
step:852/1750 train_time:81530ms step_avg:95.69ms
step:853/1750 train_time:81629ms step_avg:95.70ms
step:854/1750 train_time:81727ms step_avg:95.70ms
step:855/1750 train_time:81825ms step_avg:95.70ms
step:856/1750 train_time:81923ms step_avg:95.70ms
step:857/1750 train_time:82024ms step_avg:95.71ms
step:858/1750 train_time:82122ms step_avg:95.71ms
step:859/1750 train_time:82220ms step_avg:95.72ms
step:860/1750 train_time:82319ms step_avg:95.72ms
step:861/1750 train_time:82417ms step_avg:95.72ms
step:862/1750 train_time:82515ms step_avg:95.73ms
step:863/1750 train_time:82613ms step_avg:95.73ms
step:864/1750 train_time:82712ms step_avg:95.73ms
step:865/1750 train_time:82811ms step_avg:95.74ms
step:866/1750 train_time:82909ms step_avg:95.74ms
step:867/1750 train_time:83007ms step_avg:95.74ms
step:868/1750 train_time:83107ms step_avg:95.75ms
step:869/1750 train_time:83206ms step_avg:95.75ms
step:870/1750 train_time:83305ms step_avg:95.75ms
step:871/1750 train_time:83405ms step_avg:95.76ms
step:872/1750 train_time:83505ms step_avg:95.76ms
step:873/1750 train_time:83604ms step_avg:95.77ms
step:874/1750 train_time:83703ms step_avg:95.77ms
step:875/1750 train_time:83802ms step_avg:95.77ms
step:875/1750 val_loss:3.5496 train_time:83889ms step_avg:95.87ms
step:876/1750 train_time:83912ms step_avg:95.79ms
step:877/1750 train_time:84008ms step_avg:95.79ms
step:878/1750 train_time:84106ms step_avg:95.79ms
step:879/1750 train_time:84204ms step_avg:95.80ms
step:880/1750 train_time:84302ms step_avg:95.80ms
step:881/1750 train_time:84399ms step_avg:95.80ms
step:882/1750 train_time:84497ms step_avg:95.80ms
step:883/1750 train_time:84594ms step_avg:95.80ms
step:884/1750 train_time:84691ms step_avg:95.80ms
step:885/1750 train_time:84788ms step_avg:95.81ms
step:886/1750 train_time:84889ms step_avg:95.81ms
step:887/1750 train_time:84988ms step_avg:95.82ms
step:888/1750 train_time:85087ms step_avg:95.82ms
step:889/1750 train_time:85184ms step_avg:95.82ms
step:890/1750 train_time:85283ms step_avg:95.82ms
step:891/1750 train_time:85381ms step_avg:95.83ms
step:892/1750 train_time:85479ms step_avg:95.83ms
step:893/1750 train_time:85577ms step_avg:95.83ms
step:894/1750 train_time:85675ms step_avg:95.83ms
step:895/1750 train_time:85772ms step_avg:95.83ms
step:896/1750 train_time:85871ms step_avg:95.84ms
step:897/1750 train_time:85970ms step_avg:95.84ms
step:898/1750 train_time:86069ms step_avg:95.84ms
step:899/1750 train_time:86168ms step_avg:95.85ms
step:900/1750 train_time:86268ms step_avg:95.85ms
step:901/1750 train_time:86366ms step_avg:95.86ms
step:902/1750 train_time:86465ms step_avg:95.86ms
step:903/1750 train_time:86563ms step_avg:95.86ms
step:904/1750 train_time:86663ms step_avg:95.87ms
step:905/1750 train_time:86761ms step_avg:95.87ms
step:906/1750 train_time:86860ms step_avg:95.87ms
step:907/1750 train_time:86959ms step_avg:95.88ms
step:908/1750 train_time:87058ms step_avg:95.88ms
step:909/1750 train_time:87157ms step_avg:95.88ms
step:910/1750 train_time:87258ms step_avg:95.89ms
step:911/1750 train_time:87357ms step_avg:95.89ms
step:912/1750 train_time:87456ms step_avg:95.90ms
step:913/1750 train_time:87556ms step_avg:95.90ms
step:914/1750 train_time:87656ms step_avg:95.90ms
step:915/1750 train_time:87755ms step_avg:95.91ms
step:916/1750 train_time:87854ms step_avg:95.91ms
step:917/1750 train_time:87954ms step_avg:95.91ms
step:918/1750 train_time:88054ms step_avg:95.92ms
step:919/1750 train_time:88154ms step_avg:95.92ms
step:920/1750 train_time:88253ms step_avg:95.93ms
step:921/1750 train_time:88353ms step_avg:95.93ms
step:922/1750 train_time:88453ms step_avg:95.94ms
step:923/1750 train_time:88553ms step_avg:95.94ms
step:924/1750 train_time:88653ms step_avg:95.95ms
step:925/1750 train_time:88753ms step_avg:95.95ms
step:926/1750 train_time:88852ms step_avg:95.95ms
step:927/1750 train_time:88951ms step_avg:95.96ms
step:928/1750 train_time:89051ms step_avg:95.96ms
step:929/1750 train_time:89150ms step_avg:95.96ms
step:930/1750 train_time:89250ms step_avg:95.97ms
step:931/1750 train_time:89350ms step_avg:95.97ms
step:932/1750 train_time:89450ms step_avg:95.98ms
step:933/1750 train_time:89549ms step_avg:95.98ms
step:934/1750 train_time:89649ms step_avg:95.98ms
step:935/1750 train_time:89749ms step_avg:95.99ms
step:936/1750 train_time:89850ms step_avg:95.99ms
step:937/1750 train_time:89949ms step_avg:96.00ms
step:938/1750 train_time:90049ms step_avg:96.00ms
step:939/1750 train_time:90148ms step_avg:96.00ms
step:940/1750 train_time:90248ms step_avg:96.01ms
step:941/1750 train_time:90347ms step_avg:96.01ms
step:942/1750 train_time:90446ms step_avg:96.02ms
step:943/1750 train_time:90547ms step_avg:96.02ms
step:944/1750 train_time:90647ms step_avg:96.02ms
step:945/1750 train_time:90746ms step_avg:96.03ms
step:946/1750 train_time:90846ms step_avg:96.03ms
step:947/1750 train_time:90946ms step_avg:96.04ms
step:948/1750 train_time:91046ms step_avg:96.04ms
step:949/1750 train_time:91144ms step_avg:96.04ms
step:950/1750 train_time:91244ms step_avg:96.05ms
step:951/1750 train_time:91344ms step_avg:96.05ms
step:952/1750 train_time:91444ms step_avg:96.05ms
step:953/1750 train_time:91544ms step_avg:96.06ms
step:954/1750 train_time:91644ms step_avg:96.06ms
step:955/1750 train_time:91743ms step_avg:96.07ms
step:956/1750 train_time:91842ms step_avg:96.07ms
step:957/1750 train_time:91942ms step_avg:96.07ms
step:958/1750 train_time:92042ms step_avg:96.08ms
step:959/1750 train_time:92142ms step_avg:96.08ms
step:960/1750 train_time:92242ms step_avg:96.08ms
step:961/1750 train_time:92341ms step_avg:96.09ms
step:962/1750 train_time:92440ms step_avg:96.09ms
step:963/1750 train_time:92540ms step_avg:96.10ms
step:964/1750 train_time:92640ms step_avg:96.10ms
step:965/1750 train_time:92740ms step_avg:96.10ms
step:966/1750 train_time:92840ms step_avg:96.11ms
step:967/1750 train_time:92940ms step_avg:96.11ms
step:968/1750 train_time:93040ms step_avg:96.12ms
step:969/1750 train_time:93140ms step_avg:96.12ms
step:970/1750 train_time:93239ms step_avg:96.12ms
step:971/1750 train_time:93339ms step_avg:96.13ms
step:972/1750 train_time:93439ms step_avg:96.13ms
step:973/1750 train_time:93538ms step_avg:96.13ms
step:974/1750 train_time:93638ms step_avg:96.14ms
step:975/1750 train_time:93738ms step_avg:96.14ms
step:976/1750 train_time:93837ms step_avg:96.14ms
step:977/1750 train_time:93938ms step_avg:96.15ms
step:978/1750 train_time:94037ms step_avg:96.15ms
step:979/1750 train_time:94136ms step_avg:96.16ms
step:980/1750 train_time:94236ms step_avg:96.16ms
step:981/1750 train_time:94335ms step_avg:96.16ms
step:982/1750 train_time:94434ms step_avg:96.16ms
step:983/1750 train_time:94534ms step_avg:96.17ms
step:984/1750 train_time:94634ms step_avg:96.17ms
step:985/1750 train_time:94735ms step_avg:96.18ms
step:986/1750 train_time:94836ms step_avg:96.18ms
step:987/1750 train_time:94937ms step_avg:96.19ms
step:988/1750 train_time:95035ms step_avg:96.19ms
step:989/1750 train_time:95134ms step_avg:96.19ms
step:990/1750 train_time:95234ms step_avg:96.20ms
step:991/1750 train_time:95334ms step_avg:96.20ms
step:992/1750 train_time:95434ms step_avg:96.20ms
step:993/1750 train_time:95533ms step_avg:96.21ms
step:994/1750 train_time:95633ms step_avg:96.21ms
step:995/1750 train_time:95733ms step_avg:96.21ms
step:996/1750 train_time:95834ms step_avg:96.22ms
step:997/1750 train_time:95934ms step_avg:96.22ms
step:998/1750 train_time:96033ms step_avg:96.23ms
step:999/1750 train_time:96133ms step_avg:96.23ms
step:1000/1750 train_time:96232ms step_avg:96.23ms
step:1000/1750 val_loss:3.5094 train_time:96319ms step_avg:96.32ms
step:1001/1750 train_time:96341ms step_avg:96.25ms
step:1002/1750 train_time:96437ms step_avg:96.25ms
step:1003/1750 train_time:96539ms step_avg:96.25ms
step:1004/1750 train_time:96637ms step_avg:96.25ms
step:1005/1750 train_time:96736ms step_avg:96.25ms
step:1006/1750 train_time:96835ms step_avg:96.26ms
step:1007/1750 train_time:96934ms step_avg:96.26ms
step:1008/1750 train_time:97033ms step_avg:96.26ms
step:1009/1750 train_time:97131ms step_avg:96.26ms
step:1010/1750 train_time:97232ms step_avg:96.27ms
step:1011/1750 train_time:97334ms step_avg:96.27ms
step:1012/1750 train_time:97436ms step_avg:96.28ms
step:1013/1750 train_time:97537ms step_avg:96.29ms
step:1014/1750 train_time:97637ms step_avg:96.29ms
step:1015/1750 train_time:97736ms step_avg:96.29ms
step:1016/1750 train_time:97835ms step_avg:96.29ms
step:1017/1750 train_time:97935ms step_avg:96.30ms
step:1018/1750 train_time:98033ms step_avg:96.30ms
step:1019/1750 train_time:98132ms step_avg:96.30ms
step:1020/1750 train_time:98232ms step_avg:96.31ms
step:1021/1750 train_time:98333ms step_avg:96.31ms
step:1022/1750 train_time:98434ms step_avg:96.32ms
step:1023/1750 train_time:98535ms step_avg:96.32ms
step:1024/1750 train_time:98637ms step_avg:96.33ms
step:1025/1750 train_time:98737ms step_avg:96.33ms
step:1026/1750 train_time:98836ms step_avg:96.33ms
step:1027/1750 train_time:98936ms step_avg:96.33ms
step:1028/1750 train_time:99035ms step_avg:96.34ms
step:1029/1750 train_time:99135ms step_avg:96.34ms
step:1030/1750 train_time:99235ms step_avg:96.34ms
step:1031/1750 train_time:99336ms step_avg:96.35ms
step:1032/1750 train_time:99436ms step_avg:96.35ms
step:1033/1750 train_time:99536ms step_avg:96.36ms
step:1034/1750 train_time:99636ms step_avg:96.36ms
step:1035/1750 train_time:99736ms step_avg:96.36ms
step:1036/1750 train_time:99835ms step_avg:96.37ms
step:1037/1750 train_time:99934ms step_avg:96.37ms
step:1038/1750 train_time:100033ms step_avg:96.37ms
step:1039/1750 train_time:100133ms step_avg:96.37ms
step:1040/1750 train_time:100233ms step_avg:96.38ms
step:1041/1750 train_time:100334ms step_avg:96.38ms
step:1042/1750 train_time:100435ms step_avg:96.39ms
step:1043/1750 train_time:100535ms step_avg:96.39ms
step:1044/1750 train_time:100635ms step_avg:96.39ms
step:1045/1750 train_time:100735ms step_avg:96.40ms
step:1046/1750 train_time:100835ms step_avg:96.40ms
step:1047/1750 train_time:100935ms step_avg:96.40ms
step:1048/1750 train_time:101034ms step_avg:96.41ms
step:1049/1750 train_time:101133ms step_avg:96.41ms
step:1050/1750 train_time:101234ms step_avg:96.41ms
step:1051/1750 train_time:101335ms step_avg:96.42ms
step:1052/1750 train_time:101436ms step_avg:96.42ms
step:1053/1750 train_time:101536ms step_avg:96.43ms
step:1054/1750 train_time:101636ms step_avg:96.43ms
step:1055/1750 train_time:101736ms step_avg:96.43ms
step:1056/1750 train_time:101836ms step_avg:96.44ms
step:1057/1750 train_time:101935ms step_avg:96.44ms
step:1058/1750 train_time:102034ms step_avg:96.44ms
step:1059/1750 train_time:102134ms step_avg:96.44ms
step:1060/1750 train_time:102490ms step_avg:96.69ms
step:1061/1750 train_time:102587ms step_avg:96.69ms
step:1062/1750 train_time:102685ms step_avg:96.69ms
step:1063/1750 train_time:102784ms step_avg:96.69ms
step:1064/1750 train_time:102883ms step_avg:96.69ms
step:1065/1750 train_time:102982ms step_avg:96.70ms
step:1066/1750 train_time:103081ms step_avg:96.70ms
step:1067/1750 train_time:103179ms step_avg:96.70ms
step:1068/1750 train_time:103278ms step_avg:96.70ms
step:1069/1750 train_time:103380ms step_avg:96.71ms
step:1070/1750 train_time:103483ms step_avg:96.71ms
step:1071/1750 train_time:103586ms step_avg:96.72ms
step:1072/1750 train_time:103685ms step_avg:96.72ms
step:1073/1750 train_time:103783ms step_avg:96.72ms
step:1074/1750 train_time:103882ms step_avg:96.72ms
step:1075/1750 train_time:103980ms step_avg:96.73ms
step:1076/1750 train_time:104078ms step_avg:96.73ms
step:1077/1750 train_time:104177ms step_avg:96.73ms
step:1078/1750 train_time:104276ms step_avg:96.73ms
step:1079/1750 train_time:104377ms step_avg:96.73ms
step:1080/1750 train_time:104839ms step_avg:97.07ms
step:1081/1750 train_time:104936ms step_avg:97.07ms
step:1082/1750 train_time:105035ms step_avg:97.08ms
step:1083/1750 train_time:105134ms step_avg:97.08ms
step:1084/1750 train_time:105232ms step_avg:97.08ms
step:1085/1750 train_time:105331ms step_avg:97.08ms
step:1086/1750 train_time:105429ms step_avg:97.08ms
step:1087/1750 train_time:105528ms step_avg:97.08ms
step:1088/1750 train_time:105626ms step_avg:97.08ms
step:1089/1750 train_time:105727ms step_avg:97.09ms
step:1090/1750 train_time:105833ms step_avg:97.09ms
step:1091/1750 train_time:105935ms step_avg:97.10ms
step:1092/1750 train_time:106035ms step_avg:97.10ms
step:1093/1750 train_time:106134ms step_avg:97.10ms
step:1094/1750 train_time:106234ms step_avg:97.11ms
step:1095/1750 train_time:106333ms step_avg:97.11ms
step:1096/1750 train_time:106433ms step_avg:97.11ms
step:1097/1750 train_time:106532ms step_avg:97.11ms
step:1098/1750 train_time:106632ms step_avg:97.11ms
step:1099/1750 train_time:106732ms step_avg:97.12ms
step:1100/1750 train_time:106834ms step_avg:97.12ms
step:1101/1750 train_time:106936ms step_avg:97.13ms
step:1102/1750 train_time:107036ms step_avg:97.13ms
step:1103/1750 train_time:107136ms step_avg:97.13ms
step:1104/1750 train_time:107235ms step_avg:97.13ms
step:1105/1750 train_time:107334ms step_avg:97.14ms
step:1106/1750 train_time:107434ms step_avg:97.14ms
step:1107/1750 train_time:107534ms step_avg:97.14ms
step:1108/1750 train_time:107633ms step_avg:97.14ms
step:1109/1750 train_time:107733ms step_avg:97.14ms
step:1110/1750 train_time:107835ms step_avg:97.15ms
step:1111/1750 train_time:107937ms step_avg:97.15ms
step:1112/1750 train_time:108037ms step_avg:97.16ms
step:1113/1750 train_time:108506ms step_avg:97.49ms
step:1114/1750 train_time:108603ms step_avg:97.49ms
step:1115/1750 train_time:108701ms step_avg:97.49ms
step:1116/1750 train_time:108800ms step_avg:97.49ms
step:1117/1750 train_time:108898ms step_avg:97.49ms
step:1118/1750 train_time:108996ms step_avg:97.49ms
step:1119/1750 train_time:109095ms step_avg:97.49ms
step:1120/1750 train_time:109193ms step_avg:97.49ms
step:1121/1750 train_time:109292ms step_avg:97.50ms
step:1122/1750 train_time:109399ms step_avg:97.50ms
step:1123/1750 train_time:109505ms step_avg:97.51ms
step:1124/1750 train_time:109604ms step_avg:97.51ms
step:1125/1750 train_time:109704ms step_avg:97.51ms
step:1125/1750 val_loss:3.4578 train_time:109791ms step_avg:97.59ms
step:1126/1750 train_time:109813ms step_avg:97.52ms
step:1127/1750 train_time:109911ms step_avg:97.53ms
step:1128/1750 train_time:110011ms step_avg:97.53ms
step:1129/1750 train_time:110112ms step_avg:97.53ms
step:1130/1750 train_time:110211ms step_avg:97.53ms
step:1131/1750 train_time:110309ms step_avg:97.53ms
step:1132/1750 train_time:110408ms step_avg:97.53ms
step:1133/1750 train_time:110507ms step_avg:97.54ms
step:1134/1750 train_time:110606ms step_avg:97.54ms
step:1135/1750 train_time:110705ms step_avg:97.54ms
step:1136/1750 train_time:110806ms step_avg:97.54ms
step:1137/1750 train_time:110908ms step_avg:97.54ms
step:1138/1750 train_time:111009ms step_avg:97.55ms
step:1139/1750 train_time:111109ms step_avg:97.55ms
step:1140/1750 train_time:111208ms step_avg:97.55ms
step:1141/1750 train_time:111307ms step_avg:97.55ms
step:1142/1750 train_time:111406ms step_avg:97.55ms
step:1143/1750 train_time:111505ms step_avg:97.55ms
step:1144/1750 train_time:111605ms step_avg:97.56ms
step:1145/1750 train_time:111704ms step_avg:97.56ms
step:1146/1750 train_time:111805ms step_avg:97.56ms
step:1147/1750 train_time:111905ms step_avg:97.56ms
step:1148/1750 train_time:112007ms step_avg:97.57ms
step:1149/1750 train_time:112363ms step_avg:97.79ms
step:1150/1750 train_time:112461ms step_avg:97.79ms
step:1151/1750 train_time:112560ms step_avg:97.79ms
step:1152/1750 train_time:112658ms step_avg:97.79ms
step:1153/1750 train_time:112757ms step_avg:97.79ms
step:1154/1750 train_time:112855ms step_avg:97.79ms
step:1155/1750 train_time:112952ms step_avg:97.79ms
step:1156/1750 train_time:113050ms step_avg:97.79ms
step:1157/1750 train_time:113149ms step_avg:97.79ms
step:1158/1750 train_time:113509ms step_avg:98.02ms
step:1159/1750 train_time:113606ms step_avg:98.02ms
step:1160/1750 train_time:113705ms step_avg:98.02ms
step:1161/1750 train_time:113803ms step_avg:98.02ms
step:1162/1750 train_time:113902ms step_avg:98.02ms
step:1163/1750 train_time:114002ms step_avg:98.02ms
step:1164/1750 train_time:114101ms step_avg:98.03ms
step:1165/1750 train_time:114200ms step_avg:98.03ms
step:1166/1750 train_time:114299ms step_avg:98.03ms
step:1167/1750 train_time:114660ms step_avg:98.25ms
step:1168/1750 train_time:114758ms step_avg:98.25ms
step:1169/1750 train_time:114857ms step_avg:98.25ms
step:1170/1750 train_time:114956ms step_avg:98.25ms
step:1171/1750 train_time:115055ms step_avg:98.25ms
step:1172/1750 train_time:115155ms step_avg:98.26ms
step:1173/1750 train_time:115254ms step_avg:98.26ms
step:1174/1750 train_time:115353ms step_avg:98.26ms
step:1175/1750 train_time:115452ms step_avg:98.26ms
step:1176/1750 train_time:115559ms step_avg:98.26ms
step:1177/1750 train_time:115663ms step_avg:98.27ms
step:1178/1750 train_time:115765ms step_avg:98.27ms
step:1179/1750 train_time:115868ms step_avg:98.28ms
step:1180/1750 train_time:115967ms step_avg:98.28ms
step:1181/1750 train_time:116069ms step_avg:98.28ms
step:1182/1750 train_time:116170ms step_avg:98.28ms
step:1183/1750 train_time:116270ms step_avg:98.28ms
step:1184/1750 train_time:116372ms step_avg:98.29ms
step:1185/1750 train_time:116472ms step_avg:98.29ms
step:1186/1750 train_time:116574ms step_avg:98.29ms
step:1187/1750 train_time:116675ms step_avg:98.29ms
step:1188/1750 train_time:116778ms step_avg:98.30ms
step:1189/1750 train_time:116877ms step_avg:98.30ms
step:1190/1750 train_time:116977ms step_avg:98.30ms
step:1191/1750 train_time:117078ms step_avg:98.30ms
step:1192/1750 train_time:117178ms step_avg:98.30ms
step:1193/1750 train_time:117279ms step_avg:98.31ms
step:1194/1750 train_time:117381ms step_avg:98.31ms
step:1195/1750 train_time:117482ms step_avg:98.31ms
step:1196/1750 train_time:117584ms step_avg:98.31ms
step:1197/1750 train_time:117687ms step_avg:98.32ms
step:1198/1750 train_time:117788ms step_avg:98.32ms
step:1199/1750 train_time:117888ms step_avg:98.32ms
step:1200/1750 train_time:117989ms step_avg:98.32ms
step:1201/1750 train_time:118090ms step_avg:98.33ms
step:1202/1750 train_time:118192ms step_avg:98.33ms
step:1203/1750 train_time:118292ms step_avg:98.33ms
step:1204/1750 train_time:118393ms step_avg:98.33ms
step:1205/1750 train_time:118494ms step_avg:98.34ms
step:1206/1750 train_time:118595ms step_avg:98.34ms
step:1207/1750 train_time:118695ms step_avg:98.34ms
step:1208/1750 train_time:118796ms step_avg:98.34ms
step:1209/1750 train_time:118896ms step_avg:98.34ms
step:1210/1750 train_time:118997ms step_avg:98.34ms
step:1211/1750 train_time:119099ms step_avg:98.35ms
step:1212/1750 train_time:119199ms step_avg:98.35ms
step:1213/1750 train_time:119300ms step_avg:98.35ms
step:1214/1750 train_time:119401ms step_avg:98.35ms
step:1215/1750 train_time:119502ms step_avg:98.36ms
step:1216/1750 train_time:119606ms step_avg:98.36ms
step:1217/1750 train_time:119709ms step_avg:98.36ms
step:1218/1750 train_time:119810ms step_avg:98.37ms
step:1219/1750 train_time:119910ms step_avg:98.37ms
step:1220/1750 train_time:120012ms step_avg:98.37ms
step:1221/1750 train_time:120112ms step_avg:98.37ms
step:1222/1750 train_time:120213ms step_avg:98.37ms
step:1223/1750 train_time:120315ms step_avg:98.38ms
step:1224/1750 train_time:120415ms step_avg:98.38ms
step:1225/1750 train_time:120516ms step_avg:98.38ms
step:1226/1750 train_time:120907ms step_avg:98.62ms
step:1227/1750 train_time:121006ms step_avg:98.62ms
step:1228/1750 train_time:121105ms step_avg:98.62ms
step:1229/1750 train_time:121204ms step_avg:98.62ms
step:1230/1750 train_time:121304ms step_avg:98.62ms
step:1231/1750 train_time:121405ms step_avg:98.62ms
step:1232/1750 train_time:121504ms step_avg:98.62ms
step:1233/1750 train_time:121947ms step_avg:98.90ms
step:1234/1750 train_time:122010ms step_avg:98.87ms
step:1235/1750 train_time:122108ms step_avg:98.87ms
step:1236/1750 train_time:122208ms step_avg:98.87ms
step:1237/1750 train_time:122308ms step_avg:98.87ms
step:1238/1750 train_time:122407ms step_avg:98.87ms
step:1239/1750 train_time:122507ms step_avg:98.88ms
step:1240/1750 train_time:122606ms step_avg:98.88ms
step:1241/1750 train_time:122707ms step_avg:98.88ms
step:1242/1750 train_time:122809ms step_avg:98.88ms
step:1243/1750 train_time:122918ms step_avg:98.89ms
step:1244/1750 train_time:123019ms step_avg:98.89ms
step:1245/1750 train_time:123120ms step_avg:98.89ms
step:1246/1750 train_time:123221ms step_avg:98.89ms
step:1247/1750 train_time:123322ms step_avg:98.89ms
step:1248/1750 train_time:123424ms step_avg:98.90ms
step:1249/1750 train_time:123524ms step_avg:98.90ms
step:1250/1750 train_time:123623ms step_avg:98.90ms
step:1250/1750 val_loss:3.4120 train_time:123711ms step_avg:98.97ms
step:1251/1750 train_time:123733ms step_avg:98.91ms
step:1252/1750 train_time:123832ms step_avg:98.91ms
step:1253/1750 train_time:123935ms step_avg:98.91ms
step:1254/1750 train_time:124036ms step_avg:98.91ms
step:1255/1750 train_time:124135ms step_avg:98.91ms
step:1256/1750 train_time:124235ms step_avg:98.91ms
step:1257/1750 train_time:124335ms step_avg:98.91ms
step:1258/1750 train_time:124435ms step_avg:98.91ms
step:1259/1750 train_time:124535ms step_avg:98.92ms
step:1260/1750 train_time:124636ms step_avg:98.92ms
step:1261/1750 train_time:124741ms step_avg:98.92ms
step:1262/1750 train_time:124843ms step_avg:98.92ms
step:1263/1750 train_time:124944ms step_avg:98.93ms
step:1264/1750 train_time:125044ms step_avg:98.93ms
step:1265/1750 train_time:125144ms step_avg:98.93ms
step:1266/1750 train_time:125243ms step_avg:98.93ms
step:1267/1750 train_time:125343ms step_avg:98.93ms
step:1268/1750 train_time:125444ms step_avg:98.93ms
step:1269/1750 train_time:125544ms step_avg:98.93ms
step:1270/1750 train_time:125645ms step_avg:98.93ms
step:1271/1750 train_time:125747ms step_avg:98.94ms
step:1272/1750 train_time:125848ms step_avg:98.94ms
step:1273/1750 train_time:125949ms step_avg:98.94ms
step:1274/1750 train_time:126050ms step_avg:98.94ms
step:1275/1750 train_time:126151ms step_avg:98.94ms
step:1276/1750 train_time:126254ms step_avg:98.95ms
step:1277/1750 train_time:126356ms step_avg:98.95ms
step:1278/1750 train_time:126457ms step_avg:98.95ms
step:1279/1750 train_time:126558ms step_avg:98.95ms
step:1280/1750 train_time:126658ms step_avg:98.95ms
step:1281/1750 train_time:126759ms step_avg:98.95ms
step:1282/1750 train_time:126860ms step_avg:98.95ms
step:1283/1750 train_time:126961ms step_avg:98.96ms
step:1284/1750 train_time:127062ms step_avg:98.96ms
step:1285/1750 train_time:127162ms step_avg:98.96ms
step:1286/1750 train_time:127262ms step_avg:98.96ms
step:1287/1750 train_time:127363ms step_avg:98.96ms
step:1288/1750 train_time:127464ms step_avg:98.96ms
step:1289/1750 train_time:127565ms step_avg:98.96ms
step:1290/1750 train_time:127665ms step_avg:98.96ms
step:1291/1750 train_time:127766ms step_avg:98.97ms
step:1292/1750 train_time:127867ms step_avg:98.97ms
step:1293/1750 train_time:127969ms step_avg:98.97ms
step:1294/1750 train_time:128071ms step_avg:98.97ms
step:1295/1750 train_time:128173ms step_avg:98.97ms
step:1296/1750 train_time:128274ms step_avg:98.98ms
step:1297/1750 train_time:128375ms step_avg:98.98ms
step:1298/1750 train_time:128476ms step_avg:98.98ms
step:1299/1750 train_time:128577ms step_avg:98.98ms
step:1300/1750 train_time:128677ms step_avg:98.98ms
step:1301/1750 train_time:128778ms step_avg:98.98ms
step:1302/1750 train_time:128880ms step_avg:98.99ms
step:1303/1750 train_time:128982ms step_avg:98.99ms
step:1304/1750 train_time:129084ms step_avg:98.99ms
step:1305/1750 train_time:129185ms step_avg:98.99ms
step:1306/1750 train_time:129286ms step_avg:98.99ms
step:1307/1750 train_time:129386ms step_avg:98.99ms
step:1308/1750 train_time:129486ms step_avg:99.00ms
step:1309/1750 train_time:129587ms step_avg:99.00ms
step:1310/1750 train_time:129689ms step_avg:99.00ms
step:1311/1750 train_time:129792ms step_avg:99.00ms
step:1312/1750 train_time:129894ms step_avg:99.00ms
step:1313/1750 train_time:129997ms step_avg:99.01ms
step:1314/1750 train_time:130097ms step_avg:99.01ms
step:1315/1750 train_time:130198ms step_avg:99.01ms
step:1316/1750 train_time:130299ms step_avg:99.01ms
step:1317/1750 train_time:130399ms step_avg:99.01ms
step:1318/1750 train_time:130501ms step_avg:99.01ms
step:1319/1750 train_time:130602ms step_avg:99.02ms
step:1320/1750 train_time:130705ms step_avg:99.02ms
step:1321/1750 train_time:130806ms step_avg:99.02ms
step:1322/1750 train_time:130907ms step_avg:99.02ms
step:1323/1750 train_time:131008ms step_avg:99.02ms
step:1324/1750 train_time:131109ms step_avg:99.03ms
step:1325/1750 train_time:131212ms step_avg:99.03ms
step:1326/1750 train_time:131314ms step_avg:99.03ms
step:1327/1750 train_time:131417ms step_avg:99.03ms
step:1328/1750 train_time:131518ms step_avg:99.03ms
step:1329/1750 train_time:131618ms step_avg:99.04ms
step:1330/1750 train_time:131718ms step_avg:99.04ms
step:1331/1750 train_time:131820ms step_avg:99.04ms
step:1332/1750 train_time:131921ms step_avg:99.04ms
step:1333/1750 train_time:132023ms step_avg:99.04ms
step:1334/1750 train_time:132124ms step_avg:99.04ms
step:1335/1750 train_time:132225ms step_avg:99.04ms
step:1336/1750 train_time:132327ms step_avg:99.05ms
step:1337/1750 train_time:132428ms step_avg:99.05ms
step:1338/1750 train_time:132529ms step_avg:99.05ms
step:1339/1750 train_time:132629ms step_avg:99.05ms
step:1340/1750 train_time:132730ms step_avg:99.05ms
step:1341/1750 train_time:132832ms step_avg:99.05ms
step:1342/1750 train_time:132934ms step_avg:99.06ms
step:1343/1750 train_time:133035ms step_avg:99.06ms
step:1344/1750 train_time:133136ms step_avg:99.06ms
step:1345/1750 train_time:133237ms step_avg:99.06ms
step:1346/1750 train_time:133339ms step_avg:99.06ms
step:1347/1750 train_time:133441ms step_avg:99.07ms
step:1348/1750 train_time:133542ms step_avg:99.07ms
step:1349/1750 train_time:133643ms step_avg:99.07ms
step:1350/1750 train_time:133745ms step_avg:99.07ms
step:1351/1750 train_time:133844ms step_avg:99.07ms
step:1352/1750 train_time:133945ms step_avg:99.07ms
step:1353/1750 train_time:134046ms step_avg:99.07ms
step:1354/1750 train_time:134147ms step_avg:99.07ms
step:1355/1750 train_time:134248ms step_avg:99.08ms
step:1356/1750 train_time:134349ms step_avg:99.08ms
step:1357/1750 train_time:134451ms step_avg:99.08ms
step:1358/1750 train_time:134552ms step_avg:99.08ms
step:1359/1750 train_time:134654ms step_avg:99.08ms
step:1360/1750 train_time:134755ms step_avg:99.08ms
step:1361/1750 train_time:134856ms step_avg:99.09ms
step:1362/1750 train_time:134957ms step_avg:99.09ms
step:1363/1750 train_time:135059ms step_avg:99.09ms
step:1364/1750 train_time:135160ms step_avg:99.09ms
step:1365/1750 train_time:135262ms step_avg:99.09ms
step:1366/1750 train_time:135363ms step_avg:99.09ms
step:1367/1750 train_time:135463ms step_avg:99.10ms
step:1368/1750 train_time:135566ms step_avg:99.10ms
step:1369/1750 train_time:135666ms step_avg:99.10ms
step:1370/1750 train_time:135765ms step_avg:99.10ms
step:1371/1750 train_time:135867ms step_avg:99.10ms
step:1372/1750 train_time:135969ms step_avg:99.10ms
step:1373/1750 train_time:136070ms step_avg:99.10ms
step:1374/1750 train_time:136171ms step_avg:99.11ms
step:1375/1750 train_time:136273ms step_avg:99.11ms
step:1375/1750 val_loss:3.3721 train_time:136363ms step_avg:99.17ms
step:1376/1750 train_time:136385ms step_avg:99.12ms
step:1377/1750 train_time:136488ms step_avg:99.12ms
step:1378/1750 train_time:136590ms step_avg:99.12ms
step:1379/1750 train_time:136691ms step_avg:99.12ms
step:1380/1750 train_time:136794ms step_avg:99.13ms
step:1381/1750 train_time:136895ms step_avg:99.13ms
step:1382/1750 train_time:136994ms step_avg:99.13ms
step:1383/1750 train_time:137094ms step_avg:99.13ms
step:1384/1750 train_time:137194ms step_avg:99.13ms
step:1385/1750 train_time:137295ms step_avg:99.13ms
step:1386/1750 train_time:137398ms step_avg:99.13ms
step:1387/1750 train_time:137501ms step_avg:99.14ms
step:1388/1750 train_time:137602ms step_avg:99.14ms
step:1389/1750 train_time:137704ms step_avg:99.14ms
step:1390/1750 train_time:137805ms step_avg:99.14ms
step:1391/1750 train_time:137906ms step_avg:99.14ms
step:1392/1750 train_time:138006ms step_avg:99.14ms
step:1393/1750 train_time:138107ms step_avg:99.14ms
step:1394/1750 train_time:138208ms step_avg:99.14ms
step:1395/1750 train_time:138309ms step_avg:99.15ms
step:1396/1750 train_time:138410ms step_avg:99.15ms
step:1397/1750 train_time:138513ms step_avg:99.15ms
step:1398/1750 train_time:138614ms step_avg:99.15ms
step:1399/1750 train_time:138716ms step_avg:99.15ms
step:1400/1750 train_time:138818ms step_avg:99.16ms
step:1401/1750 train_time:138919ms step_avg:99.16ms
step:1402/1750 train_time:139021ms step_avg:99.16ms
step:1403/1750 train_time:139122ms step_avg:99.16ms
step:1404/1750 train_time:139222ms step_avg:99.16ms
step:1405/1750 train_time:139323ms step_avg:99.16ms
step:1406/1750 train_time:139424ms step_avg:99.16ms
step:1407/1750 train_time:139527ms step_avg:99.17ms
step:1408/1750 train_time:139628ms step_avg:99.17ms
step:1409/1750 train_time:139732ms step_avg:99.17ms
step:1410/1750 train_time:139833ms step_avg:99.17ms
step:1411/1750 train_time:139934ms step_avg:99.17ms
step:1412/1750 train_time:140036ms step_avg:99.18ms
step:1413/1750 train_time:140137ms step_avg:99.18ms
step:1414/1750 train_time:140237ms step_avg:99.18ms
step:1415/1750 train_time:140339ms step_avg:99.18ms
step:1416/1750 train_time:140441ms step_avg:99.18ms
step:1417/1750 train_time:140542ms step_avg:99.18ms
step:1418/1750 train_time:140644ms step_avg:99.18ms
step:1419/1750 train_time:140744ms step_avg:99.19ms
step:1420/1750 train_time:140845ms step_avg:99.19ms
step:1421/1750 train_time:140946ms step_avg:99.19ms
step:1422/1750 train_time:141047ms step_avg:99.19ms
step:1423/1750 train_time:141148ms step_avg:99.19ms
step:1424/1750 train_time:141249ms step_avg:99.19ms
step:1425/1750 train_time:141350ms step_avg:99.19ms
step:1426/1750 train_time:141451ms step_avg:99.19ms
step:1427/1750 train_time:141553ms step_avg:99.20ms
step:1428/1750 train_time:141656ms step_avg:99.20ms
step:1429/1750 train_time:141758ms step_avg:99.20ms
step:1430/1750 train_time:141861ms step_avg:99.20ms
step:1431/1750 train_time:141963ms step_avg:99.21ms
step:1432/1750 train_time:142065ms step_avg:99.21ms
step:1433/1750 train_time:142167ms step_avg:99.21ms
step:1434/1750 train_time:142267ms step_avg:99.21ms
step:1435/1750 train_time:142371ms step_avg:99.21ms
step:1436/1750 train_time:142473ms step_avg:99.22ms
step:1437/1750 train_time:142576ms step_avg:99.22ms
step:1438/1750 train_time:142677ms step_avg:99.22ms
step:1439/1750 train_time:142780ms step_avg:99.22ms
step:1440/1750 train_time:142884ms step_avg:99.22ms
step:1441/1750 train_time:142986ms step_avg:99.23ms
step:1442/1750 train_time:143086ms step_avg:99.23ms
step:1443/1750 train_time:143187ms step_avg:99.23ms
step:1444/1750 train_time:143289ms step_avg:99.23ms
step:1445/1750 train_time:143391ms step_avg:99.23ms
step:1446/1750 train_time:143492ms step_avg:99.23ms
step:1447/1750 train_time:143594ms step_avg:99.24ms
step:1448/1750 train_time:143698ms step_avg:99.24ms
step:1449/1750 train_time:143799ms step_avg:99.24ms
step:1450/1750 train_time:143902ms step_avg:99.24ms
step:1451/1750 train_time:144004ms step_avg:99.24ms
step:1452/1750 train_time:144105ms step_avg:99.25ms
step:1453/1750 train_time:144208ms step_avg:99.25ms
step:1454/1750 train_time:144311ms step_avg:99.25ms
step:1455/1750 train_time:144412ms step_avg:99.25ms
step:1456/1750 train_time:144515ms step_avg:99.25ms
step:1457/1750 train_time:144618ms step_avg:99.26ms
step:1458/1750 train_time:144721ms step_avg:99.26ms
step:1459/1750 train_time:144823ms step_avg:99.26ms
step:1460/1750 train_time:144924ms step_avg:99.26ms
step:1461/1750 train_time:145026ms step_avg:99.26ms
step:1462/1750 train_time:145128ms step_avg:99.27ms
step:1463/1750 train_time:145229ms step_avg:99.27ms
step:1464/1750 train_time:145330ms step_avg:99.27ms
step:1465/1750 train_time:145432ms step_avg:99.27ms
step:1466/1750 train_time:145534ms step_avg:99.27ms
step:1467/1750 train_time:145636ms step_avg:99.27ms
step:1468/1750 train_time:145738ms step_avg:99.28ms
step:1469/1750 train_time:145840ms step_avg:99.28ms
step:1470/1750 train_time:145942ms step_avg:99.28ms
step:1471/1750 train_time:146044ms step_avg:99.28ms
step:1472/1750 train_time:146147ms step_avg:99.28ms
step:1473/1750 train_time:146248ms step_avg:99.29ms
step:1474/1750 train_time:146350ms step_avg:99.29ms
step:1475/1750 train_time:146450ms step_avg:99.29ms
step:1476/1750 train_time:146553ms step_avg:99.29ms
step:1477/1750 train_time:146655ms step_avg:99.29ms
step:1478/1750 train_time:146758ms step_avg:99.29ms
step:1479/1750 train_time:146859ms step_avg:99.30ms
step:1480/1750 train_time:146961ms step_avg:99.30ms
step:1481/1750 train_time:147063ms step_avg:99.30ms
step:1482/1750 train_time:147165ms step_avg:99.30ms
step:1483/1750 train_time:147267ms step_avg:99.30ms
step:1484/1750 train_time:147371ms step_avg:99.31ms
step:1485/1750 train_time:147473ms step_avg:99.31ms
step:1486/1750 train_time:147575ms step_avg:99.31ms
step:1487/1750 train_time:147677ms step_avg:99.31ms
step:1488/1750 train_time:147779ms step_avg:99.31ms
step:1489/1750 train_time:147881ms step_avg:99.32ms
step:1490/1750 train_time:147984ms step_avg:99.32ms
step:1491/1750 train_time:148085ms step_avg:99.32ms
step:1492/1750 train_time:148186ms step_avg:99.32ms
step:1493/1750 train_time:148288ms step_avg:99.32ms
step:1494/1750 train_time:148390ms step_avg:99.32ms
step:1495/1750 train_time:148491ms step_avg:99.33ms
step:1496/1750 train_time:148593ms step_avg:99.33ms
step:1497/1750 train_time:148694ms step_avg:99.33ms
step:1498/1750 train_time:148796ms step_avg:99.33ms
step:1499/1750 train_time:148898ms step_avg:99.33ms
step:1500/1750 train_time:148999ms step_avg:99.33ms
step:1500/1750 val_loss:3.3374 train_time:149090ms step_avg:99.39ms
step:1501/1750 train_time:149112ms step_avg:99.34ms
step:1502/1750 train_time:149213ms step_avg:99.34ms
step:1503/1750 train_time:149315ms step_avg:99.34ms
step:1504/1750 train_time:149417ms step_avg:99.35ms
step:1505/1750 train_time:149517ms step_avg:99.35ms
step:1506/1750 train_time:149618ms step_avg:99.35ms
step:1507/1750 train_time:149720ms step_avg:99.35ms
step:1508/1750 train_time:149820ms step_avg:99.35ms
step:1509/1750 train_time:149922ms step_avg:99.35ms
step:1510/1750 train_time:150023ms step_avg:99.35ms
step:1511/1750 train_time:150127ms step_avg:99.36ms
step:1512/1750 train_time:150230ms step_avg:99.36ms
step:1513/1750 train_time:150331ms step_avg:99.36ms
step:1514/1750 train_time:150433ms step_avg:99.36ms
step:1515/1750 train_time:150536ms step_avg:99.36ms
step:1516/1750 train_time:150637ms step_avg:99.36ms
step:1517/1750 train_time:150738ms step_avg:99.37ms
step:1518/1750 train_time:150840ms step_avg:99.37ms
step:1519/1750 train_time:150942ms step_avg:99.37ms
step:1520/1750 train_time:151045ms step_avg:99.37ms
step:1521/1750 train_time:151148ms step_avg:99.37ms
step:1522/1750 train_time:151250ms step_avg:99.38ms
step:1523/1750 train_time:151351ms step_avg:99.38ms
step:1524/1750 train_time:151454ms step_avg:99.38ms
step:1525/1750 train_time:151557ms step_avg:99.38ms
step:1526/1750 train_time:151659ms step_avg:99.38ms
step:1527/1750 train_time:151759ms step_avg:99.38ms
step:1528/1750 train_time:151865ms step_avg:99.39ms
step:1529/1750 train_time:151967ms step_avg:99.39ms
step:1530/1750 train_time:152071ms step_avg:99.39ms
step:1531/1750 train_time:152172ms step_avg:99.39ms
step:1532/1750 train_time:152274ms step_avg:99.40ms
step:1533/1750 train_time:152376ms step_avg:99.40ms
step:1534/1750 train_time:152479ms step_avg:99.40ms
step:1535/1750 train_time:152581ms step_avg:99.40ms
step:1536/1750 train_time:152682ms step_avg:99.40ms
step:1537/1750 train_time:152783ms step_avg:99.40ms
step:1538/1750 train_time:152884ms step_avg:99.40ms
step:1539/1750 train_time:152986ms step_avg:99.41ms
step:1540/1750 train_time:153088ms step_avg:99.41ms
step:1541/1750 train_time:153192ms step_avg:99.41ms
step:1542/1750 train_time:153295ms step_avg:99.41ms
step:1543/1750 train_time:153398ms step_avg:99.42ms
step:1544/1750 train_time:153500ms step_avg:99.42ms
step:1545/1750 train_time:153602ms step_avg:99.42ms
step:1546/1750 train_time:153704ms step_avg:99.42ms
step:1547/1750 train_time:153806ms step_avg:99.42ms
step:1548/1750 train_time:153908ms step_avg:99.42ms
step:1549/1750 train_time:154009ms step_avg:99.43ms
step:1550/1750 train_time:154111ms step_avg:99.43ms
step:1551/1750 train_time:154215ms step_avg:99.43ms
step:1552/1750 train_time:154318ms step_avg:99.43ms
step:1553/1750 train_time:154421ms step_avg:99.43ms
step:1554/1750 train_time:154522ms step_avg:99.43ms
step:1555/1750 train_time:154623ms step_avg:99.44ms
step:1556/1750 train_time:154726ms step_avg:99.44ms
step:1557/1750 train_time:154828ms step_avg:99.44ms
step:1558/1750 train_time:154929ms step_avg:99.44ms
step:1559/1750 train_time:155031ms step_avg:99.44ms
step:1560/1750 train_time:155133ms step_avg:99.44ms
step:1561/1750 train_time:155235ms step_avg:99.45ms
step:1562/1750 train_time:155338ms step_avg:99.45ms
step:1563/1750 train_time:155444ms step_avg:99.45ms
step:1564/1750 train_time:155545ms step_avg:99.45ms
step:1565/1750 train_time:155646ms step_avg:99.45ms
step:1566/1750 train_time:155748ms step_avg:99.46ms
step:1567/1750 train_time:155849ms step_avg:99.46ms
step:1568/1750 train_time:155950ms step_avg:99.46ms
step:1569/1750 train_time:156051ms step_avg:99.46ms
step:1570/1750 train_time:156153ms step_avg:99.46ms
step:1571/1750 train_time:156256ms step_avg:99.46ms
step:1572/1750 train_time:156358ms step_avg:99.46ms
step:1573/1750 train_time:156460ms step_avg:99.47ms
step:1574/1750 train_time:156562ms step_avg:99.47ms
step:1575/1750 train_time:156663ms step_avg:99.47ms
step:1576/1750 train_time:156766ms step_avg:99.47ms
step:1577/1750 train_time:156868ms step_avg:99.47ms
step:1578/1750 train_time:156969ms step_avg:99.47ms
step:1579/1750 train_time:157071ms step_avg:99.47ms
step:1580/1750 train_time:157173ms step_avg:99.48ms
step:1581/1750 train_time:157275ms step_avg:99.48ms
step:1582/1750 train_time:157376ms step_avg:99.48ms
step:1583/1750 train_time:157482ms step_avg:99.48ms
step:1584/1750 train_time:157584ms step_avg:99.48ms
step:1585/1750 train_time:157686ms step_avg:99.49ms
step:1586/1750 train_time:157789ms step_avg:99.49ms
step:1587/1750 train_time:157889ms step_avg:99.49ms
step:1588/1750 train_time:157991ms step_avg:99.49ms
step:1589/1750 train_time:158093ms step_avg:99.49ms
step:1590/1750 train_time:158195ms step_avg:99.49ms
step:1591/1750 train_time:158297ms step_avg:99.50ms
step:1592/1750 train_time:158401ms step_avg:99.50ms
step:1593/1750 train_time:158504ms step_avg:99.50ms
step:1594/1750 train_time:158608ms step_avg:99.50ms
step:1595/1750 train_time:158710ms step_avg:99.50ms
step:1596/1750 train_time:158812ms step_avg:99.51ms
step:1597/1750 train_time:158914ms step_avg:99.51ms
step:1598/1750 train_time:159016ms step_avg:99.51ms
step:1599/1750 train_time:159117ms step_avg:99.51ms
step:1600/1750 train_time:159219ms step_avg:99.51ms
step:1601/1750 train_time:159321ms step_avg:99.51ms
step:1602/1750 train_time:159423ms step_avg:99.51ms
step:1603/1750 train_time:159525ms step_avg:99.52ms
step:1604/1750 train_time:159626ms step_avg:99.52ms
step:1605/1750 train_time:159728ms step_avg:99.52ms
step:1606/1750 train_time:159829ms step_avg:99.52ms
step:1607/1750 train_time:159931ms step_avg:99.52ms
step:1608/1750 train_time:160032ms step_avg:99.52ms
step:1609/1750 train_time:160135ms step_avg:99.52ms
step:1610/1750 train_time:160238ms step_avg:99.53ms
step:1611/1750 train_time:160341ms step_avg:99.53ms
step:1612/1750 train_time:160444ms step_avg:99.53ms
step:1613/1750 train_time:160545ms step_avg:99.53ms
step:1614/1750 train_time:160646ms step_avg:99.53ms
step:1615/1750 train_time:160746ms step_avg:99.53ms
step:1616/1750 train_time:160848ms step_avg:99.53ms
step:1617/1750 train_time:160950ms step_avg:99.54ms
step:1618/1750 train_time:161052ms step_avg:99.54ms
step:1619/1750 train_time:161155ms step_avg:99.54ms
step:1620/1750 train_time:161258ms step_avg:99.54ms
step:1621/1750 train_time:161360ms step_avg:99.54ms
step:1622/1750 train_time:161462ms step_avg:99.55ms
step:1623/1750 train_time:161564ms step_avg:99.55ms
step:1624/1750 train_time:161666ms step_avg:99.55ms
step:1625/1750 train_time:161769ms step_avg:99.55ms
step:1625/1750 val_loss:3.3076 train_time:161859ms step_avg:99.61ms
step:1626/1750 train_time:161880ms step_avg:99.56ms
step:1627/1750 train_time:161980ms step_avg:99.56ms
step:1628/1750 train_time:162081ms step_avg:99.56ms
step:1629/1750 train_time:162181ms step_avg:99.56ms
step:1630/1750 train_time:162282ms step_avg:99.56ms
step:1631/1750 train_time:162383ms step_avg:99.56ms
step:1632/1750 train_time:162485ms step_avg:99.56ms
step:1633/1750 train_time:162587ms step_avg:99.56ms
step:1634/1750 train_time:162691ms step_avg:99.57ms
step:1635/1750 train_time:162795ms step_avg:99.57ms
step:1636/1750 train_time:162898ms step_avg:99.57ms
step:1637/1750 train_time:163001ms step_avg:99.57ms
step:1638/1750 train_time:163102ms step_avg:99.57ms
step:1639/1750 train_time:163203ms step_avg:99.57ms
step:1640/1750 train_time:163305ms step_avg:99.58ms
step:1641/1750 train_time:163406ms step_avg:99.58ms
step:1642/1750 train_time:163507ms step_avg:99.58ms
step:1643/1750 train_time:163609ms step_avg:99.58ms
step:1644/1750 train_time:163712ms step_avg:99.58ms
step:1645/1750 train_time:163815ms step_avg:99.58ms
step:1646/1750 train_time:163918ms step_avg:99.59ms
step:1647/1750 train_time:164022ms step_avg:99.59ms
step:1648/1750 train_time:164125ms step_avg:99.59ms
step:1649/1750 train_time:164226ms step_avg:99.59ms
step:1650/1750 train_time:164328ms step_avg:99.59ms
step:1651/1750 train_time:164430ms step_avg:99.59ms
step:1652/1750 train_time:164532ms step_avg:99.60ms
step:1653/1750 train_time:164633ms step_avg:99.60ms
step:1654/1750 train_time:164734ms step_avg:99.60ms
step:1655/1750 train_time:164836ms step_avg:99.60ms
step:1656/1750 train_time:164939ms step_avg:99.60ms
step:1657/1750 train_time:165041ms step_avg:99.60ms
step:1658/1750 train_time:165143ms step_avg:99.60ms
step:1659/1750 train_time:165247ms step_avg:99.61ms
step:1660/1750 train_time:165350ms step_avg:99.61ms
step:1661/1750 train_time:165453ms step_avg:99.61ms
step:1662/1750 train_time:165557ms step_avg:99.61ms
step:1663/1750 train_time:165659ms step_avg:99.61ms
step:1664/1750 train_time:165761ms step_avg:99.62ms
step:1665/1750 train_time:165864ms step_avg:99.62ms
step:1666/1750 train_time:165967ms step_avg:99.62ms
step:1667/1750 train_time:166070ms step_avg:99.62ms
step:1668/1750 train_time:166175ms step_avg:99.63ms
step:1669/1750 train_time:166276ms step_avg:99.63ms
step:1670/1750 train_time:166377ms step_avg:99.63ms
step:1671/1750 train_time:166478ms step_avg:99.63ms
step:1672/1750 train_time:166580ms step_avg:99.63ms
step:1673/1750 train_time:166681ms step_avg:99.63ms
step:1674/1750 train_time:166783ms step_avg:99.63ms
step:1675/1750 train_time:166884ms step_avg:99.63ms
step:1676/1750 train_time:166989ms step_avg:99.64ms
step:1677/1750 train_time:167092ms step_avg:99.64ms
step:1678/1750 train_time:167194ms step_avg:99.64ms
step:1679/1750 train_time:167297ms step_avg:99.64ms
step:1680/1750 train_time:167398ms step_avg:99.64ms
step:1681/1750 train_time:167501ms step_avg:99.64ms
step:1682/1750 train_time:167604ms step_avg:99.65ms
step:1683/1750 train_time:167704ms step_avg:99.65ms
step:1684/1750 train_time:167807ms step_avg:99.65ms
step:1685/1750 train_time:167911ms step_avg:99.65ms
step:1686/1750 train_time:168013ms step_avg:99.65ms
step:1687/1750 train_time:168114ms step_avg:99.65ms
step:1688/1750 train_time:168217ms step_avg:99.65ms
step:1689/1750 train_time:168320ms step_avg:99.66ms
step:1690/1750 train_time:168423ms step_avg:99.66ms
step:1691/1750 train_time:168525ms step_avg:99.66ms
step:1692/1750 train_time:168627ms step_avg:99.66ms
step:1693/1750 train_time:168730ms step_avg:99.66ms
step:1694/1750 train_time:168835ms step_avg:99.67ms
step:1695/1750 train_time:168939ms step_avg:99.67ms
step:1696/1750 train_time:169042ms step_avg:99.67ms
step:1697/1750 train_time:169146ms step_avg:99.67ms
step:1698/1750 train_time:169250ms step_avg:99.68ms
step:1699/1750 train_time:169353ms step_avg:99.68ms
step:1700/1750 train_time:169456ms step_avg:99.68ms
step:1701/1750 train_time:169558ms step_avg:99.68ms
step:1702/1750 train_time:169662ms step_avg:99.68ms
step:1703/1750 train_time:169764ms step_avg:99.69ms
step:1704/1750 train_time:169866ms step_avg:99.69ms
step:1705/1750 train_time:169969ms step_avg:99.69ms
step:1706/1750 train_time:170072ms step_avg:99.69ms
step:1707/1750 train_time:170176ms step_avg:99.69ms
step:1708/1750 train_time:170279ms step_avg:99.69ms
step:1709/1750 train_time:170381ms step_avg:99.70ms
step:1710/1750 train_time:170483ms step_avg:99.70ms
step:1711/1750 train_time:170588ms step_avg:99.70ms
step:1712/1750 train_time:170690ms step_avg:99.70ms
step:1713/1750 train_time:170794ms step_avg:99.70ms
step:1714/1750 train_time:170896ms step_avg:99.71ms
step:1715/1750 train_time:171001ms step_avg:99.71ms
step:1716/1750 train_time:171103ms step_avg:99.71ms
step:1717/1750 train_time:171206ms step_avg:99.71ms
step:1718/1750 train_time:171308ms step_avg:99.71ms
step:1719/1750 train_time:171415ms step_avg:99.72ms
step:1720/1750 train_time:171516ms step_avg:99.72ms
step:1721/1750 train_time:171620ms step_avg:99.72ms
step:1722/1750 train_time:171723ms step_avg:99.72ms
step:1723/1750 train_time:171825ms step_avg:99.72ms
step:1724/1750 train_time:171929ms step_avg:99.73ms
step:1725/1750 train_time:172032ms step_avg:99.73ms
step:1726/1750 train_time:172134ms step_avg:99.73ms
step:1727/1750 train_time:172236ms step_avg:99.73ms
step:1728/1750 train_time:172341ms step_avg:99.73ms
step:1729/1750 train_time:172444ms step_avg:99.74ms
step:1730/1750 train_time:172546ms step_avg:99.74ms
step:1731/1750 train_time:172650ms step_avg:99.74ms
step:1732/1750 train_time:172752ms step_avg:99.74ms
step:1733/1750 train_time:172855ms step_avg:99.74ms
step:1734/1750 train_time:172958ms step_avg:99.75ms
step:1735/1750 train_time:173060ms step_avg:99.75ms
step:1736/1750 train_time:173163ms step_avg:99.75ms
step:1737/1750 train_time:173266ms step_avg:99.75ms
step:1738/1750 train_time:173370ms step_avg:99.75ms
step:1739/1750 train_time:173472ms step_avg:99.75ms
step:1740/1750 train_time:173574ms step_avg:99.76ms
step:1741/1750 train_time:173680ms step_avg:99.76ms
step:1742/1750 train_time:173783ms step_avg:99.76ms
step:1743/1750 train_time:173885ms step_avg:99.76ms
step:1744/1750 train_time:173987ms step_avg:99.76ms
step:1745/1750 train_time:174090ms step_avg:99.77ms
step:1746/1750 train_time:174194ms step_avg:99.77ms
step:1747/1750 train_time:174297ms step_avg:99.77ms
step:1748/1750 train_time:174402ms step_avg:99.77ms
step:1749/1750 train_time:174503ms step_avg:99.77ms
step:1750/1750 train_time:174606ms step_avg:99.78ms
step:1750/1750 val_loss:3.2838 train_time:174697ms step_avg:99.83ms
peak memory allocated: 33278 MiB reserved: 48374 MiB
