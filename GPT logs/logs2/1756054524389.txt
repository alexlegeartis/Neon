import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


class NormMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, sgd_coeff=0):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, sgd_coeff=sgd_coeff)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        self.sgd_coeff = sgd_coeff

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            sgd_coeff = group["sgd_coeff"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    eps = 1e-12
                    g_normalized = grad / (grad.norm() + eps)
                    if sgd_coeff != 1:
                        update_part = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                        update = (1 - sgd_coeff) * update_part + sgd_coeff * g_normalized
                    else:
                        update = sgd_coeff * g_normalized
                    p.add_(other=update, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()
class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)
                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_reduce_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 27.5
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos : pos + max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()
    starts = []
    for i in range(1, len(boundary_positions)):
        end = boundary_positions[i].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == dist.get_world_size():
                return starts, end - pos
            start = end
    assert False # increase max_batch_span if necessary

def distributed_data_generator(filename_pattern: str, batch_size: int, align_to_bos: bool):
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    max_batch_span = 2 * batch_size if align_to_bos else batch_size # provide buffer to handle samples up to length local_batch_size
    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            batch_span = batch_size
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1750 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
from datetime import datetime
if master_process:
    # run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_id = str(int(time.time() * 1000))
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0)
optimizer2 = NormMuon(hidden_matrix_params, lr=0.05, momentum=0.98, weight_decay=0.0, sgd_coeff=0.5)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.11 | packaged by conda-forge | (main, Jun  4 2025, 14:45:31) [GCC 13.3.0]
Running PyTorch 2.9.0.dev20250824+cu126 compiled for CUDA 12.6
Sun Aug 24 16:55:24 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:04:00.0 Off |                    0 |
| N/A   38C    P0            120W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:23:00.0 Off |                    0 |
| N/A   32C    P0            114W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:43:00.0 Off |                    0 |
| N/A   36C    P0            123W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   31C    P0            115W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:84:00.0 Off |                    0 |
| N/A   37C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:A3:00.0 Off |                    0 |
| N/A   32C    P0            121W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:C3:00.0 Off |                    0 |
| N/A   38C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E4:00.0 Off |                    0 |
| N/A   31C    P0            112W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.05ms
step:1/1750 train_time:140ms step_avg:139.95ms
step:2/1750 train_time:161ms step_avg:80.37ms
step:3/1750 train_time:243ms step_avg:81.05ms
step:4/1750 train_time:335ms step_avg:83.64ms
step:5/1750 train_time:427ms step_avg:85.33ms
step:6/1750 train_time:519ms step_avg:86.48ms
step:7/1750 train_time:611ms step_avg:87.29ms
step:8/1750 train_time:703ms step_avg:87.91ms
step:9/1750 train_time:796ms step_avg:88.43ms
step:10/1750 train_time:888ms step_avg:88.79ms
step:11/1750 train_time:980ms step_avg:89.12ms
step:12/1750 train_time:1076ms step_avg:89.67ms
step:13/1750 train_time:1173ms step_avg:90.19ms
step:14/1750 train_time:1267ms step_avg:90.50ms
step:15/1750 train_time:1360ms step_avg:90.69ms
step:16/1750 train_time:1453ms step_avg:90.80ms
step:17/1750 train_time:1545ms step_avg:90.90ms
step:18/1750 train_time:1638ms step_avg:91.01ms
step:19/1750 train_time:1731ms step_avg:91.08ms
step:20/1750 train_time:1823ms step_avg:91.17ms
step:21/1750 train_time:1917ms step_avg:91.27ms
step:22/1750 train_time:2010ms step_avg:91.39ms
step:23/1750 train_time:2105ms step_avg:91.54ms
step:24/1750 train_time:2200ms step_avg:91.65ms
step:25/1750 train_time:2294ms step_avg:91.77ms
step:26/1750 train_time:2389ms step_avg:91.87ms
step:27/1750 train_time:2481ms step_avg:91.90ms
step:28/1750 train_time:2574ms step_avg:91.94ms
step:29/1750 train_time:2667ms step_avg:91.96ms
step:30/1750 train_time:2759ms step_avg:91.98ms
step:31/1750 train_time:2852ms step_avg:91.99ms
step:32/1750 train_time:2945ms step_avg:92.03ms
step:33/1750 train_time:3038ms step_avg:92.07ms
step:34/1750 train_time:3132ms step_avg:92.11ms
step:35/1750 train_time:3226ms step_avg:92.16ms
step:36/1750 train_time:3320ms step_avg:92.21ms
step:37/1750 train_time:3414ms step_avg:92.28ms
step:38/1750 train_time:3508ms step_avg:92.30ms
step:39/1750 train_time:3601ms step_avg:92.33ms
step:40/1750 train_time:3694ms step_avg:92.36ms
step:41/1750 train_time:3788ms step_avg:92.40ms
step:42/1750 train_time:3881ms step_avg:92.40ms
step:43/1750 train_time:3974ms step_avg:92.43ms
step:44/1750 train_time:4068ms step_avg:92.45ms
step:45/1750 train_time:4162ms step_avg:92.48ms
step:46/1750 train_time:4256ms step_avg:92.51ms
step:47/1750 train_time:4350ms step_avg:92.55ms
step:48/1750 train_time:4443ms step_avg:92.57ms
step:49/1750 train_time:4536ms step_avg:92.58ms
step:50/1750 train_time:4630ms step_avg:92.59ms
step:51/1750 train_time:4723ms step_avg:92.60ms
step:52/1750 train_time:4815ms step_avg:92.60ms
step:53/1750 train_time:4908ms step_avg:92.60ms
step:54/1750 train_time:5001ms step_avg:92.61ms
step:55/1750 train_time:5094ms step_avg:92.62ms
step:56/1750 train_time:5188ms step_avg:92.65ms
step:57/1750 train_time:5282ms step_avg:92.66ms
step:58/1750 train_time:5376ms step_avg:92.69ms
step:59/1750 train_time:5469ms step_avg:92.70ms
step:60/1750 train_time:5563ms step_avg:92.71ms
step:61/1750 train_time:5657ms step_avg:92.73ms
step:62/1750 train_time:5750ms step_avg:92.74ms
step:63/1750 train_time:5843ms step_avg:92.75ms
step:64/1750 train_time:5936ms step_avg:92.76ms
step:65/1750 train_time:6030ms step_avg:92.77ms
step:66/1750 train_time:6123ms step_avg:92.78ms
step:67/1750 train_time:6217ms step_avg:92.79ms
step:68/1750 train_time:6311ms step_avg:92.81ms
step:69/1750 train_time:6404ms step_avg:92.81ms
step:70/1750 train_time:6497ms step_avg:92.82ms
step:71/1750 train_time:6591ms step_avg:92.83ms
step:72/1750 train_time:6685ms step_avg:92.85ms
step:73/1750 train_time:6779ms step_avg:92.86ms
step:74/1750 train_time:6872ms step_avg:92.86ms
step:75/1750 train_time:6965ms step_avg:92.86ms
step:76/1750 train_time:7058ms step_avg:92.87ms
step:77/1750 train_time:7152ms step_avg:92.89ms
step:78/1750 train_time:7246ms step_avg:92.90ms
step:79/1750 train_time:7339ms step_avg:92.90ms
step:80/1750 train_time:7433ms step_avg:92.91ms
step:81/1750 train_time:7526ms step_avg:92.92ms
step:82/1750 train_time:7620ms step_avg:92.93ms
step:83/1750 train_time:7714ms step_avg:92.94ms
step:84/1750 train_time:7808ms step_avg:92.95ms
step:85/1750 train_time:7900ms step_avg:92.95ms
step:86/1750 train_time:7993ms step_avg:92.94ms
step:87/1750 train_time:8086ms step_avg:92.94ms
step:88/1750 train_time:8180ms step_avg:92.96ms
step:89/1750 train_time:8274ms step_avg:92.96ms
step:90/1750 train_time:8367ms step_avg:92.97ms
step:91/1750 train_time:8460ms step_avg:92.97ms
step:92/1750 train_time:8554ms step_avg:92.98ms
step:93/1750 train_time:8647ms step_avg:92.98ms
step:94/1750 train_time:8741ms step_avg:92.99ms
step:95/1750 train_time:8834ms step_avg:92.99ms
step:96/1750 train_time:8927ms step_avg:92.99ms
step:97/1750 train_time:9021ms step_avg:93.00ms
step:98/1750 train_time:9114ms step_avg:93.00ms
step:99/1750 train_time:9209ms step_avg:93.02ms
step:100/1750 train_time:9302ms step_avg:93.02ms
step:101/1750 train_time:9396ms step_avg:93.03ms
step:102/1750 train_time:9489ms step_avg:93.03ms
step:103/1750 train_time:9582ms step_avg:93.03ms
step:104/1750 train_time:9676ms step_avg:93.04ms
step:105/1750 train_time:9769ms step_avg:93.03ms
step:106/1750 train_time:9862ms step_avg:93.04ms
step:107/1750 train_time:9955ms step_avg:93.04ms
step:108/1750 train_time:10048ms step_avg:93.04ms
step:109/1750 train_time:10141ms step_avg:93.04ms
step:110/1750 train_time:10235ms step_avg:93.04ms
step:111/1750 train_time:10328ms step_avg:93.04ms
step:112/1750 train_time:10421ms step_avg:93.05ms
step:113/1750 train_time:10514ms step_avg:93.05ms
step:114/1750 train_time:10608ms step_avg:93.06ms
step:115/1750 train_time:10702ms step_avg:93.06ms
step:116/1750 train_time:10796ms step_avg:93.07ms
step:117/1750 train_time:10889ms step_avg:93.07ms
step:118/1750 train_time:10982ms step_avg:93.07ms
step:119/1750 train_time:11077ms step_avg:93.08ms
step:120/1750 train_time:11170ms step_avg:93.08ms
step:121/1750 train_time:11263ms step_avg:93.08ms
step:122/1750 train_time:11357ms step_avg:93.09ms
step:123/1750 train_time:11450ms step_avg:93.09ms
step:124/1750 train_time:11543ms step_avg:93.09ms
step:125/1750 train_time:11637ms step_avg:93.10ms
step:125/1750 val_loss:4.6745 train_time:11721ms step_avg:93.76ms
step:126/1750 train_time:11745ms step_avg:93.21ms
step:127/1750 train_time:11834ms step_avg:93.18ms
step:128/1750 train_time:11940ms step_avg:93.28ms
step:129/1750 train_time:12036ms step_avg:93.30ms
step:130/1750 train_time:12129ms step_avg:93.30ms
step:131/1750 train_time:12221ms step_avg:93.29ms
step:132/1750 train_time:12315ms step_avg:93.29ms
step:133/1750 train_time:12408ms step_avg:93.29ms
step:134/1750 train_time:12501ms step_avg:93.29ms
step:135/1750 train_time:12594ms step_avg:93.29ms
step:136/1750 train_time:12687ms step_avg:93.29ms
step:137/1750 train_time:12780ms step_avg:93.29ms
step:138/1750 train_time:12878ms step_avg:93.32ms
step:139/1750 train_time:12974ms step_avg:93.34ms
step:140/1750 train_time:13069ms step_avg:93.35ms
step:141/1750 train_time:13163ms step_avg:93.36ms
step:142/1750 train_time:13256ms step_avg:93.35ms
step:143/1750 train_time:13350ms step_avg:93.35ms
step:144/1750 train_time:13442ms step_avg:93.34ms
step:145/1750 train_time:13534ms step_avg:93.34ms
step:146/1750 train_time:13628ms step_avg:93.34ms
step:147/1750 train_time:13721ms step_avg:93.34ms
step:148/1750 train_time:13815ms step_avg:93.35ms
step:149/1750 train_time:13911ms step_avg:93.36ms
step:150/1750 train_time:14006ms step_avg:93.38ms
step:151/1750 train_time:14101ms step_avg:93.38ms
step:152/1750 train_time:14194ms step_avg:93.38ms
step:153/1750 train_time:14288ms step_avg:93.38ms
step:154/1750 train_time:14381ms step_avg:93.38ms
step:155/1750 train_time:14475ms step_avg:93.38ms
step:156/1750 train_time:14568ms step_avg:93.38ms
step:157/1750 train_time:14661ms step_avg:93.38ms
step:158/1750 train_time:14755ms step_avg:93.38ms
step:159/1750 train_time:14849ms step_avg:93.39ms
step:160/1750 train_time:14943ms step_avg:93.39ms
step:161/1750 train_time:15037ms step_avg:93.40ms
step:162/1750 train_time:15133ms step_avg:93.41ms
step:163/1750 train_time:15227ms step_avg:93.42ms
step:164/1750 train_time:15321ms step_avg:93.42ms
step:165/1750 train_time:15414ms step_avg:93.42ms
step:166/1750 train_time:15507ms step_avg:93.42ms
step:167/1750 train_time:15600ms step_avg:93.41ms
step:168/1750 train_time:15693ms step_avg:93.41ms
step:169/1750 train_time:15787ms step_avg:93.41ms
step:170/1750 train_time:15880ms step_avg:93.41ms
step:171/1750 train_time:15975ms step_avg:93.42ms
step:172/1750 train_time:16070ms step_avg:93.43ms
step:173/1750 train_time:16164ms step_avg:93.44ms
step:174/1750 train_time:16258ms step_avg:93.44ms
step:175/1750 train_time:16352ms step_avg:93.44ms
step:176/1750 train_time:16445ms step_avg:93.44ms
step:177/1750 train_time:16539ms step_avg:93.44ms
step:178/1750 train_time:16632ms step_avg:93.44ms
step:179/1750 train_time:16726ms step_avg:93.44ms
step:180/1750 train_time:16820ms step_avg:93.45ms
step:181/1750 train_time:16915ms step_avg:93.45ms
step:182/1750 train_time:17009ms step_avg:93.46ms
step:183/1750 train_time:17103ms step_avg:93.46ms
step:184/1750 train_time:17198ms step_avg:93.46ms
step:185/1750 train_time:17291ms step_avg:93.46ms
step:186/1750 train_time:17385ms step_avg:93.47ms
step:187/1750 train_time:17478ms step_avg:93.47ms
step:188/1750 train_time:17572ms step_avg:93.47ms
step:189/1750 train_time:17666ms step_avg:93.47ms
step:190/1750 train_time:17759ms step_avg:93.47ms
step:191/1750 train_time:17853ms step_avg:93.47ms
step:192/1750 train_time:17947ms step_avg:93.48ms
step:193/1750 train_time:18041ms step_avg:93.48ms
step:194/1750 train_time:18136ms step_avg:93.48ms
step:195/1750 train_time:18230ms step_avg:93.49ms
step:196/1750 train_time:18323ms step_avg:93.49ms
step:197/1750 train_time:18417ms step_avg:93.49ms
step:198/1750 train_time:18511ms step_avg:93.49ms
step:199/1750 train_time:18605ms step_avg:93.49ms
step:200/1750 train_time:18698ms step_avg:93.49ms
step:201/1750 train_time:18792ms step_avg:93.49ms
step:202/1750 train_time:18886ms step_avg:93.50ms
step:203/1750 train_time:18980ms step_avg:93.50ms
step:204/1750 train_time:19074ms step_avg:93.50ms
step:205/1750 train_time:19169ms step_avg:93.51ms
step:206/1750 train_time:19263ms step_avg:93.51ms
step:207/1750 train_time:19356ms step_avg:93.51ms
step:208/1750 train_time:19451ms step_avg:93.51ms
step:209/1750 train_time:19545ms step_avg:93.51ms
step:210/1750 train_time:19639ms step_avg:93.52ms
step:211/1750 train_time:19732ms step_avg:93.52ms
step:212/1750 train_time:19825ms step_avg:93.52ms
step:213/1750 train_time:19919ms step_avg:93.52ms
step:214/1750 train_time:20013ms step_avg:93.52ms
step:215/1750 train_time:20107ms step_avg:93.52ms
step:216/1750 train_time:20202ms step_avg:93.53ms
step:217/1750 train_time:20295ms step_avg:93.52ms
step:218/1750 train_time:20388ms step_avg:93.52ms
step:219/1750 train_time:20483ms step_avg:93.53ms
step:220/1750 train_time:20577ms step_avg:93.53ms
step:221/1750 train_time:20670ms step_avg:93.53ms
step:222/1750 train_time:20763ms step_avg:93.53ms
step:223/1750 train_time:20857ms step_avg:93.53ms
step:224/1750 train_time:20950ms step_avg:93.53ms
step:225/1750 train_time:21044ms step_avg:93.53ms
step:226/1750 train_time:21137ms step_avg:93.53ms
step:227/1750 train_time:21231ms step_avg:93.53ms
step:228/1750 train_time:21324ms step_avg:93.53ms
step:229/1750 train_time:21417ms step_avg:93.53ms
step:230/1750 train_time:21513ms step_avg:93.53ms
step:231/1750 train_time:21607ms step_avg:93.54ms
step:232/1750 train_time:21700ms step_avg:93.54ms
step:233/1750 train_time:21794ms step_avg:93.54ms
step:234/1750 train_time:21887ms step_avg:93.53ms
step:235/1750 train_time:21980ms step_avg:93.53ms
step:236/1750 train_time:22074ms step_avg:93.53ms
step:237/1750 train_time:22168ms step_avg:93.53ms
step:238/1750 train_time:22261ms step_avg:93.53ms
step:239/1750 train_time:22355ms step_avg:93.53ms
step:240/1750 train_time:22449ms step_avg:93.54ms
step:241/1750 train_time:22543ms step_avg:93.54ms
step:242/1750 train_time:22638ms step_avg:93.54ms
step:243/1750 train_time:22731ms step_avg:93.54ms
step:244/1750 train_time:22824ms step_avg:93.54ms
step:245/1750 train_time:22918ms step_avg:93.54ms
step:246/1750 train_time:23011ms step_avg:93.54ms
step:247/1750 train_time:23105ms step_avg:93.54ms
step:248/1750 train_time:23198ms step_avg:93.54ms
step:249/1750 train_time:23292ms step_avg:93.54ms
step:250/1750 train_time:23385ms step_avg:93.54ms
step:250/1750 val_loss:4.1160 train_time:23469ms step_avg:93.88ms
step:251/1750 train_time:23490ms step_avg:93.59ms
step:252/1750 train_time:23581ms step_avg:93.57ms
step:253/1750 train_time:23679ms step_avg:93.59ms
step:254/1750 train_time:23774ms step_avg:93.60ms
step:255/1750 train_time:23867ms step_avg:93.60ms
step:256/1750 train_time:23960ms step_avg:93.59ms
step:257/1750 train_time:24053ms step_avg:93.59ms
step:258/1750 train_time:24145ms step_avg:93.59ms
step:259/1750 train_time:24238ms step_avg:93.58ms
step:260/1750 train_time:24330ms step_avg:93.58ms
step:261/1750 train_time:24424ms step_avg:93.58ms
step:262/1750 train_time:24520ms step_avg:93.59ms
step:263/1750 train_time:24616ms step_avg:93.60ms
step:264/1750 train_time:24711ms step_avg:93.60ms
step:265/1750 train_time:24806ms step_avg:93.61ms
step:266/1750 train_time:24901ms step_avg:93.61ms
step:267/1750 train_time:24994ms step_avg:93.61ms
step:268/1750 train_time:25088ms step_avg:93.61ms
step:269/1750 train_time:25181ms step_avg:93.61ms
step:270/1750 train_time:25275ms step_avg:93.61ms
step:271/1750 train_time:25369ms step_avg:93.61ms
step:272/1750 train_time:25463ms step_avg:93.61ms
step:273/1750 train_time:25558ms step_avg:93.62ms
step:274/1750 train_time:25654ms step_avg:93.63ms
step:275/1750 train_time:25749ms step_avg:93.63ms
step:276/1750 train_time:25844ms step_avg:93.64ms
step:277/1750 train_time:25938ms step_avg:93.64ms
step:278/1750 train_time:26032ms step_avg:93.64ms
step:279/1750 train_time:26125ms step_avg:93.64ms
step:280/1750 train_time:26219ms step_avg:93.64ms
step:281/1750 train_time:26313ms step_avg:93.64ms
step:282/1750 train_time:26406ms step_avg:93.64ms
step:283/1750 train_time:26501ms step_avg:93.64ms
step:284/1750 train_time:26596ms step_avg:93.65ms
step:285/1750 train_time:26690ms step_avg:93.65ms
step:286/1750 train_time:26786ms step_avg:93.66ms
step:287/1750 train_time:26880ms step_avg:93.66ms
step:288/1750 train_time:26975ms step_avg:93.66ms
step:289/1750 train_time:27069ms step_avg:93.66ms
step:290/1750 train_time:27162ms step_avg:93.66ms
step:291/1750 train_time:27256ms step_avg:93.66ms
step:292/1750 train_time:27350ms step_avg:93.66ms
step:293/1750 train_time:27444ms step_avg:93.66ms
step:294/1750 train_time:27538ms step_avg:93.67ms
step:295/1750 train_time:27631ms step_avg:93.67ms
step:296/1750 train_time:27726ms step_avg:93.67ms
step:297/1750 train_time:27820ms step_avg:93.67ms
step:298/1750 train_time:27914ms step_avg:93.67ms
step:299/1750 train_time:28009ms step_avg:93.68ms
step:300/1750 train_time:28103ms step_avg:93.68ms
step:301/1750 train_time:28196ms step_avg:93.68ms
step:302/1750 train_time:28290ms step_avg:93.68ms
step:303/1750 train_time:28384ms step_avg:93.68ms
step:304/1750 train_time:28478ms step_avg:93.68ms
step:305/1750 train_time:28572ms step_avg:93.68ms
step:306/1750 train_time:28666ms step_avg:93.68ms
step:307/1750 train_time:28760ms step_avg:93.68ms
step:308/1750 train_time:28855ms step_avg:93.68ms
step:309/1750 train_time:28949ms step_avg:93.69ms
step:310/1750 train_time:29043ms step_avg:93.69ms
step:311/1750 train_time:29137ms step_avg:93.69ms
step:312/1750 train_time:29231ms step_avg:93.69ms
step:313/1750 train_time:29325ms step_avg:93.69ms
step:314/1750 train_time:29418ms step_avg:93.69ms
step:315/1750 train_time:29512ms step_avg:93.69ms
step:316/1750 train_time:29607ms step_avg:93.69ms
step:317/1750 train_time:29700ms step_avg:93.69ms
step:318/1750 train_time:29795ms step_avg:93.70ms
step:319/1750 train_time:29890ms step_avg:93.70ms
step:320/1750 train_time:29984ms step_avg:93.70ms
step:321/1750 train_time:30078ms step_avg:93.70ms
step:322/1750 train_time:30172ms step_avg:93.70ms
step:323/1750 train_time:30266ms step_avg:93.70ms
step:324/1750 train_time:30361ms step_avg:93.71ms
step:325/1750 train_time:30455ms step_avg:93.71ms
step:326/1750 train_time:30549ms step_avg:93.71ms
step:327/1750 train_time:30643ms step_avg:93.71ms
step:328/1750 train_time:30738ms step_avg:93.71ms
step:329/1750 train_time:30832ms step_avg:93.71ms
step:330/1750 train_time:30927ms step_avg:93.72ms
step:331/1750 train_time:31021ms step_avg:93.72ms
step:332/1750 train_time:31114ms step_avg:93.72ms
step:333/1750 train_time:31208ms step_avg:93.72ms
step:334/1750 train_time:31302ms step_avg:93.72ms
step:335/1750 train_time:31396ms step_avg:93.72ms
step:336/1750 train_time:31490ms step_avg:93.72ms
step:337/1750 train_time:31585ms step_avg:93.72ms
step:338/1750 train_time:31679ms step_avg:93.72ms
step:339/1750 train_time:31773ms step_avg:93.72ms
step:340/1750 train_time:31867ms step_avg:93.73ms
step:341/1750 train_time:31961ms step_avg:93.73ms
step:342/1750 train_time:32056ms step_avg:93.73ms
step:343/1750 train_time:32150ms step_avg:93.73ms
step:344/1750 train_time:32244ms step_avg:93.73ms
step:345/1750 train_time:32338ms step_avg:93.73ms
step:346/1750 train_time:32432ms step_avg:93.74ms
step:347/1750 train_time:32526ms step_avg:93.74ms
step:348/1750 train_time:32621ms step_avg:93.74ms
step:349/1750 train_time:32715ms step_avg:93.74ms
step:350/1750 train_time:32810ms step_avg:93.74ms
step:351/1750 train_time:32905ms step_avg:93.75ms
step:352/1750 train_time:32999ms step_avg:93.75ms
step:353/1750 train_time:33094ms step_avg:93.75ms
step:354/1750 train_time:33188ms step_avg:93.75ms
step:355/1750 train_time:33283ms step_avg:93.76ms
step:356/1750 train_time:33377ms step_avg:93.75ms
step:357/1750 train_time:33471ms step_avg:93.76ms
step:358/1750 train_time:33565ms step_avg:93.76ms
step:359/1750 train_time:33659ms step_avg:93.76ms
step:360/1750 train_time:33753ms step_avg:93.76ms
step:361/1750 train_time:33848ms step_avg:93.76ms
step:362/1750 train_time:33941ms step_avg:93.76ms
step:363/1750 train_time:34036ms step_avg:93.76ms
step:364/1750 train_time:34130ms step_avg:93.76ms
step:365/1750 train_time:34224ms step_avg:93.76ms
step:366/1750 train_time:34319ms step_avg:93.77ms
step:367/1750 train_time:34413ms step_avg:93.77ms
step:368/1750 train_time:34507ms step_avg:93.77ms
step:369/1750 train_time:34600ms step_avg:93.77ms
step:370/1750 train_time:34695ms step_avg:93.77ms
step:371/1750 train_time:34789ms step_avg:93.77ms
step:372/1750 train_time:34883ms step_avg:93.77ms
step:373/1750 train_time:34977ms step_avg:93.77ms
step:374/1750 train_time:35071ms step_avg:93.77ms
step:375/1750 train_time:35166ms step_avg:93.77ms
step:375/1750 val_loss:3.8985 train_time:35249ms step_avg:94.00ms
step:376/1750 train_time:35271ms step_avg:93.81ms
step:377/1750 train_time:35361ms step_avg:93.80ms
step:378/1750 train_time:35458ms step_avg:93.80ms
step:379/1750 train_time:35552ms step_avg:93.80ms
step:380/1750 train_time:35646ms step_avg:93.81ms
step:381/1750 train_time:35739ms step_avg:93.80ms
step:382/1750 train_time:35832ms step_avg:93.80ms
step:383/1750 train_time:35925ms step_avg:93.80ms
step:384/1750 train_time:36018ms step_avg:93.80ms
step:385/1750 train_time:36111ms step_avg:93.80ms
step:386/1750 train_time:36208ms step_avg:93.80ms
step:387/1750 train_time:36304ms step_avg:93.81ms
step:388/1750 train_time:36400ms step_avg:93.81ms
step:389/1750 train_time:36495ms step_avg:93.82ms
step:390/1750 train_time:36590ms step_avg:93.82ms
step:391/1750 train_time:36685ms step_avg:93.82ms
step:392/1750 train_time:36780ms step_avg:93.83ms
step:393/1750 train_time:36875ms step_avg:93.83ms
step:394/1750 train_time:36970ms step_avg:93.83ms
step:395/1750 train_time:37065ms step_avg:93.84ms
step:396/1750 train_time:37161ms step_avg:93.84ms
step:397/1750 train_time:37259ms step_avg:93.85ms
step:398/1750 train_time:37356ms step_avg:93.86ms
step:399/1750 train_time:37454ms step_avg:93.87ms
step:400/1750 train_time:37550ms step_avg:93.88ms
step:401/1750 train_time:37646ms step_avg:93.88ms
step:402/1750 train_time:37742ms step_avg:93.89ms
step:403/1750 train_time:37838ms step_avg:93.89ms
step:404/1750 train_time:37934ms step_avg:93.90ms
step:405/1750 train_time:38029ms step_avg:93.90ms
step:406/1750 train_time:38124ms step_avg:93.90ms
step:407/1750 train_time:38220ms step_avg:93.91ms
step:408/1750 train_time:38316ms step_avg:93.91ms
step:409/1750 train_time:38413ms step_avg:93.92ms
step:410/1750 train_time:38509ms step_avg:93.92ms
step:411/1750 train_time:38606ms step_avg:93.93ms
step:412/1750 train_time:38702ms step_avg:93.94ms
step:413/1750 train_time:38797ms step_avg:93.94ms
step:414/1750 train_time:38892ms step_avg:93.94ms
step:415/1750 train_time:38987ms step_avg:93.95ms
step:416/1750 train_time:39083ms step_avg:93.95ms
step:417/1750 train_time:39178ms step_avg:93.95ms
step:418/1750 train_time:39274ms step_avg:93.96ms
step:419/1750 train_time:39370ms step_avg:93.96ms
step:420/1750 train_time:39467ms step_avg:93.97ms
step:421/1750 train_time:39563ms step_avg:93.97ms
step:422/1750 train_time:39659ms step_avg:93.98ms
step:423/1750 train_time:39755ms step_avg:93.98ms
step:424/1750 train_time:39851ms step_avg:93.99ms
step:425/1750 train_time:39946ms step_avg:93.99ms
step:426/1750 train_time:40042ms step_avg:93.99ms
step:427/1750 train_time:40137ms step_avg:94.00ms
step:428/1750 train_time:40233ms step_avg:94.00ms
step:429/1750 train_time:40329ms step_avg:94.01ms
step:430/1750 train_time:40425ms step_avg:94.01ms
step:431/1750 train_time:40522ms step_avg:94.02ms
step:432/1750 train_time:40618ms step_avg:94.02ms
step:433/1750 train_time:40714ms step_avg:94.03ms
step:434/1750 train_time:40810ms step_avg:94.03ms
step:435/1750 train_time:40906ms step_avg:94.04ms
step:436/1750 train_time:41001ms step_avg:94.04ms
step:437/1750 train_time:41097ms step_avg:94.04ms
step:438/1750 train_time:41193ms step_avg:94.05ms
step:439/1750 train_time:41288ms step_avg:94.05ms
step:440/1750 train_time:41385ms step_avg:94.06ms
step:441/1750 train_time:41481ms step_avg:94.06ms
step:442/1750 train_time:41576ms step_avg:94.06ms
step:443/1750 train_time:41673ms step_avg:94.07ms
step:444/1750 train_time:41768ms step_avg:94.07ms
step:445/1750 train_time:41864ms step_avg:94.08ms
step:446/1750 train_time:41960ms step_avg:94.08ms
step:447/1750 train_time:42056ms step_avg:94.09ms
step:448/1750 train_time:42152ms step_avg:94.09ms
step:449/1750 train_time:42248ms step_avg:94.09ms
step:450/1750 train_time:42344ms step_avg:94.10ms
step:451/1750 train_time:42440ms step_avg:94.10ms
step:452/1750 train_time:42535ms step_avg:94.10ms
step:453/1750 train_time:42632ms step_avg:94.11ms
step:454/1750 train_time:42728ms step_avg:94.11ms
step:455/1750 train_time:42823ms step_avg:94.12ms
step:456/1750 train_time:42920ms step_avg:94.12ms
step:457/1750 train_time:43015ms step_avg:94.13ms
step:458/1750 train_time:43111ms step_avg:94.13ms
step:459/1750 train_time:43207ms step_avg:94.13ms
step:460/1750 train_time:43304ms step_avg:94.14ms
step:461/1750 train_time:43400ms step_avg:94.14ms
step:462/1750 train_time:43496ms step_avg:94.15ms
step:463/1750 train_time:43592ms step_avg:94.15ms
step:464/1750 train_time:43688ms step_avg:94.15ms
step:465/1750 train_time:43784ms step_avg:94.16ms
step:466/1750 train_time:43880ms step_avg:94.16ms
step:467/1750 train_time:43976ms step_avg:94.17ms
step:468/1750 train_time:44071ms step_avg:94.17ms
step:469/1750 train_time:44168ms step_avg:94.17ms
step:470/1750 train_time:44264ms step_avg:94.18ms
step:471/1750 train_time:44360ms step_avg:94.18ms
step:472/1750 train_time:44456ms step_avg:94.19ms
step:473/1750 train_time:44552ms step_avg:94.19ms
step:474/1750 train_time:44649ms step_avg:94.20ms
step:475/1750 train_time:44744ms step_avg:94.20ms
step:476/1750 train_time:44840ms step_avg:94.20ms
step:477/1750 train_time:44936ms step_avg:94.21ms
step:478/1750 train_time:45032ms step_avg:94.21ms
step:479/1750 train_time:45127ms step_avg:94.21ms
step:480/1750 train_time:45223ms step_avg:94.22ms
step:481/1750 train_time:45319ms step_avg:94.22ms
step:482/1750 train_time:45415ms step_avg:94.22ms
step:483/1750 train_time:45512ms step_avg:94.23ms
step:484/1750 train_time:45608ms step_avg:94.23ms
step:485/1750 train_time:45703ms step_avg:94.23ms
step:486/1750 train_time:45800ms step_avg:94.24ms
step:487/1750 train_time:45897ms step_avg:94.24ms
step:488/1750 train_time:45993ms step_avg:94.25ms
step:489/1750 train_time:46088ms step_avg:94.25ms
step:490/1750 train_time:46184ms step_avg:94.25ms
step:491/1750 train_time:46279ms step_avg:94.26ms
step:492/1750 train_time:46375ms step_avg:94.26ms
step:493/1750 train_time:46472ms step_avg:94.26ms
step:494/1750 train_time:46567ms step_avg:94.27ms
step:495/1750 train_time:46663ms step_avg:94.27ms
step:496/1750 train_time:46760ms step_avg:94.27ms
step:497/1750 train_time:46856ms step_avg:94.28ms
step:498/1750 train_time:46953ms step_avg:94.28ms
step:499/1750 train_time:47049ms step_avg:94.29ms
step:500/1750 train_time:47145ms step_avg:94.29ms
step:500/1750 val_loss:3.7501 train_time:47230ms step_avg:94.46ms
step:501/1750 train_time:47254ms step_avg:94.32ms
step:502/1750 train_time:47346ms step_avg:94.31ms
step:503/1750 train_time:47450ms step_avg:94.33ms
step:504/1750 train_time:47545ms step_avg:94.33ms
step:505/1750 train_time:47640ms step_avg:94.34ms
step:506/1750 train_time:47736ms step_avg:94.34ms
step:507/1750 train_time:47831ms step_avg:94.34ms
step:508/1750 train_time:47926ms step_avg:94.34ms
step:509/1750 train_time:48021ms step_avg:94.34ms
step:510/1750 train_time:48117ms step_avg:94.35ms
step:511/1750 train_time:48213ms step_avg:94.35ms
step:512/1750 train_time:48311ms step_avg:94.36ms
step:513/1750 train_time:48410ms step_avg:94.37ms
step:514/1750 train_time:48506ms step_avg:94.37ms
step:515/1750 train_time:48603ms step_avg:94.37ms
step:516/1750 train_time:48698ms step_avg:94.38ms
step:517/1750 train_time:48794ms step_avg:94.38ms
step:518/1750 train_time:48889ms step_avg:94.38ms
step:519/1750 train_time:48984ms step_avg:94.38ms
step:520/1750 train_time:49079ms step_avg:94.38ms
step:521/1750 train_time:49175ms step_avg:94.39ms
step:522/1750 train_time:49271ms step_avg:94.39ms
step:523/1750 train_time:49369ms step_avg:94.40ms
step:524/1750 train_time:49466ms step_avg:94.40ms
step:525/1750 train_time:49563ms step_avg:94.41ms
step:526/1750 train_time:49660ms step_avg:94.41ms
step:527/1750 train_time:49757ms step_avg:94.41ms
step:528/1750 train_time:49853ms step_avg:94.42ms
step:529/1750 train_time:49949ms step_avg:94.42ms
step:530/1750 train_time:50044ms step_avg:94.42ms
step:531/1750 train_time:50139ms step_avg:94.42ms
step:532/1750 train_time:50236ms step_avg:94.43ms
step:533/1750 train_time:50333ms step_avg:94.43ms
step:534/1750 train_time:50431ms step_avg:94.44ms
step:535/1750 train_time:50528ms step_avg:94.44ms
step:536/1750 train_time:50625ms step_avg:94.45ms
step:537/1750 train_time:50722ms step_avg:94.45ms
step:538/1750 train_time:50818ms step_avg:94.46ms
step:539/1750 train_time:50914ms step_avg:94.46ms
step:540/1750 train_time:51010ms step_avg:94.46ms
step:541/1750 train_time:51105ms step_avg:94.46ms
step:542/1750 train_time:51202ms step_avg:94.47ms
step:543/1750 train_time:51299ms step_avg:94.47ms
step:544/1750 train_time:51395ms step_avg:94.48ms
step:545/1750 train_time:51492ms step_avg:94.48ms
step:546/1750 train_time:51589ms step_avg:94.49ms
step:547/1750 train_time:51686ms step_avg:94.49ms
step:548/1750 train_time:51783ms step_avg:94.49ms
step:549/1750 train_time:51880ms step_avg:94.50ms
step:550/1750 train_time:51975ms step_avg:94.50ms
step:551/1750 train_time:52071ms step_avg:94.50ms
step:552/1750 train_time:52166ms step_avg:94.50ms
step:553/1750 train_time:52263ms step_avg:94.51ms
step:554/1750 train_time:52360ms step_avg:94.51ms
step:555/1750 train_time:52457ms step_avg:94.52ms
step:556/1750 train_time:52553ms step_avg:94.52ms
step:557/1750 train_time:52650ms step_avg:94.52ms
step:558/1750 train_time:52746ms step_avg:94.53ms
step:559/1750 train_time:52843ms step_avg:94.53ms
step:560/1750 train_time:52940ms step_avg:94.54ms
step:561/1750 train_time:53036ms step_avg:94.54ms
step:562/1750 train_time:53132ms step_avg:94.54ms
step:563/1750 train_time:53228ms step_avg:94.54ms
step:564/1750 train_time:53325ms step_avg:94.55ms
step:565/1750 train_time:53422ms step_avg:94.55ms
step:566/1750 train_time:53520ms step_avg:94.56ms
step:567/1750 train_time:53616ms step_avg:94.56ms
step:568/1750 train_time:53713ms step_avg:94.56ms
step:569/1750 train_time:53809ms step_avg:94.57ms
step:570/1750 train_time:53905ms step_avg:94.57ms
step:571/1750 train_time:54002ms step_avg:94.57ms
step:572/1750 train_time:54098ms step_avg:94.58ms
step:573/1750 train_time:54194ms step_avg:94.58ms
step:574/1750 train_time:54290ms step_avg:94.58ms
step:575/1750 train_time:54387ms step_avg:94.59ms
step:576/1750 train_time:54484ms step_avg:94.59ms
step:577/1750 train_time:54580ms step_avg:94.59ms
step:578/1750 train_time:54677ms step_avg:94.60ms
step:579/1750 train_time:54774ms step_avg:94.60ms
step:580/1750 train_time:54870ms step_avg:94.60ms
step:581/1750 train_time:54966ms step_avg:94.61ms
step:582/1750 train_time:55063ms step_avg:94.61ms
step:583/1750 train_time:55159ms step_avg:94.61ms
step:584/1750 train_time:55256ms step_avg:94.62ms
step:585/1750 train_time:55353ms step_avg:94.62ms
step:586/1750 train_time:55450ms step_avg:94.62ms
step:587/1750 train_time:55547ms step_avg:94.63ms
step:588/1750 train_time:55643ms step_avg:94.63ms
step:589/1750 train_time:55740ms step_avg:94.63ms
step:590/1750 train_time:55836ms step_avg:94.64ms
step:591/1750 train_time:55933ms step_avg:94.64ms
step:592/1750 train_time:56029ms step_avg:94.64ms
step:593/1750 train_time:56125ms step_avg:94.65ms
step:594/1750 train_time:56222ms step_avg:94.65ms
step:595/1750 train_time:56319ms step_avg:94.65ms
step:596/1750 train_time:56415ms step_avg:94.66ms
step:597/1750 train_time:56512ms step_avg:94.66ms
step:598/1750 train_time:56608ms step_avg:94.66ms
step:599/1750 train_time:56704ms step_avg:94.66ms
step:600/1750 train_time:56801ms step_avg:94.67ms
step:601/1750 train_time:56897ms step_avg:94.67ms
step:602/1750 train_time:56993ms step_avg:94.67ms
step:603/1750 train_time:57090ms step_avg:94.68ms
step:604/1750 train_time:57186ms step_avg:94.68ms
step:605/1750 train_time:57283ms step_avg:94.68ms
step:606/1750 train_time:57380ms step_avg:94.69ms
step:607/1750 train_time:57477ms step_avg:94.69ms
step:608/1750 train_time:57574ms step_avg:94.69ms
step:609/1750 train_time:57669ms step_avg:94.70ms
step:610/1750 train_time:57767ms step_avg:94.70ms
step:611/1750 train_time:57863ms step_avg:94.70ms
step:612/1750 train_time:57959ms step_avg:94.70ms
step:613/1750 train_time:58057ms step_avg:94.71ms
step:614/1750 train_time:58152ms step_avg:94.71ms
step:615/1750 train_time:58248ms step_avg:94.71ms
step:616/1750 train_time:58345ms step_avg:94.72ms
step:617/1750 train_time:58441ms step_avg:94.72ms
step:618/1750 train_time:58537ms step_avg:94.72ms
step:619/1750 train_time:58634ms step_avg:94.72ms
step:620/1750 train_time:58730ms step_avg:94.73ms
step:621/1750 train_time:58826ms step_avg:94.73ms
step:622/1750 train_time:58923ms step_avg:94.73ms
step:623/1750 train_time:59020ms step_avg:94.74ms
step:624/1750 train_time:59116ms step_avg:94.74ms
step:625/1750 train_time:59212ms step_avg:94.74ms
step:625/1750 val_loss:3.6652 train_time:59297ms step_avg:94.88ms
step:626/1750 train_time:59318ms step_avg:94.76ms
step:627/1750 train_time:59410ms step_avg:94.75ms
step:628/1750 train_time:59510ms step_avg:94.76ms
step:629/1750 train_time:59606ms step_avg:94.76ms
step:630/1750 train_time:59702ms step_avg:94.76ms
step:631/1750 train_time:59797ms step_avg:94.77ms
step:632/1750 train_time:59893ms step_avg:94.77ms
step:633/1750 train_time:59988ms step_avg:94.77ms
step:634/1750 train_time:60083ms step_avg:94.77ms
step:635/1750 train_time:60179ms step_avg:94.77ms
step:636/1750 train_time:60275ms step_avg:94.77ms
step:637/1750 train_time:60374ms step_avg:94.78ms
step:638/1750 train_time:60472ms step_avg:94.78ms
step:639/1750 train_time:60568ms step_avg:94.79ms
step:640/1750 train_time:60664ms step_avg:94.79ms
step:641/1750 train_time:60760ms step_avg:94.79ms
step:642/1750 train_time:60856ms step_avg:94.79ms
step:643/1750 train_time:60952ms step_avg:94.79ms
step:644/1750 train_time:61048ms step_avg:94.79ms
step:645/1750 train_time:61143ms step_avg:94.80ms
step:646/1750 train_time:61240ms step_avg:94.80ms
step:647/1750 train_time:61337ms step_avg:94.80ms
step:648/1750 train_time:61434ms step_avg:94.81ms
step:649/1750 train_time:61532ms step_avg:94.81ms
step:650/1750 train_time:61629ms step_avg:94.81ms
step:651/1750 train_time:61726ms step_avg:94.82ms
step:652/1750 train_time:61823ms step_avg:94.82ms
step:653/1750 train_time:61921ms step_avg:94.83ms
step:654/1750 train_time:62019ms step_avg:94.83ms
step:655/1750 train_time:62117ms step_avg:94.83ms
step:656/1750 train_time:62215ms step_avg:94.84ms
step:657/1750 train_time:62313ms step_avg:94.85ms
step:658/1750 train_time:62411ms step_avg:94.85ms
step:659/1750 train_time:62510ms step_avg:94.86ms
step:660/1750 train_time:62608ms step_avg:94.86ms
step:661/1750 train_time:62705ms step_avg:94.86ms
step:662/1750 train_time:62802ms step_avg:94.87ms
step:663/1750 train_time:62900ms step_avg:94.87ms
step:664/1750 train_time:62997ms step_avg:94.88ms
step:665/1750 train_time:63095ms step_avg:94.88ms
step:666/1750 train_time:63193ms step_avg:94.88ms
step:667/1750 train_time:63290ms step_avg:94.89ms
step:668/1750 train_time:63388ms step_avg:94.89ms
step:669/1750 train_time:63487ms step_avg:94.90ms
step:670/1750 train_time:63585ms step_avg:94.90ms
step:671/1750 train_time:63683ms step_avg:94.91ms
step:672/1750 train_time:63781ms step_avg:94.91ms
step:673/1750 train_time:63878ms step_avg:94.92ms
step:674/1750 train_time:63976ms step_avg:94.92ms
step:675/1750 train_time:64074ms step_avg:94.93ms
step:676/1750 train_time:64172ms step_avg:94.93ms
step:677/1750 train_time:64270ms step_avg:94.93ms
step:678/1750 train_time:64368ms step_avg:94.94ms
step:679/1750 train_time:64466ms step_avg:94.94ms
step:680/1750 train_time:64564ms step_avg:94.95ms
step:681/1750 train_time:64662ms step_avg:94.95ms
step:682/1750 train_time:64760ms step_avg:94.96ms
step:683/1750 train_time:64858ms step_avg:94.96ms
step:684/1750 train_time:64955ms step_avg:94.96ms
step:685/1750 train_time:65053ms step_avg:94.97ms
step:686/1750 train_time:65150ms step_avg:94.97ms
step:687/1750 train_time:65247ms step_avg:94.97ms
step:688/1750 train_time:65345ms step_avg:94.98ms
step:689/1750 train_time:65443ms step_avg:94.98ms
step:690/1750 train_time:65540ms step_avg:94.99ms
step:691/1750 train_time:65639ms step_avg:94.99ms
step:692/1750 train_time:65737ms step_avg:95.00ms
step:693/1750 train_time:65835ms step_avg:95.00ms
step:694/1750 train_time:65933ms step_avg:95.00ms
step:695/1750 train_time:66031ms step_avg:95.01ms
step:696/1750 train_time:66128ms step_avg:95.01ms
step:697/1750 train_time:66225ms step_avg:95.01ms
step:698/1750 train_time:66323ms step_avg:95.02ms
step:699/1750 train_time:66420ms step_avg:95.02ms
step:700/1750 train_time:66518ms step_avg:95.03ms
step:701/1750 train_time:66617ms step_avg:95.03ms
step:702/1750 train_time:66715ms step_avg:95.04ms
step:703/1750 train_time:66813ms step_avg:95.04ms
step:704/1750 train_time:66911ms step_avg:95.04ms
step:705/1750 train_time:67008ms step_avg:95.05ms
step:706/1750 train_time:67106ms step_avg:95.05ms
step:707/1750 train_time:67204ms step_avg:95.06ms
step:708/1750 train_time:67302ms step_avg:95.06ms
step:709/1750 train_time:67401ms step_avg:95.06ms
step:710/1750 train_time:67498ms step_avg:95.07ms
step:711/1750 train_time:67596ms step_avg:95.07ms
step:712/1750 train_time:67694ms step_avg:95.08ms
step:713/1750 train_time:67792ms step_avg:95.08ms
step:714/1750 train_time:67889ms step_avg:95.08ms
step:715/1750 train_time:67987ms step_avg:95.09ms
step:716/1750 train_time:68085ms step_avg:95.09ms
step:717/1750 train_time:68183ms step_avg:95.09ms
step:718/1750 train_time:68280ms step_avg:95.10ms
step:719/1750 train_time:68378ms step_avg:95.10ms
step:720/1750 train_time:68476ms step_avg:95.11ms
step:721/1750 train_time:68574ms step_avg:95.11ms
step:722/1750 train_time:68671ms step_avg:95.11ms
step:723/1750 train_time:68769ms step_avg:95.12ms
step:724/1750 train_time:68866ms step_avg:95.12ms
step:725/1750 train_time:68964ms step_avg:95.12ms
step:726/1750 train_time:69062ms step_avg:95.13ms
step:727/1750 train_time:69160ms step_avg:95.13ms
step:728/1750 train_time:69257ms step_avg:95.13ms
step:729/1750 train_time:69356ms step_avg:95.14ms
step:730/1750 train_time:69453ms step_avg:95.14ms
step:731/1750 train_time:69551ms step_avg:95.15ms
step:732/1750 train_time:69649ms step_avg:95.15ms
step:733/1750 train_time:69746ms step_avg:95.15ms
step:734/1750 train_time:69845ms step_avg:95.16ms
step:735/1750 train_time:69943ms step_avg:95.16ms
step:736/1750 train_time:70041ms step_avg:95.16ms
step:737/1750 train_time:70139ms step_avg:95.17ms
step:738/1750 train_time:70236ms step_avg:95.17ms
step:739/1750 train_time:70334ms step_avg:95.18ms
step:740/1750 train_time:70432ms step_avg:95.18ms
step:741/1750 train_time:70529ms step_avg:95.18ms
step:742/1750 train_time:70627ms step_avg:95.18ms
step:743/1750 train_time:70725ms step_avg:95.19ms
step:744/1750 train_time:70822ms step_avg:95.19ms
step:745/1750 train_time:70920ms step_avg:95.19ms
step:746/1750 train_time:71018ms step_avg:95.20ms
step:747/1750 train_time:71116ms step_avg:95.20ms
step:748/1750 train_time:71214ms step_avg:95.21ms
step:749/1750 train_time:71313ms step_avg:95.21ms
step:750/1750 train_time:71410ms step_avg:95.21ms
step:750/1750 val_loss:3.6006 train_time:71497ms step_avg:95.33ms
step:751/1750 train_time:71518ms step_avg:95.23ms
step:752/1750 train_time:71614ms step_avg:95.23ms
step:753/1750 train_time:71713ms step_avg:95.24ms
step:754/1750 train_time:71811ms step_avg:95.24ms
step:755/1750 train_time:71909ms step_avg:95.24ms
step:756/1750 train_time:72007ms step_avg:95.25ms
step:757/1750 train_time:72103ms step_avg:95.25ms
step:758/1750 train_time:72199ms step_avg:95.25ms
step:759/1750 train_time:72297ms step_avg:95.25ms
step:760/1750 train_time:72394ms step_avg:95.26ms
step:761/1750 train_time:72493ms step_avg:95.26ms
step:762/1750 train_time:72592ms step_avg:95.27ms
step:763/1750 train_time:72691ms step_avg:95.27ms
step:764/1750 train_time:72789ms step_avg:95.27ms
step:765/1750 train_time:72887ms step_avg:95.28ms
step:766/1750 train_time:72985ms step_avg:95.28ms
step:767/1750 train_time:73083ms step_avg:95.28ms
step:768/1750 train_time:73180ms step_avg:95.29ms
step:769/1750 train_time:73277ms step_avg:95.29ms
step:770/1750 train_time:73374ms step_avg:95.29ms
step:771/1750 train_time:73472ms step_avg:95.29ms
step:772/1750 train_time:73569ms step_avg:95.30ms
step:773/1750 train_time:73669ms step_avg:95.30ms
step:774/1750 train_time:73767ms step_avg:95.31ms
step:775/1750 train_time:73865ms step_avg:95.31ms
step:776/1750 train_time:73963ms step_avg:95.31ms
step:777/1750 train_time:74060ms step_avg:95.32ms
step:778/1750 train_time:74158ms step_avg:95.32ms
step:779/1750 train_time:74256ms step_avg:95.32ms
step:780/1750 train_time:74354ms step_avg:95.33ms
step:781/1750 train_time:74452ms step_avg:95.33ms
step:782/1750 train_time:74550ms step_avg:95.33ms
step:783/1750 train_time:74648ms step_avg:95.34ms
step:784/1750 train_time:74746ms step_avg:95.34ms
step:785/1750 train_time:74844ms step_avg:95.34ms
step:786/1750 train_time:74943ms step_avg:95.35ms
step:787/1750 train_time:75041ms step_avg:95.35ms
step:788/1750 train_time:75139ms step_avg:95.35ms
step:789/1750 train_time:75236ms step_avg:95.36ms
step:790/1750 train_time:75334ms step_avg:95.36ms
step:791/1750 train_time:75431ms step_avg:95.36ms
step:792/1750 train_time:75529ms step_avg:95.37ms
step:793/1750 train_time:75628ms step_avg:95.37ms
step:794/1750 train_time:75727ms step_avg:95.37ms
step:795/1750 train_time:75825ms step_avg:95.38ms
step:796/1750 train_time:75923ms step_avg:95.38ms
step:797/1750 train_time:76021ms step_avg:95.38ms
step:798/1750 train_time:76119ms step_avg:95.39ms
step:799/1750 train_time:76217ms step_avg:95.39ms
step:800/1750 train_time:76315ms step_avg:95.39ms
step:801/1750 train_time:76412ms step_avg:95.40ms
step:802/1750 train_time:76510ms step_avg:95.40ms
step:803/1750 train_time:76608ms step_avg:95.40ms
step:804/1750 train_time:76706ms step_avg:95.41ms
step:805/1750 train_time:76804ms step_avg:95.41ms
step:806/1750 train_time:76902ms step_avg:95.41ms
step:807/1750 train_time:77000ms step_avg:95.42ms
step:808/1750 train_time:77098ms step_avg:95.42ms
step:809/1750 train_time:77196ms step_avg:95.42ms
step:810/1750 train_time:77294ms step_avg:95.42ms
step:811/1750 train_time:77393ms step_avg:95.43ms
step:812/1750 train_time:77491ms step_avg:95.43ms
step:813/1750 train_time:77589ms step_avg:95.44ms
step:814/1750 train_time:77687ms step_avg:95.44ms
step:815/1750 train_time:77785ms step_avg:95.44ms
step:816/1750 train_time:77882ms step_avg:95.44ms
step:817/1750 train_time:77980ms step_avg:95.45ms
step:818/1750 train_time:78078ms step_avg:95.45ms
step:819/1750 train_time:78176ms step_avg:95.45ms
step:820/1750 train_time:78274ms step_avg:95.46ms
step:821/1750 train_time:78372ms step_avg:95.46ms
step:822/1750 train_time:78470ms step_avg:95.46ms
step:823/1750 train_time:78568ms step_avg:95.46ms
step:824/1750 train_time:78665ms step_avg:95.47ms
step:825/1750 train_time:78763ms step_avg:95.47ms
step:826/1750 train_time:78861ms step_avg:95.47ms
step:827/1750 train_time:78959ms step_avg:95.48ms
step:828/1750 train_time:79057ms step_avg:95.48ms
step:829/1750 train_time:79156ms step_avg:95.48ms
step:830/1750 train_time:79254ms step_avg:95.49ms
step:831/1750 train_time:79353ms step_avg:95.49ms
step:832/1750 train_time:79451ms step_avg:95.49ms
step:833/1750 train_time:79548ms step_avg:95.50ms
step:834/1750 train_time:79646ms step_avg:95.50ms
step:835/1750 train_time:79744ms step_avg:95.50ms
step:836/1750 train_time:79842ms step_avg:95.51ms
step:837/1750 train_time:79940ms step_avg:95.51ms
step:838/1750 train_time:80039ms step_avg:95.51ms
step:839/1750 train_time:80136ms step_avg:95.51ms
step:840/1750 train_time:80234ms step_avg:95.52ms
step:841/1750 train_time:80332ms step_avg:95.52ms
step:842/1750 train_time:80430ms step_avg:95.52ms
step:843/1750 train_time:80529ms step_avg:95.53ms
step:844/1750 train_time:80627ms step_avg:95.53ms
step:845/1750 train_time:80725ms step_avg:95.53ms
step:846/1750 train_time:80822ms step_avg:95.53ms
step:847/1750 train_time:80920ms step_avg:95.54ms
step:848/1750 train_time:81019ms step_avg:95.54ms
step:849/1750 train_time:81117ms step_avg:95.54ms
step:850/1750 train_time:81214ms step_avg:95.55ms
step:851/1750 train_time:81311ms step_avg:95.55ms
step:852/1750 train_time:81411ms step_avg:95.55ms
step:853/1750 train_time:81509ms step_avg:95.56ms
step:854/1750 train_time:81606ms step_avg:95.56ms
step:855/1750 train_time:81704ms step_avg:95.56ms
step:856/1750 train_time:81802ms step_avg:95.56ms
step:857/1750 train_time:81900ms step_avg:95.57ms
step:858/1750 train_time:81999ms step_avg:95.57ms
step:859/1750 train_time:82097ms step_avg:95.57ms
step:860/1750 train_time:82195ms step_avg:95.58ms
step:861/1750 train_time:82293ms step_avg:95.58ms
step:862/1750 train_time:82391ms step_avg:95.58ms
step:863/1750 train_time:82489ms step_avg:95.58ms
step:864/1750 train_time:82587ms step_avg:95.59ms
step:865/1750 train_time:82685ms step_avg:95.59ms
step:866/1750 train_time:82783ms step_avg:95.59ms
step:867/1750 train_time:82881ms step_avg:95.59ms
step:868/1750 train_time:82979ms step_avg:95.60ms
step:869/1750 train_time:83077ms step_avg:95.60ms
step:870/1750 train_time:83174ms step_avg:95.60ms
step:871/1750 train_time:83272ms step_avg:95.61ms
step:872/1750 train_time:83370ms step_avg:95.61ms
step:873/1750 train_time:83468ms step_avg:95.61ms
step:874/1750 train_time:83566ms step_avg:95.61ms
step:875/1750 train_time:83664ms step_avg:95.62ms
step:875/1750 val_loss:3.5504 train_time:83751ms step_avg:95.72ms
step:876/1750 train_time:83773ms step_avg:95.63ms
step:877/1750 train_time:83869ms step_avg:95.63ms
step:878/1750 train_time:83968ms step_avg:95.64ms
step:879/1750 train_time:84066ms step_avg:95.64ms
step:880/1750 train_time:84164ms step_avg:95.64ms
step:881/1750 train_time:84261ms step_avg:95.64ms
step:882/1750 train_time:84359ms step_avg:95.65ms
step:883/1750 train_time:84457ms step_avg:95.65ms
step:884/1750 train_time:84554ms step_avg:95.65ms
step:885/1750 train_time:84651ms step_avg:95.65ms
step:886/1750 train_time:84750ms step_avg:95.65ms
step:887/1750 train_time:84851ms step_avg:95.66ms
step:888/1750 train_time:84950ms step_avg:95.66ms
step:889/1750 train_time:85048ms step_avg:95.67ms
step:890/1750 train_time:85146ms step_avg:95.67ms
step:891/1750 train_time:85244ms step_avg:95.67ms
step:892/1750 train_time:85341ms step_avg:95.67ms
step:893/1750 train_time:85438ms step_avg:95.68ms
step:894/1750 train_time:85535ms step_avg:95.68ms
step:895/1750 train_time:85633ms step_avg:95.68ms
step:896/1750 train_time:85731ms step_avg:95.68ms
step:897/1750 train_time:85831ms step_avg:95.69ms
step:898/1750 train_time:85930ms step_avg:95.69ms
step:899/1750 train_time:86030ms step_avg:95.70ms
step:900/1750 train_time:86128ms step_avg:95.70ms
step:901/1750 train_time:86226ms step_avg:95.70ms
step:902/1750 train_time:86324ms step_avg:95.70ms
step:903/1750 train_time:86421ms step_avg:95.70ms
step:904/1750 train_time:86519ms step_avg:95.71ms
step:905/1750 train_time:86616ms step_avg:95.71ms
step:906/1750 train_time:86714ms step_avg:95.71ms
step:907/1750 train_time:86812ms step_avg:95.71ms
step:908/1750 train_time:86911ms step_avg:95.72ms
step:909/1750 train_time:87009ms step_avg:95.72ms
step:910/1750 train_time:87108ms step_avg:95.72ms
step:911/1750 train_time:87207ms step_avg:95.73ms
step:912/1750 train_time:87307ms step_avg:95.73ms
step:913/1750 train_time:87406ms step_avg:95.74ms
step:914/1750 train_time:87506ms step_avg:95.74ms
step:915/1750 train_time:87605ms step_avg:95.74ms
step:916/1750 train_time:87706ms step_avg:95.75ms
step:917/1750 train_time:87806ms step_avg:95.75ms
step:918/1750 train_time:87907ms step_avg:95.76ms
step:919/1750 train_time:88006ms step_avg:95.76ms
step:920/1750 train_time:88107ms step_avg:95.77ms
step:921/1750 train_time:88205ms step_avg:95.77ms
step:922/1750 train_time:88305ms step_avg:95.78ms
step:923/1750 train_time:88405ms step_avg:95.78ms
step:924/1750 train_time:88504ms step_avg:95.78ms
step:925/1750 train_time:88603ms step_avg:95.79ms
step:926/1750 train_time:88704ms step_avg:95.79ms
step:927/1750 train_time:88804ms step_avg:95.80ms
step:928/1750 train_time:88904ms step_avg:95.80ms
step:929/1750 train_time:89004ms step_avg:95.81ms
step:930/1750 train_time:89104ms step_avg:95.81ms
step:931/1750 train_time:89204ms step_avg:95.81ms
step:932/1750 train_time:89304ms step_avg:95.82ms
step:933/1750 train_time:89403ms step_avg:95.82ms
step:934/1750 train_time:89503ms step_avg:95.83ms
step:935/1750 train_time:89603ms step_avg:95.83ms
step:936/1750 train_time:89703ms step_avg:95.84ms
step:937/1750 train_time:89803ms step_avg:95.84ms
step:938/1750 train_time:89903ms step_avg:95.85ms
step:939/1750 train_time:90003ms step_avg:95.85ms
step:940/1750 train_time:90103ms step_avg:95.85ms
step:941/1750 train_time:90204ms step_avg:95.86ms
step:942/1750 train_time:90304ms step_avg:95.86ms
step:943/1750 train_time:90403ms step_avg:95.87ms
step:944/1750 train_time:90503ms step_avg:95.87ms
step:945/1750 train_time:90602ms step_avg:95.87ms
step:946/1750 train_time:90701ms step_avg:95.88ms
step:947/1750 train_time:90801ms step_avg:95.88ms
step:948/1750 train_time:90900ms step_avg:95.89ms
step:949/1750 train_time:91001ms step_avg:95.89ms
step:950/1750 train_time:91101ms step_avg:95.90ms
step:951/1750 train_time:91201ms step_avg:95.90ms
step:952/1750 train_time:91301ms step_avg:95.90ms
step:953/1750 train_time:91401ms step_avg:95.91ms
step:954/1750 train_time:91501ms step_avg:95.91ms
step:955/1750 train_time:91600ms step_avg:95.92ms
step:956/1750 train_time:91699ms step_avg:95.92ms
step:957/1750 train_time:91799ms step_avg:95.92ms
step:958/1750 train_time:91898ms step_avg:95.93ms
step:959/1750 train_time:91999ms step_avg:95.93ms
step:960/1750 train_time:92098ms step_avg:95.94ms
step:961/1750 train_time:92199ms step_avg:95.94ms
step:962/1750 train_time:92299ms step_avg:95.95ms
step:963/1750 train_time:92398ms step_avg:95.95ms
step:964/1750 train_time:92498ms step_avg:95.95ms
step:965/1750 train_time:92598ms step_avg:95.96ms
step:966/1750 train_time:92697ms step_avg:95.96ms
step:967/1750 train_time:92796ms step_avg:95.96ms
step:968/1750 train_time:92895ms step_avg:95.97ms
step:969/1750 train_time:92995ms step_avg:95.97ms
step:970/1750 train_time:93095ms step_avg:95.97ms
step:971/1750 train_time:93194ms step_avg:95.98ms
step:972/1750 train_time:93294ms step_avg:95.98ms
step:973/1750 train_time:93394ms step_avg:95.99ms
step:974/1750 train_time:93493ms step_avg:95.99ms
step:975/1750 train_time:93593ms step_avg:95.99ms
step:976/1750 train_time:93692ms step_avg:96.00ms
step:977/1750 train_time:93792ms step_avg:96.00ms
step:978/1750 train_time:93891ms step_avg:96.00ms
step:979/1750 train_time:93991ms step_avg:96.01ms
step:980/1750 train_time:94090ms step_avg:96.01ms
step:981/1750 train_time:94190ms step_avg:96.01ms
step:982/1750 train_time:94291ms step_avg:96.02ms
step:983/1750 train_time:94391ms step_avg:96.02ms
step:984/1750 train_time:94491ms step_avg:96.03ms
step:985/1750 train_time:94589ms step_avg:96.03ms
step:986/1750 train_time:94688ms step_avg:96.03ms
step:987/1750 train_time:94789ms step_avg:96.04ms
step:988/1750 train_time:94888ms step_avg:96.04ms
step:989/1750 train_time:94987ms step_avg:96.04ms
step:990/1750 train_time:95085ms step_avg:96.05ms
step:991/1750 train_time:95185ms step_avg:96.05ms
step:992/1750 train_time:95284ms step_avg:96.05ms
step:993/1750 train_time:95384ms step_avg:96.06ms
step:994/1750 train_time:95483ms step_avg:96.06ms
step:995/1750 train_time:95583ms step_avg:96.06ms
step:996/1750 train_time:95685ms step_avg:96.07ms
step:997/1750 train_time:95785ms step_avg:96.07ms
step:998/1750 train_time:95884ms step_avg:96.08ms
step:999/1750 train_time:95984ms step_avg:96.08ms
step:1000/1750 train_time:96084ms step_avg:96.08ms
step:1000/1750 val_loss:3.5098 train_time:96172ms step_avg:96.17ms
step:1001/1750 train_time:96193ms step_avg:96.10ms
step:1002/1750 train_time:96296ms step_avg:96.10ms
step:1003/1750 train_time:96396ms step_avg:96.11ms
step:1004/1750 train_time:96496ms step_avg:96.11ms
step:1005/1750 train_time:96595ms step_avg:96.11ms
step:1006/1750 train_time:96693ms step_avg:96.12ms
step:1007/1750 train_time:96791ms step_avg:96.12ms
step:1008/1750 train_time:96890ms step_avg:96.12ms
step:1009/1750 train_time:96988ms step_avg:96.12ms
step:1010/1750 train_time:97086ms step_avg:96.13ms
step:1011/1750 train_time:97187ms step_avg:96.13ms
step:1012/1750 train_time:97288ms step_avg:96.13ms
step:1013/1750 train_time:97388ms step_avg:96.14ms
step:1014/1750 train_time:97489ms step_avg:96.14ms
step:1015/1750 train_time:97588ms step_avg:96.15ms
step:1016/1750 train_time:97687ms step_avg:96.15ms
step:1017/1750 train_time:97786ms step_avg:96.15ms
step:1018/1750 train_time:97885ms step_avg:96.15ms
step:1019/1750 train_time:97983ms step_avg:96.16ms
step:1020/1750 train_time:98082ms step_avg:96.16ms
step:1021/1750 train_time:98182ms step_avg:96.16ms
step:1022/1750 train_time:98281ms step_avg:96.17ms
step:1023/1750 train_time:98382ms step_avg:96.17ms
step:1024/1750 train_time:98484ms step_avg:96.18ms
step:1025/1750 train_time:98584ms step_avg:96.18ms
step:1026/1750 train_time:98683ms step_avg:96.18ms
step:1027/1750 train_time:98783ms step_avg:96.19ms
step:1028/1750 train_time:98881ms step_avg:96.19ms
step:1029/1750 train_time:98981ms step_avg:96.19ms
step:1030/1750 train_time:99079ms step_avg:96.19ms
step:1031/1750 train_time:99179ms step_avg:96.20ms
step:1032/1750 train_time:99278ms step_avg:96.20ms
step:1033/1750 train_time:99378ms step_avg:96.20ms
step:1034/1750 train_time:99478ms step_avg:96.21ms
step:1035/1750 train_time:99578ms step_avg:96.21ms
step:1036/1750 train_time:99679ms step_avg:96.22ms
step:1037/1750 train_time:99779ms step_avg:96.22ms
step:1038/1750 train_time:99879ms step_avg:96.22ms
step:1039/1750 train_time:99978ms step_avg:96.22ms
step:1040/1750 train_time:100078ms step_avg:96.23ms
step:1041/1750 train_time:100177ms step_avg:96.23ms
step:1042/1750 train_time:100276ms step_avg:96.23ms
step:1043/1750 train_time:100376ms step_avg:96.24ms
step:1044/1750 train_time:100476ms step_avg:96.24ms
step:1045/1750 train_time:100577ms step_avg:96.25ms
step:1046/1750 train_time:100678ms step_avg:96.25ms
step:1047/1750 train_time:100777ms step_avg:96.25ms
step:1048/1750 train_time:100876ms step_avg:96.26ms
step:1049/1750 train_time:100976ms step_avg:96.26ms
step:1050/1750 train_time:101076ms step_avg:96.26ms
step:1051/1750 train_time:101440ms step_avg:96.52ms
step:1052/1750 train_time:101539ms step_avg:96.52ms
step:1053/1750 train_time:101637ms step_avg:96.52ms
step:1054/1750 train_time:101736ms step_avg:96.52ms
step:1055/1750 train_time:101835ms step_avg:96.53ms
step:1056/1750 train_time:101933ms step_avg:96.53ms
step:1057/1750 train_time:102031ms step_avg:96.53ms
step:1058/1750 train_time:102129ms step_avg:96.53ms
step:1059/1750 train_time:102227ms step_avg:96.53ms
step:1060/1750 train_time:102328ms step_avg:96.54ms
step:1061/1750 train_time:102431ms step_avg:96.54ms
step:1062/1750 train_time:102530ms step_avg:96.54ms
step:1063/1750 train_time:102630ms step_avg:96.55ms
step:1064/1750 train_time:102731ms step_avg:96.55ms
step:1065/1750 train_time:102830ms step_avg:96.55ms
step:1066/1750 train_time:102929ms step_avg:96.56ms
step:1067/1750 train_time:103028ms step_avg:96.56ms
step:1068/1750 train_time:103127ms step_avg:96.56ms
step:1069/1750 train_time:103226ms step_avg:96.56ms
step:1070/1750 train_time:103326ms step_avg:96.57ms
step:1071/1750 train_time:103425ms step_avg:96.57ms
step:1072/1750 train_time:103526ms step_avg:96.57ms
step:1073/1750 train_time:103626ms step_avg:96.58ms
step:1074/1750 train_time:103726ms step_avg:96.58ms
step:1075/1750 train_time:103826ms step_avg:96.58ms
step:1076/1750 train_time:103926ms step_avg:96.59ms
step:1077/1750 train_time:104026ms step_avg:96.59ms
step:1078/1750 train_time:104124ms step_avg:96.59ms
step:1079/1750 train_time:104223ms step_avg:96.59ms
step:1080/1750 train_time:104323ms step_avg:96.60ms
step:1081/1750 train_time:104423ms step_avg:96.60ms
step:1082/1750 train_time:104522ms step_avg:96.60ms
step:1083/1750 train_time:104622ms step_avg:96.60ms
step:1084/1750 train_time:104723ms step_avg:96.61ms
step:1085/1750 train_time:104824ms step_avg:96.61ms
step:1086/1750 train_time:104923ms step_avg:96.61ms
step:1087/1750 train_time:105022ms step_avg:96.62ms
step:1088/1750 train_time:105122ms step_avg:96.62ms
step:1089/1750 train_time:105222ms step_avg:96.62ms
step:1090/1750 train_time:105322ms step_avg:96.63ms
step:1091/1750 train_time:105422ms step_avg:96.63ms
step:1092/1750 train_time:105521ms step_avg:96.63ms
step:1093/1750 train_time:105621ms step_avg:96.63ms
step:1094/1750 train_time:105721ms step_avg:96.64ms
step:1095/1750 train_time:105821ms step_avg:96.64ms
step:1096/1750 train_time:105921ms step_avg:96.64ms
step:1097/1750 train_time:106021ms step_avg:96.65ms
step:1098/1750 train_time:106122ms step_avg:96.65ms
step:1099/1750 train_time:106222ms step_avg:96.65ms
step:1100/1750 train_time:106322ms step_avg:96.66ms
step:1101/1750 train_time:106422ms step_avg:96.66ms
step:1102/1750 train_time:106522ms step_avg:96.66ms
step:1103/1750 train_time:106621ms step_avg:96.66ms
step:1104/1750 train_time:106722ms step_avg:96.67ms
step:1105/1750 train_time:106821ms step_avg:96.67ms
step:1106/1750 train_time:106922ms step_avg:96.67ms
step:1107/1750 train_time:107021ms step_avg:96.68ms
step:1108/1750 train_time:107121ms step_avg:96.68ms
step:1109/1750 train_time:107221ms step_avg:96.68ms
step:1110/1750 train_time:107321ms step_avg:96.69ms
step:1111/1750 train_time:107421ms step_avg:96.69ms
step:1112/1750 train_time:107522ms step_avg:96.69ms
step:1113/1750 train_time:107623ms step_avg:96.70ms
step:1114/1750 train_time:107723ms step_avg:96.70ms
step:1115/1750 train_time:107822ms step_avg:96.70ms
step:1116/1750 train_time:107922ms step_avg:96.70ms
step:1117/1750 train_time:108022ms step_avg:96.71ms
step:1118/1750 train_time:108120ms step_avg:96.71ms
step:1119/1750 train_time:108221ms step_avg:96.71ms
step:1120/1750 train_time:108321ms step_avg:96.72ms
step:1121/1750 train_time:108422ms step_avg:96.72ms
step:1122/1750 train_time:108522ms step_avg:96.72ms
step:1123/1750 train_time:108621ms step_avg:96.72ms
step:1124/1750 train_time:108721ms step_avg:96.73ms
step:1125/1750 train_time:108821ms step_avg:96.73ms
step:1125/1750 val_loss:3.4587 train_time:108910ms step_avg:96.81ms
step:1126/1750 train_time:108932ms step_avg:96.74ms
step:1127/1750 train_time:109031ms step_avg:96.74ms
step:1128/1750 train_time:109131ms step_avg:96.75ms
step:1129/1750 train_time:109231ms step_avg:96.75ms
step:1130/1750 train_time:109330ms step_avg:96.75ms
step:1131/1750 train_time:109429ms step_avg:96.75ms
step:1132/1750 train_time:109528ms step_avg:96.76ms
step:1133/1750 train_time:109627ms step_avg:96.76ms
step:1134/1750 train_time:109725ms step_avg:96.76ms
step:1135/1750 train_time:109824ms step_avg:96.76ms
step:1136/1750 train_time:109925ms step_avg:96.76ms
step:1137/1750 train_time:110025ms step_avg:96.77ms
step:1138/1750 train_time:110126ms step_avg:96.77ms
step:1139/1750 train_time:110226ms step_avg:96.77ms
step:1140/1750 train_time:110325ms step_avg:96.78ms
step:1141/1750 train_time:110424ms step_avg:96.78ms
step:1142/1750 train_time:110523ms step_avg:96.78ms
step:1143/1750 train_time:110622ms step_avg:96.78ms
step:1144/1750 train_time:110720ms step_avg:96.78ms
step:1145/1750 train_time:110819ms step_avg:96.79ms
step:1146/1750 train_time:110919ms step_avg:96.79ms
step:1147/1750 train_time:111020ms step_avg:96.79ms
step:1148/1750 train_time:111118ms step_avg:96.79ms
step:1149/1750 train_time:111219ms step_avg:96.80ms
step:1150/1750 train_time:111318ms step_avg:96.80ms
step:1151/1750 train_time:111418ms step_avg:96.80ms
step:1152/1750 train_time:111517ms step_avg:96.80ms
step:1153/1750 train_time:111617ms step_avg:96.81ms
step:1154/1750 train_time:111716ms step_avg:96.81ms
step:1155/1750 train_time:111816ms step_avg:96.81ms
step:1156/1750 train_time:111916ms step_avg:96.81ms
step:1157/1750 train_time:112016ms step_avg:96.82ms
step:1158/1750 train_time:112117ms step_avg:96.82ms
step:1159/1750 train_time:112469ms step_avg:97.04ms
step:1160/1750 train_time:112568ms step_avg:97.04ms
step:1161/1750 train_time:112667ms step_avg:97.04ms
step:1162/1750 train_time:112765ms step_avg:97.04ms
step:1163/1750 train_time:112863ms step_avg:97.04ms
step:1164/1750 train_time:112962ms step_avg:97.05ms
step:1165/1750 train_time:113061ms step_avg:97.05ms
step:1166/1750 train_time:113159ms step_avg:97.05ms
step:1167/1750 train_time:113514ms step_avg:97.27ms
step:1168/1750 train_time:113613ms step_avg:97.27ms
step:1169/1750 train_time:113986ms step_avg:97.51ms
step:1170/1750 train_time:114084ms step_avg:97.51ms
step:1171/1750 train_time:114183ms step_avg:97.51ms
step:1172/1750 train_time:114282ms step_avg:97.51ms
step:1173/1750 train_time:114382ms step_avg:97.51ms
step:1174/1750 train_time:114481ms step_avg:97.51ms
step:1175/1750 train_time:114580ms step_avg:97.52ms
step:1176/1750 train_time:114970ms step_avg:97.76ms
step:1177/1750 train_time:115068ms step_avg:97.76ms
step:1178/1750 train_time:115167ms step_avg:97.76ms
step:1179/1750 train_time:115270ms step_avg:97.77ms
step:1180/1750 train_time:115370ms step_avg:97.77ms
step:1181/1750 train_time:115470ms step_avg:97.77ms
step:1182/1750 train_time:115569ms step_avg:97.77ms
step:1183/1750 train_time:115668ms step_avg:97.78ms
step:1184/1750 train_time:115769ms step_avg:97.78ms
step:1185/1750 train_time:115874ms step_avg:97.78ms
step:1186/1750 train_time:115977ms step_avg:97.79ms
step:1187/1750 train_time:116077ms step_avg:97.79ms
step:1188/1750 train_time:116178ms step_avg:97.79ms
step:1189/1750 train_time:116278ms step_avg:97.80ms
step:1190/1750 train_time:116380ms step_avg:97.80ms
step:1191/1750 train_time:116480ms step_avg:97.80ms
step:1192/1750 train_time:116579ms step_avg:97.80ms
step:1193/1750 train_time:116679ms step_avg:97.80ms
step:1194/1750 train_time:116780ms step_avg:97.81ms
step:1195/1750 train_time:116881ms step_avg:97.81ms
step:1196/1750 train_time:116981ms step_avg:97.81ms
step:1197/1750 train_time:117083ms step_avg:97.81ms
step:1198/1750 train_time:117184ms step_avg:97.82ms
step:1199/1750 train_time:117285ms step_avg:97.82ms
step:1200/1750 train_time:117384ms step_avg:97.82ms
step:1201/1750 train_time:117485ms step_avg:97.82ms
step:1202/1750 train_time:117586ms step_avg:97.83ms
step:1203/1750 train_time:117686ms step_avg:97.83ms
step:1204/1750 train_time:117786ms step_avg:97.83ms
step:1205/1750 train_time:117886ms step_avg:97.83ms
step:1206/1750 train_time:117987ms step_avg:97.83ms
step:1207/1750 train_time:118089ms step_avg:97.84ms
step:1208/1750 train_time:118190ms step_avg:97.84ms
step:1209/1750 train_time:118291ms step_avg:97.84ms
step:1210/1750 train_time:118392ms step_avg:97.84ms
step:1211/1750 train_time:118492ms step_avg:97.85ms
step:1212/1750 train_time:118593ms step_avg:97.85ms
step:1213/1750 train_time:118694ms step_avg:97.85ms
step:1214/1750 train_time:118794ms step_avg:97.85ms
step:1215/1750 train_time:118895ms step_avg:97.86ms
step:1216/1750 train_time:119391ms step_avg:98.18ms
step:1217/1750 train_time:119454ms step_avg:98.15ms
step:1218/1750 train_time:119554ms step_avg:98.16ms
step:1219/1750 train_time:119653ms step_avg:98.16ms
step:1220/1750 train_time:119753ms step_avg:98.16ms
step:1221/1750 train_time:119853ms step_avg:98.16ms
step:1222/1750 train_time:119953ms step_avg:98.16ms
step:1223/1750 train_time:120352ms step_avg:98.41ms
step:1224/1750 train_time:120450ms step_avg:98.41ms
step:1225/1750 train_time:120550ms step_avg:98.41ms
step:1226/1750 train_time:120649ms step_avg:98.41ms
step:1227/1750 train_time:120749ms step_avg:98.41ms
step:1228/1750 train_time:120848ms step_avg:98.41ms
step:1229/1750 train_time:120947ms step_avg:98.41ms
step:1230/1750 train_time:121046ms step_avg:98.41ms
step:1231/1750 train_time:121146ms step_avg:98.41ms
step:1232/1750 train_time:121252ms step_avg:98.42ms
step:1233/1750 train_time:121645ms step_avg:98.66ms
step:1234/1750 train_time:121744ms step_avg:98.66ms
step:1235/1750 train_time:121844ms step_avg:98.66ms
step:1236/1750 train_time:121943ms step_avg:98.66ms
step:1237/1750 train_time:122043ms step_avg:98.66ms
step:1238/1750 train_time:122142ms step_avg:98.66ms
step:1239/1750 train_time:122242ms step_avg:98.66ms
step:1240/1750 train_time:122340ms step_avg:98.66ms
step:1241/1750 train_time:122441ms step_avg:98.66ms
step:1242/1750 train_time:122543ms step_avg:98.67ms
step:1243/1750 train_time:122647ms step_avg:98.67ms
step:1244/1750 train_time:122747ms step_avg:98.67ms
step:1245/1750 train_time:122847ms step_avg:98.67ms
step:1246/1750 train_time:122948ms step_avg:98.67ms
step:1247/1750 train_time:123048ms step_avg:98.68ms
step:1248/1750 train_time:123149ms step_avg:98.68ms
step:1249/1750 train_time:123249ms step_avg:98.68ms
step:1250/1750 train_time:123350ms step_avg:98.68ms
step:1250/1750 val_loss:3.4142 train_time:123439ms step_avg:98.75ms
step:1251/1750 train_time:123461ms step_avg:98.69ms
step:1252/1750 train_time:123562ms step_avg:98.69ms
step:1253/1750 train_time:123663ms step_avg:98.69ms
step:1254/1750 train_time:123764ms step_avg:98.70ms
step:1255/1750 train_time:123864ms step_avg:98.70ms
step:1256/1750 train_time:123965ms step_avg:98.70ms
step:1257/1750 train_time:124065ms step_avg:98.70ms
step:1258/1750 train_time:124165ms step_avg:98.70ms
step:1259/1750 train_time:124265ms step_avg:98.70ms
step:1260/1750 train_time:124367ms step_avg:98.70ms
step:1261/1750 train_time:124471ms step_avg:98.71ms
step:1262/1750 train_time:124573ms step_avg:98.71ms
step:1263/1750 train_time:124674ms step_avg:98.71ms
step:1264/1750 train_time:124774ms step_avg:98.71ms
step:1265/1750 train_time:124874ms step_avg:98.71ms
step:1266/1750 train_time:124976ms step_avg:98.72ms
step:1267/1750 train_time:125078ms step_avg:98.72ms
step:1268/1750 train_time:125178ms step_avg:98.72ms
step:1269/1750 train_time:125279ms step_avg:98.72ms
step:1270/1750 train_time:125380ms step_avg:98.72ms
step:1271/1750 train_time:125483ms step_avg:98.73ms
step:1272/1750 train_time:125584ms step_avg:98.73ms
step:1273/1750 train_time:125684ms step_avg:98.73ms
step:1274/1750 train_time:125784ms step_avg:98.73ms
step:1275/1750 train_time:125885ms step_avg:98.73ms
step:1276/1750 train_time:125986ms step_avg:98.73ms
step:1277/1750 train_time:126086ms step_avg:98.74ms
step:1278/1750 train_time:126186ms step_avg:98.74ms
step:1279/1750 train_time:126288ms step_avg:98.74ms
step:1280/1750 train_time:126389ms step_avg:98.74ms
step:1281/1750 train_time:126490ms step_avg:98.74ms
step:1282/1750 train_time:126591ms step_avg:98.75ms
step:1283/1750 train_time:126690ms step_avg:98.75ms
step:1284/1750 train_time:126791ms step_avg:98.75ms
step:1285/1750 train_time:126892ms step_avg:98.75ms
step:1286/1750 train_time:126992ms step_avg:98.75ms
step:1287/1750 train_time:127094ms step_avg:98.75ms
step:1288/1750 train_time:127195ms step_avg:98.75ms
step:1289/1750 train_time:127296ms step_avg:98.76ms
step:1290/1750 train_time:127398ms step_avg:98.76ms
step:1291/1750 train_time:127499ms step_avg:98.76ms
step:1292/1750 train_time:127600ms step_avg:98.76ms
step:1293/1750 train_time:127701ms step_avg:98.76ms
step:1294/1750 train_time:127802ms step_avg:98.77ms
step:1295/1750 train_time:127903ms step_avg:98.77ms
step:1296/1750 train_time:128005ms step_avg:98.77ms
step:1297/1750 train_time:128107ms step_avg:98.77ms
step:1298/1750 train_time:128207ms step_avg:98.77ms
step:1299/1750 train_time:128308ms step_avg:98.77ms
step:1300/1750 train_time:128409ms step_avg:98.78ms
step:1301/1750 train_time:128510ms step_avg:98.78ms
step:1302/1750 train_time:128611ms step_avg:98.78ms
step:1303/1750 train_time:128712ms step_avg:98.78ms
step:1304/1750 train_time:128814ms step_avg:98.78ms
step:1305/1750 train_time:128915ms step_avg:98.79ms
step:1306/1750 train_time:129016ms step_avg:98.79ms
step:1307/1750 train_time:129117ms step_avg:98.79ms
step:1308/1750 train_time:129218ms step_avg:98.79ms
step:1309/1750 train_time:129320ms step_avg:98.79ms
step:1310/1750 train_time:129420ms step_avg:98.79ms
step:1311/1750 train_time:129521ms step_avg:98.80ms
step:1312/1750 train_time:129622ms step_avg:98.80ms
step:1313/1750 train_time:129723ms step_avg:98.80ms
step:1314/1750 train_time:129824ms step_avg:98.80ms
step:1315/1750 train_time:129926ms step_avg:98.80ms
step:1316/1750 train_time:130026ms step_avg:98.80ms
step:1317/1750 train_time:130127ms step_avg:98.81ms
step:1318/1750 train_time:130228ms step_avg:98.81ms
step:1319/1750 train_time:130329ms step_avg:98.81ms
step:1320/1750 train_time:130430ms step_avg:98.81ms
step:1321/1750 train_time:130531ms step_avg:98.81ms
step:1322/1750 train_time:130631ms step_avg:98.81ms
step:1323/1750 train_time:130731ms step_avg:98.81ms
step:1324/1750 train_time:130832ms step_avg:98.82ms
step:1325/1750 train_time:130933ms step_avg:98.82ms
step:1326/1750 train_time:131035ms step_avg:98.82ms
step:1327/1750 train_time:131137ms step_avg:98.82ms
step:1328/1750 train_time:131238ms step_avg:98.82ms
step:1329/1750 train_time:131340ms step_avg:98.83ms
step:1330/1750 train_time:131440ms step_avg:98.83ms
step:1331/1750 train_time:131541ms step_avg:98.83ms
step:1332/1750 train_time:131641ms step_avg:98.83ms
step:1333/1750 train_time:131742ms step_avg:98.83ms
step:1334/1750 train_time:131843ms step_avg:98.83ms
step:1335/1750 train_time:131944ms step_avg:98.83ms
step:1336/1750 train_time:132046ms step_avg:98.84ms
step:1337/1750 train_time:132147ms step_avg:98.84ms
step:1338/1750 train_time:132247ms step_avg:98.84ms
step:1339/1750 train_time:132348ms step_avg:98.84ms
step:1340/1750 train_time:132449ms step_avg:98.84ms
step:1341/1750 train_time:132550ms step_avg:98.84ms
step:1342/1750 train_time:132650ms step_avg:98.85ms
step:1343/1750 train_time:132751ms step_avg:98.85ms
step:1344/1750 train_time:132852ms step_avg:98.85ms
step:1345/1750 train_time:132954ms step_avg:98.85ms
step:1346/1750 train_time:133057ms step_avg:98.85ms
step:1347/1750 train_time:133158ms step_avg:98.86ms
step:1348/1750 train_time:133260ms step_avg:98.86ms
step:1349/1750 train_time:133359ms step_avg:98.86ms
step:1350/1750 train_time:133461ms step_avg:98.86ms
step:1351/1750 train_time:133562ms step_avg:98.86ms
step:1352/1750 train_time:133662ms step_avg:98.86ms
step:1353/1750 train_time:133763ms step_avg:98.86ms
step:1354/1750 train_time:133865ms step_avg:98.87ms
step:1355/1750 train_time:133967ms step_avg:98.87ms
step:1356/1750 train_time:134068ms step_avg:98.87ms
step:1357/1750 train_time:134169ms step_avg:98.87ms
step:1358/1750 train_time:134270ms step_avg:98.87ms
step:1359/1750 train_time:134370ms step_avg:98.87ms
step:1360/1750 train_time:134470ms step_avg:98.88ms
step:1361/1750 train_time:134570ms step_avg:98.88ms
step:1362/1750 train_time:134671ms step_avg:98.88ms
step:1363/1750 train_time:134772ms step_avg:98.88ms
step:1364/1750 train_time:134874ms step_avg:98.88ms
step:1365/1750 train_time:134976ms step_avg:98.88ms
step:1366/1750 train_time:135078ms step_avg:98.89ms
step:1367/1750 train_time:135178ms step_avg:98.89ms
step:1368/1750 train_time:135280ms step_avg:98.89ms
step:1369/1750 train_time:135380ms step_avg:98.89ms
step:1370/1750 train_time:135481ms step_avg:98.89ms
step:1371/1750 train_time:135582ms step_avg:98.89ms
step:1372/1750 train_time:135683ms step_avg:98.89ms
step:1373/1750 train_time:135784ms step_avg:98.90ms
step:1374/1750 train_time:135886ms step_avg:98.90ms
step:1375/1750 train_time:135989ms step_avg:98.90ms
step:1375/1750 val_loss:3.3740 train_time:136078ms step_avg:98.97ms
step:1376/1750 train_time:136100ms step_avg:98.91ms
step:1377/1750 train_time:136202ms step_avg:98.91ms
step:1378/1750 train_time:136304ms step_avg:98.91ms
step:1379/1750 train_time:136406ms step_avg:98.92ms
step:1380/1750 train_time:136508ms step_avg:98.92ms
step:1381/1750 train_time:136607ms step_avg:98.92ms
step:1382/1750 train_time:136707ms step_avg:98.92ms
step:1383/1750 train_time:136807ms step_avg:98.92ms
step:1384/1750 train_time:136906ms step_avg:98.92ms
step:1385/1750 train_time:137006ms step_avg:98.92ms
step:1386/1750 train_time:137110ms step_avg:98.93ms
step:1387/1750 train_time:137214ms step_avg:98.93ms
step:1388/1750 train_time:137316ms step_avg:98.93ms
step:1389/1750 train_time:137416ms step_avg:98.93ms
step:1390/1750 train_time:137517ms step_avg:98.93ms
step:1391/1750 train_time:137617ms step_avg:98.93ms
step:1392/1750 train_time:137718ms step_avg:98.94ms
step:1393/1750 train_time:137819ms step_avg:98.94ms
step:1394/1750 train_time:137919ms step_avg:98.94ms
step:1395/1750 train_time:138021ms step_avg:98.94ms
step:1396/1750 train_time:138124ms step_avg:98.94ms
step:1397/1750 train_time:138227ms step_avg:98.95ms
step:1398/1750 train_time:138328ms step_avg:98.95ms
step:1399/1750 train_time:138429ms step_avg:98.95ms
step:1400/1750 train_time:138530ms step_avg:98.95ms
step:1401/1750 train_time:138631ms step_avg:98.95ms
step:1402/1750 train_time:138732ms step_avg:98.95ms
step:1403/1750 train_time:138833ms step_avg:98.95ms
step:1404/1750 train_time:138934ms step_avg:98.96ms
step:1405/1750 train_time:139035ms step_avg:98.96ms
step:1406/1750 train_time:139137ms step_avg:98.96ms
step:1407/1750 train_time:139238ms step_avg:98.96ms
step:1408/1750 train_time:139339ms step_avg:98.96ms
step:1409/1750 train_time:139441ms step_avg:98.96ms
step:1410/1750 train_time:139543ms step_avg:98.97ms
step:1411/1750 train_time:139644ms step_avg:98.97ms
step:1412/1750 train_time:139746ms step_avg:98.97ms
step:1413/1750 train_time:139847ms step_avg:98.97ms
step:1414/1750 train_time:139947ms step_avg:98.97ms
step:1415/1750 train_time:140050ms step_avg:98.98ms
step:1416/1750 train_time:140151ms step_avg:98.98ms
step:1417/1750 train_time:140253ms step_avg:98.98ms
step:1418/1750 train_time:140353ms step_avg:98.98ms
step:1419/1750 train_time:140454ms step_avg:98.98ms
step:1420/1750 train_time:140555ms step_avg:98.98ms
step:1421/1750 train_time:140656ms step_avg:98.98ms
step:1422/1750 train_time:140756ms step_avg:98.98ms
step:1423/1750 train_time:140857ms step_avg:98.99ms
step:1424/1750 train_time:140958ms step_avg:98.99ms
step:1425/1750 train_time:141058ms step_avg:98.99ms
step:1426/1750 train_time:141161ms step_avg:98.99ms
step:1427/1750 train_time:141263ms step_avg:98.99ms
step:1428/1750 train_time:141365ms step_avg:99.00ms
step:1429/1750 train_time:141467ms step_avg:99.00ms
step:1430/1750 train_time:141570ms step_avg:99.00ms
step:1431/1750 train_time:141672ms step_avg:99.00ms
step:1432/1750 train_time:141773ms step_avg:99.00ms
step:1433/1750 train_time:141875ms step_avg:99.01ms
step:1434/1750 train_time:141976ms step_avg:99.01ms
step:1435/1750 train_time:142079ms step_avg:99.01ms
step:1436/1750 train_time:142181ms step_avg:99.01ms
step:1437/1750 train_time:142283ms step_avg:99.01ms
step:1438/1750 train_time:142385ms step_avg:99.02ms
step:1439/1750 train_time:142489ms step_avg:99.02ms
step:1440/1750 train_time:142592ms step_avg:99.02ms
step:1441/1750 train_time:142694ms step_avg:99.02ms
step:1442/1750 train_time:142794ms step_avg:99.03ms
step:1443/1750 train_time:142895ms step_avg:99.03ms
step:1444/1750 train_time:142997ms step_avg:99.03ms
step:1445/1750 train_time:143098ms step_avg:99.03ms
step:1446/1750 train_time:143198ms step_avg:99.03ms
step:1447/1750 train_time:143300ms step_avg:99.03ms
step:1448/1750 train_time:143407ms step_avg:99.04ms
step:1449/1750 train_time:143508ms step_avg:99.04ms
step:1450/1750 train_time:143609ms step_avg:99.04ms
step:1451/1750 train_time:143711ms step_avg:99.04ms
step:1452/1750 train_time:143812ms step_avg:99.04ms
step:1453/1750 train_time:143915ms step_avg:99.05ms
step:1454/1750 train_time:144018ms step_avg:99.05ms
step:1455/1750 train_time:144119ms step_avg:99.05ms
step:1456/1750 train_time:144220ms step_avg:99.05ms
step:1457/1750 train_time:144322ms step_avg:99.05ms
step:1458/1750 train_time:144425ms step_avg:99.06ms
step:1459/1750 train_time:144528ms step_avg:99.06ms
step:1460/1750 train_time:144629ms step_avg:99.06ms
step:1461/1750 train_time:144732ms step_avg:99.06ms
step:1462/1750 train_time:144834ms step_avg:99.07ms
step:1463/1750 train_time:144935ms step_avg:99.07ms
step:1464/1750 train_time:145036ms step_avg:99.07ms
step:1465/1750 train_time:145138ms step_avg:99.07ms
step:1466/1750 train_time:145239ms step_avg:99.07ms
step:1467/1750 train_time:145340ms step_avg:99.07ms
step:1468/1750 train_time:145443ms step_avg:99.08ms
step:1469/1750 train_time:145545ms step_avg:99.08ms
step:1470/1750 train_time:145648ms step_avg:99.08ms
step:1471/1750 train_time:145750ms step_avg:99.08ms
step:1472/1750 train_time:145851ms step_avg:99.08ms
step:1473/1750 train_time:145952ms step_avg:99.09ms
step:1474/1750 train_time:146054ms step_avg:99.09ms
step:1475/1750 train_time:146155ms step_avg:99.09ms
step:1476/1750 train_time:146257ms step_avg:99.09ms
step:1477/1750 train_time:146360ms step_avg:99.09ms
step:1478/1750 train_time:146462ms step_avg:99.09ms
step:1479/1750 train_time:146564ms step_avg:99.10ms
step:1480/1750 train_time:146666ms step_avg:99.10ms
step:1481/1750 train_time:146768ms step_avg:99.10ms
step:1482/1750 train_time:146871ms step_avg:99.10ms
step:1483/1750 train_time:146972ms step_avg:99.10ms
step:1484/1750 train_time:147074ms step_avg:99.11ms
step:1485/1750 train_time:147177ms step_avg:99.11ms
step:1486/1750 train_time:147279ms step_avg:99.11ms
step:1487/1750 train_time:147381ms step_avg:99.11ms
step:1488/1750 train_time:147484ms step_avg:99.12ms
step:1489/1750 train_time:147586ms step_avg:99.12ms
step:1490/1750 train_time:147688ms step_avg:99.12ms
step:1491/1750 train_time:147789ms step_avg:99.12ms
step:1492/1750 train_time:147891ms step_avg:99.12ms
step:1493/1750 train_time:147992ms step_avg:99.12ms
step:1494/1750 train_time:148094ms step_avg:99.13ms
step:1495/1750 train_time:148195ms step_avg:99.13ms
step:1496/1750 train_time:148297ms step_avg:99.13ms
step:1497/1750 train_time:148397ms step_avg:99.13ms
step:1498/1750 train_time:148499ms step_avg:99.13ms
step:1499/1750 train_time:148600ms step_avg:99.13ms
step:1500/1750 train_time:148702ms step_avg:99.13ms
step:1500/1750 val_loss:3.3385 train_time:148793ms step_avg:99.20ms
step:1501/1750 train_time:148815ms step_avg:99.14ms
step:1502/1750 train_time:148918ms step_avg:99.15ms
step:1503/1750 train_time:149019ms step_avg:99.15ms
step:1504/1750 train_time:149120ms step_avg:99.15ms
step:1505/1750 train_time:149221ms step_avg:99.15ms
step:1506/1750 train_time:149322ms step_avg:99.15ms
step:1507/1750 train_time:149422ms step_avg:99.15ms
step:1508/1750 train_time:149522ms step_avg:99.15ms
step:1509/1750 train_time:149624ms step_avg:99.15ms
step:1510/1750 train_time:149727ms step_avg:99.16ms
step:1511/1750 train_time:149833ms step_avg:99.16ms
step:1512/1750 train_time:149936ms step_avg:99.16ms
step:1513/1750 train_time:150038ms step_avg:99.17ms
step:1514/1750 train_time:150140ms step_avg:99.17ms
step:1515/1750 train_time:150243ms step_avg:99.17ms
step:1516/1750 train_time:150344ms step_avg:99.17ms
step:1517/1750 train_time:150445ms step_avg:99.17ms
step:1518/1750 train_time:150544ms step_avg:99.17ms
step:1519/1750 train_time:150646ms step_avg:99.17ms
step:1520/1750 train_time:150749ms step_avg:99.18ms
step:1521/1750 train_time:150851ms step_avg:99.18ms
step:1522/1750 train_time:150953ms step_avg:99.18ms
step:1523/1750 train_time:151054ms step_avg:99.18ms
step:1524/1750 train_time:151158ms step_avg:99.19ms
step:1525/1750 train_time:151262ms step_avg:99.19ms
step:1526/1750 train_time:151364ms step_avg:99.19ms
step:1527/1750 train_time:151465ms step_avg:99.19ms
step:1528/1750 train_time:151570ms step_avg:99.19ms
step:1529/1750 train_time:151672ms step_avg:99.20ms
step:1530/1750 train_time:151775ms step_avg:99.20ms
step:1531/1750 train_time:151877ms step_avg:99.20ms
step:1532/1750 train_time:151979ms step_avg:99.20ms
step:1533/1750 train_time:152081ms step_avg:99.20ms
step:1534/1750 train_time:152183ms step_avg:99.21ms
step:1535/1750 train_time:152285ms step_avg:99.21ms
step:1536/1750 train_time:152385ms step_avg:99.21ms
step:1537/1750 train_time:152487ms step_avg:99.21ms
step:1538/1750 train_time:152587ms step_avg:99.21ms
step:1539/1750 train_time:152689ms step_avg:99.21ms
step:1540/1750 train_time:152791ms step_avg:99.22ms
step:1541/1750 train_time:152895ms step_avg:99.22ms
step:1542/1750 train_time:152999ms step_avg:99.22ms
step:1543/1750 train_time:153101ms step_avg:99.22ms
step:1544/1750 train_time:153203ms step_avg:99.22ms
step:1545/1750 train_time:153305ms step_avg:99.23ms
step:1546/1750 train_time:153406ms step_avg:99.23ms
step:1547/1750 train_time:153507ms step_avg:99.23ms
step:1548/1750 train_time:153609ms step_avg:99.23ms
step:1549/1750 train_time:153710ms step_avg:99.23ms
step:1550/1750 train_time:153812ms step_avg:99.23ms
step:1551/1750 train_time:153915ms step_avg:99.24ms
step:1552/1750 train_time:154017ms step_avg:99.24ms
step:1553/1750 train_time:154119ms step_avg:99.24ms
step:1554/1750 train_time:154219ms step_avg:99.24ms
step:1555/1750 train_time:154322ms step_avg:99.24ms
step:1556/1750 train_time:154424ms step_avg:99.24ms
step:1557/1750 train_time:154526ms step_avg:99.25ms
step:1558/1750 train_time:154628ms step_avg:99.25ms
step:1559/1750 train_time:154730ms step_avg:99.25ms
step:1560/1750 train_time:154831ms step_avg:99.25ms
step:1561/1750 train_time:154933ms step_avg:99.25ms
step:1562/1750 train_time:155035ms step_avg:99.25ms
step:1563/1750 train_time:155140ms step_avg:99.26ms
step:1564/1750 train_time:155241ms step_avg:99.26ms
step:1565/1750 train_time:155342ms step_avg:99.26ms
step:1566/1750 train_time:155444ms step_avg:99.26ms
step:1567/1750 train_time:155546ms step_avg:99.26ms
step:1568/1750 train_time:155647ms step_avg:99.26ms
step:1569/1750 train_time:155749ms step_avg:99.27ms
step:1570/1750 train_time:155853ms step_avg:99.27ms
step:1571/1750 train_time:155955ms step_avg:99.27ms
step:1572/1750 train_time:156057ms step_avg:99.27ms
step:1573/1750 train_time:156159ms step_avg:99.27ms
step:1574/1750 train_time:156261ms step_avg:99.28ms
step:1575/1750 train_time:156363ms step_avg:99.28ms
step:1576/1750 train_time:156465ms step_avg:99.28ms
step:1577/1750 train_time:156568ms step_avg:99.28ms
step:1578/1750 train_time:156668ms step_avg:99.28ms
step:1579/1750 train_time:156771ms step_avg:99.28ms
step:1580/1750 train_time:156873ms step_avg:99.29ms
step:1581/1750 train_time:156975ms step_avg:99.29ms
step:1582/1750 train_time:157075ms step_avg:99.29ms
step:1583/1750 train_time:157179ms step_avg:99.29ms
step:1584/1750 train_time:157282ms step_avg:99.29ms
step:1585/1750 train_time:157385ms step_avg:99.30ms
step:1586/1750 train_time:157488ms step_avg:99.30ms
step:1587/1750 train_time:157590ms step_avg:99.30ms
step:1588/1750 train_time:157691ms step_avg:99.30ms
step:1589/1750 train_time:157792ms step_avg:99.30ms
step:1590/1750 train_time:157894ms step_avg:99.30ms
step:1591/1750 train_time:157996ms step_avg:99.31ms
step:1592/1750 train_time:158097ms step_avg:99.31ms
step:1593/1750 train_time:158199ms step_avg:99.31ms
step:1594/1750 train_time:158304ms step_avg:99.31ms
step:1595/1750 train_time:158406ms step_avg:99.31ms
step:1596/1750 train_time:158507ms step_avg:99.32ms
step:1597/1750 train_time:158609ms step_avg:99.32ms
step:1598/1750 train_time:158711ms step_avg:99.32ms
step:1599/1750 train_time:158812ms step_avg:99.32ms
step:1600/1750 train_time:158914ms step_avg:99.32ms
step:1601/1750 train_time:159016ms step_avg:99.32ms
step:1602/1750 train_time:159118ms step_avg:99.32ms
step:1603/1750 train_time:159220ms step_avg:99.33ms
step:1604/1750 train_time:159322ms step_avg:99.33ms
step:1605/1750 train_time:159426ms step_avg:99.33ms
step:1606/1750 train_time:159528ms step_avg:99.33ms
step:1607/1750 train_time:159628ms step_avg:99.33ms
step:1608/1750 train_time:159730ms step_avg:99.33ms
step:1609/1750 train_time:159832ms step_avg:99.34ms
step:1610/1750 train_time:159935ms step_avg:99.34ms
step:1611/1750 train_time:160037ms step_avg:99.34ms
step:1612/1750 train_time:160140ms step_avg:99.34ms
step:1613/1750 train_time:160242ms step_avg:99.34ms
step:1614/1750 train_time:160343ms step_avg:99.34ms
step:1615/1750 train_time:160444ms step_avg:99.35ms
step:1616/1750 train_time:160546ms step_avg:99.35ms
step:1617/1750 train_time:160648ms step_avg:99.35ms
step:1618/1750 train_time:160750ms step_avg:99.35ms
step:1619/1750 train_time:160852ms step_avg:99.35ms
step:1620/1750 train_time:160956ms step_avg:99.36ms
step:1621/1750 train_time:161057ms step_avg:99.36ms
step:1622/1750 train_time:161160ms step_avg:99.36ms
step:1623/1750 train_time:161262ms step_avg:99.36ms
step:1624/1750 train_time:161365ms step_avg:99.36ms
step:1625/1750 train_time:161468ms step_avg:99.36ms
step:1625/1750 val_loss:3.3087 train_time:161558ms step_avg:99.42ms
step:1626/1750 train_time:161579ms step_avg:99.37ms
step:1627/1750 train_time:161681ms step_avg:99.37ms
step:1628/1750 train_time:161782ms step_avg:99.37ms
step:1629/1750 train_time:161884ms step_avg:99.38ms
step:1630/1750 train_time:161984ms step_avg:99.38ms
step:1631/1750 train_time:162085ms step_avg:99.38ms
step:1632/1750 train_time:162186ms step_avg:99.38ms
step:1633/1750 train_time:162287ms step_avg:99.38ms
step:1634/1750 train_time:162391ms step_avg:99.38ms
step:1635/1750 train_time:162491ms step_avg:99.38ms
step:1636/1750 train_time:162596ms step_avg:99.39ms
step:1637/1750 train_time:162699ms step_avg:99.39ms
step:1638/1750 train_time:162801ms step_avg:99.39ms
step:1639/1750 train_time:162902ms step_avg:99.39ms
step:1640/1750 train_time:163003ms step_avg:99.39ms
step:1641/1750 train_time:163104ms step_avg:99.39ms
step:1642/1750 train_time:163204ms step_avg:99.39ms
step:1643/1750 train_time:163306ms step_avg:99.39ms
step:1644/1750 train_time:163408ms step_avg:99.40ms
step:1645/1750 train_time:163510ms step_avg:99.40ms
step:1646/1750 train_time:163612ms step_avg:99.40ms
step:1647/1750 train_time:163716ms step_avg:99.40ms
step:1648/1750 train_time:163820ms step_avg:99.41ms
step:1649/1750 train_time:163922ms step_avg:99.41ms
step:1650/1750 train_time:164023ms step_avg:99.41ms
step:1651/1750 train_time:164125ms step_avg:99.41ms
step:1652/1750 train_time:164226ms step_avg:99.41ms
step:1653/1750 train_time:164328ms step_avg:99.41ms
step:1654/1750 train_time:164430ms step_avg:99.41ms
step:1655/1750 train_time:164533ms step_avg:99.42ms
step:1656/1750 train_time:164637ms step_avg:99.42ms
step:1657/1750 train_time:164738ms step_avg:99.42ms
step:1658/1750 train_time:164840ms step_avg:99.42ms
step:1659/1750 train_time:164944ms step_avg:99.42ms
step:1660/1750 train_time:165045ms step_avg:99.42ms
step:1661/1750 train_time:165148ms step_avg:99.43ms
step:1662/1750 train_time:165251ms step_avg:99.43ms
step:1663/1750 train_time:165353ms step_avg:99.43ms
step:1664/1750 train_time:165456ms step_avg:99.43ms
step:1665/1750 train_time:165561ms step_avg:99.44ms
step:1666/1750 train_time:165664ms step_avg:99.44ms
step:1667/1750 train_time:165765ms step_avg:99.44ms
step:1668/1750 train_time:165869ms step_avg:99.44ms
step:1669/1750 train_time:165970ms step_avg:99.44ms
step:1670/1750 train_time:166072ms step_avg:99.44ms
step:1671/1750 train_time:166174ms step_avg:99.45ms
step:1672/1750 train_time:166276ms step_avg:99.45ms
step:1673/1750 train_time:166377ms step_avg:99.45ms
step:1674/1750 train_time:166480ms step_avg:99.45ms
step:1675/1750 train_time:166582ms step_avg:99.45ms
step:1676/1750 train_time:166685ms step_avg:99.45ms
step:1677/1750 train_time:166787ms step_avg:99.46ms
step:1678/1750 train_time:166889ms step_avg:99.46ms
step:1679/1750 train_time:166991ms step_avg:99.46ms
step:1680/1750 train_time:167092ms step_avg:99.46ms
step:1681/1750 train_time:167195ms step_avg:99.46ms
step:1682/1750 train_time:167300ms step_avg:99.47ms
step:1683/1750 train_time:167401ms step_avg:99.47ms
step:1684/1750 train_time:167503ms step_avg:99.47ms
step:1685/1750 train_time:167606ms step_avg:99.47ms
step:1686/1750 train_time:167707ms step_avg:99.47ms
step:1687/1750 train_time:167810ms step_avg:99.47ms
step:1688/1750 train_time:167911ms step_avg:99.47ms
step:1689/1750 train_time:168013ms step_avg:99.48ms
step:1690/1750 train_time:168116ms step_avg:99.48ms
step:1691/1750 train_time:168219ms step_avg:99.48ms
step:1692/1750 train_time:168321ms step_avg:99.48ms
step:1693/1750 train_time:168424ms step_avg:99.48ms
step:1694/1750 train_time:168527ms step_avg:99.48ms
step:1695/1750 train_time:168630ms step_avg:99.49ms
step:1696/1750 train_time:168733ms step_avg:99.49ms
step:1697/1750 train_time:168837ms step_avg:99.49ms
step:1698/1750 train_time:168939ms step_avg:99.49ms
step:1699/1750 train_time:169041ms step_avg:99.49ms
step:1700/1750 train_time:169144ms step_avg:99.50ms
step:1701/1750 train_time:169246ms step_avg:99.50ms
step:1702/1750 train_time:169351ms step_avg:99.50ms
step:1703/1750 train_time:169454ms step_avg:99.50ms
step:1704/1750 train_time:169558ms step_avg:99.51ms
step:1705/1750 train_time:169660ms step_avg:99.51ms
step:1706/1750 train_time:169762ms step_avg:99.51ms
step:1707/1750 train_time:169866ms step_avg:99.51ms
step:1708/1750 train_time:169969ms step_avg:99.51ms
step:1709/1750 train_time:170071ms step_avg:99.51ms
step:1710/1750 train_time:170174ms step_avg:99.52ms
step:1711/1750 train_time:170278ms step_avg:99.52ms
step:1712/1750 train_time:170380ms step_avg:99.52ms
step:1713/1750 train_time:170484ms step_avg:99.52ms
step:1714/1750 train_time:170587ms step_avg:99.53ms
step:1715/1750 train_time:170691ms step_avg:99.53ms
step:1716/1750 train_time:170794ms step_avg:99.53ms
step:1717/1750 train_time:170898ms step_avg:99.53ms
step:1718/1750 train_time:171000ms step_avg:99.53ms
step:1719/1750 train_time:171105ms step_avg:99.54ms
step:1720/1750 train_time:171206ms step_avg:99.54ms
step:1721/1750 train_time:171310ms step_avg:99.54ms
step:1722/1750 train_time:171412ms step_avg:99.54ms
step:1723/1750 train_time:171515ms step_avg:99.54ms
step:1724/1750 train_time:171619ms step_avg:99.55ms
step:1725/1750 train_time:171723ms step_avg:99.55ms
step:1726/1750 train_time:171826ms step_avg:99.55ms
step:1727/1750 train_time:171928ms step_avg:99.55ms
step:1728/1750 train_time:172032ms step_avg:99.56ms
step:1729/1750 train_time:172134ms step_avg:99.56ms
step:1730/1750 train_time:172238ms step_avg:99.56ms
step:1731/1750 train_time:172342ms step_avg:99.56ms
step:1732/1750 train_time:172445ms step_avg:99.56ms
step:1733/1750 train_time:172548ms step_avg:99.57ms
step:1734/1750 train_time:172653ms step_avg:99.57ms
step:1735/1750 train_time:172756ms step_avg:99.57ms
step:1736/1750 train_time:172859ms step_avg:99.57ms
step:1737/1750 train_time:172961ms step_avg:99.57ms
step:1738/1750 train_time:173064ms step_avg:99.58ms
step:1739/1750 train_time:173166ms step_avg:99.58ms
step:1740/1750 train_time:173269ms step_avg:99.58ms
step:1741/1750 train_time:173375ms step_avg:99.58ms
step:1742/1750 train_time:173478ms step_avg:99.59ms
step:1743/1750 train_time:173582ms step_avg:99.59ms
step:1744/1750 train_time:173686ms step_avg:99.59ms
step:1745/1750 train_time:173787ms step_avg:99.59ms
step:1746/1750 train_time:173889ms step_avg:99.59ms
step:1747/1750 train_time:173993ms step_avg:99.60ms
step:1748/1750 train_time:174097ms step_avg:99.60ms
step:1749/1750 train_time:174200ms step_avg:99.60ms
step:1750/1750 train_time:174302ms step_avg:99.60ms
step:1750/1750 val_loss:3.2854 train_time:174393ms step_avg:99.65ms
peak memory allocated: 33278 MiB reserved: 48954 MiB
