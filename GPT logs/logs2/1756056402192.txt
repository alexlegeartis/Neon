import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X
'''
import torch.utils.dlpack as thd
from cupyx.scipy.sparse.linalg import svds as cupyx_svds

def several_sv_svds_approximation(W_torch, k, num_iter=50):
    """SVD approximation using the top k singular values and corresponding vectors."""
    # Store original device and dtype
    original_device = W_torch.device
    original_dtype = W_torch.dtype
    
    W = cp.from_dlpack(thd.to_dlpack(W_torch)).astype(cp.float32)
    U, S, Vt = cupyx_svds(W, k=min([k, W.shape[0] - 1, W.shape[1] - 1]), maxiter=num_iter, which='LM')

    # Convert back to torch tensors and ensure they're on the correct device
    approx_torch_U = thd.from_dlpack(U.toDlpack()).to(device=original_device, dtype=original_dtype)
    approx_torch_S = thd.from_dlpack(S.toDlpack()).to(device=original_device, dtype=original_dtype)
    approx_torch_Vt = thd.from_dlpack(Vt.toDlpack()).to(device=original_device, dtype=original_dtype)
    
    return approx_torch_U, approx_torch_S, approx_torch_Vt

class NormNeon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    # v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    u, s, vt = several_sv_svds_approximation(grad.bfloat16(), 5)
                    p.add_(other=u@vt, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()
'''
class FastNormMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, sgd_coeff=0):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, sgd_coeff=sgd_coeff)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        self.sgd_coeff = sgd_coeff

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            sgd_coeff = group["sgd_coeff"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    # assume: grad is a Tensor, momentum_buffer exists, momentum, sgd_coeff, eps, eff_lr are defined
                    with torch.no_grad():
                        # inplace blend with momentum (keeps original semantic: grad becomes the lerped tensor)
                        grad.lerp_(momentum_buffer, momentum)

                        # compute zeropower part from the (post-lerp) grad, in bfloat16 to match the original code
                        if sgd_coeff != 1:
                            # create update_part in bfloat16 (may allocate once per call; unavoidable if zeropower expects bfloat16)
                            update_part = zeropower_via_newtonschulz5(grad.to(torch.bfloat16), 5)

                            # upcast update_part to grad dtype to do mixed-dtype blending without extra temporaries later
                            if update_part.dtype != grad.dtype:
                                update_part = update_part.to(grad.dtype)

                        # compute inverse norm scalar efficiently (no large temp tensors)
                        # - view(-1) is cheap (no copy)
                        # - dot(view, view) is a single reduction
                        norm_sq = grad.view(-1).dot(grad.view(-1))
                        inv_norm = (norm_sq + eps).rsqrt()  # stable rsqrt on the scalar

                        # normalize grad in-place (so we avoid allocating g_normalized)
                        grad.mul_(inv_norm)   # now grad == g_normalized (in-place)

                        # build final update (reuse update_part buffer if available)
                        if sgd_coeff != 1:
                            # update_part := (1 - sgd_coeff) * update_part + sgd_coeff * grad
                            # do it in-place on update_part to avoid creating another temp
                            update_part.mul_(1 - sgd_coeff)    # in-place scale
                            update_part.lerp_(grad, sgd_coeff) # in-place linear interpolation
                            update = update_part
                        else:
                            # if sgd_coeff == 1 we can use grad directly (already normalized in-place)
                            update = grad

                        # apply update (same as original)
                        p.add_(other=update, alpha=-eff_lr)
                    '''
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    eps = 1e-12
                    g_normalized = grad / (grad.norm() + eps)
                    if sgd_coeff != 1:
                        update_part = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                        update = (1 - sgd_coeff) * update_part + sgd_coeff * g_normalized
                    else:
                        update = sgd_coeff * g_normalized
                    p.add_(other=update, alpha=-eff_lr)
                    '''
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class NormMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, sgd_coeff=0):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, sgd_coeff=sgd_coeff)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        self.sgd_coeff = sgd_coeff

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            sgd_coeff = group["sgd_coeff"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    eps = 1e-12
                    g_normalized = grad / (grad.norm() + eps)
                    if sgd_coeff != 1:
                        update_part = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                        update = (1 - sgd_coeff) * update_part + sgd_coeff * g_normalized
                    else:
                        update = sgd_coeff * g_normalized
                    p.add_(other=update, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()
class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)
                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_reduce_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 27.5
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos : pos + max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()
    starts = []
    for i in range(1, len(boundary_positions)):
        end = boundary_positions[i].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == dist.get_world_size():
                return starts, end - pos
            start = end
    assert False # increase max_batch_span if necessary

def distributed_data_generator(filename_pattern: str, batch_size: int, align_to_bos: bool):
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    max_batch_span = 2 * batch_size if align_to_bos else batch_size # provide buffer to handle samples up to length local_batch_size
    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            batch_span = batch_size
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1750 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
from datetime import datetime
if master_process:
    # run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_id = str(int(time.time() * 1000))
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0)
optimizer2 = NormMuon(hidden_matrix_params, lr=0.4, momentum=0.65, weight_decay=0.0, sgd_coeff=0.5)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.11 | packaged by conda-forge | (main, Jun  4 2025, 14:45:31) [GCC 13.3.0]
Running PyTorch 2.9.0.dev20250824+cu126 compiled for CUDA 12.6
Sun Aug 24 17:26:42 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:04:00.0 Off |                    0 |
| N/A   31C    P0            116W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:23:00.0 Off |                    0 |
| N/A   28C    P0            113W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:43:00.0 Off |                    0 |
| N/A   30C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   28C    P0            113W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:84:00.0 Off |                    0 |
| N/A   31C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:A3:00.0 Off |                    0 |
| N/A   28C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:C3:00.0 Off |                    0 |
| N/A   30C    P0            115W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E4:00.0 Off |                    0 |
| N/A   27C    P0            111W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.05ms
step:1/1750 train_time:153ms step_avg:153.47ms
step:2/1750 train_time:174ms step_avg:86.94ms
step:3/1750 train_time:256ms step_avg:85.23ms
step:4/1750 train_time:347ms step_avg:86.79ms
step:5/1750 train_time:439ms step_avg:87.78ms
step:6/1750 train_time:531ms step_avg:88.53ms
step:7/1750 train_time:624ms step_avg:89.16ms
step:8/1750 train_time:716ms step_avg:89.51ms
step:9/1750 train_time:808ms step_avg:89.73ms
step:10/1750 train_time:900ms step_avg:89.98ms
step:11/1750 train_time:992ms step_avg:90.19ms
step:12/1750 train_time:1086ms step_avg:90.47ms
step:13/1750 train_time:1181ms step_avg:90.82ms
step:14/1750 train_time:1276ms step_avg:91.12ms
step:15/1750 train_time:1370ms step_avg:91.32ms
step:16/1750 train_time:1463ms step_avg:91.44ms
step:17/1750 train_time:1556ms step_avg:91.51ms
step:18/1750 train_time:1648ms step_avg:91.57ms
step:19/1750 train_time:1741ms step_avg:91.62ms
step:20/1750 train_time:1833ms step_avg:91.64ms
step:21/1750 train_time:1925ms step_avg:91.68ms
step:22/1750 train_time:2018ms step_avg:91.73ms
step:23/1750 train_time:2111ms step_avg:91.78ms
step:24/1750 train_time:2204ms step_avg:91.85ms
step:25/1750 train_time:2299ms step_avg:91.96ms
step:26/1750 train_time:2392ms step_avg:92.01ms
step:27/1750 train_time:2485ms step_avg:92.04ms
step:28/1750 train_time:2578ms step_avg:92.06ms
step:29/1750 train_time:2670ms step_avg:92.07ms
step:30/1750 train_time:2763ms step_avg:92.10ms
step:31/1750 train_time:2856ms step_avg:92.12ms
step:32/1750 train_time:2949ms step_avg:92.15ms
step:33/1750 train_time:3042ms step_avg:92.17ms
step:34/1750 train_time:3135ms step_avg:92.21ms
step:35/1750 train_time:3229ms step_avg:92.26ms
step:36/1750 train_time:3323ms step_avg:92.30ms
step:37/1750 train_time:3417ms step_avg:92.34ms
step:38/1750 train_time:3509ms step_avg:92.34ms
step:39/1750 train_time:3602ms step_avg:92.37ms
step:40/1750 train_time:3695ms step_avg:92.38ms
step:41/1750 train_time:3788ms step_avg:92.39ms
step:42/1750 train_time:3881ms step_avg:92.41ms
step:43/1750 train_time:3974ms step_avg:92.42ms
step:44/1750 train_time:4067ms step_avg:92.44ms
step:45/1750 train_time:4161ms step_avg:92.46ms
step:46/1750 train_time:4254ms step_avg:92.47ms
step:47/1750 train_time:4347ms step_avg:92.50ms
step:48/1750 train_time:4441ms step_avg:92.51ms
step:49/1750 train_time:4534ms step_avg:92.53ms
step:50/1750 train_time:4627ms step_avg:92.54ms
step:51/1750 train_time:4719ms step_avg:92.53ms
step:52/1750 train_time:4811ms step_avg:92.53ms
step:53/1750 train_time:4904ms step_avg:92.52ms
step:54/1750 train_time:4996ms step_avg:92.52ms
step:55/1750 train_time:5089ms step_avg:92.52ms
step:56/1750 train_time:5182ms step_avg:92.53ms
step:57/1750 train_time:5276ms step_avg:92.56ms
step:58/1750 train_time:5369ms step_avg:92.57ms
step:59/1750 train_time:5463ms step_avg:92.59ms
step:60/1750 train_time:5556ms step_avg:92.60ms
step:61/1750 train_time:5649ms step_avg:92.60ms
step:62/1750 train_time:5742ms step_avg:92.61ms
step:63/1750 train_time:5835ms step_avg:92.62ms
step:64/1750 train_time:5928ms step_avg:92.63ms
step:65/1750 train_time:6022ms step_avg:92.64ms
step:66/1750 train_time:6115ms step_avg:92.65ms
step:67/1750 train_time:6208ms step_avg:92.65ms
step:68/1750 train_time:6301ms step_avg:92.67ms
step:69/1750 train_time:6394ms step_avg:92.67ms
step:70/1750 train_time:6488ms step_avg:92.68ms
step:71/1750 train_time:6581ms step_avg:92.69ms
step:72/1750 train_time:6673ms step_avg:92.69ms
step:73/1750 train_time:6767ms step_avg:92.70ms
step:74/1750 train_time:6860ms step_avg:92.70ms
step:75/1750 train_time:6953ms step_avg:92.70ms
step:76/1750 train_time:7046ms step_avg:92.70ms
step:77/1750 train_time:7138ms step_avg:92.71ms
step:78/1750 train_time:7232ms step_avg:92.71ms
step:79/1750 train_time:7325ms step_avg:92.73ms
step:80/1750 train_time:7419ms step_avg:92.73ms
step:81/1750 train_time:7511ms step_avg:92.73ms
step:82/1750 train_time:7604ms step_avg:92.74ms
step:83/1750 train_time:7698ms step_avg:92.75ms
step:84/1750 train_time:7791ms step_avg:92.75ms
step:85/1750 train_time:7884ms step_avg:92.76ms
step:86/1750 train_time:7977ms step_avg:92.76ms
step:87/1750 train_time:8071ms step_avg:92.77ms
step:88/1750 train_time:8164ms step_avg:92.78ms
step:89/1750 train_time:8257ms step_avg:92.78ms
step:90/1750 train_time:8350ms step_avg:92.78ms
step:91/1750 train_time:8444ms step_avg:92.79ms
step:92/1750 train_time:8537ms step_avg:92.80ms
step:93/1750 train_time:8631ms step_avg:92.80ms
step:94/1750 train_time:8724ms step_avg:92.81ms
step:95/1750 train_time:8817ms step_avg:92.81ms
step:96/1750 train_time:8910ms step_avg:92.81ms
step:97/1750 train_time:9003ms step_avg:92.82ms
step:98/1750 train_time:9096ms step_avg:92.82ms
step:99/1750 train_time:9189ms step_avg:92.82ms
step:100/1750 train_time:9283ms step_avg:92.83ms
step:101/1750 train_time:9375ms step_avg:92.83ms
step:102/1750 train_time:9469ms step_avg:92.84ms
step:103/1750 train_time:9563ms step_avg:92.85ms
step:104/1750 train_time:9655ms step_avg:92.84ms
step:105/1750 train_time:9749ms step_avg:92.84ms
step:106/1750 train_time:9842ms step_avg:92.85ms
step:107/1750 train_time:9934ms step_avg:92.85ms
step:108/1750 train_time:10028ms step_avg:92.85ms
step:109/1750 train_time:10120ms step_avg:92.85ms
step:110/1750 train_time:10213ms step_avg:92.85ms
step:111/1750 train_time:10307ms step_avg:92.85ms
step:112/1750 train_time:10400ms step_avg:92.85ms
step:113/1750 train_time:10493ms step_avg:92.86ms
step:114/1750 train_time:10586ms step_avg:92.86ms
step:115/1750 train_time:10679ms step_avg:92.87ms
step:116/1750 train_time:10772ms step_avg:92.86ms
step:117/1750 train_time:10865ms step_avg:92.86ms
step:118/1750 train_time:10959ms step_avg:92.87ms
step:119/1750 train_time:11052ms step_avg:92.87ms
step:120/1750 train_time:11145ms step_avg:92.88ms
step:121/1750 train_time:11238ms step_avg:92.88ms
step:122/1750 train_time:11332ms step_avg:92.88ms
step:123/1750 train_time:11426ms step_avg:92.89ms
step:124/1750 train_time:11519ms step_avg:92.89ms
step:125/1750 train_time:11611ms step_avg:92.89ms
step:125/1750 val_loss:4.9447 train_time:11694ms step_avg:93.55ms
step:126/1750 train_time:11715ms step_avg:92.98ms
step:127/1750 train_time:11805ms step_avg:92.96ms
step:128/1750 train_time:11909ms step_avg:93.04ms
step:129/1750 train_time:12003ms step_avg:93.05ms
step:130/1750 train_time:12097ms step_avg:93.05ms
step:131/1750 train_time:12190ms step_avg:93.05ms
step:132/1750 train_time:12282ms step_avg:93.05ms
step:133/1750 train_time:12375ms step_avg:93.05ms
step:134/1750 train_time:12468ms step_avg:93.05ms
step:135/1750 train_time:12561ms step_avg:93.04ms
step:136/1750 train_time:12654ms step_avg:93.04ms
step:137/1750 train_time:12747ms step_avg:93.05ms
step:138/1750 train_time:12844ms step_avg:93.07ms
step:139/1750 train_time:12940ms step_avg:93.09ms
step:140/1750 train_time:13034ms step_avg:93.10ms
step:141/1750 train_time:13129ms step_avg:93.11ms
step:142/1750 train_time:13222ms step_avg:93.11ms
step:143/1750 train_time:13316ms step_avg:93.12ms
step:144/1750 train_time:13408ms step_avg:93.11ms
step:145/1750 train_time:13501ms step_avg:93.11ms
step:146/1750 train_time:13594ms step_avg:93.11ms
step:147/1750 train_time:13687ms step_avg:93.11ms
step:148/1750 train_time:13782ms step_avg:93.12ms
step:149/1750 train_time:13876ms step_avg:93.13ms
step:150/1750 train_time:13971ms step_avg:93.14ms
step:151/1750 train_time:14065ms step_avg:93.15ms
step:152/1750 train_time:14159ms step_avg:93.15ms
step:153/1750 train_time:14253ms step_avg:93.16ms
step:154/1750 train_time:14346ms step_avg:93.16ms
step:155/1750 train_time:14439ms step_avg:93.15ms
step:156/1750 train_time:14532ms step_avg:93.16ms
step:157/1750 train_time:14625ms step_avg:93.15ms
step:158/1750 train_time:14718ms step_avg:93.15ms
step:159/1750 train_time:14813ms step_avg:93.17ms
step:160/1750 train_time:14907ms step_avg:93.17ms
step:161/1750 train_time:15001ms step_avg:93.18ms
step:162/1750 train_time:15096ms step_avg:93.18ms
step:163/1750 train_time:15190ms step_avg:93.19ms
step:164/1750 train_time:15283ms step_avg:93.19ms
step:165/1750 train_time:15377ms step_avg:93.19ms
step:166/1750 train_time:15471ms step_avg:93.20ms
step:167/1750 train_time:15564ms step_avg:93.19ms
step:168/1750 train_time:15656ms step_avg:93.19ms
step:169/1750 train_time:15750ms step_avg:93.20ms
step:170/1750 train_time:15844ms step_avg:93.20ms
step:171/1750 train_time:15938ms step_avg:93.21ms
step:172/1750 train_time:16032ms step_avg:93.21ms
step:173/1750 train_time:16127ms step_avg:93.22ms
step:174/1750 train_time:16220ms step_avg:93.22ms
step:175/1750 train_time:16315ms step_avg:93.23ms
step:176/1750 train_time:16409ms step_avg:93.23ms
step:177/1750 train_time:16502ms step_avg:93.23ms
step:178/1750 train_time:16596ms step_avg:93.23ms
step:179/1750 train_time:16689ms step_avg:93.23ms
step:180/1750 train_time:16782ms step_avg:93.24ms
step:181/1750 train_time:16877ms step_avg:93.24ms
step:182/1750 train_time:16970ms step_avg:93.24ms
step:183/1750 train_time:17064ms step_avg:93.25ms
step:184/1750 train_time:17159ms step_avg:93.25ms
step:185/1750 train_time:17253ms step_avg:93.26ms
step:186/1750 train_time:17347ms step_avg:93.26ms
step:187/1750 train_time:17440ms step_avg:93.26ms
step:188/1750 train_time:17534ms step_avg:93.27ms
step:189/1750 train_time:17628ms step_avg:93.27ms
step:190/1750 train_time:17722ms step_avg:93.27ms
step:191/1750 train_time:17816ms step_avg:93.28ms
step:192/1750 train_time:17910ms step_avg:93.28ms
step:193/1750 train_time:18003ms step_avg:93.28ms
step:194/1750 train_time:18097ms step_avg:93.28ms
step:195/1750 train_time:18191ms step_avg:93.29ms
step:196/1750 train_time:18285ms step_avg:93.29ms
step:197/1750 train_time:18378ms step_avg:93.29ms
step:198/1750 train_time:18472ms step_avg:93.29ms
step:199/1750 train_time:18565ms step_avg:93.29ms
step:200/1750 train_time:18658ms step_avg:93.29ms
step:201/1750 train_time:18752ms step_avg:93.29ms
step:202/1750 train_time:18846ms step_avg:93.30ms
step:203/1750 train_time:18940ms step_avg:93.30ms
step:204/1750 train_time:19034ms step_avg:93.30ms
step:205/1750 train_time:19128ms step_avg:93.31ms
step:206/1750 train_time:19222ms step_avg:93.31ms
step:207/1750 train_time:19316ms step_avg:93.32ms
step:208/1750 train_time:19410ms step_avg:93.32ms
step:209/1750 train_time:19504ms step_avg:93.32ms
step:210/1750 train_time:19598ms step_avg:93.32ms
step:211/1750 train_time:19692ms step_avg:93.33ms
step:212/1750 train_time:19786ms step_avg:93.33ms
step:213/1750 train_time:19879ms step_avg:93.33ms
step:214/1750 train_time:20287ms step_avg:94.80ms
step:215/1750 train_time:20379ms step_avg:94.79ms
step:216/1750 train_time:20472ms step_avg:94.78ms
step:217/1750 train_time:20565ms step_avg:94.77ms
step:218/1750 train_time:20657ms step_avg:94.76ms
step:219/1750 train_time:20750ms step_avg:94.75ms
step:220/1750 train_time:20843ms step_avg:94.74ms
step:221/1750 train_time:20936ms step_avg:94.73ms
step:222/1750 train_time:21029ms step_avg:94.72ms
step:223/1750 train_time:21121ms step_avg:94.72ms
step:224/1750 train_time:21219ms step_avg:94.73ms
step:225/1750 train_time:21316ms step_avg:94.74ms
step:226/1750 train_time:21410ms step_avg:94.74ms
step:227/1750 train_time:21504ms step_avg:94.73ms
step:228/1750 train_time:21597ms step_avg:94.73ms
step:229/1750 train_time:21691ms step_avg:94.72ms
step:230/1750 train_time:21783ms step_avg:94.71ms
step:231/1750 train_time:21876ms step_avg:94.70ms
step:232/1750 train_time:21969ms step_avg:94.70ms
step:233/1750 train_time:22062ms step_avg:94.69ms
step:234/1750 train_time:22157ms step_avg:94.69ms
step:235/1750 train_time:22254ms step_avg:94.70ms
step:236/1750 train_time:22349ms step_avg:94.70ms
step:237/1750 train_time:22442ms step_avg:94.69ms
step:238/1750 train_time:22536ms step_avg:94.69ms
step:239/1750 train_time:22630ms step_avg:94.69ms
step:240/1750 train_time:22723ms step_avg:94.68ms
step:241/1750 train_time:22817ms step_avg:94.68ms
step:242/1750 train_time:22911ms step_avg:94.67ms
step:243/1750 train_time:23003ms step_avg:94.66ms
step:244/1750 train_time:23097ms step_avg:94.66ms
step:245/1750 train_time:23192ms step_avg:94.66ms
step:246/1750 train_time:23286ms step_avg:94.66ms
step:247/1750 train_time:23382ms step_avg:94.66ms
step:248/1750 train_time:23476ms step_avg:94.66ms
step:249/1750 train_time:23570ms step_avg:94.66ms
step:250/1750 train_time:23663ms step_avg:94.65ms
step:250/1750 val_loss:4.2428 train_time:23747ms step_avg:94.99ms
step:251/1750 train_time:23768ms step_avg:94.69ms
step:252/1750 train_time:23861ms step_avg:94.69ms
step:253/1750 train_time:23961ms step_avg:94.71ms
step:254/1750 train_time:24055ms step_avg:94.70ms
step:255/1750 train_time:24147ms step_avg:94.70ms
step:256/1750 train_time:24240ms step_avg:94.69ms
step:257/1750 train_time:24333ms step_avg:94.68ms
step:258/1750 train_time:24425ms step_avg:94.67ms
step:259/1750 train_time:24518ms step_avg:94.66ms
step:260/1750 train_time:24612ms step_avg:94.66ms
step:261/1750 train_time:24704ms step_avg:94.65ms
step:262/1750 train_time:24800ms step_avg:94.66ms
step:263/1750 train_time:24897ms step_avg:94.67ms
step:264/1750 train_time:24994ms step_avg:94.67ms
step:265/1750 train_time:25088ms step_avg:94.67ms
step:266/1750 train_time:25182ms step_avg:94.67ms
step:267/1750 train_time:25276ms step_avg:94.67ms
step:268/1750 train_time:25370ms step_avg:94.66ms
step:269/1750 train_time:25463ms step_avg:94.66ms
step:270/1750 train_time:25557ms step_avg:94.65ms
step:271/1750 train_time:25650ms step_avg:94.65ms
step:272/1750 train_time:25744ms step_avg:94.65ms
step:273/1750 train_time:25840ms step_avg:94.65ms
step:274/1750 train_time:25935ms step_avg:94.66ms
step:275/1750 train_time:26032ms step_avg:94.66ms
step:276/1750 train_time:26125ms step_avg:94.66ms
step:277/1750 train_time:26221ms step_avg:94.66ms
step:278/1750 train_time:26315ms step_avg:94.66ms
step:279/1750 train_time:26408ms step_avg:94.65ms
step:280/1750 train_time:26501ms step_avg:94.65ms
step:281/1750 train_time:26595ms step_avg:94.64ms
step:282/1750 train_time:26689ms step_avg:94.64ms
step:283/1750 train_time:26782ms step_avg:94.64ms
step:284/1750 train_time:26877ms step_avg:94.64ms
step:285/1750 train_time:26972ms step_avg:94.64ms
step:286/1750 train_time:27068ms step_avg:94.64ms
step:287/1750 train_time:27162ms step_avg:94.64ms
step:288/1750 train_time:27256ms step_avg:94.64ms
step:289/1750 train_time:27351ms step_avg:94.64ms
step:290/1750 train_time:27445ms step_avg:94.64ms
step:291/1750 train_time:27538ms step_avg:94.63ms
step:292/1750 train_time:27632ms step_avg:94.63ms
step:293/1750 train_time:27726ms step_avg:94.63ms
step:294/1750 train_time:27821ms step_avg:94.63ms
step:295/1750 train_time:27916ms step_avg:94.63ms
step:296/1750 train_time:28011ms step_avg:94.63ms
step:297/1750 train_time:28107ms step_avg:94.64ms
step:298/1750 train_time:28201ms step_avg:94.64ms
step:299/1750 train_time:28296ms step_avg:94.63ms
step:300/1750 train_time:28390ms step_avg:94.63ms
step:301/1750 train_time:28483ms step_avg:94.63ms
step:302/1750 train_time:28577ms step_avg:94.63ms
step:303/1750 train_time:28672ms step_avg:94.63ms
step:304/1750 train_time:28765ms step_avg:94.62ms
step:305/1750 train_time:28860ms step_avg:94.62ms
step:306/1750 train_time:28955ms step_avg:94.62ms
step:307/1750 train_time:29050ms step_avg:94.62ms
step:308/1750 train_time:29144ms step_avg:94.62ms
step:309/1750 train_time:29239ms step_avg:94.62ms
step:310/1750 train_time:29333ms step_avg:94.62ms
step:311/1750 train_time:29426ms step_avg:94.62ms
step:312/1750 train_time:29520ms step_avg:94.62ms
step:313/1750 train_time:29614ms step_avg:94.61ms
step:314/1750 train_time:29707ms step_avg:94.61ms
step:315/1750 train_time:29802ms step_avg:94.61ms
step:316/1750 train_time:29896ms step_avg:94.61ms
step:317/1750 train_time:29991ms step_avg:94.61ms
step:318/1750 train_time:30085ms step_avg:94.61ms
step:319/1750 train_time:30179ms step_avg:94.61ms
step:320/1750 train_time:30274ms step_avg:94.60ms
step:321/1750 train_time:30368ms step_avg:94.60ms
step:322/1750 train_time:30462ms step_avg:94.60ms
step:323/1750 train_time:30556ms step_avg:94.60ms
step:324/1750 train_time:30650ms step_avg:94.60ms
step:325/1750 train_time:30744ms step_avg:94.60ms
step:326/1750 train_time:30838ms step_avg:94.60ms
step:327/1750 train_time:30933ms step_avg:94.60ms
step:328/1750 train_time:31027ms step_avg:94.60ms
step:329/1750 train_time:31122ms step_avg:94.60ms
step:330/1750 train_time:31216ms step_avg:94.59ms
step:331/1750 train_time:31311ms step_avg:94.60ms
step:332/1750 train_time:31404ms step_avg:94.59ms
step:333/1750 train_time:31499ms step_avg:94.59ms
step:334/1750 train_time:31594ms step_avg:94.59ms
step:335/1750 train_time:31688ms step_avg:94.59ms
step:336/1750 train_time:31782ms step_avg:94.59ms
step:337/1750 train_time:31877ms step_avg:94.59ms
step:338/1750 train_time:31971ms step_avg:94.59ms
step:339/1750 train_time:32065ms step_avg:94.59ms
step:340/1750 train_time:32160ms step_avg:94.59ms
step:341/1750 train_time:32255ms step_avg:94.59ms
step:342/1750 train_time:32349ms step_avg:94.59ms
step:343/1750 train_time:32443ms step_avg:94.59ms
step:344/1750 train_time:32537ms step_avg:94.59ms
step:345/1750 train_time:32632ms step_avg:94.59ms
step:346/1750 train_time:32726ms step_avg:94.58ms
step:347/1750 train_time:32821ms step_avg:94.58ms
step:348/1750 train_time:32915ms step_avg:94.58ms
step:349/1750 train_time:33008ms step_avg:94.58ms
step:350/1750 train_time:33103ms step_avg:94.58ms
step:351/1750 train_time:33198ms step_avg:94.58ms
step:352/1750 train_time:33292ms step_avg:94.58ms
step:353/1750 train_time:33385ms step_avg:94.58ms
step:354/1750 train_time:33480ms step_avg:94.58ms
step:355/1750 train_time:33574ms step_avg:94.58ms
step:356/1750 train_time:33669ms step_avg:94.58ms
step:357/1750 train_time:33762ms step_avg:94.57ms
step:358/1750 train_time:33856ms step_avg:94.57ms
step:359/1750 train_time:33950ms step_avg:94.57ms
step:360/1750 train_time:34044ms step_avg:94.57ms
step:361/1750 train_time:34138ms step_avg:94.56ms
step:362/1750 train_time:34232ms step_avg:94.56ms
step:363/1750 train_time:34326ms step_avg:94.56ms
step:364/1750 train_time:34420ms step_avg:94.56ms
step:365/1750 train_time:34515ms step_avg:94.56ms
step:366/1750 train_time:34609ms step_avg:94.56ms
step:367/1750 train_time:34703ms step_avg:94.56ms
step:368/1750 train_time:34798ms step_avg:94.56ms
step:369/1750 train_time:34892ms step_avg:94.56ms
step:370/1750 train_time:34986ms step_avg:94.56ms
step:371/1750 train_time:35080ms step_avg:94.55ms
step:372/1750 train_time:35174ms step_avg:94.55ms
step:373/1750 train_time:35268ms step_avg:94.55ms
step:374/1750 train_time:35362ms step_avg:94.55ms
step:375/1750 train_time:35457ms step_avg:94.55ms
step:375/1750 val_loss:3.9934 train_time:35541ms step_avg:94.78ms
step:376/1750 train_time:35564ms step_avg:94.58ms
step:377/1750 train_time:35655ms step_avg:94.57ms
step:378/1750 train_time:35753ms step_avg:94.58ms
step:379/1750 train_time:35847ms step_avg:94.58ms
step:380/1750 train_time:35940ms step_avg:94.58ms
step:381/1750 train_time:36033ms step_avg:94.58ms
step:382/1750 train_time:36127ms step_avg:94.57ms
step:383/1750 train_time:36220ms step_avg:94.57ms
step:384/1750 train_time:36313ms step_avg:94.57ms
step:385/1750 train_time:36406ms step_avg:94.56ms
step:386/1750 train_time:36502ms step_avg:94.56ms
step:387/1750 train_time:36599ms step_avg:94.57ms
step:388/1750 train_time:36696ms step_avg:94.58ms
step:389/1750 train_time:36792ms step_avg:94.58ms
step:390/1750 train_time:36887ms step_avg:94.58ms
step:391/1750 train_time:36982ms step_avg:94.58ms
step:392/1750 train_time:37077ms step_avg:94.59ms
step:393/1750 train_time:37173ms step_avg:94.59ms
step:394/1750 train_time:37268ms step_avg:94.59ms
step:395/1750 train_time:37363ms step_avg:94.59ms
step:396/1750 train_time:37459ms step_avg:94.59ms
step:397/1750 train_time:37555ms step_avg:94.60ms
step:398/1750 train_time:37652ms step_avg:94.60ms
step:399/1750 train_time:37749ms step_avg:94.61ms
step:400/1750 train_time:37845ms step_avg:94.61ms
step:401/1750 train_time:37941ms step_avg:94.62ms
step:402/1750 train_time:38037ms step_avg:94.62ms
step:403/1750 train_time:38133ms step_avg:94.62ms
step:404/1750 train_time:38229ms step_avg:94.63ms
step:405/1750 train_time:38325ms step_avg:94.63ms
step:406/1750 train_time:38420ms step_avg:94.63ms
step:407/1750 train_time:38516ms step_avg:94.63ms
step:408/1750 train_time:38612ms step_avg:94.64ms
step:409/1750 train_time:38709ms step_avg:94.64ms
step:410/1750 train_time:38806ms step_avg:94.65ms
step:411/1750 train_time:38903ms step_avg:94.65ms
step:412/1750 train_time:38998ms step_avg:94.66ms
step:413/1750 train_time:39094ms step_avg:94.66ms
step:414/1750 train_time:39190ms step_avg:94.66ms
step:415/1750 train_time:39286ms step_avg:94.66ms
step:416/1750 train_time:39382ms step_avg:94.67ms
step:417/1750 train_time:39477ms step_avg:94.67ms
step:418/1750 train_time:39573ms step_avg:94.67ms
step:419/1750 train_time:39669ms step_avg:94.68ms
step:420/1750 train_time:39765ms step_avg:94.68ms
step:421/1750 train_time:39861ms step_avg:94.68ms
step:422/1750 train_time:39958ms step_avg:94.69ms
step:423/1750 train_time:40054ms step_avg:94.69ms
step:424/1750 train_time:40149ms step_avg:94.69ms
step:425/1750 train_time:40245ms step_avg:94.69ms
step:426/1750 train_time:40340ms step_avg:94.70ms
step:427/1750 train_time:40436ms step_avg:94.70ms
step:428/1750 train_time:40532ms step_avg:94.70ms
step:429/1750 train_time:40628ms step_avg:94.70ms
step:430/1750 train_time:40724ms step_avg:94.71ms
step:431/1750 train_time:40820ms step_avg:94.71ms
step:432/1750 train_time:40916ms step_avg:94.71ms
step:433/1750 train_time:41012ms step_avg:94.71ms
step:434/1750 train_time:41107ms step_avg:94.72ms
step:435/1750 train_time:41203ms step_avg:94.72ms
step:436/1750 train_time:41300ms step_avg:94.72ms
step:437/1750 train_time:41395ms step_avg:94.73ms
step:438/1750 train_time:41491ms step_avg:94.73ms
step:439/1750 train_time:41587ms step_avg:94.73ms
step:440/1750 train_time:41683ms step_avg:94.73ms
step:441/1750 train_time:41779ms step_avg:94.74ms
step:442/1750 train_time:41875ms step_avg:94.74ms
step:443/1750 train_time:41972ms step_avg:94.74ms
step:444/1750 train_time:42068ms step_avg:94.75ms
step:445/1750 train_time:42164ms step_avg:94.75ms
step:446/1750 train_time:42260ms step_avg:94.75ms
step:447/1750 train_time:42356ms step_avg:94.76ms
step:448/1750 train_time:42452ms step_avg:94.76ms
step:449/1750 train_time:42548ms step_avg:94.76ms
step:450/1750 train_time:42644ms step_avg:94.76ms
step:451/1750 train_time:42739ms step_avg:94.77ms
step:452/1750 train_time:42835ms step_avg:94.77ms
step:453/1750 train_time:42932ms step_avg:94.77ms
step:454/1750 train_time:43028ms step_avg:94.78ms
step:455/1750 train_time:43124ms step_avg:94.78ms
step:456/1750 train_time:43220ms step_avg:94.78ms
step:457/1750 train_time:43316ms step_avg:94.78ms
step:458/1750 train_time:43412ms step_avg:94.79ms
step:459/1750 train_time:43508ms step_avg:94.79ms
step:460/1750 train_time:43603ms step_avg:94.79ms
step:461/1750 train_time:43698ms step_avg:94.79ms
step:462/1750 train_time:43794ms step_avg:94.79ms
step:463/1750 train_time:43890ms step_avg:94.80ms
step:464/1750 train_time:43986ms step_avg:94.80ms
step:465/1750 train_time:44082ms step_avg:94.80ms
step:466/1750 train_time:44177ms step_avg:94.80ms
step:467/1750 train_time:44274ms step_avg:94.80ms
step:468/1750 train_time:44370ms step_avg:94.81ms
step:469/1750 train_time:44466ms step_avg:94.81ms
step:470/1750 train_time:44562ms step_avg:94.81ms
step:471/1750 train_time:44658ms step_avg:94.82ms
step:472/1750 train_time:44755ms step_avg:94.82ms
step:473/1750 train_time:44852ms step_avg:94.82ms
step:474/1750 train_time:44947ms step_avg:94.83ms
step:475/1750 train_time:45043ms step_avg:94.83ms
step:476/1750 train_time:45138ms step_avg:94.83ms
step:477/1750 train_time:45234ms step_avg:94.83ms
step:478/1750 train_time:45330ms step_avg:94.83ms
step:479/1750 train_time:45426ms step_avg:94.84ms
step:480/1750 train_time:45522ms step_avg:94.84ms
step:481/1750 train_time:45617ms step_avg:94.84ms
step:482/1750 train_time:45714ms step_avg:94.84ms
step:483/1750 train_time:45810ms step_avg:94.85ms
step:484/1750 train_time:45907ms step_avg:94.85ms
step:485/1750 train_time:46004ms step_avg:94.85ms
step:486/1750 train_time:46100ms step_avg:94.86ms
step:487/1750 train_time:46195ms step_avg:94.86ms
step:488/1750 train_time:46291ms step_avg:94.86ms
step:489/1750 train_time:46387ms step_avg:94.86ms
step:490/1750 train_time:46484ms step_avg:94.87ms
step:491/1750 train_time:46580ms step_avg:94.87ms
step:492/1750 train_time:46676ms step_avg:94.87ms
step:493/1750 train_time:46773ms step_avg:94.87ms
step:494/1750 train_time:46869ms step_avg:94.88ms
step:495/1750 train_time:46965ms step_avg:94.88ms
step:496/1750 train_time:47061ms step_avg:94.88ms
step:497/1750 train_time:47157ms step_avg:94.88ms
step:498/1750 train_time:47252ms step_avg:94.88ms
step:499/1750 train_time:47348ms step_avg:94.89ms
step:500/1750 train_time:47445ms step_avg:94.89ms
step:500/1750 val_loss:3.8226 train_time:47530ms step_avg:95.06ms
step:501/1750 train_time:47551ms step_avg:94.91ms
step:502/1750 train_time:47645ms step_avg:94.91ms
step:503/1750 train_time:47744ms step_avg:94.92ms
step:504/1750 train_time:47840ms step_avg:94.92ms
step:505/1750 train_time:47935ms step_avg:94.92ms
step:506/1750 train_time:48030ms step_avg:94.92ms
step:507/1750 train_time:48125ms step_avg:94.92ms
step:508/1750 train_time:48220ms step_avg:94.92ms
step:509/1750 train_time:48315ms step_avg:94.92ms
step:510/1750 train_time:48410ms step_avg:94.92ms
step:511/1750 train_time:48506ms step_avg:94.92ms
step:512/1750 train_time:48604ms step_avg:94.93ms
step:513/1750 train_time:48701ms step_avg:94.93ms
step:514/1750 train_time:48798ms step_avg:94.94ms
step:515/1750 train_time:48894ms step_avg:94.94ms
step:516/1750 train_time:48990ms step_avg:94.94ms
step:517/1750 train_time:49086ms step_avg:94.94ms
step:518/1750 train_time:49181ms step_avg:94.94ms
step:519/1750 train_time:49276ms step_avg:94.94ms
step:520/1750 train_time:49372ms step_avg:94.95ms
step:521/1750 train_time:49468ms step_avg:94.95ms
step:522/1750 train_time:49567ms step_avg:94.96ms
step:523/1750 train_time:49665ms step_avg:94.96ms
step:524/1750 train_time:49761ms step_avg:94.96ms
step:525/1750 train_time:49858ms step_avg:94.97ms
step:526/1750 train_time:49955ms step_avg:94.97ms
step:527/1750 train_time:50051ms step_avg:94.97ms
step:528/1750 train_time:50147ms step_avg:94.98ms
step:529/1750 train_time:50242ms step_avg:94.97ms
step:530/1750 train_time:50338ms step_avg:94.98ms
step:531/1750 train_time:50434ms step_avg:94.98ms
step:532/1750 train_time:50531ms step_avg:94.98ms
step:533/1750 train_time:50627ms step_avg:94.99ms
step:534/1750 train_time:50725ms step_avg:94.99ms
step:535/1750 train_time:50821ms step_avg:94.99ms
step:536/1750 train_time:50918ms step_avg:95.00ms
step:537/1750 train_time:51014ms step_avg:95.00ms
step:538/1750 train_time:51110ms step_avg:95.00ms
step:539/1750 train_time:51206ms step_avg:95.00ms
step:540/1750 train_time:51302ms step_avg:95.00ms
step:541/1750 train_time:51398ms step_avg:95.01ms
step:542/1750 train_time:51495ms step_avg:95.01ms
step:543/1750 train_time:51592ms step_avg:95.01ms
step:544/1750 train_time:51689ms step_avg:95.02ms
step:545/1750 train_time:51786ms step_avg:95.02ms
step:546/1750 train_time:51883ms step_avg:95.02ms
step:547/1750 train_time:51979ms step_avg:95.03ms
step:548/1750 train_time:52075ms step_avg:95.03ms
step:549/1750 train_time:52171ms step_avg:95.03ms
step:550/1750 train_time:52267ms step_avg:95.03ms
step:551/1750 train_time:52362ms step_avg:95.03ms
step:552/1750 train_time:52459ms step_avg:95.03ms
step:553/1750 train_time:52555ms step_avg:95.04ms
step:554/1750 train_time:52652ms step_avg:95.04ms
step:555/1750 train_time:52749ms step_avg:95.04ms
step:556/1750 train_time:52847ms step_avg:95.05ms
step:557/1750 train_time:52945ms step_avg:95.05ms
step:558/1750 train_time:53041ms step_avg:95.06ms
step:559/1750 train_time:53137ms step_avg:95.06ms
step:560/1750 train_time:53233ms step_avg:95.06ms
step:561/1750 train_time:53330ms step_avg:95.06ms
step:562/1750 train_time:53426ms step_avg:95.06ms
step:563/1750 train_time:53522ms step_avg:95.07ms
step:564/1750 train_time:53618ms step_avg:95.07ms
step:565/1750 train_time:53715ms step_avg:95.07ms
step:566/1750 train_time:53812ms step_avg:95.07ms
step:567/1750 train_time:53909ms step_avg:95.08ms
step:568/1750 train_time:54005ms step_avg:95.08ms
step:569/1750 train_time:54100ms step_avg:95.08ms
step:570/1750 train_time:54196ms step_avg:95.08ms
step:571/1750 train_time:54293ms step_avg:95.08ms
step:572/1750 train_time:54389ms step_avg:95.09ms
step:573/1750 train_time:54486ms step_avg:95.09ms
step:574/1750 train_time:54582ms step_avg:95.09ms
step:575/1750 train_time:54679ms step_avg:95.09ms
step:576/1750 train_time:54776ms step_avg:95.10ms
step:577/1750 train_time:54873ms step_avg:95.10ms
step:578/1750 train_time:54971ms step_avg:95.10ms
step:579/1750 train_time:55067ms step_avg:95.11ms
step:580/1750 train_time:55164ms step_avg:95.11ms
step:581/1750 train_time:55260ms step_avg:95.11ms
step:582/1750 train_time:55356ms step_avg:95.11ms
step:583/1750 train_time:55453ms step_avg:95.12ms
step:584/1750 train_time:55550ms step_avg:95.12ms
step:585/1750 train_time:55647ms step_avg:95.12ms
step:586/1750 train_time:55743ms step_avg:95.13ms
step:587/1750 train_time:55840ms step_avg:95.13ms
step:588/1750 train_time:55936ms step_avg:95.13ms
step:589/1750 train_time:56033ms step_avg:95.13ms
step:590/1750 train_time:56130ms step_avg:95.13ms
step:591/1750 train_time:56226ms step_avg:95.14ms
step:592/1750 train_time:56322ms step_avg:95.14ms
step:593/1750 train_time:56419ms step_avg:95.14ms
step:594/1750 train_time:56515ms step_avg:95.14ms
step:595/1750 train_time:56612ms step_avg:95.15ms
step:596/1750 train_time:56710ms step_avg:95.15ms
step:597/1750 train_time:56806ms step_avg:95.15ms
step:598/1750 train_time:56902ms step_avg:95.15ms
step:599/1750 train_time:56998ms step_avg:95.16ms
step:600/1750 train_time:57095ms step_avg:95.16ms
step:601/1750 train_time:57192ms step_avg:95.16ms
step:602/1750 train_time:57288ms step_avg:95.16ms
step:603/1750 train_time:57385ms step_avg:95.17ms
step:604/1750 train_time:57480ms step_avg:95.17ms
step:605/1750 train_time:57577ms step_avg:95.17ms
step:606/1750 train_time:57675ms step_avg:95.17ms
step:607/1750 train_time:57772ms step_avg:95.18ms
step:608/1750 train_time:57868ms step_avg:95.18ms
step:609/1750 train_time:57964ms step_avg:95.18ms
step:610/1750 train_time:58060ms step_avg:95.18ms
step:611/1750 train_time:58156ms step_avg:95.18ms
step:612/1750 train_time:58253ms step_avg:95.18ms
step:613/1750 train_time:58350ms step_avg:95.19ms
step:614/1750 train_time:58446ms step_avg:95.19ms
step:615/1750 train_time:58542ms step_avg:95.19ms
step:616/1750 train_time:58638ms step_avg:95.19ms
step:617/1750 train_time:58735ms step_avg:95.19ms
step:618/1750 train_time:58832ms step_avg:95.20ms
step:619/1750 train_time:58929ms step_avg:95.20ms
step:620/1750 train_time:59026ms step_avg:95.20ms
step:621/1750 train_time:59122ms step_avg:95.20ms
step:622/1750 train_time:59218ms step_avg:95.21ms
step:623/1750 train_time:59315ms step_avg:95.21ms
step:624/1750 train_time:59411ms step_avg:95.21ms
step:625/1750 train_time:59507ms step_avg:95.21ms
step:625/1750 val_loss:3.7208 train_time:59593ms step_avg:95.35ms
step:626/1750 train_time:59613ms step_avg:95.23ms
step:627/1750 train_time:59709ms step_avg:95.23ms
step:628/1750 train_time:59808ms step_avg:95.24ms
step:629/1750 train_time:59905ms step_avg:95.24ms
step:630/1750 train_time:60000ms step_avg:95.24ms
step:631/1750 train_time:60095ms step_avg:95.24ms
step:632/1750 train_time:60191ms step_avg:95.24ms
step:633/1750 train_time:60286ms step_avg:95.24ms
step:634/1750 train_time:60382ms step_avg:95.24ms
step:635/1750 train_time:60477ms step_avg:95.24ms
step:636/1750 train_time:60574ms step_avg:95.24ms
step:637/1750 train_time:60672ms step_avg:95.25ms
step:638/1750 train_time:60770ms step_avg:95.25ms
step:639/1750 train_time:60867ms step_avg:95.25ms
step:640/1750 train_time:60965ms step_avg:95.26ms
step:641/1750 train_time:61061ms step_avg:95.26ms
step:642/1750 train_time:61157ms step_avg:95.26ms
step:643/1750 train_time:61253ms step_avg:95.26ms
step:644/1750 train_time:61349ms step_avg:95.26ms
step:645/1750 train_time:61445ms step_avg:95.26ms
step:646/1750 train_time:61541ms step_avg:95.27ms
step:647/1750 train_time:61639ms step_avg:95.27ms
step:648/1750 train_time:61736ms step_avg:95.27ms
step:649/1750 train_time:61833ms step_avg:95.27ms
step:650/1750 train_time:61930ms step_avg:95.28ms
step:651/1750 train_time:62028ms step_avg:95.28ms
step:652/1750 train_time:62126ms step_avg:95.29ms
step:653/1750 train_time:62224ms step_avg:95.29ms
step:654/1750 train_time:62321ms step_avg:95.29ms
step:655/1750 train_time:62418ms step_avg:95.30ms
step:656/1750 train_time:62516ms step_avg:95.30ms
step:657/1750 train_time:62613ms step_avg:95.30ms
step:658/1750 train_time:62712ms step_avg:95.31ms
step:659/1750 train_time:62810ms step_avg:95.31ms
step:660/1750 train_time:62908ms step_avg:95.32ms
step:661/1750 train_time:63006ms step_avg:95.32ms
step:662/1750 train_time:63104ms step_avg:95.32ms
step:663/1750 train_time:63201ms step_avg:95.33ms
step:664/1750 train_time:63298ms step_avg:95.33ms
step:665/1750 train_time:63394ms step_avg:95.33ms
step:666/1750 train_time:63493ms step_avg:95.33ms
step:667/1750 train_time:63591ms step_avg:95.34ms
step:668/1750 train_time:63689ms step_avg:95.34ms
step:669/1750 train_time:63787ms step_avg:95.35ms
step:670/1750 train_time:63887ms step_avg:95.35ms
step:671/1750 train_time:63985ms step_avg:95.36ms
step:672/1750 train_time:64082ms step_avg:95.36ms
step:673/1750 train_time:64180ms step_avg:95.36ms
step:674/1750 train_time:64277ms step_avg:95.37ms
step:675/1750 train_time:64375ms step_avg:95.37ms
step:676/1750 train_time:64472ms step_avg:95.37ms
step:677/1750 train_time:64570ms step_avg:95.38ms
step:678/1750 train_time:64668ms step_avg:95.38ms
step:679/1750 train_time:64766ms step_avg:95.38ms
step:680/1750 train_time:64864ms step_avg:95.39ms
step:681/1750 train_time:64962ms step_avg:95.39ms
step:682/1750 train_time:65060ms step_avg:95.40ms
step:683/1750 train_time:65158ms step_avg:95.40ms
step:684/1750 train_time:65255ms step_avg:95.40ms
step:685/1750 train_time:65352ms step_avg:95.40ms
step:686/1750 train_time:65449ms step_avg:95.41ms
step:687/1750 train_time:65547ms step_avg:95.41ms
step:688/1750 train_time:65645ms step_avg:95.41ms
step:689/1750 train_time:65743ms step_avg:95.42ms
step:690/1750 train_time:65841ms step_avg:95.42ms
step:691/1750 train_time:65939ms step_avg:95.43ms
step:692/1750 train_time:66036ms step_avg:95.43ms
step:693/1750 train_time:66135ms step_avg:95.43ms
step:694/1750 train_time:66232ms step_avg:95.44ms
step:695/1750 train_time:66330ms step_avg:95.44ms
step:696/1750 train_time:66428ms step_avg:95.44ms
step:697/1750 train_time:66526ms step_avg:95.45ms
step:698/1750 train_time:66623ms step_avg:95.45ms
step:699/1750 train_time:66721ms step_avg:95.45ms
step:700/1750 train_time:66819ms step_avg:95.46ms
step:701/1750 train_time:66917ms step_avg:95.46ms
step:702/1750 train_time:67016ms step_avg:95.46ms
step:703/1750 train_time:67115ms step_avg:95.47ms
step:704/1750 train_time:67212ms step_avg:95.47ms
step:705/1750 train_time:67310ms step_avg:95.48ms
step:706/1750 train_time:67408ms step_avg:95.48ms
step:707/1750 train_time:67506ms step_avg:95.48ms
step:708/1750 train_time:67604ms step_avg:95.49ms
step:709/1750 train_time:67701ms step_avg:95.49ms
step:710/1750 train_time:67799ms step_avg:95.49ms
step:711/1750 train_time:67897ms step_avg:95.49ms
step:712/1750 train_time:67994ms step_avg:95.50ms
step:713/1750 train_time:68093ms step_avg:95.50ms
step:714/1750 train_time:68191ms step_avg:95.51ms
step:715/1750 train_time:68289ms step_avg:95.51ms
step:716/1750 train_time:68386ms step_avg:95.51ms
step:717/1750 train_time:68484ms step_avg:95.51ms
step:718/1750 train_time:68582ms step_avg:95.52ms
step:719/1750 train_time:68680ms step_avg:95.52ms
step:720/1750 train_time:68778ms step_avg:95.52ms
step:721/1750 train_time:68876ms step_avg:95.53ms
step:722/1750 train_time:68974ms step_avg:95.53ms
step:723/1750 train_time:69072ms step_avg:95.53ms
step:724/1750 train_time:69170ms step_avg:95.54ms
step:725/1750 train_time:69267ms step_avg:95.54ms
step:726/1750 train_time:69364ms step_avg:95.54ms
step:727/1750 train_time:69461ms step_avg:95.55ms
step:728/1750 train_time:69560ms step_avg:95.55ms
step:729/1750 train_time:69658ms step_avg:95.55ms
step:730/1750 train_time:69755ms step_avg:95.56ms
step:731/1750 train_time:69853ms step_avg:95.56ms
step:732/1750 train_time:69951ms step_avg:95.56ms
step:733/1750 train_time:70048ms step_avg:95.56ms
step:734/1750 train_time:70147ms step_avg:95.57ms
step:735/1750 train_time:70246ms step_avg:95.57ms
step:736/1750 train_time:70344ms step_avg:95.58ms
step:737/1750 train_time:70442ms step_avg:95.58ms
step:738/1750 train_time:70540ms step_avg:95.58ms
step:739/1750 train_time:70638ms step_avg:95.59ms
step:740/1750 train_time:70736ms step_avg:95.59ms
step:741/1750 train_time:70833ms step_avg:95.59ms
step:742/1750 train_time:70931ms step_avg:95.59ms
step:743/1750 train_time:71029ms step_avg:95.60ms
step:744/1750 train_time:71127ms step_avg:95.60ms
step:745/1750 train_time:71225ms step_avg:95.60ms
step:746/1750 train_time:71322ms step_avg:95.61ms
step:747/1750 train_time:71420ms step_avg:95.61ms
step:748/1750 train_time:71518ms step_avg:95.61ms
step:749/1750 train_time:71616ms step_avg:95.62ms
step:750/1750 train_time:71714ms step_avg:95.62ms
step:750/1750 val_loss:3.6518 train_time:71800ms step_avg:95.73ms
step:751/1750 train_time:71820ms step_avg:95.63ms
step:752/1750 train_time:71918ms step_avg:95.64ms
step:753/1750 train_time:72019ms step_avg:95.64ms
step:754/1750 train_time:72118ms step_avg:95.65ms
step:755/1750 train_time:72215ms step_avg:95.65ms
step:756/1750 train_time:72312ms step_avg:95.65ms
step:757/1750 train_time:72410ms step_avg:95.65ms
step:758/1750 train_time:72506ms step_avg:95.65ms
step:759/1750 train_time:72603ms step_avg:95.66ms
step:760/1750 train_time:72700ms step_avg:95.66ms
step:761/1750 train_time:72799ms step_avg:95.66ms
step:762/1750 train_time:72900ms step_avg:95.67ms
step:763/1750 train_time:72998ms step_avg:95.67ms
step:764/1750 train_time:73097ms step_avg:95.68ms
step:765/1750 train_time:73194ms step_avg:95.68ms
step:766/1750 train_time:73292ms step_avg:95.68ms
step:767/1750 train_time:73390ms step_avg:95.68ms
step:768/1750 train_time:73486ms step_avg:95.68ms
step:769/1750 train_time:73583ms step_avg:95.69ms
step:770/1750 train_time:73680ms step_avg:95.69ms
step:771/1750 train_time:73777ms step_avg:95.69ms
step:772/1750 train_time:73877ms step_avg:95.69ms
step:773/1750 train_time:73974ms step_avg:95.70ms
step:774/1750 train_time:74074ms step_avg:95.70ms
step:775/1750 train_time:74170ms step_avg:95.70ms
step:776/1750 train_time:74268ms step_avg:95.71ms
step:777/1750 train_time:74365ms step_avg:95.71ms
step:778/1750 train_time:74463ms step_avg:95.71ms
step:779/1750 train_time:74560ms step_avg:95.71ms
step:780/1750 train_time:74658ms step_avg:95.72ms
step:781/1750 train_time:74756ms step_avg:95.72ms
step:782/1750 train_time:74856ms step_avg:95.72ms
step:783/1750 train_time:74954ms step_avg:95.73ms
step:784/1750 train_time:75054ms step_avg:95.73ms
step:785/1750 train_time:75152ms step_avg:95.73ms
step:786/1750 train_time:75250ms step_avg:95.74ms
step:787/1750 train_time:75347ms step_avg:95.74ms
step:788/1750 train_time:75445ms step_avg:95.74ms
step:789/1750 train_time:75542ms step_avg:95.74ms
step:790/1750 train_time:75640ms step_avg:95.75ms
step:791/1750 train_time:75739ms step_avg:95.75ms
step:792/1750 train_time:75837ms step_avg:95.75ms
step:793/1750 train_time:75935ms step_avg:95.76ms
step:794/1750 train_time:76034ms step_avg:95.76ms
step:795/1750 train_time:76132ms step_avg:95.76ms
step:796/1750 train_time:76232ms step_avg:95.77ms
step:797/1750 train_time:76329ms step_avg:95.77ms
step:798/1750 train_time:76427ms step_avg:95.77ms
step:799/1750 train_time:76526ms step_avg:95.78ms
step:800/1750 train_time:76624ms step_avg:95.78ms
step:801/1750 train_time:76722ms step_avg:95.78ms
step:802/1750 train_time:76821ms step_avg:95.79ms
step:803/1750 train_time:76920ms step_avg:95.79ms
step:804/1750 train_time:77019ms step_avg:95.79ms
step:805/1750 train_time:77118ms step_avg:95.80ms
step:806/1750 train_time:77217ms step_avg:95.80ms
step:807/1750 train_time:77316ms step_avg:95.81ms
step:808/1750 train_time:77415ms step_avg:95.81ms
step:809/1750 train_time:77513ms step_avg:95.81ms
step:810/1750 train_time:77611ms step_avg:95.82ms
step:811/1750 train_time:77710ms step_avg:95.82ms
step:812/1750 train_time:77809ms step_avg:95.82ms
step:813/1750 train_time:77907ms step_avg:95.83ms
step:814/1750 train_time:78005ms step_avg:95.83ms
step:815/1750 train_time:78103ms step_avg:95.83ms
step:816/1750 train_time:78202ms step_avg:95.84ms
step:817/1750 train_time:78300ms step_avg:95.84ms
step:818/1750 train_time:78398ms step_avg:95.84ms
step:819/1750 train_time:78498ms step_avg:95.85ms
step:820/1750 train_time:78597ms step_avg:95.85ms
step:821/1750 train_time:78696ms step_avg:95.85ms
step:822/1750 train_time:78794ms step_avg:95.86ms
step:823/1750 train_time:78892ms step_avg:95.86ms
step:824/1750 train_time:78991ms step_avg:95.86ms
step:825/1750 train_time:79089ms step_avg:95.87ms
step:826/1750 train_time:79188ms step_avg:95.87ms
step:827/1750 train_time:79287ms step_avg:95.87ms
step:828/1750 train_time:79385ms step_avg:95.88ms
step:829/1750 train_time:79483ms step_avg:95.88ms
step:830/1750 train_time:79581ms step_avg:95.88ms
step:831/1750 train_time:79681ms step_avg:95.89ms
step:832/1750 train_time:79780ms step_avg:95.89ms
step:833/1750 train_time:79879ms step_avg:95.89ms
step:834/1750 train_time:79977ms step_avg:95.90ms
step:835/1750 train_time:80075ms step_avg:95.90ms
step:836/1750 train_time:80174ms step_avg:95.90ms
step:837/1750 train_time:80273ms step_avg:95.91ms
step:838/1750 train_time:80372ms step_avg:95.91ms
step:839/1750 train_time:80472ms step_avg:95.91ms
step:840/1750 train_time:80572ms step_avg:95.92ms
step:841/1750 train_time:80670ms step_avg:95.92ms
step:842/1750 train_time:80769ms step_avg:95.93ms
step:843/1750 train_time:80868ms step_avg:95.93ms
step:844/1750 train_time:80966ms step_avg:95.93ms
step:845/1750 train_time:81063ms step_avg:95.93ms
step:846/1750 train_time:81161ms step_avg:95.94ms
step:847/1750 train_time:81259ms step_avg:95.94ms
step:848/1750 train_time:81358ms step_avg:95.94ms
step:849/1750 train_time:81456ms step_avg:95.94ms
step:850/1750 train_time:81555ms step_avg:95.95ms
step:851/1750 train_time:81654ms step_avg:95.95ms
step:852/1750 train_time:81752ms step_avg:95.95ms
step:853/1750 train_time:81850ms step_avg:95.96ms
step:854/1750 train_time:81949ms step_avg:95.96ms
step:855/1750 train_time:82047ms step_avg:95.96ms
step:856/1750 train_time:82146ms step_avg:95.96ms
step:857/1750 train_time:82244ms step_avg:95.97ms
step:858/1750 train_time:82342ms step_avg:95.97ms
step:859/1750 train_time:82441ms step_avg:95.97ms
step:860/1750 train_time:82539ms step_avg:95.98ms
step:861/1750 train_time:82637ms step_avg:95.98ms
step:862/1750 train_time:82737ms step_avg:95.98ms
step:863/1750 train_time:82835ms step_avg:95.99ms
step:864/1750 train_time:82934ms step_avg:95.99ms
step:865/1750 train_time:83033ms step_avg:95.99ms
step:866/1750 train_time:83131ms step_avg:95.99ms
step:867/1750 train_time:83230ms step_avg:96.00ms
step:868/1750 train_time:83327ms step_avg:96.00ms
step:869/1750 train_time:83426ms step_avg:96.00ms
step:870/1750 train_time:83524ms step_avg:96.00ms
step:871/1750 train_time:83622ms step_avg:96.01ms
step:872/1750 train_time:83721ms step_avg:96.01ms
step:873/1750 train_time:83821ms step_avg:96.01ms
step:874/1750 train_time:83920ms step_avg:96.02ms
step:875/1750 train_time:84019ms step_avg:96.02ms
step:875/1750 val_loss:3.5982 train_time:84107ms step_avg:96.12ms
step:876/1750 train_time:84127ms step_avg:96.04ms
step:877/1750 train_time:84228ms step_avg:96.04ms
step:878/1750 train_time:84329ms step_avg:96.05ms
step:879/1750 train_time:84427ms step_avg:96.05ms
step:880/1750 train_time:84524ms step_avg:96.05ms
step:881/1750 train_time:84622ms step_avg:96.05ms
step:882/1750 train_time:84719ms step_avg:96.05ms
step:883/1750 train_time:84816ms step_avg:96.05ms
step:884/1750 train_time:84913ms step_avg:96.06ms
step:885/1750 train_time:85010ms step_avg:96.06ms
step:886/1750 train_time:85109ms step_avg:96.06ms
step:887/1750 train_time:85209ms step_avg:96.06ms
step:888/1750 train_time:85308ms step_avg:96.07ms
step:889/1750 train_time:85407ms step_avg:96.07ms
step:890/1750 train_time:85506ms step_avg:96.07ms
step:891/1750 train_time:85604ms step_avg:96.08ms
step:892/1750 train_time:85702ms step_avg:96.08ms
step:893/1750 train_time:85800ms step_avg:96.08ms
step:894/1750 train_time:85897ms step_avg:96.08ms
step:895/1750 train_time:85994ms step_avg:96.08ms
step:896/1750 train_time:86093ms step_avg:96.09ms
step:897/1750 train_time:86192ms step_avg:96.09ms
step:898/1750 train_time:86290ms step_avg:96.09ms
step:899/1750 train_time:86389ms step_avg:96.09ms
step:900/1750 train_time:86488ms step_avg:96.10ms
step:901/1750 train_time:86585ms step_avg:96.10ms
step:902/1750 train_time:86685ms step_avg:96.10ms
step:903/1750 train_time:86784ms step_avg:96.11ms
step:904/1750 train_time:86881ms step_avg:96.11ms
step:905/1750 train_time:86979ms step_avg:96.11ms
step:906/1750 train_time:87077ms step_avg:96.11ms
step:907/1750 train_time:87175ms step_avg:96.11ms
step:908/1750 train_time:87273ms step_avg:96.12ms
step:909/1750 train_time:87373ms step_avg:96.12ms
step:910/1750 train_time:87473ms step_avg:96.12ms
step:911/1750 train_time:87574ms step_avg:96.13ms
step:912/1750 train_time:87675ms step_avg:96.13ms
step:913/1750 train_time:87775ms step_avg:96.14ms
step:914/1750 train_time:87875ms step_avg:96.14ms
step:915/1750 train_time:87974ms step_avg:96.15ms
step:916/1750 train_time:88073ms step_avg:96.15ms
step:917/1750 train_time:88171ms step_avg:96.15ms
step:918/1750 train_time:88271ms step_avg:96.16ms
step:919/1750 train_time:88370ms step_avg:96.16ms
step:920/1750 train_time:88471ms step_avg:96.16ms
step:921/1750 train_time:88571ms step_avg:96.17ms
step:922/1750 train_time:88671ms step_avg:96.17ms
step:923/1750 train_time:88769ms step_avg:96.17ms
step:924/1750 train_time:88871ms step_avg:96.18ms
step:925/1750 train_time:88969ms step_avg:96.18ms
step:926/1750 train_time:89068ms step_avg:96.19ms
step:927/1750 train_time:89167ms step_avg:96.19ms
step:928/1750 train_time:89267ms step_avg:96.19ms
step:929/1750 train_time:89366ms step_avg:96.20ms
step:930/1750 train_time:89466ms step_avg:96.20ms
step:931/1750 train_time:89567ms step_avg:96.20ms
step:932/1750 train_time:89667ms step_avg:96.21ms
step:933/1750 train_time:89767ms step_avg:96.21ms
step:934/1750 train_time:89868ms step_avg:96.22ms
step:935/1750 train_time:89968ms step_avg:96.22ms
step:936/1750 train_time:90068ms step_avg:96.23ms
step:937/1750 train_time:90168ms step_avg:96.23ms
step:938/1750 train_time:90267ms step_avg:96.23ms
step:939/1750 train_time:90367ms step_avg:96.24ms
step:940/1750 train_time:90467ms step_avg:96.24ms
step:941/1750 train_time:90568ms step_avg:96.25ms
step:942/1750 train_time:90668ms step_avg:96.25ms
step:943/1750 train_time:90768ms step_avg:96.25ms
step:944/1750 train_time:90867ms step_avg:96.26ms
step:945/1750 train_time:90967ms step_avg:96.26ms
step:946/1750 train_time:91066ms step_avg:96.26ms
step:947/1750 train_time:91166ms step_avg:96.27ms
step:948/1750 train_time:91266ms step_avg:96.27ms
step:949/1750 train_time:91365ms step_avg:96.27ms
step:950/1750 train_time:91465ms step_avg:96.28ms
step:951/1750 train_time:91565ms step_avg:96.28ms
step:952/1750 train_time:91665ms step_avg:96.29ms
step:953/1750 train_time:91765ms step_avg:96.29ms
step:954/1750 train_time:91866ms step_avg:96.30ms
step:955/1750 train_time:91967ms step_avg:96.30ms
step:956/1750 train_time:92066ms step_avg:96.30ms
step:957/1750 train_time:92166ms step_avg:96.31ms
step:958/1750 train_time:92266ms step_avg:96.31ms
step:959/1750 train_time:92366ms step_avg:96.31ms
step:960/1750 train_time:92466ms step_avg:96.32ms
step:961/1750 train_time:92565ms step_avg:96.32ms
step:962/1750 train_time:92665ms step_avg:96.33ms
step:963/1750 train_time:92766ms step_avg:96.33ms
step:964/1750 train_time:92867ms step_avg:96.33ms
step:965/1750 train_time:92966ms step_avg:96.34ms
step:966/1750 train_time:93065ms step_avg:96.34ms
step:967/1750 train_time:93166ms step_avg:96.35ms
step:968/1750 train_time:93267ms step_avg:96.35ms
step:969/1750 train_time:93367ms step_avg:96.35ms
step:970/1750 train_time:93467ms step_avg:96.36ms
step:971/1750 train_time:93568ms step_avg:96.36ms
step:972/1750 train_time:93668ms step_avg:96.37ms
step:973/1750 train_time:93767ms step_avg:96.37ms
step:974/1750 train_time:93867ms step_avg:96.37ms
step:975/1750 train_time:93967ms step_avg:96.38ms
step:976/1750 train_time:94066ms step_avg:96.38ms
step:977/1750 train_time:94166ms step_avg:96.38ms
step:978/1750 train_time:94267ms step_avg:96.39ms
step:979/1750 train_time:94367ms step_avg:96.39ms
step:980/1750 train_time:94466ms step_avg:96.39ms
step:981/1750 train_time:94566ms step_avg:96.40ms
step:982/1750 train_time:94667ms step_avg:96.40ms
step:983/1750 train_time:94766ms step_avg:96.41ms
step:984/1750 train_time:94865ms step_avg:96.41ms
step:985/1750 train_time:94965ms step_avg:96.41ms
step:986/1750 train_time:95064ms step_avg:96.41ms
step:987/1750 train_time:95164ms step_avg:96.42ms
step:988/1750 train_time:95264ms step_avg:96.42ms
step:989/1750 train_time:95363ms step_avg:96.42ms
step:990/1750 train_time:95464ms step_avg:96.43ms
step:991/1750 train_time:95564ms step_avg:96.43ms
step:992/1750 train_time:95664ms step_avg:96.44ms
step:993/1750 train_time:95764ms step_avg:96.44ms
step:994/1750 train_time:95864ms step_avg:96.44ms
step:995/1750 train_time:95964ms step_avg:96.45ms
step:996/1750 train_time:96063ms step_avg:96.45ms
step:997/1750 train_time:96163ms step_avg:96.45ms
step:998/1750 train_time:96262ms step_avg:96.46ms
step:999/1750 train_time:96363ms step_avg:96.46ms
step:1000/1750 train_time:96463ms step_avg:96.46ms
step:1000/1750 val_loss:3.5512 train_time:96552ms step_avg:96.55ms
step:1001/1750 train_time:96575ms step_avg:96.48ms
step:1002/1750 train_time:96669ms step_avg:96.48ms
step:1003/1750 train_time:96774ms step_avg:96.48ms
step:1004/1750 train_time:96873ms step_avg:96.49ms
step:1005/1750 train_time:96971ms step_avg:96.49ms
step:1006/1750 train_time:97070ms step_avg:96.49ms
step:1007/1750 train_time:97169ms step_avg:96.49ms
step:1008/1750 train_time:97268ms step_avg:96.50ms
step:1009/1750 train_time:97367ms step_avg:96.50ms
step:1010/1750 train_time:97467ms step_avg:96.50ms
step:1011/1750 train_time:97568ms step_avg:96.51ms
step:1012/1750 train_time:97669ms step_avg:96.51ms
step:1013/1750 train_time:97770ms step_avg:96.52ms
step:1014/1750 train_time:97871ms step_avg:96.52ms
step:1015/1750 train_time:97970ms step_avg:96.52ms
step:1016/1750 train_time:98069ms step_avg:96.52ms
step:1017/1750 train_time:98168ms step_avg:96.53ms
step:1018/1750 train_time:98267ms step_avg:96.53ms
step:1019/1750 train_time:98365ms step_avg:96.53ms
step:1020/1750 train_time:98465ms step_avg:96.53ms
step:1021/1750 train_time:98565ms step_avg:96.54ms
step:1022/1750 train_time:98666ms step_avg:96.54ms
step:1023/1750 train_time:98767ms step_avg:96.55ms
step:1024/1750 train_time:98867ms step_avg:96.55ms
step:1025/1750 train_time:98967ms step_avg:96.55ms
step:1026/1750 train_time:99067ms step_avg:96.56ms
step:1027/1750 train_time:99166ms step_avg:96.56ms
step:1028/1750 train_time:99265ms step_avg:96.56ms
step:1029/1750 train_time:99364ms step_avg:96.56ms
step:1030/1750 train_time:99463ms step_avg:96.57ms
step:1031/1750 train_time:99564ms step_avg:96.57ms
step:1032/1750 train_time:99664ms step_avg:96.57ms
step:1033/1750 train_time:99765ms step_avg:96.58ms
step:1034/1750 train_time:99865ms step_avg:96.58ms
step:1035/1750 train_time:99965ms step_avg:96.58ms
step:1036/1750 train_time:100065ms step_avg:96.59ms
step:1037/1750 train_time:100166ms step_avg:96.59ms
step:1038/1750 train_time:100265ms step_avg:96.59ms
step:1039/1750 train_time:100363ms step_avg:96.60ms
step:1040/1750 train_time:100463ms step_avg:96.60ms
step:1041/1750 train_time:100563ms step_avg:96.60ms
step:1042/1750 train_time:100663ms step_avg:96.61ms
step:1043/1750 train_time:100763ms step_avg:96.61ms
step:1044/1750 train_time:100864ms step_avg:96.61ms
step:1045/1750 train_time:100964ms step_avg:96.62ms
step:1046/1750 train_time:101064ms step_avg:96.62ms
step:1047/1750 train_time:101165ms step_avg:96.62ms
step:1048/1750 train_time:101263ms step_avg:96.63ms
step:1049/1750 train_time:101363ms step_avg:96.63ms
step:1050/1750 train_time:101463ms step_avg:96.63ms
step:1051/1750 train_time:101814ms step_avg:96.87ms
step:1052/1750 train_time:101912ms step_avg:96.87ms
step:1053/1750 train_time:102012ms step_avg:96.88ms
step:1054/1750 train_time:102111ms step_avg:96.88ms
step:1055/1750 train_time:102211ms step_avg:96.88ms
step:1056/1750 train_time:102310ms step_avg:96.88ms
step:1057/1750 train_time:102409ms step_avg:96.89ms
step:1058/1750 train_time:102507ms step_avg:96.89ms
step:1059/1750 train_time:102605ms step_avg:96.89ms
step:1060/1750 train_time:102708ms step_avg:96.89ms
step:1061/1750 train_time:102813ms step_avg:96.90ms
step:1062/1750 train_time:102913ms step_avg:96.91ms
step:1063/1750 train_time:103014ms step_avg:96.91ms
step:1064/1750 train_time:103113ms step_avg:96.91ms
step:1065/1750 train_time:103214ms step_avg:96.91ms
step:1066/1750 train_time:103312ms step_avg:96.92ms
step:1067/1750 train_time:103413ms step_avg:96.92ms
step:1068/1750 train_time:103512ms step_avg:96.92ms
step:1069/1750 train_time:103614ms step_avg:96.93ms
step:1070/1750 train_time:103716ms step_avg:96.93ms
step:1071/1750 train_time:103818ms step_avg:96.94ms
step:1072/1750 train_time:103919ms step_avg:96.94ms
step:1073/1750 train_time:104019ms step_avg:96.94ms
step:1074/1750 train_time:104119ms step_avg:96.95ms
step:1075/1750 train_time:104219ms step_avg:96.95ms
step:1076/1750 train_time:104601ms step_avg:97.21ms
step:1077/1750 train_time:104699ms step_avg:97.21ms
step:1078/1750 train_time:104798ms step_avg:97.21ms
step:1079/1750 train_time:104897ms step_avg:97.22ms
step:1080/1750 train_time:104995ms step_avg:97.22ms
step:1081/1750 train_time:105094ms step_avg:97.22ms
step:1082/1750 train_time:105193ms step_avg:97.22ms
step:1083/1750 train_time:105291ms step_avg:97.22ms
step:1084/1750 train_time:105391ms step_avg:97.22ms
step:1085/1750 train_time:105493ms step_avg:97.23ms
step:1086/1750 train_time:105601ms step_avg:97.24ms
step:1087/1750 train_time:105701ms step_avg:97.24ms
step:1088/1750 train_time:105799ms step_avg:97.24ms
step:1089/1750 train_time:105898ms step_avg:97.24ms
step:1090/1750 train_time:105997ms step_avg:97.25ms
step:1091/1750 train_time:106096ms step_avg:97.25ms
step:1092/1750 train_time:106196ms step_avg:97.25ms
step:1093/1750 train_time:106295ms step_avg:97.25ms
step:1094/1750 train_time:106395ms step_avg:97.25ms
step:1095/1750 train_time:106498ms step_avg:97.26ms
step:1096/1750 train_time:106892ms step_avg:97.53ms
step:1097/1750 train_time:106989ms step_avg:97.53ms
step:1098/1750 train_time:107089ms step_avg:97.53ms
step:1099/1750 train_time:107189ms step_avg:97.53ms
step:1100/1750 train_time:107286ms step_avg:97.53ms
step:1101/1750 train_time:107385ms step_avg:97.53ms
step:1102/1750 train_time:107484ms step_avg:97.54ms
step:1103/1750 train_time:107582ms step_avg:97.54ms
step:1104/1750 train_time:107680ms step_avg:97.54ms
step:1105/1750 train_time:107787ms step_avg:97.54ms
step:1106/1750 train_time:107890ms step_avg:97.55ms
step:1107/1750 train_time:107989ms step_avg:97.55ms
step:1108/1750 train_time:108089ms step_avg:97.55ms
step:1109/1750 train_time:108189ms step_avg:97.56ms
step:1110/1750 train_time:108289ms step_avg:97.56ms
step:1111/1750 train_time:108388ms step_avg:97.56ms
step:1112/1750 train_time:108488ms step_avg:97.56ms
step:1113/1750 train_time:108588ms step_avg:97.56ms
step:1114/1750 train_time:108688ms step_avg:97.57ms
step:1115/1750 train_time:108789ms step_avg:97.57ms
step:1116/1750 train_time:108890ms step_avg:97.57ms
step:1117/1750 train_time:108991ms step_avg:97.57ms
step:1118/1750 train_time:109091ms step_avg:97.58ms
step:1119/1750 train_time:109190ms step_avg:97.58ms
step:1120/1750 train_time:109290ms step_avg:97.58ms
step:1121/1750 train_time:109390ms step_avg:97.58ms
step:1122/1750 train_time:109490ms step_avg:97.58ms
step:1123/1750 train_time:109589ms step_avg:97.59ms
step:1124/1750 train_time:109690ms step_avg:97.59ms
step:1125/1750 train_time:109790ms step_avg:97.59ms
step:1125/1750 val_loss:3.4974 train_time:109880ms step_avg:97.67ms
step:1126/1750 train_time:109900ms step_avg:97.60ms
step:1127/1750 train_time:109999ms step_avg:97.60ms
step:1128/1750 train_time:110100ms step_avg:97.61ms
step:1129/1750 train_time:110200ms step_avg:97.61ms
step:1130/1750 train_time:110300ms step_avg:97.61ms
step:1131/1750 train_time:110399ms step_avg:97.61ms
step:1132/1750 train_time:110497ms step_avg:97.61ms
step:1133/1750 train_time:110596ms step_avg:97.61ms
step:1134/1750 train_time:110694ms step_avg:97.61ms
step:1135/1750 train_time:110793ms step_avg:97.61ms
step:1136/1750 train_time:110894ms step_avg:97.62ms
step:1137/1750 train_time:110995ms step_avg:97.62ms
step:1138/1750 train_time:111096ms step_avg:97.62ms
step:1139/1750 train_time:111196ms step_avg:97.63ms
step:1140/1750 train_time:111296ms step_avg:97.63ms
step:1141/1750 train_time:111395ms step_avg:97.63ms
step:1142/1750 train_time:111494ms step_avg:97.63ms
step:1143/1750 train_time:111593ms step_avg:97.63ms
step:1144/1750 train_time:111692ms step_avg:97.63ms
step:1145/1750 train_time:111792ms step_avg:97.64ms
step:1146/1750 train_time:111892ms step_avg:97.64ms
step:1147/1750 train_time:111993ms step_avg:97.64ms
step:1148/1750 train_time:112095ms step_avg:97.64ms
step:1149/1750 train_time:112453ms step_avg:97.87ms
step:1150/1750 train_time:112550ms step_avg:97.87ms
step:1151/1750 train_time:112649ms step_avg:97.87ms
step:1152/1750 train_time:112747ms step_avg:97.87ms
step:1153/1750 train_time:112846ms step_avg:97.87ms
step:1154/1750 train_time:112944ms step_avg:97.87ms
step:1155/1750 train_time:113042ms step_avg:97.87ms
step:1156/1750 train_time:113140ms step_avg:97.87ms
step:1157/1750 train_time:113239ms step_avg:97.87ms
step:1158/1750 train_time:113341ms step_avg:97.88ms
step:1159/1750 train_time:113445ms step_avg:97.88ms
step:1160/1750 train_time:113545ms step_avg:97.88ms
step:1161/1750 train_time:113645ms step_avg:97.89ms
step:1162/1750 train_time:113745ms step_avg:97.89ms
step:1163/1750 train_time:113845ms step_avg:97.89ms
step:1164/1750 train_time:113945ms step_avg:97.89ms
step:1165/1750 train_time:114044ms step_avg:97.89ms
step:1166/1750 train_time:114144ms step_avg:97.89ms
step:1167/1750 train_time:114244ms step_avg:97.90ms
step:1168/1750 train_time:114592ms step_avg:98.11ms
step:1169/1750 train_time:114691ms step_avg:98.11ms
step:1170/1750 train_time:114790ms step_avg:98.11ms
step:1171/1750 train_time:114890ms step_avg:98.11ms
step:1172/1750 train_time:114990ms step_avg:98.11ms
step:1173/1750 train_time:115091ms step_avg:98.12ms
step:1174/1750 train_time:115191ms step_avg:98.12ms
step:1175/1750 train_time:115290ms step_avg:98.12ms
step:1176/1750 train_time:115652ms step_avg:98.34ms
step:1177/1750 train_time:115751ms step_avg:98.34ms
step:1178/1750 train_time:115851ms step_avg:98.35ms
step:1179/1750 train_time:115954ms step_avg:98.35ms
step:1180/1750 train_time:116054ms step_avg:98.35ms
step:1181/1750 train_time:116155ms step_avg:98.35ms
step:1182/1750 train_time:116255ms step_avg:98.35ms
step:1183/1750 train_time:116354ms step_avg:98.36ms
step:1184/1750 train_time:116455ms step_avg:98.36ms
step:1185/1750 train_time:116560ms step_avg:98.36ms
step:1186/1750 train_time:116663ms step_avg:98.37ms
step:1187/1750 train_time:116764ms step_avg:98.37ms
step:1188/1750 train_time:116865ms step_avg:98.37ms
step:1189/1750 train_time:116965ms step_avg:98.37ms
step:1190/1750 train_time:117065ms step_avg:98.37ms
step:1191/1750 train_time:117166ms step_avg:98.38ms
step:1192/1750 train_time:117267ms step_avg:98.38ms
step:1193/1750 train_time:117367ms step_avg:98.38ms
step:1194/1750 train_time:117469ms step_avg:98.38ms
step:1195/1750 train_time:117572ms step_avg:98.39ms
step:1196/1750 train_time:117674ms step_avg:98.39ms
step:1197/1750 train_time:117775ms step_avg:98.39ms
step:1198/1750 train_time:117876ms step_avg:98.39ms
step:1199/1750 train_time:117976ms step_avg:98.40ms
step:1200/1750 train_time:118077ms step_avg:98.40ms
step:1201/1750 train_time:118179ms step_avg:98.40ms
step:1202/1750 train_time:118281ms step_avg:98.40ms
step:1203/1750 train_time:118381ms step_avg:98.40ms
step:1204/1750 train_time:118482ms step_avg:98.41ms
step:1205/1750 train_time:118921ms step_avg:98.69ms
step:1206/1750 train_time:118985ms step_avg:98.66ms
step:1207/1750 train_time:119083ms step_avg:98.66ms
step:1208/1750 train_time:119182ms step_avg:98.66ms
step:1209/1750 train_time:119282ms step_avg:98.66ms
step:1210/1750 train_time:119381ms step_avg:98.66ms
step:1211/1750 train_time:119481ms step_avg:98.66ms
step:1212/1750 train_time:119581ms step_avg:98.66ms
step:1213/1750 train_time:119680ms step_avg:98.66ms
step:1214/1750 train_time:119779ms step_avg:98.66ms
step:1215/1750 train_time:119882ms step_avg:98.67ms
step:1216/1750 train_time:119990ms step_avg:98.68ms
step:1217/1750 train_time:120092ms step_avg:98.68ms
step:1218/1750 train_time:120193ms step_avg:98.68ms
step:1219/1750 train_time:120293ms step_avg:98.68ms
step:1220/1750 train_time:120394ms step_avg:98.68ms
step:1221/1750 train_time:120494ms step_avg:98.68ms
step:1222/1750 train_time:120595ms step_avg:98.69ms
step:1223/1750 train_time:120695ms step_avg:98.69ms
step:1224/1750 train_time:120796ms step_avg:98.69ms
step:1225/1750 train_time:120898ms step_avg:98.69ms
step:1226/1750 train_time:121001ms step_avg:98.70ms
step:1227/1750 train_time:121101ms step_avg:98.70ms
step:1228/1750 train_time:121201ms step_avg:98.70ms
step:1229/1750 train_time:121301ms step_avg:98.70ms
step:1230/1750 train_time:121402ms step_avg:98.70ms
step:1231/1750 train_time:121502ms step_avg:98.70ms
step:1232/1750 train_time:121605ms step_avg:98.71ms
step:1233/1750 train_time:121706ms step_avg:98.71ms
step:1234/1750 train_time:121807ms step_avg:98.71ms
step:1235/1750 train_time:121909ms step_avg:98.71ms
step:1236/1750 train_time:122010ms step_avg:98.71ms
step:1237/1750 train_time:122112ms step_avg:98.72ms
step:1238/1750 train_time:122213ms step_avg:98.72ms
step:1239/1750 train_time:122315ms step_avg:98.72ms
step:1240/1750 train_time:122417ms step_avg:98.72ms
step:1241/1750 train_time:122518ms step_avg:98.73ms
step:1242/1750 train_time:122618ms step_avg:98.73ms
step:1243/1750 train_time:122718ms step_avg:98.73ms
step:1244/1750 train_time:122819ms step_avg:98.73ms
step:1245/1750 train_time:122921ms step_avg:98.73ms
step:1246/1750 train_time:123022ms step_avg:98.73ms
step:1247/1750 train_time:123125ms step_avg:98.74ms
step:1248/1750 train_time:123227ms step_avg:98.74ms
step:1249/1750 train_time:123328ms step_avg:98.74ms
step:1250/1750 train_time:123430ms step_avg:98.74ms
step:1250/1750 val_loss:3.4499 train_time:123520ms step_avg:98.82ms
step:1251/1750 train_time:123540ms step_avg:98.75ms
step:1252/1750 train_time:123643ms step_avg:98.76ms
step:1253/1750 train_time:123746ms step_avg:98.76ms
step:1254/1750 train_time:123847ms step_avg:98.76ms
step:1255/1750 train_time:123946ms step_avg:98.76ms
step:1256/1750 train_time:124046ms step_avg:98.76ms
step:1257/1750 train_time:124146ms step_avg:98.76ms
step:1258/1750 train_time:124246ms step_avg:98.76ms
step:1259/1750 train_time:124345ms step_avg:98.77ms
step:1260/1750 train_time:124446ms step_avg:98.77ms
step:1261/1750 train_time:124549ms step_avg:98.77ms
step:1262/1750 train_time:124651ms step_avg:98.77ms
step:1263/1750 train_time:124754ms step_avg:98.78ms
step:1264/1750 train_time:124854ms step_avg:98.78ms
step:1265/1750 train_time:124953ms step_avg:98.78ms
step:1266/1750 train_time:125053ms step_avg:98.78ms
step:1267/1750 train_time:125154ms step_avg:98.78ms
step:1268/1750 train_time:125255ms step_avg:98.78ms
step:1269/1750 train_time:125357ms step_avg:98.78ms
step:1270/1750 train_time:125458ms step_avg:98.79ms
step:1271/1750 train_time:125561ms step_avg:98.79ms
step:1272/1750 train_time:125663ms step_avg:98.79ms
step:1273/1750 train_time:125766ms step_avg:98.79ms
step:1274/1750 train_time:125866ms step_avg:98.80ms
step:1275/1750 train_time:125966ms step_avg:98.80ms
step:1276/1750 train_time:126067ms step_avg:98.80ms
step:1277/1750 train_time:126167ms step_avg:98.80ms
step:1278/1750 train_time:126267ms step_avg:98.80ms
step:1279/1750 train_time:126368ms step_avg:98.80ms
step:1280/1750 train_time:126469ms step_avg:98.80ms
step:1281/1750 train_time:126571ms step_avg:98.81ms
step:1282/1750 train_time:126673ms step_avg:98.81ms
step:1283/1750 train_time:126774ms step_avg:98.81ms
step:1284/1750 train_time:126874ms step_avg:98.81ms
step:1285/1750 train_time:126975ms step_avg:98.81ms
step:1286/1750 train_time:127076ms step_avg:98.81ms
step:1287/1750 train_time:127177ms step_avg:98.82ms
step:1288/1750 train_time:127277ms step_avg:98.82ms
step:1289/1750 train_time:127379ms step_avg:98.82ms
step:1290/1750 train_time:127481ms step_avg:98.82ms
step:1291/1750 train_time:127582ms step_avg:98.82ms
step:1292/1750 train_time:127684ms step_avg:98.83ms
step:1293/1750 train_time:127786ms step_avg:98.83ms
step:1294/1750 train_time:127887ms step_avg:98.83ms
step:1295/1750 train_time:127989ms step_avg:98.83ms
step:1296/1750 train_time:128089ms step_avg:98.83ms
step:1297/1750 train_time:128190ms step_avg:98.84ms
step:1298/1750 train_time:128291ms step_avg:98.84ms
step:1299/1750 train_time:128392ms step_avg:98.84ms
step:1300/1750 train_time:128493ms step_avg:98.84ms
step:1301/1750 train_time:128595ms step_avg:98.84ms
step:1302/1750 train_time:128696ms step_avg:98.84ms
step:1303/1750 train_time:128797ms step_avg:98.85ms
step:1304/1750 train_time:128898ms step_avg:98.85ms
step:1305/1750 train_time:129000ms step_avg:98.85ms
step:1306/1750 train_time:129101ms step_avg:98.85ms
step:1307/1750 train_time:129204ms step_avg:98.86ms
step:1308/1750 train_time:129304ms step_avg:98.86ms
step:1309/1750 train_time:129405ms step_avg:98.86ms
step:1310/1750 train_time:129507ms step_avg:98.86ms
step:1311/1750 train_time:129609ms step_avg:98.86ms
step:1312/1750 train_time:129710ms step_avg:98.86ms
step:1313/1750 train_time:129811ms step_avg:98.87ms
step:1314/1750 train_time:129912ms step_avg:98.87ms
step:1315/1750 train_time:130013ms step_avg:98.87ms
step:1316/1750 train_time:130113ms step_avg:98.87ms
step:1317/1750 train_time:130213ms step_avg:98.87ms
step:1318/1750 train_time:130313ms step_avg:98.87ms
step:1319/1750 train_time:130416ms step_avg:98.88ms
step:1320/1750 train_time:130519ms step_avg:98.88ms
step:1321/1750 train_time:130620ms step_avg:98.88ms
step:1322/1750 train_time:130724ms step_avg:98.88ms
step:1323/1750 train_time:130824ms step_avg:98.88ms
step:1324/1750 train_time:130925ms step_avg:98.89ms
step:1325/1750 train_time:131026ms step_avg:98.89ms
step:1326/1750 train_time:131127ms step_avg:98.89ms
step:1327/1750 train_time:131228ms step_avg:98.89ms
step:1328/1750 train_time:131328ms step_avg:98.89ms
step:1329/1750 train_time:131428ms step_avg:98.89ms
step:1330/1750 train_time:131531ms step_avg:98.90ms
step:1331/1750 train_time:131633ms step_avg:98.90ms
step:1332/1750 train_time:131735ms step_avg:98.90ms
step:1333/1750 train_time:131836ms step_avg:98.90ms
step:1334/1750 train_time:131936ms step_avg:98.90ms
step:1335/1750 train_time:132038ms step_avg:98.90ms
step:1336/1750 train_time:132139ms step_avg:98.91ms
step:1337/1750 train_time:132240ms step_avg:98.91ms
step:1338/1750 train_time:132342ms step_avg:98.91ms
step:1339/1750 train_time:132445ms step_avg:98.91ms
step:1340/1750 train_time:132546ms step_avg:98.92ms
step:1341/1750 train_time:132647ms step_avg:98.92ms
step:1342/1750 train_time:132748ms step_avg:98.92ms
step:1343/1750 train_time:132848ms step_avg:98.92ms
step:1344/1750 train_time:132948ms step_avg:98.92ms
step:1345/1750 train_time:133049ms step_avg:98.92ms
step:1346/1750 train_time:133151ms step_avg:98.92ms
step:1347/1750 train_time:133252ms step_avg:98.92ms
step:1348/1750 train_time:133354ms step_avg:98.93ms
step:1349/1750 train_time:133454ms step_avg:98.93ms
step:1350/1750 train_time:133555ms step_avg:98.93ms
step:1351/1750 train_time:133658ms step_avg:98.93ms
step:1352/1750 train_time:133759ms step_avg:98.93ms
step:1353/1750 train_time:133859ms step_avg:98.94ms
step:1354/1750 train_time:133960ms step_avg:98.94ms
step:1355/1750 train_time:134061ms step_avg:98.94ms
step:1356/1750 train_time:134163ms step_avg:98.94ms
step:1357/1750 train_time:134265ms step_avg:98.94ms
step:1358/1750 train_time:134366ms step_avg:98.94ms
step:1359/1750 train_time:134467ms step_avg:98.95ms
step:1360/1750 train_time:134569ms step_avg:98.95ms
step:1361/1750 train_time:134669ms step_avg:98.95ms
step:1362/1750 train_time:134770ms step_avg:98.95ms
step:1363/1750 train_time:134872ms step_avg:98.95ms
step:1364/1750 train_time:134975ms step_avg:98.96ms
step:1365/1750 train_time:135076ms step_avg:98.96ms
step:1366/1750 train_time:135177ms step_avg:98.96ms
step:1367/1750 train_time:135277ms step_avg:98.96ms
step:1368/1750 train_time:135379ms step_avg:98.96ms
step:1369/1750 train_time:135481ms step_avg:98.96ms
step:1370/1750 train_time:135583ms step_avg:98.97ms
step:1371/1750 train_time:135684ms step_avg:98.97ms
step:1372/1750 train_time:135785ms step_avg:98.97ms
step:1373/1750 train_time:135887ms step_avg:98.97ms
step:1374/1750 train_time:135988ms step_avg:98.97ms
step:1375/1750 train_time:136089ms step_avg:98.97ms
step:1375/1750 val_loss:3.4067 train_time:136179ms step_avg:99.04ms
step:1376/1750 train_time:136199ms step_avg:98.98ms
step:1377/1750 train_time:136304ms step_avg:98.99ms
step:1378/1750 train_time:136405ms step_avg:98.99ms
step:1379/1750 train_time:136506ms step_avg:98.99ms
step:1380/1750 train_time:136608ms step_avg:98.99ms
step:1381/1750 train_time:136708ms step_avg:98.99ms
step:1382/1750 train_time:136807ms step_avg:98.99ms
step:1383/1750 train_time:136907ms step_avg:98.99ms
step:1384/1750 train_time:137007ms step_avg:98.99ms
step:1385/1750 train_time:137107ms step_avg:98.99ms
step:1386/1750 train_time:137209ms step_avg:99.00ms
step:1387/1750 train_time:137312ms step_avg:99.00ms
step:1388/1750 train_time:137415ms step_avg:99.00ms
step:1389/1750 train_time:137517ms step_avg:99.00ms
step:1390/1750 train_time:137617ms step_avg:99.01ms
step:1391/1750 train_time:137719ms step_avg:99.01ms
step:1392/1750 train_time:137821ms step_avg:99.01ms
step:1393/1750 train_time:137921ms step_avg:99.01ms
step:1394/1750 train_time:138021ms step_avg:99.01ms
step:1395/1750 train_time:138123ms step_avg:99.01ms
step:1396/1750 train_time:138226ms step_avg:99.02ms
step:1397/1750 train_time:138329ms step_avg:99.02ms
step:1398/1750 train_time:138430ms step_avg:99.02ms
step:1399/1750 train_time:138533ms step_avg:99.02ms
step:1400/1750 train_time:138635ms step_avg:99.03ms
step:1401/1750 train_time:138736ms step_avg:99.03ms
step:1402/1750 train_time:138836ms step_avg:99.03ms
step:1403/1750 train_time:138936ms step_avg:99.03ms
step:1404/1750 train_time:139038ms step_avg:99.03ms
step:1405/1750 train_time:139139ms step_avg:99.03ms
step:1406/1750 train_time:139240ms step_avg:99.03ms
step:1407/1750 train_time:139343ms step_avg:99.04ms
step:1408/1750 train_time:139445ms step_avg:99.04ms
step:1409/1750 train_time:139547ms step_avg:99.04ms
step:1410/1750 train_time:139648ms step_avg:99.04ms
step:1411/1750 train_time:139749ms step_avg:99.04ms
step:1412/1750 train_time:139850ms step_avg:99.04ms
step:1413/1750 train_time:139951ms step_avg:99.05ms
step:1414/1750 train_time:140053ms step_avg:99.05ms
step:1415/1750 train_time:140153ms step_avg:99.05ms
step:1416/1750 train_time:140253ms step_avg:99.05ms
step:1417/1750 train_time:140354ms step_avg:99.05ms
step:1418/1750 train_time:140455ms step_avg:99.05ms
step:1419/1750 train_time:140558ms step_avg:99.05ms
step:1420/1750 train_time:140660ms step_avg:99.06ms
step:1421/1750 train_time:140762ms step_avg:99.06ms
step:1422/1750 train_time:140864ms step_avg:99.06ms
step:1423/1750 train_time:140966ms step_avg:99.06ms
step:1424/1750 train_time:141067ms step_avg:99.06ms
step:1425/1750 train_time:141168ms step_avg:99.06ms
step:1426/1750 train_time:141270ms step_avg:99.07ms
step:1427/1750 train_time:141371ms step_avg:99.07ms
step:1428/1750 train_time:141473ms step_avg:99.07ms
step:1429/1750 train_time:141574ms step_avg:99.07ms
step:1430/1750 train_time:141677ms step_avg:99.07ms
step:1431/1750 train_time:141780ms step_avg:99.08ms
step:1432/1750 train_time:141882ms step_avg:99.08ms
step:1433/1750 train_time:141983ms step_avg:99.08ms
step:1434/1750 train_time:142085ms step_avg:99.08ms
step:1435/1750 train_time:142188ms step_avg:99.09ms
step:1436/1750 train_time:142289ms step_avg:99.09ms
step:1437/1750 train_time:142391ms step_avg:99.09ms
step:1438/1750 train_time:142493ms step_avg:99.09ms
step:1439/1750 train_time:142596ms step_avg:99.09ms
step:1440/1750 train_time:142699ms step_avg:99.10ms
step:1441/1750 train_time:142800ms step_avg:99.10ms
step:1442/1750 train_time:142901ms step_avg:99.10ms
step:1443/1750 train_time:143003ms step_avg:99.10ms
step:1444/1750 train_time:143105ms step_avg:99.10ms
step:1445/1750 train_time:143206ms step_avg:99.10ms
step:1446/1750 train_time:143309ms step_avg:99.11ms
step:1447/1750 train_time:143411ms step_avg:99.11ms
step:1448/1750 train_time:143514ms step_avg:99.11ms
step:1449/1750 train_time:143614ms step_avg:99.11ms
step:1450/1750 train_time:143716ms step_avg:99.11ms
step:1451/1750 train_time:143818ms step_avg:99.12ms
step:1452/1750 train_time:143920ms step_avg:99.12ms
step:1453/1750 train_time:144022ms step_avg:99.12ms
step:1454/1750 train_time:144127ms step_avg:99.12ms
step:1455/1750 train_time:144230ms step_avg:99.13ms
step:1456/1750 train_time:144331ms step_avg:99.13ms
step:1457/1750 train_time:144433ms step_avg:99.13ms
step:1458/1750 train_time:144535ms step_avg:99.13ms
step:1459/1750 train_time:144636ms step_avg:99.13ms
step:1460/1750 train_time:144737ms step_avg:99.13ms
step:1461/1750 train_time:144839ms step_avg:99.14ms
step:1462/1750 train_time:144941ms step_avg:99.14ms
step:1463/1750 train_time:145044ms step_avg:99.14ms
step:1464/1750 train_time:145146ms step_avg:99.14ms
step:1465/1750 train_time:145248ms step_avg:99.15ms
step:1466/1750 train_time:145351ms step_avg:99.15ms
step:1467/1750 train_time:145453ms step_avg:99.15ms
step:1468/1750 train_time:145555ms step_avg:99.15ms
step:1469/1750 train_time:145658ms step_avg:99.15ms
step:1470/1750 train_time:145759ms step_avg:99.16ms
step:1471/1750 train_time:145861ms step_avg:99.16ms
step:1472/1750 train_time:145962ms step_avg:99.16ms
step:1473/1750 train_time:146063ms step_avg:99.16ms
step:1474/1750 train_time:146165ms step_avg:99.16ms
step:1475/1750 train_time:146267ms step_avg:99.16ms
step:1476/1750 train_time:146369ms step_avg:99.17ms
step:1477/1750 train_time:146471ms step_avg:99.17ms
step:1478/1750 train_time:146573ms step_avg:99.17ms
step:1479/1750 train_time:146674ms step_avg:99.17ms
step:1480/1750 train_time:146776ms step_avg:99.17ms
step:1481/1750 train_time:146878ms step_avg:99.17ms
step:1482/1750 train_time:146981ms step_avg:99.18ms
step:1483/1750 train_time:147083ms step_avg:99.18ms
step:1484/1750 train_time:147185ms step_avg:99.18ms
step:1485/1750 train_time:147289ms step_avg:99.18ms
step:1486/1750 train_time:147390ms step_avg:99.19ms
step:1487/1750 train_time:147492ms step_avg:99.19ms
step:1488/1750 train_time:147595ms step_avg:99.19ms
step:1489/1750 train_time:147697ms step_avg:99.19ms
step:1490/1750 train_time:147798ms step_avg:99.19ms
step:1491/1750 train_time:147900ms step_avg:99.20ms
step:1492/1750 train_time:148001ms step_avg:99.20ms
step:1493/1750 train_time:148103ms step_avg:99.20ms
step:1494/1750 train_time:148205ms step_avg:99.20ms
step:1495/1750 train_time:148307ms step_avg:99.20ms
step:1496/1750 train_time:148409ms step_avg:99.20ms
step:1497/1750 train_time:148510ms step_avg:99.21ms
step:1498/1750 train_time:148612ms step_avg:99.21ms
step:1499/1750 train_time:148713ms step_avg:99.21ms
step:1500/1750 train_time:148817ms step_avg:99.21ms
step:1500/1750 val_loss:3.3706 train_time:148907ms step_avg:99.27ms
step:1501/1750 train_time:148927ms step_avg:99.22ms
step:1502/1750 train_time:149029ms step_avg:99.22ms
step:1503/1750 train_time:149131ms step_avg:99.22ms
step:1504/1750 train_time:149232ms step_avg:99.22ms
step:1505/1750 train_time:149334ms step_avg:99.23ms
step:1506/1750 train_time:149436ms step_avg:99.23ms
step:1507/1750 train_time:149538ms step_avg:99.23ms
step:1508/1750 train_time:149639ms step_avg:99.23ms
step:1509/1750 train_time:149740ms step_avg:99.23ms
step:1510/1750 train_time:149843ms step_avg:99.23ms
step:1511/1750 train_time:149947ms step_avg:99.24ms
step:1512/1750 train_time:150050ms step_avg:99.24ms
step:1513/1750 train_time:150151ms step_avg:99.24ms
step:1514/1750 train_time:150253ms step_avg:99.24ms
step:1515/1750 train_time:150358ms step_avg:99.25ms
step:1516/1750 train_time:150459ms step_avg:99.25ms
step:1517/1750 train_time:150559ms step_avg:99.25ms
step:1518/1750 train_time:150660ms step_avg:99.25ms
step:1519/1750 train_time:150763ms step_avg:99.25ms
step:1520/1750 train_time:150866ms step_avg:99.25ms
step:1521/1750 train_time:150968ms step_avg:99.26ms
step:1522/1750 train_time:151070ms step_avg:99.26ms
step:1523/1750 train_time:151171ms step_avg:99.26ms
step:1524/1750 train_time:151275ms step_avg:99.26ms
step:1525/1750 train_time:151379ms step_avg:99.27ms
step:1526/1750 train_time:151481ms step_avg:99.27ms
step:1527/1750 train_time:151582ms step_avg:99.27ms
step:1528/1750 train_time:151687ms step_avg:99.27ms
step:1529/1750 train_time:151789ms step_avg:99.27ms
step:1530/1750 train_time:151892ms step_avg:99.28ms
step:1531/1750 train_time:151994ms step_avg:99.28ms
step:1532/1750 train_time:152097ms step_avg:99.28ms
step:1533/1750 train_time:152200ms step_avg:99.28ms
step:1534/1750 train_time:152302ms step_avg:99.28ms
step:1535/1750 train_time:152404ms step_avg:99.29ms
step:1536/1750 train_time:152504ms step_avg:99.29ms
step:1537/1750 train_time:152605ms step_avg:99.29ms
step:1538/1750 train_time:152707ms step_avg:99.29ms
step:1539/1750 train_time:152809ms step_avg:99.29ms
step:1540/1750 train_time:152911ms step_avg:99.29ms
step:1541/1750 train_time:153014ms step_avg:99.30ms
step:1542/1750 train_time:153117ms step_avg:99.30ms
step:1543/1750 train_time:153220ms step_avg:99.30ms
step:1544/1750 train_time:153322ms step_avg:99.30ms
step:1545/1750 train_time:153424ms step_avg:99.30ms
step:1546/1750 train_time:153525ms step_avg:99.30ms
step:1547/1750 train_time:153627ms step_avg:99.31ms
step:1548/1750 train_time:153730ms step_avg:99.31ms
step:1549/1750 train_time:153832ms step_avg:99.31ms
step:1550/1750 train_time:153934ms step_avg:99.31ms
step:1551/1750 train_time:154038ms step_avg:99.32ms
step:1552/1750 train_time:154140ms step_avg:99.32ms
step:1553/1750 train_time:154244ms step_avg:99.32ms
step:1554/1750 train_time:154345ms step_avg:99.32ms
step:1555/1750 train_time:154446ms step_avg:99.32ms
step:1556/1750 train_time:154547ms step_avg:99.32ms
step:1557/1750 train_time:154649ms step_avg:99.33ms
step:1558/1750 train_time:154752ms step_avg:99.33ms
step:1559/1750 train_time:154853ms step_avg:99.33ms
step:1560/1750 train_time:154955ms step_avg:99.33ms
step:1561/1750 train_time:155059ms step_avg:99.33ms
step:1562/1750 train_time:155163ms step_avg:99.34ms
step:1563/1750 train_time:155268ms step_avg:99.34ms
step:1564/1750 train_time:155370ms step_avg:99.34ms
step:1565/1750 train_time:155471ms step_avg:99.34ms
step:1566/1750 train_time:155573ms step_avg:99.34ms
step:1567/1750 train_time:155675ms step_avg:99.35ms
step:1568/1750 train_time:155777ms step_avg:99.35ms
step:1569/1750 train_time:155878ms step_avg:99.35ms
step:1570/1750 train_time:155981ms step_avg:99.35ms
step:1571/1750 train_time:156083ms step_avg:99.35ms
step:1572/1750 train_time:156185ms step_avg:99.35ms
step:1573/1750 train_time:156287ms step_avg:99.36ms
step:1574/1750 train_time:156389ms step_avg:99.36ms
step:1575/1750 train_time:156491ms step_avg:99.36ms
step:1576/1750 train_time:156594ms step_avg:99.36ms
step:1577/1750 train_time:156697ms step_avg:99.36ms
step:1578/1750 train_time:156798ms step_avg:99.37ms
step:1579/1750 train_time:156901ms step_avg:99.37ms
step:1580/1750 train_time:157003ms step_avg:99.37ms
step:1581/1750 train_time:157105ms step_avg:99.37ms
step:1582/1750 train_time:157207ms step_avg:99.37ms
step:1583/1750 train_time:157310ms step_avg:99.37ms
step:1584/1750 train_time:157413ms step_avg:99.38ms
step:1585/1750 train_time:157515ms step_avg:99.38ms
step:1586/1750 train_time:157618ms step_avg:99.38ms
step:1587/1750 train_time:157720ms step_avg:99.38ms
step:1588/1750 train_time:157822ms step_avg:99.38ms
step:1589/1750 train_time:157923ms step_avg:99.39ms
step:1590/1750 train_time:158024ms step_avg:99.39ms
step:1591/1750 train_time:158126ms step_avg:99.39ms
step:1592/1750 train_time:158228ms step_avg:99.39ms
step:1593/1750 train_time:158330ms step_avg:99.39ms
step:1594/1750 train_time:158435ms step_avg:99.39ms
step:1595/1750 train_time:158537ms step_avg:99.40ms
step:1596/1750 train_time:158640ms step_avg:99.40ms
step:1597/1750 train_time:158741ms step_avg:99.40ms
step:1598/1750 train_time:158844ms step_avg:99.40ms
step:1599/1750 train_time:158945ms step_avg:99.40ms
step:1600/1750 train_time:159047ms step_avg:99.40ms
step:1601/1750 train_time:159149ms step_avg:99.41ms
step:1602/1750 train_time:159250ms step_avg:99.41ms
step:1603/1750 train_time:159352ms step_avg:99.41ms
step:1604/1750 train_time:159454ms step_avg:99.41ms
step:1605/1750 train_time:159557ms step_avg:99.41ms
step:1606/1750 train_time:159660ms step_avg:99.41ms
step:1607/1750 train_time:159762ms step_avg:99.42ms
step:1608/1750 train_time:159864ms step_avg:99.42ms
step:1609/1750 train_time:159965ms step_avg:99.42ms
step:1610/1750 train_time:160067ms step_avg:99.42ms
step:1611/1750 train_time:160169ms step_avg:99.42ms
step:1612/1750 train_time:160271ms step_avg:99.42ms
step:1613/1750 train_time:160373ms step_avg:99.43ms
step:1614/1750 train_time:160474ms step_avg:99.43ms
step:1615/1750 train_time:160576ms step_avg:99.43ms
step:1616/1750 train_time:160679ms step_avg:99.43ms
step:1617/1750 train_time:160782ms step_avg:99.43ms
step:1618/1750 train_time:160883ms step_avg:99.43ms
step:1619/1750 train_time:160985ms step_avg:99.43ms
step:1620/1750 train_time:161088ms step_avg:99.44ms
step:1621/1750 train_time:161188ms step_avg:99.44ms
step:1622/1750 train_time:161290ms step_avg:99.44ms
step:1623/1750 train_time:161393ms step_avg:99.44ms
step:1624/1750 train_time:161496ms step_avg:99.44ms
step:1625/1750 train_time:161600ms step_avg:99.45ms
step:1625/1750 val_loss:3.3394 train_time:161690ms step_avg:99.50ms
step:1626/1750 train_time:161711ms step_avg:99.45ms
step:1627/1750 train_time:161810ms step_avg:99.45ms
step:1628/1750 train_time:161915ms step_avg:99.46ms
step:1629/1750 train_time:162017ms step_avg:99.46ms
step:1630/1750 train_time:162118ms step_avg:99.46ms
step:1631/1750 train_time:162220ms step_avg:99.46ms
step:1632/1750 train_time:162320ms step_avg:99.46ms
step:1633/1750 train_time:162421ms step_avg:99.46ms
step:1634/1750 train_time:162525ms step_avg:99.46ms
step:1635/1750 train_time:162628ms step_avg:99.47ms
step:1636/1750 train_time:162731ms step_avg:99.47ms
step:1637/1750 train_time:162834ms step_avg:99.47ms
step:1638/1750 train_time:162936ms step_avg:99.47ms
step:1639/1750 train_time:163038ms step_avg:99.47ms
step:1640/1750 train_time:163140ms step_avg:99.48ms
step:1641/1750 train_time:163241ms step_avg:99.48ms
step:1642/1750 train_time:163342ms step_avg:99.48ms
step:1643/1750 train_time:163443ms step_avg:99.48ms
step:1644/1750 train_time:163545ms step_avg:99.48ms
step:1645/1750 train_time:163646ms step_avg:99.48ms
step:1646/1750 train_time:163750ms step_avg:99.48ms
step:1647/1750 train_time:163855ms step_avg:99.49ms
step:1648/1750 train_time:163959ms step_avg:99.49ms
step:1649/1750 train_time:164061ms step_avg:99.49ms
step:1650/1750 train_time:164162ms step_avg:99.49ms
step:1651/1750 train_time:164263ms step_avg:99.49ms
step:1652/1750 train_time:164366ms step_avg:99.49ms
step:1653/1750 train_time:164468ms step_avg:99.50ms
step:1654/1750 train_time:164569ms step_avg:99.50ms
step:1655/1750 train_time:164672ms step_avg:99.50ms
step:1656/1750 train_time:164775ms step_avg:99.50ms
step:1657/1750 train_time:164876ms step_avg:99.50ms
step:1658/1750 train_time:164979ms step_avg:99.50ms
step:1659/1750 train_time:165083ms step_avg:99.51ms
step:1660/1750 train_time:165184ms step_avg:99.51ms
step:1661/1750 train_time:165288ms step_avg:99.51ms
step:1662/1750 train_time:165391ms step_avg:99.51ms
step:1663/1750 train_time:165493ms step_avg:99.51ms
step:1664/1750 train_time:165596ms step_avg:99.52ms
step:1665/1750 train_time:165702ms step_avg:99.52ms
step:1666/1750 train_time:165804ms step_avg:99.52ms
step:1667/1750 train_time:165906ms step_avg:99.52ms
step:1668/1750 train_time:166009ms step_avg:99.53ms
step:1669/1750 train_time:166113ms step_avg:99.53ms
step:1670/1750 train_time:166215ms step_avg:99.53ms
step:1671/1750 train_time:166317ms step_avg:99.53ms
step:1672/1750 train_time:166419ms step_avg:99.53ms
step:1673/1750 train_time:166520ms step_avg:99.53ms
step:1674/1750 train_time:166622ms step_avg:99.54ms
step:1675/1750 train_time:166724ms step_avg:99.54ms
step:1676/1750 train_time:166827ms step_avg:99.54ms
step:1677/1750 train_time:166929ms step_avg:99.54ms
step:1678/1750 train_time:167031ms step_avg:99.54ms
step:1679/1750 train_time:167134ms step_avg:99.54ms
step:1680/1750 train_time:167236ms step_avg:99.55ms
step:1681/1750 train_time:167339ms step_avg:99.55ms
step:1682/1750 train_time:167443ms step_avg:99.55ms
step:1683/1750 train_time:167544ms step_avg:99.55ms
step:1684/1750 train_time:167647ms step_avg:99.55ms
step:1685/1750 train_time:167749ms step_avg:99.55ms
step:1686/1750 train_time:167850ms step_avg:99.56ms
step:1687/1750 train_time:167952ms step_avg:99.56ms
step:1688/1750 train_time:168056ms step_avg:99.56ms
step:1689/1750 train_time:168157ms step_avg:99.56ms
step:1690/1750 train_time:168260ms step_avg:99.56ms
step:1691/1750 train_time:168361ms step_avg:99.56ms
step:1692/1750 train_time:168464ms step_avg:99.57ms
step:1693/1750 train_time:168568ms step_avg:99.57ms
step:1694/1750 train_time:168671ms step_avg:99.57ms
step:1695/1750 train_time:168775ms step_avg:99.57ms
step:1696/1750 train_time:168878ms step_avg:99.57ms
step:1697/1750 train_time:168982ms step_avg:99.58ms
step:1698/1750 train_time:169085ms step_avg:99.58ms
step:1699/1750 train_time:169187ms step_avg:99.58ms
step:1700/1750 train_time:169289ms step_avg:99.58ms
step:1701/1750 train_time:169392ms step_avg:99.58ms
step:1702/1750 train_time:169499ms step_avg:99.59ms
step:1703/1750 train_time:169601ms step_avg:99.59ms
step:1704/1750 train_time:169703ms step_avg:99.59ms
step:1705/1750 train_time:169805ms step_avg:99.59ms
step:1706/1750 train_time:169909ms step_avg:99.59ms
step:1707/1750 train_time:170012ms step_avg:99.60ms
step:1708/1750 train_time:170116ms step_avg:99.60ms
step:1709/1750 train_time:170218ms step_avg:99.60ms
step:1710/1750 train_time:170320ms step_avg:99.60ms
step:1711/1750 train_time:170425ms step_avg:99.61ms
step:1712/1750 train_time:170528ms step_avg:99.61ms
step:1713/1750 train_time:170632ms step_avg:99.61ms
step:1714/1750 train_time:170735ms step_avg:99.61ms
step:1715/1750 train_time:170840ms step_avg:99.61ms
step:1716/1750 train_time:170942ms step_avg:99.62ms
step:1717/1750 train_time:171044ms step_avg:99.62ms
step:1718/1750 train_time:171146ms step_avg:99.62ms
step:1719/1750 train_time:171253ms step_avg:99.62ms
step:1720/1750 train_time:171356ms step_avg:99.63ms
step:1721/1750 train_time:171459ms step_avg:99.63ms
step:1722/1750 train_time:171562ms step_avg:99.63ms
step:1723/1750 train_time:171663ms step_avg:99.63ms
step:1724/1750 train_time:171766ms step_avg:99.63ms
step:1725/1750 train_time:171871ms step_avg:99.64ms
step:1726/1750 train_time:171974ms step_avg:99.64ms
step:1727/1750 train_time:172077ms step_avg:99.64ms
step:1728/1750 train_time:172181ms step_avg:99.64ms
step:1729/1750 train_time:172285ms step_avg:99.64ms
step:1730/1750 train_time:172386ms step_avg:99.65ms
step:1731/1750 train_time:172490ms step_avg:99.65ms
step:1732/1750 train_time:172594ms step_avg:99.65ms
step:1733/1750 train_time:172696ms step_avg:99.65ms
step:1734/1750 train_time:172800ms step_avg:99.65ms
step:1735/1750 train_time:172903ms step_avg:99.66ms
step:1736/1750 train_time:173005ms step_avg:99.66ms
step:1737/1750 train_time:173109ms step_avg:99.66ms
step:1738/1750 train_time:173212ms step_avg:99.66ms
step:1739/1750 train_time:173315ms step_avg:99.66ms
step:1740/1750 train_time:173417ms step_avg:99.66ms
step:1741/1750 train_time:173523ms step_avg:99.67ms
step:1742/1750 train_time:173627ms step_avg:99.67ms
step:1743/1750 train_time:173730ms step_avg:99.67ms
step:1744/1750 train_time:173832ms step_avg:99.67ms
step:1745/1750 train_time:173935ms step_avg:99.68ms
step:1746/1750 train_time:174039ms step_avg:99.68ms
step:1747/1750 train_time:174143ms step_avg:99.68ms
step:1748/1750 train_time:174247ms step_avg:99.68ms
step:1749/1750 train_time:174350ms step_avg:99.69ms
step:1750/1750 train_time:174454ms step_avg:99.69ms
step:1750/1750 val_loss:3.3149 train_time:174545ms step_avg:99.74ms
peak memory allocated: 33278 MiB reserved: 48694 MiB
