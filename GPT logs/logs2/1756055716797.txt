import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


def several_sv_svds_approximation(W_torch, k, num_iter=50):
    """SVD approximation using the top k singular values and corresponding vectors."""
    # Store original device and dtype
    original_device = W_torch.device
    original_dtype = W_torch.dtype
    
    W = cp.from_dlpack(thd.to_dlpack(W_torch)).astype(cp.float32)
    U, S, Vt = cupyx_svds(W, k=min([k, W.shape[0] - 1, W.shape[1] - 1]), maxiter=num_iter, which='LM')

    # Convert back to torch tensors and ensure they're on the correct device
    approx_torch_U = thd.from_dlpack(U.toDlpack()).to(device=original_device, dtype=original_dtype)
    approx_torch_S = thd.from_dlpack(S.toDlpack()).to(device=original_device, dtype=original_dtype)
    approx_torch_Vt = thd.from_dlpack(Vt.toDlpack()).to(device=original_device, dtype=original_dtype)
    
    return approx_torch_U, approx_torch_S, approx_torch_Vt

class FastNormMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, sgd_coeff=0):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, sgd_coeff=sgd_coeff)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        self.sgd_coeff = sgd_coeff

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            sgd_coeff = group["sgd_coeff"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    # assume: grad is a Tensor, momentum_buffer exists, momentum, sgd_coeff, eps, eff_lr are defined
                    with torch.no_grad():
                        # inplace blend with momentum (keeps original semantic: grad becomes the lerped tensor)
                        grad.lerp_(momentum_buffer, momentum)

                        # compute zeropower part from the (post-lerp) grad, in bfloat16 to match the original code
                        if sgd_coeff != 1:
                            # create update_part in bfloat16 (may allocate once per call; unavoidable if zeropower expects bfloat16)
                            update_part = zeropower_via_newtonschulz5(grad.to(torch.bfloat16), 5)

                            # upcast update_part to grad dtype to do mixed-dtype blending without extra temporaries later
                            if update_part.dtype != grad.dtype:
                                update_part = update_part.to(grad.dtype)

                        # compute inverse norm scalar efficiently (no large temp tensors)
                        # - view(-1) is cheap (no copy)
                        # - dot(view, view) is a single reduction
                        norm_sq = grad.view(-1).dot(grad.view(-1))
                        inv_norm = (norm_sq + eps).rsqrt()  # stable rsqrt on the scalar

                        # normalize grad in-place (so we avoid allocating g_normalized)
                        grad.mul_(inv_norm)   # now grad == g_normalized (in-place)

                        # build final update (reuse update_part buffer if available)
                        if sgd_coeff != 1:
                            # update_part := (1 - sgd_coeff) * update_part + sgd_coeff * grad
                            # do it in-place on update_part to avoid creating another temp
                            update_part.mul_(1 - sgd_coeff)    # in-place scale
                            update_part.lerp_(grad, sgd_coeff) # in-place linear interpolation
                            update = update_part
                        else:
                            # if sgd_coeff == 1 we can use grad directly (already normalized in-place)
                            update = grad

                        # apply update (same as original)
                        p.add_(other=update, alpha=-eff_lr)
                    '''
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    eps = 1e-12
                    g_normalized = grad / (grad.norm() + eps)
                    if sgd_coeff != 1:
                        update_part = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                        update = (1 - sgd_coeff) * update_part + sgd_coeff * g_normalized
                    else:
                        update = sgd_coeff * g_normalized
                    p.add_(other=update, alpha=-eff_lr)
                    '''
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class NormMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, sgd_coeff=0):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, sgd_coeff=sgd_coeff)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        self.sgd_coeff = sgd_coeff

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            sgd_coeff = group["sgd_coeff"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    eps = 1e-12
                    g_normalized = grad / (grad.norm() + eps)
                    if sgd_coeff != 1:
                        update_part = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                        update = (1 - sgd_coeff) * update_part + sgd_coeff * g_normalized
                    else:
                        update = sgd_coeff * g_normalized
                    p.add_(other=update, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()
class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)
                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_reduce_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 27.5
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos : pos + max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()
    starts = []
    for i in range(1, len(boundary_positions)):
        end = boundary_positions[i].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == dist.get_world_size():
                return starts, end - pos
            start = end
    assert False # increase max_batch_span if necessary

def distributed_data_generator(filename_pattern: str, batch_size: int, align_to_bos: bool):
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    max_batch_span = 2 * batch_size if align_to_bos else batch_size # provide buffer to handle samples up to length local_batch_size
    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            batch_span = batch_size
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1750 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
from datetime import datetime
if master_process:
    # run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_id = str(int(time.time() * 1000))
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0)
optimizer2 = NormMuon(hidden_matrix_params, lr=0.05, momentum=0.95, weight_decay=0.0, sgd_coeff=0.3)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.11 | packaged by conda-forge | (main, Jun  4 2025, 14:45:31) [GCC 13.3.0]
Running PyTorch 2.9.0.dev20250824+cu126 compiled for CUDA 12.6
Sun Aug 24 17:15:17 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:04:00.0 Off |                    0 |
| N/A   37C    P0            119W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:23:00.0 Off |                    0 |
| N/A   31C    P0            114W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:43:00.0 Off |                    0 |
| N/A   35C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   30C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:84:00.0 Off |                    0 |
| N/A   36C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:A3:00.0 Off |                    0 |
| N/A   31C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:C3:00.0 Off |                    0 |
| N/A   36C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E4:00.0 Off |                    0 |
| N/A   30C    P0            112W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.05ms
step:1/1750 train_time:151ms step_avg:151.49ms
step:2/1750 train_time:172ms step_avg:86.13ms
step:3/1750 train_time:253ms step_avg:84.32ms
step:4/1750 train_time:345ms step_avg:86.14ms
step:5/1750 train_time:437ms step_avg:87.38ms
step:6/1750 train_time:530ms step_avg:88.29ms
step:7/1750 train_time:621ms step_avg:88.78ms
step:8/1750 train_time:714ms step_avg:89.19ms
step:9/1750 train_time:806ms step_avg:89.54ms
step:10/1750 train_time:898ms step_avg:89.80ms
step:11/1750 train_time:990ms step_avg:90.03ms
step:12/1750 train_time:1086ms step_avg:90.48ms
step:13/1750 train_time:1183ms step_avg:91.01ms
step:14/1750 train_time:1277ms step_avg:91.18ms
step:15/1750 train_time:1370ms step_avg:91.32ms
step:16/1750 train_time:1462ms step_avg:91.40ms
step:17/1750 train_time:1555ms step_avg:91.49ms
step:18/1750 train_time:1648ms step_avg:91.56ms
step:19/1750 train_time:1741ms step_avg:91.61ms
step:20/1750 train_time:1834ms step_avg:91.70ms
step:21/1750 train_time:1927ms step_avg:91.76ms
step:22/1750 train_time:2020ms step_avg:91.84ms
step:23/1750 train_time:2114ms step_avg:91.91ms
step:24/1750 train_time:2208ms step_avg:91.99ms
step:25/1750 train_time:2301ms step_avg:92.05ms
step:26/1750 train_time:2394ms step_avg:92.09ms
step:27/1750 train_time:2487ms step_avg:92.12ms
step:28/1750 train_time:2580ms step_avg:92.15ms
step:29/1750 train_time:2673ms step_avg:92.18ms
step:30/1750 train_time:2766ms step_avg:92.21ms
step:31/1750 train_time:2858ms step_avg:92.20ms
step:32/1750 train_time:2952ms step_avg:92.24ms
step:33/1750 train_time:3046ms step_avg:92.31ms
step:34/1750 train_time:3140ms step_avg:92.35ms
step:35/1750 train_time:3234ms step_avg:92.41ms
step:36/1750 train_time:3328ms step_avg:92.43ms
step:37/1750 train_time:3422ms step_avg:92.48ms
step:38/1750 train_time:3515ms step_avg:92.51ms
step:39/1750 train_time:3609ms step_avg:92.54ms
step:40/1750 train_time:3702ms step_avg:92.56ms
step:41/1750 train_time:3796ms step_avg:92.58ms
step:42/1750 train_time:3888ms step_avg:92.58ms
step:43/1750 train_time:3981ms step_avg:92.59ms
step:44/1750 train_time:4075ms step_avg:92.61ms
step:45/1750 train_time:4168ms step_avg:92.62ms
step:46/1750 train_time:4262ms step_avg:92.65ms
step:47/1750 train_time:4357ms step_avg:92.69ms
step:48/1750 train_time:4451ms step_avg:92.72ms
step:49/1750 train_time:4544ms step_avg:92.74ms
step:50/1750 train_time:4637ms step_avg:92.74ms
step:51/1750 train_time:4731ms step_avg:92.76ms
step:52/1750 train_time:4824ms step_avg:92.77ms
step:53/1750 train_time:4917ms step_avg:92.77ms
step:54/1750 train_time:5010ms step_avg:92.77ms
step:55/1750 train_time:5103ms step_avg:92.78ms
step:56/1750 train_time:5196ms step_avg:92.79ms
step:57/1750 train_time:5290ms step_avg:92.80ms
step:58/1750 train_time:5384ms step_avg:92.82ms
step:59/1750 train_time:5477ms step_avg:92.84ms
step:60/1750 train_time:5571ms step_avg:92.85ms
step:61/1750 train_time:5665ms step_avg:92.86ms
step:62/1750 train_time:5758ms step_avg:92.88ms
step:63/1750 train_time:5852ms step_avg:92.89ms
step:64/1750 train_time:5945ms step_avg:92.90ms
step:65/1750 train_time:6038ms step_avg:92.89ms
step:66/1750 train_time:6132ms step_avg:92.90ms
step:67/1750 train_time:6225ms step_avg:92.91ms
step:68/1750 train_time:6319ms step_avg:92.92ms
step:69/1750 train_time:6412ms step_avg:92.93ms
step:70/1750 train_time:6506ms step_avg:92.94ms
step:71/1750 train_time:6599ms step_avg:92.94ms
step:72/1750 train_time:6693ms step_avg:92.95ms
step:73/1750 train_time:6787ms step_avg:92.97ms
step:74/1750 train_time:6881ms step_avg:92.99ms
step:75/1750 train_time:6974ms step_avg:92.99ms
step:76/1750 train_time:7067ms step_avg:92.99ms
step:77/1750 train_time:7161ms step_avg:92.99ms
step:78/1750 train_time:7253ms step_avg:92.99ms
step:79/1750 train_time:7347ms step_avg:93.00ms
step:80/1750 train_time:7441ms step_avg:93.01ms
step:81/1750 train_time:7535ms step_avg:93.02ms
step:82/1750 train_time:7628ms step_avg:93.02ms
step:83/1750 train_time:7721ms step_avg:93.02ms
step:84/1750 train_time:7814ms step_avg:93.03ms
step:85/1750 train_time:7907ms step_avg:93.02ms
step:86/1750 train_time:8000ms step_avg:93.02ms
step:87/1750 train_time:8093ms step_avg:93.02ms
step:88/1750 train_time:8186ms step_avg:93.03ms
step:89/1750 train_time:8279ms step_avg:93.02ms
step:90/1750 train_time:8372ms step_avg:93.03ms
step:91/1750 train_time:8466ms step_avg:93.03ms
step:92/1750 train_time:8559ms step_avg:93.03ms
step:93/1750 train_time:8652ms step_avg:93.04ms
step:94/1750 train_time:8746ms step_avg:93.04ms
step:95/1750 train_time:8839ms step_avg:93.04ms
step:96/1750 train_time:8932ms step_avg:93.05ms
step:97/1750 train_time:9026ms step_avg:93.05ms
step:98/1750 train_time:9119ms step_avg:93.05ms
step:99/1750 train_time:9212ms step_avg:93.05ms
step:100/1750 train_time:9306ms step_avg:93.06ms
step:101/1750 train_time:9399ms step_avg:93.06ms
step:102/1750 train_time:9493ms step_avg:93.07ms
step:103/1750 train_time:9585ms step_avg:93.06ms
step:104/1750 train_time:9678ms step_avg:93.06ms
step:105/1750 train_time:9772ms step_avg:93.07ms
step:106/1750 train_time:9866ms step_avg:93.07ms
step:107/1750 train_time:9959ms step_avg:93.07ms
step:108/1750 train_time:10051ms step_avg:93.07ms
step:109/1750 train_time:10145ms step_avg:93.08ms
step:110/1750 train_time:10240ms step_avg:93.09ms
step:111/1750 train_time:10332ms step_avg:93.08ms
step:112/1750 train_time:10426ms step_avg:93.09ms
step:113/1750 train_time:10520ms step_avg:93.10ms
step:114/1750 train_time:10613ms step_avg:93.10ms
step:115/1750 train_time:10706ms step_avg:93.10ms
step:116/1750 train_time:10800ms step_avg:93.10ms
step:117/1750 train_time:10893ms step_avg:93.10ms
step:118/1750 train_time:10986ms step_avg:93.10ms
step:119/1750 train_time:11079ms step_avg:93.10ms
step:120/1750 train_time:11173ms step_avg:93.11ms
step:121/1750 train_time:11266ms step_avg:93.11ms
step:122/1750 train_time:11360ms step_avg:93.11ms
step:123/1750 train_time:11453ms step_avg:93.12ms
step:124/1750 train_time:11547ms step_avg:93.12ms
step:125/1750 train_time:11640ms step_avg:93.12ms
step:125/1750 val_loss:4.6419 train_time:11723ms step_avg:93.79ms
step:126/1750 train_time:11747ms step_avg:93.23ms
step:127/1750 train_time:11837ms step_avg:93.20ms
step:128/1750 train_time:11939ms step_avg:93.27ms
step:129/1750 train_time:12034ms step_avg:93.29ms
step:130/1750 train_time:12127ms step_avg:93.28ms
step:131/1750 train_time:12219ms step_avg:93.28ms
step:132/1750 train_time:12312ms step_avg:93.27ms
step:133/1750 train_time:12405ms step_avg:93.27ms
step:134/1750 train_time:12498ms step_avg:93.27ms
step:135/1750 train_time:12591ms step_avg:93.26ms
step:136/1750 train_time:12685ms step_avg:93.27ms
step:137/1750 train_time:12780ms step_avg:93.29ms
step:138/1750 train_time:12876ms step_avg:93.31ms
step:139/1750 train_time:12972ms step_avg:93.32ms
step:140/1750 train_time:13066ms step_avg:93.33ms
step:141/1750 train_time:13161ms step_avg:93.34ms
step:142/1750 train_time:13254ms step_avg:93.34ms
step:143/1750 train_time:13348ms step_avg:93.34ms
step:144/1750 train_time:13441ms step_avg:93.34ms
step:145/1750 train_time:13533ms step_avg:93.33ms
step:146/1750 train_time:13626ms step_avg:93.33ms
step:147/1750 train_time:13720ms step_avg:93.33ms
step:148/1750 train_time:13815ms step_avg:93.34ms
step:149/1750 train_time:13909ms step_avg:93.35ms
step:150/1750 train_time:14004ms step_avg:93.36ms
step:151/1750 train_time:14098ms step_avg:93.36ms
step:152/1750 train_time:14192ms step_avg:93.37ms
step:153/1750 train_time:14285ms step_avg:93.37ms
step:154/1750 train_time:14379ms step_avg:93.37ms
step:155/1750 train_time:14473ms step_avg:93.38ms
step:156/1750 train_time:14566ms step_avg:93.37ms
step:157/1750 train_time:14659ms step_avg:93.37ms
step:158/1750 train_time:14753ms step_avg:93.37ms
step:159/1750 train_time:14846ms step_avg:93.37ms
step:160/1750 train_time:14941ms step_avg:93.38ms
step:161/1750 train_time:15035ms step_avg:93.39ms
step:162/1750 train_time:15130ms step_avg:93.39ms
step:163/1750 train_time:15223ms step_avg:93.39ms
step:164/1750 train_time:15317ms step_avg:93.40ms
step:165/1750 train_time:15411ms step_avg:93.40ms
step:166/1750 train_time:15504ms step_avg:93.40ms
step:167/1750 train_time:15598ms step_avg:93.40ms
step:168/1750 train_time:15692ms step_avg:93.40ms
step:169/1750 train_time:15785ms step_avg:93.40ms
step:170/1750 train_time:15879ms step_avg:93.41ms
step:171/1750 train_time:15973ms step_avg:93.41ms
step:172/1750 train_time:16067ms step_avg:93.41ms
step:173/1750 train_time:16160ms step_avg:93.41ms
step:174/1750 train_time:16254ms step_avg:93.41ms
step:175/1750 train_time:16347ms step_avg:93.41ms
step:176/1750 train_time:16441ms step_avg:93.42ms
step:177/1750 train_time:16535ms step_avg:93.42ms
step:178/1750 train_time:16628ms step_avg:93.41ms
step:179/1750 train_time:16721ms step_avg:93.41ms
step:180/1750 train_time:16816ms step_avg:93.42ms
step:181/1750 train_time:16910ms step_avg:93.42ms
step:182/1750 train_time:17003ms step_avg:93.42ms
step:183/1750 train_time:17097ms step_avg:93.43ms
step:184/1750 train_time:17191ms step_avg:93.43ms
step:185/1750 train_time:17285ms step_avg:93.43ms
step:186/1750 train_time:17378ms step_avg:93.43ms
step:187/1750 train_time:17472ms step_avg:93.44ms
step:188/1750 train_time:17565ms step_avg:93.43ms
step:189/1750 train_time:17659ms step_avg:93.43ms
step:190/1750 train_time:17752ms step_avg:93.43ms
step:191/1750 train_time:17846ms step_avg:93.43ms
step:192/1750 train_time:17939ms step_avg:93.43ms
step:193/1750 train_time:18034ms step_avg:93.44ms
step:194/1750 train_time:18128ms step_avg:93.44ms
step:195/1750 train_time:18221ms step_avg:93.44ms
step:196/1750 train_time:18314ms step_avg:93.44ms
step:197/1750 train_time:18407ms step_avg:93.44ms
step:198/1750 train_time:18501ms step_avg:93.44ms
step:199/1750 train_time:18595ms step_avg:93.44ms
step:200/1750 train_time:18689ms step_avg:93.44ms
step:201/1750 train_time:18783ms step_avg:93.45ms
step:202/1750 train_time:18876ms step_avg:93.45ms
step:203/1750 train_time:18969ms step_avg:93.45ms
step:204/1750 train_time:19064ms step_avg:93.45ms
step:205/1750 train_time:19158ms step_avg:93.45ms
step:206/1750 train_time:19252ms step_avg:93.46ms
step:207/1750 train_time:19345ms step_avg:93.46ms
step:208/1750 train_time:19440ms step_avg:93.46ms
step:209/1750 train_time:19533ms step_avg:93.46ms
step:210/1750 train_time:19627ms step_avg:93.46ms
step:211/1750 train_time:19721ms step_avg:93.47ms
step:212/1750 train_time:19816ms step_avg:93.47ms
step:213/1750 train_time:19908ms step_avg:93.47ms
step:214/1750 train_time:20002ms step_avg:93.47ms
step:215/1750 train_time:20096ms step_avg:93.47ms
step:216/1750 train_time:20189ms step_avg:93.47ms
step:217/1750 train_time:20282ms step_avg:93.47ms
step:218/1750 train_time:20376ms step_avg:93.47ms
step:219/1750 train_time:20469ms step_avg:93.47ms
step:220/1750 train_time:20563ms step_avg:93.47ms
step:221/1750 train_time:20658ms step_avg:93.47ms
step:222/1750 train_time:20752ms step_avg:93.48ms
step:223/1750 train_time:20845ms step_avg:93.48ms
step:224/1750 train_time:20938ms step_avg:93.47ms
step:225/1750 train_time:21031ms step_avg:93.47ms
step:226/1750 train_time:21125ms step_avg:93.47ms
step:227/1750 train_time:21219ms step_avg:93.47ms
step:228/1750 train_time:21312ms step_avg:93.47ms
step:229/1750 train_time:21406ms step_avg:93.47ms
step:230/1750 train_time:21500ms step_avg:93.48ms
step:231/1750 train_time:21593ms step_avg:93.48ms
step:232/1750 train_time:21688ms step_avg:93.48ms
step:233/1750 train_time:21781ms step_avg:93.48ms
step:234/1750 train_time:21875ms step_avg:93.48ms
step:235/1750 train_time:21968ms step_avg:93.48ms
step:236/1750 train_time:22062ms step_avg:93.48ms
step:237/1750 train_time:22156ms step_avg:93.48ms
step:238/1750 train_time:22250ms step_avg:93.49ms
step:239/1750 train_time:22343ms step_avg:93.49ms
step:240/1750 train_time:22437ms step_avg:93.49ms
step:241/1750 train_time:22531ms step_avg:93.49ms
step:242/1750 train_time:22625ms step_avg:93.49ms
step:243/1750 train_time:22719ms step_avg:93.49ms
step:244/1750 train_time:22813ms step_avg:93.49ms
step:245/1750 train_time:22906ms step_avg:93.49ms
step:246/1750 train_time:23000ms step_avg:93.49ms
step:247/1750 train_time:23093ms step_avg:93.49ms
step:248/1750 train_time:23186ms step_avg:93.49ms
step:249/1750 train_time:23280ms step_avg:93.49ms
step:250/1750 train_time:23373ms step_avg:93.49ms
step:250/1750 val_loss:4.0949 train_time:23456ms step_avg:93.83ms
step:251/1750 train_time:23478ms step_avg:93.54ms
step:252/1750 train_time:23567ms step_avg:93.52ms
step:253/1750 train_time:23669ms step_avg:93.56ms
step:254/1750 train_time:23765ms step_avg:93.56ms
step:255/1750 train_time:23858ms step_avg:93.56ms
step:256/1750 train_time:23951ms step_avg:93.56ms
step:257/1750 train_time:24044ms step_avg:93.55ms
step:258/1750 train_time:24136ms step_avg:93.55ms
step:259/1750 train_time:24229ms step_avg:93.55ms
step:260/1750 train_time:24321ms step_avg:93.54ms
step:261/1750 train_time:24415ms step_avg:93.54ms
step:262/1750 train_time:24510ms step_avg:93.55ms
step:263/1750 train_time:24607ms step_avg:93.56ms
step:264/1750 train_time:24704ms step_avg:93.58ms
step:265/1750 train_time:24799ms step_avg:93.58ms
step:266/1750 train_time:24893ms step_avg:93.58ms
step:267/1750 train_time:24987ms step_avg:93.58ms
step:268/1750 train_time:25081ms step_avg:93.59ms
step:269/1750 train_time:25174ms step_avg:93.58ms
step:270/1750 train_time:25268ms step_avg:93.58ms
step:271/1750 train_time:25362ms step_avg:93.59ms
step:272/1750 train_time:25456ms step_avg:93.59ms
step:273/1750 train_time:25551ms step_avg:93.59ms
step:274/1750 train_time:25646ms step_avg:93.60ms
step:275/1750 train_time:25742ms step_avg:93.61ms
step:276/1750 train_time:25836ms step_avg:93.61ms
step:277/1750 train_time:25931ms step_avg:93.61ms
step:278/1750 train_time:26025ms step_avg:93.62ms
step:279/1750 train_time:26119ms step_avg:93.62ms
step:280/1750 train_time:26212ms step_avg:93.61ms
step:281/1750 train_time:26306ms step_avg:93.62ms
step:282/1750 train_time:26399ms step_avg:93.62ms
step:283/1750 train_time:26494ms step_avg:93.62ms
step:284/1750 train_time:26589ms step_avg:93.62ms
step:285/1750 train_time:26684ms step_avg:93.63ms
step:286/1750 train_time:26778ms step_avg:93.63ms
step:287/1750 train_time:26873ms step_avg:93.63ms
step:288/1750 train_time:26967ms step_avg:93.64ms
step:289/1750 train_time:27062ms step_avg:93.64ms
step:290/1750 train_time:27155ms step_avg:93.64ms
step:291/1750 train_time:27249ms step_avg:93.64ms
step:292/1750 train_time:27342ms step_avg:93.64ms
step:293/1750 train_time:27436ms step_avg:93.64ms
step:294/1750 train_time:27530ms step_avg:93.64ms
step:295/1750 train_time:27624ms step_avg:93.64ms
step:296/1750 train_time:27719ms step_avg:93.64ms
step:297/1750 train_time:27813ms step_avg:93.64ms
step:298/1750 train_time:27907ms step_avg:93.65ms
step:299/1750 train_time:28001ms step_avg:93.65ms
step:300/1750 train_time:28095ms step_avg:93.65ms
step:301/1750 train_time:28189ms step_avg:93.65ms
step:302/1750 train_time:28283ms step_avg:93.65ms
step:303/1750 train_time:28377ms step_avg:93.65ms
step:304/1750 train_time:28471ms step_avg:93.65ms
step:305/1750 train_time:28564ms step_avg:93.65ms
step:306/1750 train_time:28659ms step_avg:93.66ms
step:307/1750 train_time:28754ms step_avg:93.66ms
step:308/1750 train_time:28848ms step_avg:93.66ms
step:309/1750 train_time:28943ms step_avg:93.67ms
step:310/1750 train_time:29038ms step_avg:93.67ms
step:311/1750 train_time:29132ms step_avg:93.67ms
step:312/1750 train_time:29227ms step_avg:93.67ms
step:313/1750 train_time:29320ms step_avg:93.67ms
step:314/1750 train_time:29414ms step_avg:93.68ms
step:315/1750 train_time:29509ms step_avg:93.68ms
step:316/1750 train_time:29602ms step_avg:93.68ms
step:317/1750 train_time:29697ms step_avg:93.68ms
step:318/1750 train_time:29792ms step_avg:93.69ms
step:319/1750 train_time:29886ms step_avg:93.69ms
step:320/1750 train_time:29981ms step_avg:93.69ms
step:321/1750 train_time:30075ms step_avg:93.69ms
step:322/1750 train_time:30169ms step_avg:93.69ms
step:323/1750 train_time:30264ms step_avg:93.70ms
step:324/1750 train_time:30358ms step_avg:93.70ms
step:325/1750 train_time:30452ms step_avg:93.70ms
step:326/1750 train_time:30547ms step_avg:93.70ms
step:327/1750 train_time:30641ms step_avg:93.70ms
step:328/1750 train_time:30735ms step_avg:93.70ms
step:329/1750 train_time:30829ms step_avg:93.70ms
step:330/1750 train_time:30923ms step_avg:93.71ms
step:331/1750 train_time:31017ms step_avg:93.71ms
step:332/1750 train_time:31111ms step_avg:93.71ms
step:333/1750 train_time:31206ms step_avg:93.71ms
step:334/1750 train_time:31301ms step_avg:93.72ms
step:335/1750 train_time:31395ms step_avg:93.72ms
step:336/1750 train_time:31489ms step_avg:93.72ms
step:337/1750 train_time:31584ms step_avg:93.72ms
step:338/1750 train_time:31678ms step_avg:93.72ms
step:339/1750 train_time:31771ms step_avg:93.72ms
step:340/1750 train_time:31866ms step_avg:93.72ms
step:341/1750 train_time:31960ms step_avg:93.72ms
step:342/1750 train_time:32054ms step_avg:93.72ms
step:343/1750 train_time:32148ms step_avg:93.73ms
step:344/1750 train_time:32242ms step_avg:93.73ms
step:345/1750 train_time:32336ms step_avg:93.73ms
step:346/1750 train_time:32430ms step_avg:93.73ms
step:347/1750 train_time:32525ms step_avg:93.73ms
step:348/1750 train_time:32619ms step_avg:93.73ms
step:349/1750 train_time:32713ms step_avg:93.73ms
step:350/1750 train_time:32807ms step_avg:93.74ms
step:351/1750 train_time:32902ms step_avg:93.74ms
step:352/1750 train_time:32996ms step_avg:93.74ms
step:353/1750 train_time:33090ms step_avg:93.74ms
step:354/1750 train_time:33184ms step_avg:93.74ms
step:355/1750 train_time:33278ms step_avg:93.74ms
step:356/1750 train_time:33372ms step_avg:93.74ms
step:357/1750 train_time:33467ms step_avg:93.75ms
step:358/1750 train_time:33562ms step_avg:93.75ms
step:359/1750 train_time:33656ms step_avg:93.75ms
step:360/1750 train_time:33750ms step_avg:93.75ms
step:361/1750 train_time:33844ms step_avg:93.75ms
step:362/1750 train_time:33938ms step_avg:93.75ms
step:363/1750 train_time:34032ms step_avg:93.75ms
step:364/1750 train_time:34127ms step_avg:93.75ms
step:365/1750 train_time:34220ms step_avg:93.75ms
step:366/1750 train_time:34315ms step_avg:93.76ms
step:367/1750 train_time:34408ms step_avg:93.76ms
step:368/1750 train_time:34503ms step_avg:93.76ms
step:369/1750 train_time:34597ms step_avg:93.76ms
step:370/1750 train_time:34691ms step_avg:93.76ms
step:371/1750 train_time:34785ms step_avg:93.76ms
step:372/1750 train_time:34879ms step_avg:93.76ms
step:373/1750 train_time:34973ms step_avg:93.76ms
step:374/1750 train_time:35067ms step_avg:93.76ms
step:375/1750 train_time:35162ms step_avg:93.77ms
step:375/1750 val_loss:3.8938 train_time:35245ms step_avg:93.99ms
step:376/1750 train_time:35265ms step_avg:93.79ms
step:377/1750 train_time:35358ms step_avg:93.79ms
step:378/1750 train_time:35459ms step_avg:93.81ms
step:379/1750 train_time:35553ms step_avg:93.81ms
step:380/1750 train_time:35648ms step_avg:93.81ms
step:381/1750 train_time:35740ms step_avg:93.81ms
step:382/1750 train_time:35833ms step_avg:93.80ms
step:383/1750 train_time:35927ms step_avg:93.80ms
step:384/1750 train_time:36020ms step_avg:93.80ms
step:385/1750 train_time:36113ms step_avg:93.80ms
step:386/1750 train_time:36207ms step_avg:93.80ms
step:387/1750 train_time:36303ms step_avg:93.81ms
step:388/1750 train_time:36400ms step_avg:93.81ms
step:389/1750 train_time:36495ms step_avg:93.82ms
step:390/1750 train_time:36591ms step_avg:93.82ms
step:391/1750 train_time:36687ms step_avg:93.83ms
step:392/1750 train_time:36782ms step_avg:93.83ms
step:393/1750 train_time:36878ms step_avg:93.84ms
step:394/1750 train_time:36974ms step_avg:93.84ms
step:395/1750 train_time:37069ms step_avg:93.84ms
step:396/1750 train_time:37164ms step_avg:93.85ms
step:397/1750 train_time:37261ms step_avg:93.86ms
step:398/1750 train_time:37358ms step_avg:93.86ms
step:399/1750 train_time:37455ms step_avg:93.87ms
step:400/1750 train_time:37551ms step_avg:93.88ms
step:401/1750 train_time:37648ms step_avg:93.89ms
step:402/1750 train_time:37743ms step_avg:93.89ms
step:403/1750 train_time:37838ms step_avg:93.89ms
step:404/1750 train_time:37934ms step_avg:93.90ms
step:405/1750 train_time:38029ms step_avg:93.90ms
step:406/1750 train_time:38125ms step_avg:93.90ms
step:407/1750 train_time:38221ms step_avg:93.91ms
step:408/1750 train_time:38317ms step_avg:93.91ms
step:409/1750 train_time:38413ms step_avg:93.92ms
step:410/1750 train_time:38510ms step_avg:93.93ms
step:411/1750 train_time:38607ms step_avg:93.93ms
step:412/1750 train_time:38703ms step_avg:93.94ms
step:413/1750 train_time:38799ms step_avg:93.94ms
step:414/1750 train_time:38895ms step_avg:93.95ms
step:415/1750 train_time:38990ms step_avg:93.95ms
step:416/1750 train_time:39085ms step_avg:93.96ms
step:417/1750 train_time:39181ms step_avg:93.96ms
step:418/1750 train_time:39277ms step_avg:93.96ms
step:419/1750 train_time:39373ms step_avg:93.97ms
step:420/1750 train_time:39470ms step_avg:93.98ms
step:421/1750 train_time:39566ms step_avg:93.98ms
step:422/1750 train_time:39662ms step_avg:93.99ms
step:423/1750 train_time:39758ms step_avg:93.99ms
step:424/1750 train_time:39854ms step_avg:94.00ms
step:425/1750 train_time:39949ms step_avg:94.00ms
step:426/1750 train_time:40045ms step_avg:94.00ms
step:427/1750 train_time:40140ms step_avg:94.01ms
step:428/1750 train_time:40236ms step_avg:94.01ms
step:429/1750 train_time:40333ms step_avg:94.02ms
step:430/1750 train_time:40428ms step_avg:94.02ms
step:431/1750 train_time:40525ms step_avg:94.02ms
step:432/1750 train_time:40621ms step_avg:94.03ms
step:433/1750 train_time:40717ms step_avg:94.03ms
step:434/1750 train_time:40813ms step_avg:94.04ms
step:435/1750 train_time:40909ms step_avg:94.04ms
step:436/1750 train_time:41005ms step_avg:94.05ms
step:437/1750 train_time:41100ms step_avg:94.05ms
step:438/1750 train_time:41196ms step_avg:94.05ms
step:439/1750 train_time:41292ms step_avg:94.06ms
step:440/1750 train_time:41388ms step_avg:94.06ms
step:441/1750 train_time:41484ms step_avg:94.07ms
step:442/1750 train_time:41580ms step_avg:94.07ms
step:443/1750 train_time:41677ms step_avg:94.08ms
step:444/1750 train_time:41774ms step_avg:94.09ms
step:445/1750 train_time:41870ms step_avg:94.09ms
step:446/1750 train_time:41966ms step_avg:94.09ms
step:447/1750 train_time:42062ms step_avg:94.10ms
step:448/1750 train_time:42157ms step_avg:94.10ms
step:449/1750 train_time:42253ms step_avg:94.10ms
step:450/1750 train_time:42349ms step_avg:94.11ms
step:451/1750 train_time:42445ms step_avg:94.11ms
step:452/1750 train_time:42541ms step_avg:94.12ms
step:453/1750 train_time:42637ms step_avg:94.12ms
step:454/1750 train_time:42733ms step_avg:94.13ms
step:455/1750 train_time:42830ms step_avg:94.13ms
step:456/1750 train_time:42925ms step_avg:94.13ms
step:457/1750 train_time:43021ms step_avg:94.14ms
step:458/1750 train_time:43117ms step_avg:94.14ms
step:459/1750 train_time:43213ms step_avg:94.15ms
step:460/1750 train_time:43308ms step_avg:94.15ms
step:461/1750 train_time:43404ms step_avg:94.15ms
step:462/1750 train_time:43500ms step_avg:94.16ms
step:463/1750 train_time:43596ms step_avg:94.16ms
step:464/1750 train_time:43692ms step_avg:94.16ms
step:465/1750 train_time:43788ms step_avg:94.17ms
step:466/1750 train_time:43884ms step_avg:94.17ms
step:467/1750 train_time:43980ms step_avg:94.18ms
step:468/1750 train_time:44077ms step_avg:94.18ms
step:469/1750 train_time:44172ms step_avg:94.18ms
step:470/1750 train_time:44269ms step_avg:94.19ms
step:471/1750 train_time:44364ms step_avg:94.19ms
step:472/1750 train_time:44460ms step_avg:94.20ms
step:473/1750 train_time:44556ms step_avg:94.20ms
step:474/1750 train_time:44652ms step_avg:94.20ms
step:475/1750 train_time:44748ms step_avg:94.21ms
step:476/1750 train_time:44844ms step_avg:94.21ms
step:477/1750 train_time:44941ms step_avg:94.22ms
step:478/1750 train_time:45038ms step_avg:94.22ms
step:479/1750 train_time:45134ms step_avg:94.22ms
step:480/1750 train_time:45230ms step_avg:94.23ms
step:481/1750 train_time:45326ms step_avg:94.23ms
step:482/1750 train_time:45421ms step_avg:94.23ms
step:483/1750 train_time:45517ms step_avg:94.24ms
step:484/1750 train_time:45612ms step_avg:94.24ms
step:485/1750 train_time:45708ms step_avg:94.24ms
step:486/1750 train_time:45804ms step_avg:94.25ms
step:487/1750 train_time:45900ms step_avg:94.25ms
step:488/1750 train_time:45996ms step_avg:94.25ms
step:489/1750 train_time:46093ms step_avg:94.26ms
step:490/1750 train_time:46189ms step_avg:94.26ms
step:491/1750 train_time:46285ms step_avg:94.27ms
step:492/1750 train_time:46381ms step_avg:94.27ms
step:493/1750 train_time:46477ms step_avg:94.27ms
step:494/1750 train_time:46572ms step_avg:94.28ms
step:495/1750 train_time:46668ms step_avg:94.28ms
step:496/1750 train_time:46764ms step_avg:94.28ms
step:497/1750 train_time:46860ms step_avg:94.29ms
step:498/1750 train_time:46956ms step_avg:94.29ms
step:499/1750 train_time:47052ms step_avg:94.29ms
step:500/1750 train_time:47148ms step_avg:94.30ms
step:500/1750 val_loss:3.7484 train_time:47233ms step_avg:94.47ms
step:501/1750 train_time:47253ms step_avg:94.32ms
step:502/1750 train_time:47347ms step_avg:94.32ms
step:503/1750 train_time:47444ms step_avg:94.32ms
step:504/1750 train_time:47541ms step_avg:94.33ms
step:505/1750 train_time:47636ms step_avg:94.33ms
step:506/1750 train_time:47731ms step_avg:94.33ms
step:507/1750 train_time:47826ms step_avg:94.33ms
step:508/1750 train_time:47921ms step_avg:94.33ms
step:509/1750 train_time:48016ms step_avg:94.33ms
step:510/1750 train_time:48112ms step_avg:94.34ms
step:511/1750 train_time:48214ms step_avg:94.35ms
step:512/1750 train_time:48312ms step_avg:94.36ms
step:513/1750 train_time:48409ms step_avg:94.37ms
step:514/1750 train_time:48505ms step_avg:94.37ms
step:515/1750 train_time:48601ms step_avg:94.37ms
step:516/1750 train_time:48697ms step_avg:94.37ms
step:517/1750 train_time:48792ms step_avg:94.38ms
step:518/1750 train_time:48887ms step_avg:94.38ms
step:519/1750 train_time:48983ms step_avg:94.38ms
step:520/1750 train_time:49079ms step_avg:94.38ms
step:521/1750 train_time:49177ms step_avg:94.39ms
step:522/1750 train_time:49275ms step_avg:94.40ms
step:523/1750 train_time:49372ms step_avg:94.40ms
step:524/1750 train_time:49469ms step_avg:94.41ms
step:525/1750 train_time:49565ms step_avg:94.41ms
step:526/1750 train_time:49662ms step_avg:94.41ms
step:527/1750 train_time:49758ms step_avg:94.42ms
step:528/1750 train_time:49854ms step_avg:94.42ms
step:529/1750 train_time:49950ms step_avg:94.42ms
step:530/1750 train_time:50046ms step_avg:94.43ms
step:531/1750 train_time:50143ms step_avg:94.43ms
step:532/1750 train_time:50241ms step_avg:94.44ms
step:533/1750 train_time:50338ms step_avg:94.44ms
step:534/1750 train_time:50435ms step_avg:94.45ms
step:535/1750 train_time:50531ms step_avg:94.45ms
step:536/1750 train_time:50628ms step_avg:94.46ms
step:537/1750 train_time:50724ms step_avg:94.46ms
step:538/1750 train_time:50820ms step_avg:94.46ms
step:539/1750 train_time:50916ms step_avg:94.46ms
step:540/1750 train_time:51012ms step_avg:94.47ms
step:541/1750 train_time:51109ms step_avg:94.47ms
step:542/1750 train_time:51206ms step_avg:94.48ms
step:543/1750 train_time:51303ms step_avg:94.48ms
step:544/1750 train_time:51400ms step_avg:94.49ms
step:545/1750 train_time:51496ms step_avg:94.49ms
step:546/1750 train_time:51594ms step_avg:94.49ms
step:547/1750 train_time:51692ms step_avg:94.50ms
step:548/1750 train_time:51789ms step_avg:94.50ms
step:549/1750 train_time:51884ms step_avg:94.51ms
step:550/1750 train_time:51981ms step_avg:94.51ms
step:551/1750 train_time:52077ms step_avg:94.51ms
step:552/1750 train_time:52173ms step_avg:94.52ms
step:553/1750 train_time:52270ms step_avg:94.52ms
step:554/1750 train_time:52367ms step_avg:94.52ms
step:555/1750 train_time:52464ms step_avg:94.53ms
step:556/1750 train_time:52560ms step_avg:94.53ms
step:557/1750 train_time:52656ms step_avg:94.54ms
step:558/1750 train_time:52753ms step_avg:94.54ms
step:559/1750 train_time:52851ms step_avg:94.55ms
step:560/1750 train_time:52949ms step_avg:94.55ms
step:561/1750 train_time:53044ms step_avg:94.55ms
step:562/1750 train_time:53140ms step_avg:94.56ms
step:563/1750 train_time:53236ms step_avg:94.56ms
step:564/1750 train_time:53333ms step_avg:94.56ms
step:565/1750 train_time:53428ms step_avg:94.56ms
step:566/1750 train_time:53525ms step_avg:94.57ms
step:567/1750 train_time:53622ms step_avg:94.57ms
step:568/1750 train_time:53719ms step_avg:94.58ms
step:569/1750 train_time:53816ms step_avg:94.58ms
step:570/1750 train_time:53913ms step_avg:94.58ms
step:571/1750 train_time:54010ms step_avg:94.59ms
step:572/1750 train_time:54106ms step_avg:94.59ms
step:573/1750 train_time:54202ms step_avg:94.59ms
step:574/1750 train_time:54298ms step_avg:94.60ms
step:575/1750 train_time:54394ms step_avg:94.60ms
step:576/1750 train_time:54491ms step_avg:94.60ms
step:577/1750 train_time:54587ms step_avg:94.61ms
step:578/1750 train_time:54684ms step_avg:94.61ms
step:579/1750 train_time:54781ms step_avg:94.61ms
step:580/1750 train_time:54879ms step_avg:94.62ms
step:581/1750 train_time:54975ms step_avg:94.62ms
step:582/1750 train_time:55071ms step_avg:94.62ms
step:583/1750 train_time:55167ms step_avg:94.63ms
step:584/1750 train_time:55263ms step_avg:94.63ms
step:585/1750 train_time:55360ms step_avg:94.63ms
step:586/1750 train_time:55456ms step_avg:94.64ms
step:587/1750 train_time:55554ms step_avg:94.64ms
step:588/1750 train_time:55651ms step_avg:94.64ms
step:589/1750 train_time:55747ms step_avg:94.65ms
step:590/1750 train_time:55844ms step_avg:94.65ms
step:591/1750 train_time:55941ms step_avg:94.65ms
step:592/1750 train_time:56037ms step_avg:94.66ms
step:593/1750 train_time:56134ms step_avg:94.66ms
step:594/1750 train_time:56230ms step_avg:94.66ms
step:595/1750 train_time:56326ms step_avg:94.67ms
step:596/1750 train_time:56423ms step_avg:94.67ms
step:597/1750 train_time:56519ms step_avg:94.67ms
step:598/1750 train_time:56616ms step_avg:94.67ms
step:599/1750 train_time:56712ms step_avg:94.68ms
step:600/1750 train_time:56809ms step_avg:94.68ms
step:601/1750 train_time:56905ms step_avg:94.68ms
step:602/1750 train_time:57001ms step_avg:94.69ms
step:603/1750 train_time:57097ms step_avg:94.69ms
step:604/1750 train_time:57193ms step_avg:94.69ms
step:605/1750 train_time:57290ms step_avg:94.69ms
step:606/1750 train_time:57386ms step_avg:94.70ms
step:607/1750 train_time:57482ms step_avg:94.70ms
step:608/1750 train_time:57579ms step_avg:94.70ms
step:609/1750 train_time:57675ms step_avg:94.71ms
step:610/1750 train_time:57773ms step_avg:94.71ms
step:611/1750 train_time:57870ms step_avg:94.71ms
step:612/1750 train_time:57967ms step_avg:94.72ms
step:613/1750 train_time:58063ms step_avg:94.72ms
step:614/1750 train_time:58159ms step_avg:94.72ms
step:615/1750 train_time:58255ms step_avg:94.72ms
step:616/1750 train_time:58352ms step_avg:94.73ms
step:617/1750 train_time:58448ms step_avg:94.73ms
step:618/1750 train_time:58545ms step_avg:94.73ms
step:619/1750 train_time:58642ms step_avg:94.74ms
step:620/1750 train_time:58739ms step_avg:94.74ms
step:621/1750 train_time:58835ms step_avg:94.74ms
step:622/1750 train_time:58931ms step_avg:94.74ms
step:623/1750 train_time:59028ms step_avg:94.75ms
step:624/1750 train_time:59124ms step_avg:94.75ms
step:625/1750 train_time:59220ms step_avg:94.75ms
step:625/1750 val_loss:3.6616 train_time:59305ms step_avg:94.89ms
step:626/1750 train_time:59325ms step_avg:94.77ms
step:627/1750 train_time:59422ms step_avg:94.77ms
step:628/1750 train_time:59520ms step_avg:94.78ms
step:629/1750 train_time:59617ms step_avg:94.78ms
step:630/1750 train_time:59712ms step_avg:94.78ms
step:631/1750 train_time:59808ms step_avg:94.78ms
step:632/1750 train_time:59904ms step_avg:94.78ms
step:633/1750 train_time:59999ms step_avg:94.79ms
step:634/1750 train_time:60095ms step_avg:94.79ms
step:635/1750 train_time:60191ms step_avg:94.79ms
step:636/1750 train_time:60288ms step_avg:94.79ms
step:637/1750 train_time:60387ms step_avg:94.80ms
step:638/1750 train_time:60484ms step_avg:94.80ms
step:639/1750 train_time:60581ms step_avg:94.81ms
step:640/1750 train_time:60677ms step_avg:94.81ms
step:641/1750 train_time:60774ms step_avg:94.81ms
step:642/1750 train_time:60870ms step_avg:94.81ms
step:643/1750 train_time:60965ms step_avg:94.81ms
step:644/1750 train_time:61061ms step_avg:94.82ms
step:645/1750 train_time:61157ms step_avg:94.82ms
step:646/1750 train_time:61253ms step_avg:94.82ms
step:647/1750 train_time:61350ms step_avg:94.82ms
step:648/1750 train_time:61448ms step_avg:94.83ms
step:649/1750 train_time:61545ms step_avg:94.83ms
step:650/1750 train_time:61642ms step_avg:94.83ms
step:651/1750 train_time:61740ms step_avg:94.84ms
step:652/1750 train_time:61838ms step_avg:94.84ms
step:653/1750 train_time:61936ms step_avg:94.85ms
step:654/1750 train_time:62033ms step_avg:94.85ms
step:655/1750 train_time:62130ms step_avg:94.86ms
step:656/1750 train_time:62228ms step_avg:94.86ms
step:657/1750 train_time:62326ms step_avg:94.86ms
step:658/1750 train_time:62424ms step_avg:94.87ms
step:659/1750 train_time:62521ms step_avg:94.87ms
step:660/1750 train_time:62619ms step_avg:94.88ms
step:661/1750 train_time:62717ms step_avg:94.88ms
step:662/1750 train_time:62815ms step_avg:94.89ms
step:663/1750 train_time:62913ms step_avg:94.89ms
step:664/1750 train_time:63010ms step_avg:94.89ms
step:665/1750 train_time:63109ms step_avg:94.90ms
step:666/1750 train_time:63206ms step_avg:94.90ms
step:667/1750 train_time:63304ms step_avg:94.91ms
step:668/1750 train_time:63402ms step_avg:94.91ms
step:669/1750 train_time:63501ms step_avg:94.92ms
step:670/1750 train_time:63598ms step_avg:94.92ms
step:671/1750 train_time:63696ms step_avg:94.93ms
step:672/1750 train_time:63793ms step_avg:94.93ms
step:673/1750 train_time:63892ms step_avg:94.94ms
step:674/1750 train_time:63989ms step_avg:94.94ms
step:675/1750 train_time:64087ms step_avg:94.94ms
step:676/1750 train_time:64185ms step_avg:94.95ms
step:677/1750 train_time:64282ms step_avg:94.95ms
step:678/1750 train_time:64380ms step_avg:94.96ms
step:679/1750 train_time:64478ms step_avg:94.96ms
step:680/1750 train_time:64576ms step_avg:94.97ms
step:681/1750 train_time:64675ms step_avg:94.97ms
step:682/1750 train_time:64772ms step_avg:94.97ms
step:683/1750 train_time:64870ms step_avg:94.98ms
step:684/1750 train_time:64968ms step_avg:94.98ms
step:685/1750 train_time:65067ms step_avg:94.99ms
step:686/1750 train_time:65164ms step_avg:94.99ms
step:687/1750 train_time:65262ms step_avg:94.99ms
step:688/1750 train_time:65359ms step_avg:95.00ms
step:689/1750 train_time:65456ms step_avg:95.00ms
step:690/1750 train_time:65554ms step_avg:95.01ms
step:691/1750 train_time:65651ms step_avg:95.01ms
step:692/1750 train_time:65749ms step_avg:95.01ms
step:693/1750 train_time:65849ms step_avg:95.02ms
step:694/1750 train_time:65946ms step_avg:95.02ms
step:695/1750 train_time:66044ms step_avg:95.03ms
step:696/1750 train_time:66141ms step_avg:95.03ms
step:697/1750 train_time:66240ms step_avg:95.04ms
step:698/1750 train_time:66337ms step_avg:95.04ms
step:699/1750 train_time:66435ms step_avg:95.04ms
step:700/1750 train_time:66532ms step_avg:95.05ms
step:701/1750 train_time:66630ms step_avg:95.05ms
step:702/1750 train_time:66728ms step_avg:95.05ms
step:703/1750 train_time:66828ms step_avg:95.06ms
step:704/1750 train_time:66926ms step_avg:95.07ms
step:705/1750 train_time:67024ms step_avg:95.07ms
step:706/1750 train_time:67122ms step_avg:95.07ms
step:707/1750 train_time:67219ms step_avg:95.08ms
step:708/1750 train_time:67317ms step_avg:95.08ms
step:709/1750 train_time:67416ms step_avg:95.09ms
step:710/1750 train_time:67513ms step_avg:95.09ms
step:711/1750 train_time:67611ms step_avg:95.09ms
step:712/1750 train_time:67708ms step_avg:95.10ms
step:713/1750 train_time:67806ms step_avg:95.10ms
step:714/1750 train_time:67905ms step_avg:95.10ms
step:715/1750 train_time:68002ms step_avg:95.11ms
step:716/1750 train_time:68100ms step_avg:95.11ms
step:717/1750 train_time:68198ms step_avg:95.12ms
step:718/1750 train_time:68295ms step_avg:95.12ms
step:719/1750 train_time:68392ms step_avg:95.12ms
step:720/1750 train_time:68489ms step_avg:95.12ms
step:721/1750 train_time:68587ms step_avg:95.13ms
step:722/1750 train_time:68685ms step_avg:95.13ms
step:723/1750 train_time:68782ms step_avg:95.13ms
step:724/1750 train_time:68880ms step_avg:95.14ms
step:725/1750 train_time:68978ms step_avg:95.14ms
step:726/1750 train_time:69076ms step_avg:95.15ms
step:727/1750 train_time:69174ms step_avg:95.15ms
step:728/1750 train_time:69271ms step_avg:95.15ms
step:729/1750 train_time:69369ms step_avg:95.16ms
step:730/1750 train_time:69467ms step_avg:95.16ms
step:731/1750 train_time:69564ms step_avg:95.16ms
step:732/1750 train_time:69662ms step_avg:95.17ms
step:733/1750 train_time:69759ms step_avg:95.17ms
step:734/1750 train_time:69857ms step_avg:95.17ms
step:735/1750 train_time:69955ms step_avg:95.18ms
step:736/1750 train_time:70053ms step_avg:95.18ms
step:737/1750 train_time:70151ms step_avg:95.18ms
step:738/1750 train_time:70249ms step_avg:95.19ms
step:739/1750 train_time:70347ms step_avg:95.19ms
step:740/1750 train_time:70445ms step_avg:95.20ms
step:741/1750 train_time:70543ms step_avg:95.20ms
step:742/1750 train_time:70641ms step_avg:95.20ms
step:743/1750 train_time:70739ms step_avg:95.21ms
step:744/1750 train_time:70836ms step_avg:95.21ms
step:745/1750 train_time:70934ms step_avg:95.21ms
step:746/1750 train_time:71032ms step_avg:95.22ms
step:747/1750 train_time:71130ms step_avg:95.22ms
step:748/1750 train_time:71227ms step_avg:95.22ms
step:749/1750 train_time:71325ms step_avg:95.23ms
step:750/1750 train_time:71422ms step_avg:95.23ms
step:750/1750 val_loss:3.6009 train_time:71509ms step_avg:95.35ms
step:751/1750 train_time:71531ms step_avg:95.25ms
step:752/1750 train_time:71629ms step_avg:95.25ms
step:753/1750 train_time:71728ms step_avg:95.26ms
step:754/1750 train_time:71826ms step_avg:95.26ms
step:755/1750 train_time:71923ms step_avg:95.26ms
step:756/1750 train_time:72020ms step_avg:95.26ms
step:757/1750 train_time:72117ms step_avg:95.27ms
step:758/1750 train_time:72214ms step_avg:95.27ms
step:759/1750 train_time:72311ms step_avg:95.27ms
step:760/1750 train_time:72408ms step_avg:95.27ms
step:761/1750 train_time:72507ms step_avg:95.28ms
step:762/1750 train_time:72607ms step_avg:95.29ms
step:763/1750 train_time:72706ms step_avg:95.29ms
step:764/1750 train_time:72805ms step_avg:95.29ms
step:765/1750 train_time:72903ms step_avg:95.30ms
step:766/1750 train_time:73001ms step_avg:95.30ms
step:767/1750 train_time:73098ms step_avg:95.30ms
step:768/1750 train_time:73196ms step_avg:95.31ms
step:769/1750 train_time:73293ms step_avg:95.31ms
step:770/1750 train_time:73390ms step_avg:95.31ms
step:771/1750 train_time:73488ms step_avg:95.32ms
step:772/1750 train_time:73587ms step_avg:95.32ms
step:773/1750 train_time:73685ms step_avg:95.32ms
step:774/1750 train_time:73785ms step_avg:95.33ms
step:775/1750 train_time:73884ms step_avg:95.33ms
step:776/1750 train_time:73981ms step_avg:95.34ms
step:777/1750 train_time:74078ms step_avg:95.34ms
step:778/1750 train_time:74175ms step_avg:95.34ms
step:779/1750 train_time:74273ms step_avg:95.34ms
step:780/1750 train_time:74370ms step_avg:95.35ms
step:781/1750 train_time:74468ms step_avg:95.35ms
step:782/1750 train_time:74566ms step_avg:95.35ms
step:783/1750 train_time:74665ms step_avg:95.36ms
step:784/1750 train_time:74763ms step_avg:95.36ms
step:785/1750 train_time:74862ms step_avg:95.37ms
step:786/1750 train_time:74960ms step_avg:95.37ms
step:787/1750 train_time:75057ms step_avg:95.37ms
step:788/1750 train_time:75154ms step_avg:95.37ms
step:789/1750 train_time:75252ms step_avg:95.38ms
step:790/1750 train_time:75349ms step_avg:95.38ms
step:791/1750 train_time:75447ms step_avg:95.38ms
step:792/1750 train_time:75546ms step_avg:95.39ms
step:793/1750 train_time:75644ms step_avg:95.39ms
step:794/1750 train_time:75743ms step_avg:95.39ms
step:795/1750 train_time:75841ms step_avg:95.40ms
step:796/1750 train_time:75940ms step_avg:95.40ms
step:797/1750 train_time:76039ms step_avg:95.41ms
step:798/1750 train_time:76136ms step_avg:95.41ms
step:799/1750 train_time:76234ms step_avg:95.41ms
step:800/1750 train_time:76332ms step_avg:95.42ms
step:801/1750 train_time:76429ms step_avg:95.42ms
step:802/1750 train_time:76527ms step_avg:95.42ms
step:803/1750 train_time:76625ms step_avg:95.42ms
step:804/1750 train_time:76724ms step_avg:95.43ms
step:805/1750 train_time:76822ms step_avg:95.43ms
step:806/1750 train_time:76920ms step_avg:95.43ms
step:807/1750 train_time:77018ms step_avg:95.44ms
step:808/1750 train_time:77117ms step_avg:95.44ms
step:809/1750 train_time:77215ms step_avg:95.45ms
step:810/1750 train_time:77314ms step_avg:95.45ms
step:811/1750 train_time:77411ms step_avg:95.45ms
step:812/1750 train_time:77511ms step_avg:95.46ms
step:813/1750 train_time:77610ms step_avg:95.46ms
step:814/1750 train_time:77709ms step_avg:95.47ms
step:815/1750 train_time:77809ms step_avg:95.47ms
step:816/1750 train_time:77908ms step_avg:95.48ms
step:817/1750 train_time:78007ms step_avg:95.48ms
step:818/1750 train_time:78105ms step_avg:95.48ms
step:819/1750 train_time:78204ms step_avg:95.49ms
step:820/1750 train_time:78303ms step_avg:95.49ms
step:821/1750 train_time:78402ms step_avg:95.50ms
step:822/1750 train_time:78501ms step_avg:95.50ms
step:823/1750 train_time:78599ms step_avg:95.50ms
step:824/1750 train_time:78698ms step_avg:95.51ms
step:825/1750 train_time:78797ms step_avg:95.51ms
step:826/1750 train_time:78895ms step_avg:95.51ms
step:827/1750 train_time:78994ms step_avg:95.52ms
step:828/1750 train_time:79093ms step_avg:95.52ms
step:829/1750 train_time:79192ms step_avg:95.53ms
step:830/1750 train_time:79291ms step_avg:95.53ms
step:831/1750 train_time:79389ms step_avg:95.53ms
step:832/1750 train_time:79487ms step_avg:95.54ms
step:833/1750 train_time:79584ms step_avg:95.54ms
step:834/1750 train_time:79682ms step_avg:95.54ms
step:835/1750 train_time:79779ms step_avg:95.54ms
step:836/1750 train_time:79877ms step_avg:95.55ms
step:837/1750 train_time:79976ms step_avg:95.55ms
step:838/1750 train_time:80074ms step_avg:95.55ms
step:839/1750 train_time:80172ms step_avg:95.56ms
step:840/1750 train_time:80270ms step_avg:95.56ms
step:841/1750 train_time:80369ms step_avg:95.56ms
step:842/1750 train_time:80467ms step_avg:95.57ms
step:843/1750 train_time:80565ms step_avg:95.57ms
step:844/1750 train_time:80663ms step_avg:95.57ms
step:845/1750 train_time:80761ms step_avg:95.57ms
step:846/1750 train_time:80858ms step_avg:95.58ms
step:847/1750 train_time:80956ms step_avg:95.58ms
step:848/1750 train_time:81054ms step_avg:95.58ms
step:849/1750 train_time:81152ms step_avg:95.59ms
step:850/1750 train_time:81251ms step_avg:95.59ms
step:851/1750 train_time:81348ms step_avg:95.59ms
step:852/1750 train_time:81446ms step_avg:95.59ms
step:853/1750 train_time:81545ms step_avg:95.60ms
step:854/1750 train_time:81642ms step_avg:95.60ms
step:855/1750 train_time:81740ms step_avg:95.60ms
step:856/1750 train_time:81838ms step_avg:95.61ms
step:857/1750 train_time:81936ms step_avg:95.61ms
step:858/1750 train_time:82034ms step_avg:95.61ms
step:859/1750 train_time:82132ms step_avg:95.61ms
step:860/1750 train_time:82230ms step_avg:95.62ms
step:861/1750 train_time:82329ms step_avg:95.62ms
step:862/1750 train_time:82426ms step_avg:95.62ms
step:863/1750 train_time:82524ms step_avg:95.62ms
step:864/1750 train_time:82622ms step_avg:95.63ms
step:865/1750 train_time:82719ms step_avg:95.63ms
step:866/1750 train_time:82818ms step_avg:95.63ms
step:867/1750 train_time:82917ms step_avg:95.64ms
step:868/1750 train_time:83015ms step_avg:95.64ms
step:869/1750 train_time:83114ms step_avg:95.64ms
step:870/1750 train_time:83212ms step_avg:95.65ms
step:871/1750 train_time:83310ms step_avg:95.65ms
step:872/1750 train_time:83407ms step_avg:95.65ms
step:873/1750 train_time:83505ms step_avg:95.65ms
step:874/1750 train_time:83604ms step_avg:95.66ms
step:875/1750 train_time:83701ms step_avg:95.66ms
step:875/1750 val_loss:3.5513 train_time:83788ms step_avg:95.76ms
step:876/1750 train_time:83808ms step_avg:95.67ms
step:877/1750 train_time:83903ms step_avg:95.67ms
step:878/1750 train_time:84004ms step_avg:95.68ms
step:879/1750 train_time:84102ms step_avg:95.68ms
step:880/1750 train_time:84199ms step_avg:95.68ms
step:881/1750 train_time:84296ms step_avg:95.68ms
step:882/1750 train_time:84394ms step_avg:95.68ms
step:883/1750 train_time:84491ms step_avg:95.69ms
step:884/1750 train_time:84589ms step_avg:95.69ms
step:885/1750 train_time:84686ms step_avg:95.69ms
step:886/1750 train_time:84786ms step_avg:95.69ms
step:887/1750 train_time:84886ms step_avg:95.70ms
step:888/1750 train_time:84986ms step_avg:95.71ms
step:889/1750 train_time:85084ms step_avg:95.71ms
step:890/1750 train_time:85183ms step_avg:95.71ms
step:891/1750 train_time:85281ms step_avg:95.71ms
step:892/1750 train_time:85379ms step_avg:95.72ms
step:893/1750 train_time:85477ms step_avg:95.72ms
step:894/1750 train_time:85574ms step_avg:95.72ms
step:895/1750 train_time:85672ms step_avg:95.72ms
step:896/1750 train_time:85770ms step_avg:95.73ms
step:897/1750 train_time:85869ms step_avg:95.73ms
step:898/1750 train_time:85968ms step_avg:95.73ms
step:899/1750 train_time:86066ms step_avg:95.74ms
step:900/1750 train_time:86165ms step_avg:95.74ms
step:901/1750 train_time:86264ms step_avg:95.74ms
step:902/1750 train_time:86362ms step_avg:95.75ms
step:903/1750 train_time:86460ms step_avg:95.75ms
step:904/1750 train_time:86558ms step_avg:95.75ms
step:905/1750 train_time:86655ms step_avg:95.75ms
step:906/1750 train_time:86753ms step_avg:95.75ms
step:907/1750 train_time:86852ms step_avg:95.76ms
step:908/1750 train_time:86950ms step_avg:95.76ms
step:909/1750 train_time:87049ms step_avg:95.76ms
step:910/1750 train_time:87149ms step_avg:95.77ms
step:911/1750 train_time:87248ms step_avg:95.77ms
step:912/1750 train_time:87348ms step_avg:95.78ms
step:913/1750 train_time:87448ms step_avg:95.78ms
step:914/1750 train_time:87548ms step_avg:95.79ms
step:915/1750 train_time:87648ms step_avg:95.79ms
step:916/1750 train_time:87748ms step_avg:95.79ms
step:917/1750 train_time:87848ms step_avg:95.80ms
step:918/1750 train_time:87948ms step_avg:95.80ms
step:919/1750 train_time:88047ms step_avg:95.81ms
step:920/1750 train_time:88146ms step_avg:95.81ms
step:921/1750 train_time:88245ms step_avg:95.81ms
step:922/1750 train_time:88345ms step_avg:95.82ms
step:923/1750 train_time:88444ms step_avg:95.82ms
step:924/1750 train_time:88544ms step_avg:95.83ms
step:925/1750 train_time:88644ms step_avg:95.83ms
step:926/1750 train_time:88745ms step_avg:95.84ms
step:927/1750 train_time:88845ms step_avg:95.84ms
step:928/1750 train_time:88945ms step_avg:95.85ms
step:929/1750 train_time:89044ms step_avg:95.85ms
step:930/1750 train_time:89143ms step_avg:95.85ms
step:931/1750 train_time:89242ms step_avg:95.86ms
step:932/1750 train_time:89341ms step_avg:95.86ms
step:933/1750 train_time:89440ms step_avg:95.86ms
step:934/1750 train_time:89540ms step_avg:95.87ms
step:935/1750 train_time:89639ms step_avg:95.87ms
step:936/1750 train_time:89739ms step_avg:95.87ms
step:937/1750 train_time:89840ms step_avg:95.88ms
step:938/1750 train_time:89941ms step_avg:95.89ms
step:939/1750 train_time:90041ms step_avg:95.89ms
step:940/1750 train_time:90140ms step_avg:95.89ms
step:941/1750 train_time:90241ms step_avg:95.90ms
step:942/1750 train_time:90340ms step_avg:95.90ms
step:943/1750 train_time:90439ms step_avg:95.91ms
step:944/1750 train_time:90539ms step_avg:95.91ms
step:945/1750 train_time:90639ms step_avg:95.91ms
step:946/1750 train_time:90738ms step_avg:95.92ms
step:947/1750 train_time:90839ms step_avg:95.92ms
step:948/1750 train_time:90940ms step_avg:95.93ms
step:949/1750 train_time:91040ms step_avg:95.93ms
step:950/1750 train_time:91140ms step_avg:95.94ms
step:951/1750 train_time:91239ms step_avg:95.94ms
step:952/1750 train_time:91339ms step_avg:95.94ms
step:953/1750 train_time:91439ms step_avg:95.95ms
step:954/1750 train_time:91538ms step_avg:95.95ms
step:955/1750 train_time:91639ms step_avg:95.96ms
step:956/1750 train_time:91738ms step_avg:95.96ms
step:957/1750 train_time:91838ms step_avg:95.96ms
step:958/1750 train_time:91939ms step_avg:95.97ms
step:959/1750 train_time:92039ms step_avg:95.97ms
step:960/1750 train_time:92139ms step_avg:95.98ms
step:961/1750 train_time:92240ms step_avg:95.98ms
step:962/1750 train_time:92340ms step_avg:95.99ms
step:963/1750 train_time:92440ms step_avg:95.99ms
step:964/1750 train_time:92540ms step_avg:96.00ms
step:965/1750 train_time:92640ms step_avg:96.00ms
step:966/1750 train_time:92739ms step_avg:96.00ms
step:967/1750 train_time:92839ms step_avg:96.01ms
step:968/1750 train_time:92939ms step_avg:96.01ms
step:969/1750 train_time:93040ms step_avg:96.02ms
step:970/1750 train_time:93140ms step_avg:96.02ms
step:971/1750 train_time:93240ms step_avg:96.03ms
step:972/1750 train_time:93340ms step_avg:96.03ms
step:973/1750 train_time:93440ms step_avg:96.03ms
step:974/1750 train_time:93539ms step_avg:96.04ms
step:975/1750 train_time:93638ms step_avg:96.04ms
step:976/1750 train_time:93738ms step_avg:96.04ms
step:977/1750 train_time:93838ms step_avg:96.05ms
step:978/1750 train_time:93938ms step_avg:96.05ms
step:979/1750 train_time:94039ms step_avg:96.06ms
step:980/1750 train_time:94138ms step_avg:96.06ms
step:981/1750 train_time:94238ms step_avg:96.06ms
step:982/1750 train_time:94337ms step_avg:96.07ms
step:983/1750 train_time:94438ms step_avg:96.07ms
step:984/1750 train_time:94538ms step_avg:96.07ms
step:985/1750 train_time:94638ms step_avg:96.08ms
step:986/1750 train_time:94738ms step_avg:96.08ms
step:987/1750 train_time:94838ms step_avg:96.09ms
step:988/1750 train_time:94938ms step_avg:96.09ms
step:989/1750 train_time:95037ms step_avg:96.09ms
step:990/1750 train_time:95137ms step_avg:96.10ms
step:991/1750 train_time:95237ms step_avg:96.10ms
step:992/1750 train_time:95337ms step_avg:96.11ms
step:993/1750 train_time:95436ms step_avg:96.11ms
step:994/1750 train_time:95535ms step_avg:96.11ms
step:995/1750 train_time:95634ms step_avg:96.11ms
step:996/1750 train_time:95734ms step_avg:96.12ms
step:997/1750 train_time:95833ms step_avg:96.12ms
step:998/1750 train_time:95933ms step_avg:96.12ms
step:999/1750 train_time:96032ms step_avg:96.13ms
step:1000/1750 train_time:96132ms step_avg:96.13ms
step:1000/1750 val_loss:3.5097 train_time:96220ms step_avg:96.22ms
step:1001/1750 train_time:96240ms step_avg:96.14ms
step:1002/1750 train_time:96340ms step_avg:96.15ms
step:1003/1750 train_time:96442ms step_avg:96.15ms
step:1004/1750 train_time:96542ms step_avg:96.16ms
step:1005/1750 train_time:96641ms step_avg:96.16ms
step:1006/1750 train_time:96740ms step_avg:96.16ms
step:1007/1750 train_time:96839ms step_avg:96.17ms
step:1008/1750 train_time:96938ms step_avg:96.17ms
step:1009/1750 train_time:97038ms step_avg:96.17ms
step:1010/1750 train_time:97137ms step_avg:96.18ms
step:1011/1750 train_time:97240ms step_avg:96.18ms
step:1012/1750 train_time:97342ms step_avg:96.19ms
step:1013/1750 train_time:97443ms step_avg:96.19ms
step:1014/1750 train_time:97543ms step_avg:96.20ms
step:1015/1750 train_time:97642ms step_avg:96.20ms
step:1016/1750 train_time:97741ms step_avg:96.20ms
step:1017/1750 train_time:97839ms step_avg:96.20ms
step:1018/1750 train_time:97938ms step_avg:96.21ms
step:1019/1750 train_time:98037ms step_avg:96.21ms
step:1020/1750 train_time:98137ms step_avg:96.21ms
step:1021/1750 train_time:98238ms step_avg:96.22ms
step:1022/1750 train_time:98340ms step_avg:96.22ms
step:1023/1750 train_time:98440ms step_avg:96.23ms
step:1024/1750 train_time:98541ms step_avg:96.23ms
step:1025/1750 train_time:98641ms step_avg:96.24ms
step:1026/1750 train_time:98740ms step_avg:96.24ms
step:1027/1750 train_time:98839ms step_avg:96.24ms
step:1028/1750 train_time:98938ms step_avg:96.24ms
step:1029/1750 train_time:99038ms step_avg:96.25ms
step:1030/1750 train_time:99137ms step_avg:96.25ms
step:1031/1750 train_time:99238ms step_avg:96.25ms
step:1032/1750 train_time:99339ms step_avg:96.26ms
step:1033/1750 train_time:99440ms step_avg:96.26ms
step:1034/1750 train_time:99540ms step_avg:96.27ms
step:1035/1750 train_time:99641ms step_avg:96.27ms
step:1036/1750 train_time:99740ms step_avg:96.27ms
step:1037/1750 train_time:99840ms step_avg:96.28ms
step:1038/1750 train_time:99939ms step_avg:96.28ms
step:1039/1750 train_time:100037ms step_avg:96.28ms
step:1040/1750 train_time:100137ms step_avg:96.29ms
step:1041/1750 train_time:100238ms step_avg:96.29ms
step:1042/1750 train_time:100339ms step_avg:96.29ms
step:1043/1750 train_time:100440ms step_avg:96.30ms
step:1044/1750 train_time:100540ms step_avg:96.30ms
step:1045/1750 train_time:100640ms step_avg:96.31ms
step:1046/1750 train_time:100740ms step_avg:96.31ms
step:1047/1750 train_time:100840ms step_avg:96.31ms
step:1048/1750 train_time:100940ms step_avg:96.32ms
step:1049/1750 train_time:101038ms step_avg:96.32ms
step:1050/1750 train_time:101139ms step_avg:96.32ms
step:1051/1750 train_time:101239ms step_avg:96.33ms
step:1052/1750 train_time:101339ms step_avg:96.33ms
step:1053/1750 train_time:101440ms step_avg:96.33ms
step:1054/1750 train_time:101540ms step_avg:96.34ms
step:1055/1750 train_time:101641ms step_avg:96.34ms
step:1056/1750 train_time:101740ms step_avg:96.35ms
step:1057/1750 train_time:101840ms step_avg:96.35ms
step:1058/1750 train_time:101939ms step_avg:96.35ms
step:1059/1750 train_time:102039ms step_avg:96.35ms
step:1060/1750 train_time:102139ms step_avg:96.36ms
step:1061/1750 train_time:102239ms step_avg:96.36ms
step:1062/1750 train_time:102339ms step_avg:96.36ms
step:1063/1750 train_time:102440ms step_avg:96.37ms
step:1064/1750 train_time:102540ms step_avg:96.37ms
step:1065/1750 train_time:102641ms step_avg:96.38ms
step:1066/1750 train_time:102740ms step_avg:96.38ms
step:1067/1750 train_time:102840ms step_avg:96.38ms
step:1068/1750 train_time:102939ms step_avg:96.39ms
step:1069/1750 train_time:103038ms step_avg:96.39ms
step:1070/1750 train_time:103138ms step_avg:96.39ms
step:1071/1750 train_time:103238ms step_avg:96.39ms
step:1072/1750 train_time:103338ms step_avg:96.40ms
step:1073/1750 train_time:103439ms step_avg:96.40ms
step:1074/1750 train_time:103539ms step_avg:96.41ms
step:1075/1750 train_time:103639ms step_avg:96.41ms
step:1076/1750 train_time:103739ms step_avg:96.41ms
step:1077/1750 train_time:103840ms step_avg:96.42ms
step:1078/1750 train_time:103939ms step_avg:96.42ms
step:1079/1750 train_time:104040ms step_avg:96.42ms
step:1080/1750 train_time:104139ms step_avg:96.43ms
step:1081/1750 train_time:104239ms step_avg:96.43ms
step:1082/1750 train_time:104340ms step_avg:96.43ms
step:1083/1750 train_time:104439ms step_avg:96.44ms
step:1084/1750 train_time:104539ms step_avg:96.44ms
step:1085/1750 train_time:104640ms step_avg:96.44ms
step:1086/1750 train_time:104740ms step_avg:96.45ms
step:1087/1750 train_time:104840ms step_avg:96.45ms
step:1088/1750 train_time:104940ms step_avg:96.45ms
step:1089/1750 train_time:105041ms step_avg:96.46ms
step:1090/1750 train_time:105141ms step_avg:96.46ms
step:1091/1750 train_time:105241ms step_avg:96.46ms
step:1092/1750 train_time:105341ms step_avg:96.47ms
step:1093/1750 train_time:105440ms step_avg:96.47ms
step:1094/1750 train_time:105540ms step_avg:96.47ms
step:1095/1750 train_time:105639ms step_avg:96.47ms
step:1096/1750 train_time:105740ms step_avg:96.48ms
step:1097/1750 train_time:105838ms step_avg:96.48ms
step:1098/1750 train_time:105940ms step_avg:96.48ms
step:1099/1750 train_time:106040ms step_avg:96.49ms
step:1100/1750 train_time:106140ms step_avg:96.49ms
step:1101/1750 train_time:106239ms step_avg:96.49ms
step:1102/1750 train_time:106339ms step_avg:96.50ms
step:1103/1750 train_time:106439ms step_avg:96.50ms
step:1104/1750 train_time:106539ms step_avg:96.50ms
step:1105/1750 train_time:106640ms step_avg:96.51ms
step:1106/1750 train_time:106741ms step_avg:96.51ms
step:1107/1750 train_time:106841ms step_avg:96.51ms
step:1108/1750 train_time:106941ms step_avg:96.52ms
step:1109/1750 train_time:107040ms step_avg:96.52ms
step:1110/1750 train_time:107141ms step_avg:96.52ms
step:1111/1750 train_time:107241ms step_avg:96.53ms
step:1112/1750 train_time:107341ms step_avg:96.53ms
step:1113/1750 train_time:107442ms step_avg:96.53ms
step:1114/1750 train_time:107542ms step_avg:96.54ms
step:1115/1750 train_time:107641ms step_avg:96.54ms
step:1116/1750 train_time:107742ms step_avg:96.54ms
step:1117/1750 train_time:107842ms step_avg:96.55ms
step:1118/1750 train_time:107941ms step_avg:96.55ms
step:1119/1750 train_time:108041ms step_avg:96.55ms
step:1120/1750 train_time:108141ms step_avg:96.55ms
step:1121/1750 train_time:108241ms step_avg:96.56ms
step:1122/1750 train_time:108341ms step_avg:96.56ms
step:1123/1750 train_time:108441ms step_avg:96.56ms
step:1124/1750 train_time:108540ms step_avg:96.57ms
step:1125/1750 train_time:108641ms step_avg:96.57ms
step:1125/1750 val_loss:3.4574 train_time:108730ms step_avg:96.65ms
step:1126/1750 train_time:108750ms step_avg:96.58ms
step:1127/1750 train_time:108850ms step_avg:96.58ms
step:1128/1750 train_time:108953ms step_avg:96.59ms
step:1129/1750 train_time:109052ms step_avg:96.59ms
step:1130/1750 train_time:109151ms step_avg:96.59ms
step:1131/1750 train_time:109249ms step_avg:96.60ms
step:1132/1750 train_time:109349ms step_avg:96.60ms
step:1133/1750 train_time:109448ms step_avg:96.60ms
step:1134/1750 train_time:109547ms step_avg:96.60ms
step:1135/1750 train_time:109645ms step_avg:96.60ms
step:1136/1750 train_time:109747ms step_avg:96.61ms
step:1137/1750 train_time:109849ms step_avg:96.61ms
step:1138/1750 train_time:109951ms step_avg:96.62ms
step:1139/1750 train_time:110051ms step_avg:96.62ms
step:1140/1750 train_time:110150ms step_avg:96.62ms
step:1141/1750 train_time:110250ms step_avg:96.63ms
step:1142/1750 train_time:110349ms step_avg:96.63ms
step:1143/1750 train_time:110448ms step_avg:96.63ms
step:1144/1750 train_time:110547ms step_avg:96.63ms
step:1145/1750 train_time:110646ms step_avg:96.63ms
step:1146/1750 train_time:110746ms step_avg:96.64ms
step:1147/1750 train_time:110847ms step_avg:96.64ms
step:1148/1750 train_time:110948ms step_avg:96.65ms
step:1149/1750 train_time:111049ms step_avg:96.65ms
step:1150/1750 train_time:111149ms step_avg:96.65ms
step:1151/1750 train_time:111248ms step_avg:96.65ms
step:1152/1750 train_time:111348ms step_avg:96.66ms
step:1153/1750 train_time:111447ms step_avg:96.66ms
step:1154/1750 train_time:111546ms step_avg:96.66ms
step:1155/1750 train_time:111645ms step_avg:96.66ms
step:1156/1750 train_time:111744ms step_avg:96.66ms
step:1157/1750 train_time:111844ms step_avg:96.67ms
step:1158/1750 train_time:111945ms step_avg:96.67ms
step:1159/1750 train_time:112046ms step_avg:96.67ms
step:1160/1750 train_time:112146ms step_avg:96.68ms
step:1161/1750 train_time:112246ms step_avg:96.68ms
step:1162/1750 train_time:112345ms step_avg:96.68ms
step:1163/1750 train_time:112446ms step_avg:96.69ms
step:1164/1750 train_time:112544ms step_avg:96.69ms
step:1165/1750 train_time:112644ms step_avg:96.69ms
step:1166/1750 train_time:112744ms step_avg:96.69ms
step:1167/1750 train_time:112843ms step_avg:96.70ms
step:1168/1750 train_time:112945ms step_avg:96.70ms
step:1169/1750 train_time:113047ms step_avg:96.70ms
step:1170/1750 train_time:113147ms step_avg:96.71ms
step:1171/1750 train_time:113249ms step_avg:96.71ms
step:1172/1750 train_time:113350ms step_avg:96.71ms
step:1173/1750 train_time:113450ms step_avg:96.72ms
step:1174/1750 train_time:113551ms step_avg:96.72ms
step:1175/1750 train_time:113651ms step_avg:96.72ms
step:1176/1750 train_time:113752ms step_avg:96.73ms
step:1177/1750 train_time:113852ms step_avg:96.73ms
step:1178/1750 train_time:113953ms step_avg:96.73ms
step:1179/1750 train_time:114056ms step_avg:96.74ms
step:1180/1750 train_time:114158ms step_avg:96.74ms
step:1181/1750 train_time:114259ms step_avg:96.75ms
step:1182/1750 train_time:114361ms step_avg:96.75ms
step:1183/1750 train_time:114461ms step_avg:96.76ms
step:1184/1750 train_time:114562ms step_avg:96.76ms
step:1185/1750 train_time:114663ms step_avg:96.76ms
step:1186/1750 train_time:114764ms step_avg:96.77ms
step:1187/1750 train_time:114864ms step_avg:96.77ms
step:1188/1750 train_time:114965ms step_avg:96.77ms
step:1189/1750 train_time:115066ms step_avg:96.78ms
step:1190/1750 train_time:115167ms step_avg:96.78ms
step:1191/1750 train_time:115270ms step_avg:96.78ms
step:1192/1750 train_time:115372ms step_avg:96.79ms
step:1193/1750 train_time:115473ms step_avg:96.79ms
step:1194/1750 train_time:115574ms step_avg:96.80ms
step:1195/1750 train_time:115674ms step_avg:96.80ms
step:1196/1750 train_time:115774ms step_avg:96.80ms
step:1197/1750 train_time:115876ms step_avg:96.81ms
step:1198/1750 train_time:115978ms step_avg:96.81ms
step:1199/1750 train_time:116078ms step_avg:96.81ms
step:1200/1750 train_time:116179ms step_avg:96.82ms
step:1201/1750 train_time:116281ms step_avg:96.82ms
step:1202/1750 train_time:116383ms step_avg:96.82ms
step:1203/1750 train_time:116484ms step_avg:96.83ms
step:1204/1750 train_time:116584ms step_avg:96.83ms
step:1205/1750 train_time:116684ms step_avg:96.83ms
step:1206/1750 train_time:116785ms step_avg:96.84ms
step:1207/1750 train_time:116887ms step_avg:96.84ms
step:1208/1750 train_time:116988ms step_avg:96.84ms
step:1209/1750 train_time:117090ms step_avg:96.85ms
step:1210/1750 train_time:117192ms step_avg:96.85ms
step:1211/1750 train_time:117293ms step_avg:96.86ms
step:1212/1750 train_time:117394ms step_avg:96.86ms
step:1213/1750 train_time:117493ms step_avg:96.86ms
step:1214/1750 train_time:117593ms step_avg:96.86ms
step:1215/1750 train_time:117695ms step_avg:96.87ms
step:1216/1750 train_time:117797ms step_avg:96.87ms
step:1217/1750 train_time:117897ms step_avg:96.88ms
step:1218/1750 train_time:118000ms step_avg:96.88ms
step:1219/1750 train_time:118102ms step_avg:96.88ms
step:1220/1750 train_time:118203ms step_avg:96.89ms
step:1221/1750 train_time:118304ms step_avg:96.89ms
step:1222/1750 train_time:118405ms step_avg:96.89ms
step:1223/1750 train_time:118505ms step_avg:96.90ms
step:1224/1750 train_time:118605ms step_avg:96.90ms
step:1225/1750 train_time:118706ms step_avg:96.90ms
step:1226/1750 train_time:118807ms step_avg:96.91ms
step:1227/1750 train_time:118909ms step_avg:96.91ms
step:1228/1750 train_time:119011ms step_avg:96.91ms
step:1229/1750 train_time:119112ms step_avg:96.92ms
step:1230/1750 train_time:119212ms step_avg:96.92ms
step:1231/1750 train_time:119313ms step_avg:96.92ms
step:1232/1750 train_time:119414ms step_avg:96.93ms
step:1233/1750 train_time:119514ms step_avg:96.93ms
step:1234/1750 train_time:119615ms step_avg:96.93ms
step:1235/1750 train_time:119715ms step_avg:96.94ms
step:1236/1750 train_time:119818ms step_avg:96.94ms
step:1237/1750 train_time:119919ms step_avg:96.94ms
step:1238/1750 train_time:120020ms step_avg:96.95ms
step:1239/1750 train_time:120121ms step_avg:96.95ms
step:1240/1750 train_time:120222ms step_avg:96.95ms
step:1241/1750 train_time:120324ms step_avg:96.96ms
step:1242/1750 train_time:120425ms step_avg:96.96ms
step:1243/1750 train_time:120526ms step_avg:96.96ms
step:1244/1750 train_time:120626ms step_avg:96.97ms
step:1245/1750 train_time:120727ms step_avg:96.97ms
step:1246/1750 train_time:120830ms step_avg:96.97ms
step:1247/1750 train_time:120932ms step_avg:96.98ms
step:1248/1750 train_time:121032ms step_avg:96.98ms
step:1249/1750 train_time:121133ms step_avg:96.98ms
step:1250/1750 train_time:121233ms step_avg:96.99ms
step:1250/1750 val_loss:3.4120 train_time:121322ms step_avg:97.06ms
step:1251/1750 train_time:121342ms step_avg:97.00ms
step:1252/1750 train_time:121442ms step_avg:97.00ms
step:1253/1750 train_time:121542ms step_avg:97.00ms
step:1254/1750 train_time:121643ms step_avg:97.00ms
step:1255/1750 train_time:121743ms step_avg:97.01ms
step:1256/1750 train_time:121842ms step_avg:97.01ms
step:1257/1750 train_time:121943ms step_avg:97.01ms
step:1258/1750 train_time:122042ms step_avg:97.01ms
step:1259/1750 train_time:122142ms step_avg:97.01ms
step:1260/1750 train_time:122242ms step_avg:97.02ms
step:1261/1750 train_time:122345ms step_avg:97.02ms
step:1262/1750 train_time:122446ms step_avg:97.03ms
step:1263/1750 train_time:122547ms step_avg:97.03ms
step:1264/1750 train_time:122648ms step_avg:97.03ms
step:1265/1750 train_time:122749ms step_avg:97.03ms
step:1266/1750 train_time:122848ms step_avg:97.04ms
step:1267/1750 train_time:122950ms step_avg:97.04ms
step:1268/1750 train_time:123050ms step_avg:97.04ms
step:1269/1750 train_time:123150ms step_avg:97.05ms
step:1270/1750 train_time:123252ms step_avg:97.05ms
step:1271/1750 train_time:123354ms step_avg:97.05ms
step:1272/1750 train_time:123454ms step_avg:97.06ms
step:1273/1750 train_time:123554ms step_avg:97.06ms
step:1274/1750 train_time:123655ms step_avg:97.06ms
step:1275/1750 train_time:123755ms step_avg:97.06ms
step:1276/1750 train_time:123857ms step_avg:97.07ms
step:1277/1750 train_time:123958ms step_avg:97.07ms
step:1278/1750 train_time:124059ms step_avg:97.07ms
step:1279/1750 train_time:124160ms step_avg:97.08ms
step:1280/1750 train_time:124261ms step_avg:97.08ms
step:1281/1750 train_time:124362ms step_avg:97.08ms
step:1282/1750 train_time:124463ms step_avg:97.09ms
step:1283/1750 train_time:124563ms step_avg:97.09ms
step:1284/1750 train_time:124664ms step_avg:97.09ms
step:1285/1750 train_time:124765ms step_avg:97.09ms
step:1286/1750 train_time:124866ms step_avg:97.10ms
step:1287/1750 train_time:124967ms step_avg:97.10ms
step:1288/1750 train_time:125068ms step_avg:97.10ms
step:1289/1750 train_time:125169ms step_avg:97.11ms
step:1290/1750 train_time:125270ms step_avg:97.11ms
step:1291/1750 train_time:125371ms step_avg:97.11ms
step:1292/1750 train_time:125471ms step_avg:97.11ms
step:1293/1750 train_time:125572ms step_avg:97.12ms
step:1294/1750 train_time:125673ms step_avg:97.12ms
step:1295/1750 train_time:125774ms step_avg:97.12ms
step:1296/1750 train_time:125874ms step_avg:97.13ms
step:1297/1750 train_time:125975ms step_avg:97.13ms
step:1298/1750 train_time:126076ms step_avg:97.13ms
step:1299/1750 train_time:126176ms step_avg:97.13ms
step:1300/1750 train_time:126278ms step_avg:97.14ms
step:1301/1750 train_time:126379ms step_avg:97.14ms
step:1302/1750 train_time:126481ms step_avg:97.14ms
step:1303/1750 train_time:126582ms step_avg:97.15ms
step:1304/1750 train_time:126684ms step_avg:97.15ms
step:1305/1750 train_time:126785ms step_avg:97.15ms
step:1306/1750 train_time:126885ms step_avg:97.16ms
step:1307/1750 train_time:126987ms step_avg:97.16ms
step:1308/1750 train_time:127087ms step_avg:97.16ms
step:1309/1750 train_time:127189ms step_avg:97.17ms
step:1310/1750 train_time:127293ms step_avg:97.17ms
step:1311/1750 train_time:127395ms step_avg:97.17ms
step:1312/1750 train_time:127496ms step_avg:97.18ms
step:1313/1750 train_time:127596ms step_avg:97.18ms
step:1314/1750 train_time:127699ms step_avg:97.18ms
step:1315/1750 train_time:127800ms step_avg:97.19ms
step:1316/1750 train_time:127900ms step_avg:97.19ms
step:1317/1750 train_time:128001ms step_avg:97.19ms
step:1318/1750 train_time:128102ms step_avg:97.19ms
step:1319/1750 train_time:128204ms step_avg:97.20ms
step:1320/1750 train_time:128306ms step_avg:97.20ms
step:1321/1750 train_time:128406ms step_avg:97.20ms
step:1322/1750 train_time:128508ms step_avg:97.21ms
step:1323/1750 train_time:128610ms step_avg:97.21ms
step:1324/1750 train_time:128711ms step_avg:97.21ms
step:1325/1750 train_time:128812ms step_avg:97.22ms
step:1326/1750 train_time:128914ms step_avg:97.22ms
step:1327/1750 train_time:129015ms step_avg:97.22ms
step:1328/1750 train_time:129115ms step_avg:97.22ms
step:1329/1750 train_time:129215ms step_avg:97.23ms
step:1330/1750 train_time:129318ms step_avg:97.23ms
step:1331/1750 train_time:129419ms step_avg:97.23ms
step:1332/1750 train_time:129520ms step_avg:97.24ms
step:1333/1750 train_time:129622ms step_avg:97.24ms
step:1334/1750 train_time:129722ms step_avg:97.24ms
step:1335/1750 train_time:129824ms step_avg:97.25ms
step:1336/1750 train_time:129926ms step_avg:97.25ms
step:1337/1750 train_time:130026ms step_avg:97.25ms
step:1338/1750 train_time:130126ms step_avg:97.25ms
step:1339/1750 train_time:130227ms step_avg:97.26ms
step:1340/1750 train_time:130329ms step_avg:97.26ms
step:1341/1750 train_time:130430ms step_avg:97.26ms
step:1342/1750 train_time:130532ms step_avg:97.27ms
step:1343/1750 train_time:130633ms step_avg:97.27ms
step:1344/1750 train_time:130733ms step_avg:97.27ms
step:1345/1750 train_time:130834ms step_avg:97.27ms
step:1346/1750 train_time:130935ms step_avg:97.28ms
step:1347/1750 train_time:131035ms step_avg:97.28ms
step:1348/1750 train_time:131136ms step_avg:97.28ms
step:1349/1750 train_time:131237ms step_avg:97.28ms
step:1350/1750 train_time:131340ms step_avg:97.29ms
step:1351/1750 train_time:131442ms step_avg:97.29ms
step:1352/1750 train_time:131543ms step_avg:97.29ms
step:1353/1750 train_time:131645ms step_avg:97.30ms
step:1354/1750 train_time:131744ms step_avg:97.30ms
step:1355/1750 train_time:131845ms step_avg:97.30ms
step:1356/1750 train_time:131945ms step_avg:97.30ms
step:1357/1750 train_time:132046ms step_avg:97.31ms
step:1358/1750 train_time:132146ms step_avg:97.31ms
step:1359/1750 train_time:132248ms step_avg:97.31ms
step:1360/1750 train_time:132350ms step_avg:97.32ms
step:1361/1750 train_time:132452ms step_avg:97.32ms
step:1362/1750 train_time:132552ms step_avg:97.32ms
step:1363/1750 train_time:132653ms step_avg:97.32ms
step:1364/1750 train_time:132755ms step_avg:97.33ms
step:1365/1750 train_time:132856ms step_avg:97.33ms
step:1366/1750 train_time:132958ms step_avg:97.33ms
step:1367/1750 train_time:133058ms step_avg:97.34ms
step:1368/1750 train_time:133161ms step_avg:97.34ms
step:1369/1750 train_time:133261ms step_avg:97.34ms
step:1370/1750 train_time:133362ms step_avg:97.34ms
step:1371/1750 train_time:133464ms step_avg:97.35ms
step:1372/1750 train_time:133564ms step_avg:97.35ms
step:1373/1750 train_time:133665ms step_avg:97.35ms
step:1374/1750 train_time:133766ms step_avg:97.36ms
step:1375/1750 train_time:133867ms step_avg:97.36ms
step:1375/1750 val_loss:3.3726 train_time:133958ms step_avg:97.42ms
step:1376/1750 train_time:133978ms step_avg:97.37ms
step:1377/1750 train_time:134082ms step_avg:97.37ms
step:1378/1750 train_time:134182ms step_avg:97.37ms
step:1379/1750 train_time:134282ms step_avg:97.38ms
step:1380/1750 train_time:134383ms step_avg:97.38ms
step:1381/1750 train_time:134483ms step_avg:97.38ms
step:1382/1750 train_time:134583ms step_avg:97.38ms
step:1383/1750 train_time:134684ms step_avg:97.39ms
step:1384/1750 train_time:134785ms step_avg:97.39ms
step:1385/1750 train_time:134886ms step_avg:97.39ms
step:1386/1750 train_time:134989ms step_avg:97.39ms
step:1387/1750 train_time:135090ms step_avg:97.40ms
step:1388/1750 train_time:135191ms step_avg:97.40ms
step:1389/1750 train_time:135293ms step_avg:97.40ms
step:1390/1750 train_time:135394ms step_avg:97.41ms
step:1391/1750 train_time:135495ms step_avg:97.41ms
step:1392/1750 train_time:135597ms step_avg:97.41ms
step:1393/1750 train_time:135698ms step_avg:97.41ms
step:1394/1750 train_time:135799ms step_avg:97.42ms
step:1395/1750 train_time:135900ms step_avg:97.42ms
step:1396/1750 train_time:136002ms step_avg:97.42ms
step:1397/1750 train_time:136104ms step_avg:97.43ms
step:1398/1750 train_time:136205ms step_avg:97.43ms
step:1399/1750 train_time:136307ms step_avg:97.43ms
step:1400/1750 train_time:136408ms step_avg:97.43ms
step:1401/1750 train_time:136510ms step_avg:97.44ms
step:1402/1750 train_time:136610ms step_avg:97.44ms
step:1403/1750 train_time:136711ms step_avg:97.44ms
step:1404/1750 train_time:136812ms step_avg:97.44ms
step:1405/1750 train_time:136915ms step_avg:97.45ms
step:1406/1750 train_time:137017ms step_avg:97.45ms
step:1407/1750 train_time:137119ms step_avg:97.45ms
step:1408/1750 train_time:137219ms step_avg:97.46ms
step:1409/1750 train_time:137322ms step_avg:97.46ms
step:1410/1750 train_time:137423ms step_avg:97.46ms
step:1411/1750 train_time:137524ms step_avg:97.47ms
step:1412/1750 train_time:137625ms step_avg:97.47ms
step:1413/1750 train_time:137725ms step_avg:97.47ms
step:1414/1750 train_time:137826ms step_avg:97.47ms
step:1415/1750 train_time:137929ms step_avg:97.48ms
step:1416/1750 train_time:138030ms step_avg:97.48ms
step:1417/1750 train_time:138130ms step_avg:97.48ms
step:1418/1750 train_time:138230ms step_avg:97.48ms
step:1419/1750 train_time:138331ms step_avg:97.48ms
step:1420/1750 train_time:138432ms step_avg:97.49ms
step:1421/1750 train_time:138535ms step_avg:97.49ms
step:1422/1750 train_time:138636ms step_avg:97.49ms
step:1423/1750 train_time:138737ms step_avg:97.50ms
step:1424/1750 train_time:138838ms step_avg:97.50ms
step:1425/1750 train_time:138939ms step_avg:97.50ms
step:1426/1750 train_time:139039ms step_avg:97.50ms
step:1427/1750 train_time:139140ms step_avg:97.51ms
step:1428/1750 train_time:139241ms step_avg:97.51ms
step:1429/1750 train_time:139343ms step_avg:97.51ms
step:1430/1750 train_time:139447ms step_avg:97.52ms
step:1431/1750 train_time:139550ms step_avg:97.52ms
step:1432/1750 train_time:139651ms step_avg:97.52ms
step:1433/1750 train_time:139753ms step_avg:97.52ms
step:1434/1750 train_time:139854ms step_avg:97.53ms
step:1435/1750 train_time:139957ms step_avg:97.53ms
step:1436/1750 train_time:140058ms step_avg:97.53ms
step:1437/1750 train_time:140161ms step_avg:97.54ms
step:1438/1750 train_time:140262ms step_avg:97.54ms
step:1439/1750 train_time:140365ms step_avg:97.54ms
step:1440/1750 train_time:140468ms step_avg:97.55ms
step:1441/1750 train_time:140570ms step_avg:97.55ms
step:1442/1750 train_time:140671ms step_avg:97.55ms
step:1443/1750 train_time:140771ms step_avg:97.55ms
step:1444/1750 train_time:140873ms step_avg:97.56ms
step:1445/1750 train_time:140975ms step_avg:97.56ms
step:1446/1750 train_time:141078ms step_avg:97.56ms
step:1447/1750 train_time:141179ms step_avg:97.57ms
step:1448/1750 train_time:141284ms step_avg:97.57ms
step:1449/1750 train_time:141385ms step_avg:97.57ms
step:1450/1750 train_time:141487ms step_avg:97.58ms
step:1451/1750 train_time:141589ms step_avg:97.58ms
step:1452/1750 train_time:141690ms step_avg:97.58ms
step:1453/1750 train_time:141792ms step_avg:97.59ms
step:1454/1750 train_time:141895ms step_avg:97.59ms
step:1455/1750 train_time:141997ms step_avg:97.59ms
step:1456/1750 train_time:142099ms step_avg:97.60ms
step:1457/1750 train_time:142201ms step_avg:97.60ms
step:1458/1750 train_time:142304ms step_avg:97.60ms
step:1459/1750 train_time:142406ms step_avg:97.61ms
step:1460/1750 train_time:142507ms step_avg:97.61ms
step:1461/1750 train_time:142609ms step_avg:97.61ms
step:1462/1750 train_time:142711ms step_avg:97.61ms
step:1463/1750 train_time:142813ms step_avg:97.62ms
step:1464/1750 train_time:142915ms step_avg:97.62ms
step:1465/1750 train_time:143017ms step_avg:97.62ms
step:1466/1750 train_time:143119ms step_avg:97.63ms
step:1467/1750 train_time:143220ms step_avg:97.63ms
step:1468/1750 train_time:143322ms step_avg:97.63ms
step:1469/1750 train_time:143425ms step_avg:97.63ms
step:1470/1750 train_time:143526ms step_avg:97.64ms
step:1471/1750 train_time:143628ms step_avg:97.64ms
step:1472/1750 train_time:143730ms step_avg:97.64ms
step:1473/1750 train_time:143831ms step_avg:97.64ms
step:1474/1750 train_time:143932ms step_avg:97.65ms
step:1475/1750 train_time:144033ms step_avg:97.65ms
step:1476/1750 train_time:144136ms step_avg:97.65ms
step:1477/1750 train_time:144239ms step_avg:97.66ms
step:1478/1750 train_time:144341ms step_avg:97.66ms
step:1479/1750 train_time:144443ms step_avg:97.66ms
step:1480/1750 train_time:144544ms step_avg:97.67ms
step:1481/1750 train_time:144646ms step_avg:97.67ms
step:1482/1750 train_time:144747ms step_avg:97.67ms
step:1483/1750 train_time:144848ms step_avg:97.67ms
step:1484/1750 train_time:144951ms step_avg:97.68ms
step:1485/1750 train_time:145054ms step_avg:97.68ms
step:1486/1750 train_time:145156ms step_avg:97.68ms
step:1487/1750 train_time:145259ms step_avg:97.69ms
step:1488/1750 train_time:145361ms step_avg:97.69ms
step:1489/1750 train_time:145463ms step_avg:97.69ms
step:1490/1750 train_time:145565ms step_avg:97.69ms
step:1491/1750 train_time:145667ms step_avg:97.70ms
step:1492/1750 train_time:145768ms step_avg:97.70ms
step:1493/1750 train_time:145869ms step_avg:97.70ms
step:1494/1750 train_time:145970ms step_avg:97.70ms
step:1495/1750 train_time:146071ms step_avg:97.71ms
step:1496/1750 train_time:146173ms step_avg:97.71ms
step:1497/1750 train_time:146275ms step_avg:97.71ms
step:1498/1750 train_time:146378ms step_avg:97.72ms
step:1499/1750 train_time:146480ms step_avg:97.72ms
step:1500/1750 train_time:146583ms step_avg:97.72ms
step:1500/1750 val_loss:3.3371 train_time:146673ms step_avg:97.78ms
step:1501/1750 train_time:146693ms step_avg:97.73ms
step:1502/1750 train_time:146793ms step_avg:97.73ms
step:1503/1750 train_time:146896ms step_avg:97.73ms
step:1504/1750 train_time:146996ms step_avg:97.74ms
step:1505/1750 train_time:147096ms step_avg:97.74ms
step:1506/1750 train_time:147197ms step_avg:97.74ms
step:1507/1750 train_time:147297ms step_avg:97.74ms
step:1508/1750 train_time:147398ms step_avg:97.74ms
step:1509/1750 train_time:147499ms step_avg:97.75ms
step:1510/1750 train_time:147600ms step_avg:97.75ms
step:1511/1750 train_time:147706ms step_avg:97.75ms
step:1512/1750 train_time:147810ms step_avg:97.76ms
step:1513/1750 train_time:147913ms step_avg:97.76ms
step:1514/1750 train_time:148015ms step_avg:97.76ms
step:1515/1750 train_time:148120ms step_avg:97.77ms
step:1516/1750 train_time:148220ms step_avg:97.77ms
step:1517/1750 train_time:148320ms step_avg:97.77ms
step:1518/1750 train_time:148421ms step_avg:97.77ms
step:1519/1750 train_time:148523ms step_avg:97.78ms
step:1520/1750 train_time:148625ms step_avg:97.78ms
step:1521/1750 train_time:148727ms step_avg:97.78ms
step:1522/1750 train_time:148830ms step_avg:97.79ms
step:1523/1750 train_time:148933ms step_avg:97.79ms
step:1524/1750 train_time:149038ms step_avg:97.79ms
step:1525/1750 train_time:149140ms step_avg:97.80ms
step:1526/1750 train_time:149241ms step_avg:97.80ms
step:1527/1750 train_time:149343ms step_avg:97.80ms
step:1528/1750 train_time:149449ms step_avg:97.81ms
step:1529/1750 train_time:149550ms step_avg:97.81ms
step:1530/1750 train_time:149653ms step_avg:97.81ms
step:1531/1750 train_time:149753ms step_avg:97.81ms
step:1532/1750 train_time:149856ms step_avg:97.82ms
step:1533/1750 train_time:149958ms step_avg:97.82ms
step:1534/1750 train_time:150059ms step_avg:97.82ms
step:1535/1750 train_time:150161ms step_avg:97.82ms
step:1536/1750 train_time:150262ms step_avg:97.83ms
step:1537/1750 train_time:150363ms step_avg:97.83ms
step:1538/1750 train_time:150465ms step_avg:97.83ms
step:1539/1750 train_time:150568ms step_avg:97.83ms
step:1540/1750 train_time:150671ms step_avg:97.84ms
step:1541/1750 train_time:150774ms step_avg:97.84ms
step:1542/1750 train_time:150878ms step_avg:97.85ms
step:1543/1750 train_time:150981ms step_avg:97.85ms
step:1544/1750 train_time:151082ms step_avg:97.85ms
step:1545/1750 train_time:151183ms step_avg:97.85ms
step:1546/1750 train_time:151284ms step_avg:97.86ms
step:1547/1750 train_time:151387ms step_avg:97.86ms
step:1548/1750 train_time:151489ms step_avg:97.86ms
step:1549/1750 train_time:151591ms step_avg:97.86ms
step:1550/1750 train_time:151693ms step_avg:97.87ms
step:1551/1750 train_time:151795ms step_avg:97.87ms
step:1552/1750 train_time:151897ms step_avg:97.87ms
step:1553/1750 train_time:152000ms step_avg:97.87ms
step:1554/1750 train_time:152100ms step_avg:97.88ms
step:1555/1750 train_time:152202ms step_avg:97.88ms
step:1556/1750 train_time:152304ms step_avg:97.88ms
step:1557/1750 train_time:152407ms step_avg:97.89ms
step:1558/1750 train_time:152509ms step_avg:97.89ms
step:1559/1750 train_time:152612ms step_avg:97.89ms
step:1560/1750 train_time:152714ms step_avg:97.89ms
step:1561/1750 train_time:152815ms step_avg:97.90ms
step:1562/1750 train_time:152918ms step_avg:97.90ms
step:1563/1750 train_time:153023ms step_avg:97.90ms
step:1564/1750 train_time:153125ms step_avg:97.91ms
step:1565/1750 train_time:153226ms step_avg:97.91ms
step:1566/1750 train_time:153328ms step_avg:97.91ms
step:1567/1750 train_time:153429ms step_avg:97.91ms
step:1568/1750 train_time:153530ms step_avg:97.91ms
step:1569/1750 train_time:153633ms step_avg:97.92ms
step:1570/1750 train_time:153736ms step_avg:97.92ms
step:1571/1750 train_time:153837ms step_avg:97.92ms
step:1572/1750 train_time:153939ms step_avg:97.93ms
step:1573/1750 train_time:154041ms step_avg:97.93ms
step:1574/1750 train_time:154144ms step_avg:97.93ms
step:1575/1750 train_time:154245ms step_avg:97.93ms
step:1576/1750 train_time:154348ms step_avg:97.94ms
step:1577/1750 train_time:154451ms step_avg:97.94ms
step:1578/1750 train_time:154552ms step_avg:97.94ms
step:1579/1750 train_time:154653ms step_avg:97.94ms
step:1580/1750 train_time:154755ms step_avg:97.95ms
step:1581/1750 train_time:154858ms step_avg:97.95ms
step:1582/1750 train_time:154959ms step_avg:97.95ms
step:1583/1750 train_time:155063ms step_avg:97.96ms
step:1584/1750 train_time:155166ms step_avg:97.96ms
step:1585/1750 train_time:155268ms step_avg:97.96ms
step:1586/1750 train_time:155371ms step_avg:97.96ms
step:1587/1750 train_time:155473ms step_avg:97.97ms
step:1588/1750 train_time:155575ms step_avg:97.97ms
step:1589/1750 train_time:155677ms step_avg:97.97ms
step:1590/1750 train_time:155779ms step_avg:97.97ms
step:1591/1750 train_time:155880ms step_avg:97.98ms
step:1592/1750 train_time:155982ms step_avg:97.98ms
step:1593/1750 train_time:156084ms step_avg:97.98ms
step:1594/1750 train_time:156190ms step_avg:97.99ms
step:1595/1750 train_time:156292ms step_avg:97.99ms
step:1596/1750 train_time:156393ms step_avg:97.99ms
step:1597/1750 train_time:156494ms step_avg:97.99ms
step:1598/1750 train_time:156598ms step_avg:98.00ms
step:1599/1750 train_time:156699ms step_avg:98.00ms
step:1600/1750 train_time:156800ms step_avg:98.00ms
step:1601/1750 train_time:156901ms step_avg:98.00ms
step:1602/1750 train_time:157003ms step_avg:98.00ms
step:1603/1750 train_time:157105ms step_avg:98.01ms
step:1604/1750 train_time:157207ms step_avg:98.01ms
step:1605/1750 train_time:157309ms step_avg:98.01ms
step:1606/1750 train_time:157412ms step_avg:98.01ms
step:1607/1750 train_time:157513ms step_avg:98.02ms
step:1608/1750 train_time:157614ms step_avg:98.02ms
step:1609/1750 train_time:157717ms step_avg:98.02ms
step:1610/1750 train_time:157820ms step_avg:98.02ms
step:1611/1750 train_time:157923ms step_avg:98.03ms
step:1612/1750 train_time:158025ms step_avg:98.03ms
step:1613/1750 train_time:158127ms step_avg:98.03ms
step:1614/1750 train_time:158228ms step_avg:98.03ms
step:1615/1750 train_time:158329ms step_avg:98.04ms
step:1616/1750 train_time:158431ms step_avg:98.04ms
step:1617/1750 train_time:158534ms step_avg:98.04ms
step:1618/1750 train_time:158635ms step_avg:98.04ms
step:1619/1750 train_time:158737ms step_avg:98.05ms
step:1620/1750 train_time:158839ms step_avg:98.05ms
step:1621/1750 train_time:158940ms step_avg:98.05ms
step:1622/1750 train_time:159042ms step_avg:98.05ms
step:1623/1750 train_time:159144ms step_avg:98.06ms
step:1624/1750 train_time:159248ms step_avg:98.06ms
step:1625/1750 train_time:159352ms step_avg:98.06ms
step:1625/1750 val_loss:3.3068 train_time:159442ms step_avg:98.12ms
step:1626/1750 train_time:159462ms step_avg:98.07ms
step:1627/1750 train_time:159566ms step_avg:98.07ms
step:1628/1750 train_time:159668ms step_avg:98.08ms
step:1629/1750 train_time:159771ms step_avg:98.08ms
step:1630/1750 train_time:159871ms step_avg:98.08ms
step:1631/1750 train_time:159972ms step_avg:98.08ms
step:1632/1750 train_time:160073ms step_avg:98.08ms
step:1633/1750 train_time:160175ms step_avg:98.09ms
step:1634/1750 train_time:160278ms step_avg:98.09ms
step:1635/1750 train_time:160380ms step_avg:98.09ms
step:1636/1750 train_time:160484ms step_avg:98.10ms
step:1637/1750 train_time:160587ms step_avg:98.10ms
step:1638/1750 train_time:160689ms step_avg:98.10ms
step:1639/1750 train_time:160791ms step_avg:98.10ms
step:1640/1750 train_time:160892ms step_avg:98.10ms
step:1641/1750 train_time:160993ms step_avg:98.11ms
step:1642/1750 train_time:161094ms step_avg:98.11ms
step:1643/1750 train_time:161194ms step_avg:98.11ms
step:1644/1750 train_time:161296ms step_avg:98.11ms
step:1645/1750 train_time:161399ms step_avg:98.11ms
step:1646/1750 train_time:161503ms step_avg:98.12ms
step:1647/1750 train_time:161606ms step_avg:98.12ms
step:1648/1750 train_time:161709ms step_avg:98.12ms
step:1649/1750 train_time:161811ms step_avg:98.13ms
step:1650/1750 train_time:161912ms step_avg:98.13ms
step:1651/1750 train_time:162014ms step_avg:98.13ms
step:1652/1750 train_time:162115ms step_avg:98.13ms
step:1653/1750 train_time:162216ms step_avg:98.13ms
step:1654/1750 train_time:162317ms step_avg:98.14ms
step:1655/1750 train_time:162419ms step_avg:98.14ms
step:1656/1750 train_time:162522ms step_avg:98.14ms
step:1657/1750 train_time:162625ms step_avg:98.14ms
step:1658/1750 train_time:162728ms step_avg:98.15ms
step:1659/1750 train_time:162832ms step_avg:98.15ms
step:1660/1750 train_time:162933ms step_avg:98.15ms
step:1661/1750 train_time:163036ms step_avg:98.16ms
step:1662/1750 train_time:163138ms step_avg:98.16ms
step:1663/1750 train_time:163241ms step_avg:98.16ms
step:1664/1750 train_time:163343ms step_avg:98.16ms
step:1665/1750 train_time:163447ms step_avg:98.17ms
step:1666/1750 train_time:163550ms step_avg:98.17ms
step:1667/1750 train_time:163653ms step_avg:98.17ms
step:1668/1750 train_time:163758ms step_avg:98.18ms
step:1669/1750 train_time:163861ms step_avg:98.18ms
step:1670/1750 train_time:163962ms step_avg:98.18ms
step:1671/1750 train_time:164064ms step_avg:98.18ms
step:1672/1750 train_time:164166ms step_avg:98.19ms
step:1673/1750 train_time:164267ms step_avg:98.19ms
step:1674/1750 train_time:164370ms step_avg:98.19ms
step:1675/1750 train_time:164471ms step_avg:98.19ms
step:1676/1750 train_time:164575ms step_avg:98.19ms
step:1677/1750 train_time:164676ms step_avg:98.20ms
step:1678/1750 train_time:164781ms step_avg:98.20ms
step:1679/1750 train_time:164885ms step_avg:98.20ms
step:1680/1750 train_time:164985ms step_avg:98.21ms
step:1681/1750 train_time:165088ms step_avg:98.21ms
step:1682/1750 train_time:165192ms step_avg:98.21ms
step:1683/1750 train_time:165292ms step_avg:98.21ms
step:1684/1750 train_time:165395ms step_avg:98.22ms
step:1685/1750 train_time:165497ms step_avg:98.22ms
step:1686/1750 train_time:165600ms step_avg:98.22ms
step:1687/1750 train_time:165702ms step_avg:98.22ms
step:1688/1750 train_time:165805ms step_avg:98.23ms
step:1689/1750 train_time:165907ms step_avg:98.23ms
step:1690/1750 train_time:166010ms step_avg:98.23ms
step:1691/1750 train_time:166111ms step_avg:98.23ms
step:1692/1750 train_time:166213ms step_avg:98.23ms
step:1693/1750 train_time:166316ms step_avg:98.24ms
step:1694/1750 train_time:166419ms step_avg:98.24ms
step:1695/1750 train_time:166522ms step_avg:98.24ms
step:1696/1750 train_time:166624ms step_avg:98.25ms
step:1697/1750 train_time:166728ms step_avg:98.25ms
step:1698/1750 train_time:166831ms step_avg:98.25ms
step:1699/1750 train_time:166933ms step_avg:98.25ms
step:1700/1750 train_time:167037ms step_avg:98.26ms
step:1701/1750 train_time:167140ms step_avg:98.26ms
step:1702/1750 train_time:167245ms step_avg:98.26ms
step:1703/1750 train_time:167347ms step_avg:98.27ms
step:1704/1750 train_time:167450ms step_avg:98.27ms
step:1705/1750 train_time:167552ms step_avg:98.27ms
step:1706/1750 train_time:167656ms step_avg:98.27ms
step:1707/1750 train_time:167760ms step_avg:98.28ms
step:1708/1750 train_time:167863ms step_avg:98.28ms
step:1709/1750 train_time:167967ms step_avg:98.28ms
step:1710/1750 train_time:168068ms step_avg:98.29ms
step:1711/1750 train_time:168172ms step_avg:98.29ms
step:1712/1750 train_time:168274ms step_avg:98.29ms
step:1713/1750 train_time:168379ms step_avg:98.29ms
step:1714/1750 train_time:168481ms step_avg:98.30ms
step:1715/1750 train_time:168585ms step_avg:98.30ms
step:1716/1750 train_time:168689ms step_avg:98.30ms
step:1717/1750 train_time:168791ms step_avg:98.31ms
step:1718/1750 train_time:168893ms step_avg:98.31ms
step:1719/1750 train_time:169001ms step_avg:98.31ms
step:1720/1750 train_time:169102ms step_avg:98.32ms
step:1721/1750 train_time:169204ms step_avg:98.32ms
step:1722/1750 train_time:169308ms step_avg:98.32ms
step:1723/1750 train_time:169412ms step_avg:98.32ms
step:1724/1750 train_time:169515ms step_avg:98.33ms
step:1725/1750 train_time:169619ms step_avg:98.33ms
step:1726/1750 train_time:169721ms step_avg:98.33ms
step:1727/1750 train_time:169824ms step_avg:98.33ms
step:1728/1750 train_time:169929ms step_avg:98.34ms
step:1729/1750 train_time:170032ms step_avg:98.34ms
step:1730/1750 train_time:170134ms step_avg:98.34ms
step:1731/1750 train_time:170239ms step_avg:98.35ms
step:1732/1750 train_time:170342ms step_avg:98.35ms
step:1733/1750 train_time:170445ms step_avg:98.35ms
step:1734/1750 train_time:170549ms step_avg:98.36ms
step:1735/1750 train_time:170651ms step_avg:98.36ms
step:1736/1750 train_time:170754ms step_avg:98.36ms
step:1737/1750 train_time:170856ms step_avg:98.36ms
step:1738/1750 train_time:170960ms step_avg:98.37ms
step:1739/1750 train_time:171063ms step_avg:98.37ms
step:1740/1750 train_time:171165ms step_avg:98.37ms
step:1741/1750 train_time:171271ms step_avg:98.37ms
step:1742/1750 train_time:171375ms step_avg:98.38ms
step:1743/1750 train_time:171477ms step_avg:98.38ms
step:1744/1750 train_time:171578ms step_avg:98.38ms
step:1745/1750 train_time:171680ms step_avg:98.38ms
step:1746/1750 train_time:171782ms step_avg:98.39ms
step:1747/1750 train_time:171885ms step_avg:98.39ms
step:1748/1750 train_time:171988ms step_avg:98.39ms
step:1749/1750 train_time:172091ms step_avg:98.39ms
step:1750/1750 train_time:172194ms step_avg:98.40ms
step:1750/1750 val_loss:3.2835 train_time:172285ms step_avg:98.45ms
peak memory allocated: 33278 MiB reserved: 48754 MiB
