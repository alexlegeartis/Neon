import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


class NormMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, sgd_coeff=0):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, sgd_coeff=sgd_coeff)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        self.sgd_coeff = sgd_coeff

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            sgd_coeff = group["sgd_coeff"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    eps = 1e-12
                    g_normalized = grad / (grad.norm() + eps)
                    if sgd_coeff != 1:
                        update_part = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                        update = (1 - sgd_coeff) * update_part + sgd_coeff * g_normalized
                    else:
                        update = sgd_coeff * g_normalized
                    p.add_(other=update, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()
class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * world_size
            for base_i in range(0, len(params), world_size):
                if base_i + rank < len(params):
                    grad = params[base_i + rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + rank < len(params):
                    p = params[base_i + rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                all_reduce_futures.append(dist.all_gather(params_pad[base_i:base_i + world_size], params_pad[base_i + rank], async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_reduce_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)
                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_reduce_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_reduce_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 27.5
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos : pos + max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()
    starts = []
    for i in range(1, len(boundary_positions)):
        end = boundary_positions[i].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == dist.get_world_size():
                return starts, end - pos
            start = end
    assert False # increase max_batch_span if necessary

def distributed_data_generator(filename_pattern: str, batch_size: int, align_to_bos: bool):
    rank = dist.get_rank()
    world_size = dist.get_world_size()
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    max_batch_span = 2 * batch_size if align_to_bos else batch_size # provide buffer to handle samples up to length local_batch_size
    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            batch_span = batch_size
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1750 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
from datetime import datetime
if master_process:
    # run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_id = str(int(time.time() * 1000))
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0)
optimizer2 = NormMuon(hidden_matrix_params, lr=0.05, momentum=0.96, weight_decay=0.0, sgd_coeff=0.5)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, align_to_bos=True)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.12.11 | packaged by conda-forge | (main, Jun  4 2025, 14:45:31) [GCC 13.3.0]
Running PyTorch 2.9.0.dev20250824+cu126 compiled for CUDA 12.6
Sun Aug 24 16:46:13 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.158.01             Driver Version: 570.158.01     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:04:00.0 Off |                    0 |
| N/A   40C    P0            122W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:23:00.0 Off |                    0 |
| N/A   33C    P0            115W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:43:00.0 Off |                    0 |
| N/A   38C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   32C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:84:00.0 Off |                    0 |
| N/A   38C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:A3:00.0 Off |                    0 |
| N/A   33C    P0            121W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:C3:00.0 Off |                    0 |
| N/A   39C    P0            121W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:E4:00.0 Off |                    0 |
| N/A   32C    P0            113W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.06ms
step:1/1750 train_time:150ms step_avg:150.17ms
step:2/1750 train_time:171ms step_avg:85.59ms
step:3/1750 train_time:251ms step_avg:83.81ms
step:4/1750 train_time:343ms step_avg:85.80ms
step:5/1750 train_time:436ms step_avg:87.11ms
step:6/1750 train_time:528ms step_avg:87.96ms
step:7/1750 train_time:621ms step_avg:88.69ms
step:8/1750 train_time:713ms step_avg:89.10ms
step:9/1750 train_time:806ms step_avg:89.53ms
step:10/1750 train_time:898ms step_avg:89.79ms
step:11/1750 train_time:991ms step_avg:90.05ms
step:12/1750 train_time:1086ms step_avg:90.50ms
step:13/1750 train_time:1181ms step_avg:90.84ms
step:14/1750 train_time:1276ms step_avg:91.11ms
step:15/1750 train_time:1369ms step_avg:91.25ms
step:16/1750 train_time:1462ms step_avg:91.39ms
step:17/1750 train_time:1555ms step_avg:91.46ms
step:18/1750 train_time:1648ms step_avg:91.53ms
step:19/1750 train_time:1741ms step_avg:91.63ms
step:20/1750 train_time:1833ms step_avg:91.67ms
step:21/1750 train_time:1926ms step_avg:91.73ms
step:22/1750 train_time:2020ms step_avg:91.80ms
step:23/1750 train_time:2114ms step_avg:91.91ms
step:24/1750 train_time:2209ms step_avg:92.03ms
step:25/1750 train_time:2303ms step_avg:92.14ms
step:26/1750 train_time:2397ms step_avg:92.20ms
step:27/1750 train_time:2490ms step_avg:92.21ms
step:28/1750 train_time:2583ms step_avg:92.25ms
step:29/1750 train_time:2676ms step_avg:92.26ms
step:30/1750 train_time:2768ms step_avg:92.28ms
step:31/1750 train_time:2861ms step_avg:92.29ms
step:32/1750 train_time:2954ms step_avg:92.32ms
step:33/1750 train_time:3048ms step_avg:92.35ms
step:34/1750 train_time:3141ms step_avg:92.40ms
step:35/1750 train_time:3236ms step_avg:92.44ms
step:36/1750 train_time:3330ms step_avg:92.50ms
step:37/1750 train_time:3425ms step_avg:92.56ms
step:38/1750 train_time:3518ms step_avg:92.59ms
step:39/1750 train_time:3611ms step_avg:92.59ms
step:40/1750 train_time:3705ms step_avg:92.62ms
step:41/1750 train_time:3798ms step_avg:92.64ms
step:42/1750 train_time:3890ms step_avg:92.63ms
step:43/1750 train_time:3984ms step_avg:92.65ms
step:44/1750 train_time:4077ms step_avg:92.66ms
step:45/1750 train_time:4171ms step_avg:92.68ms
step:46/1750 train_time:4265ms step_avg:92.71ms
step:47/1750 train_time:4358ms step_avg:92.73ms
step:48/1750 train_time:4453ms step_avg:92.76ms
step:49/1750 train_time:4546ms step_avg:92.79ms
step:50/1750 train_time:4640ms step_avg:92.80ms
step:51/1750 train_time:4733ms step_avg:92.80ms
step:52/1750 train_time:4827ms step_avg:92.82ms
step:53/1750 train_time:4920ms step_avg:92.83ms
step:54/1750 train_time:5013ms step_avg:92.83ms
step:55/1750 train_time:5106ms step_avg:92.84ms
step:56/1750 train_time:5200ms step_avg:92.85ms
step:57/1750 train_time:5294ms step_avg:92.87ms
step:58/1750 train_time:5387ms step_avg:92.89ms
step:59/1750 train_time:5482ms step_avg:92.91ms
step:60/1750 train_time:5576ms step_avg:92.93ms
step:61/1750 train_time:5669ms step_avg:92.94ms
step:62/1750 train_time:5763ms step_avg:92.95ms
step:63/1750 train_time:5856ms step_avg:92.96ms
step:64/1750 train_time:5949ms step_avg:92.96ms
step:65/1750 train_time:6043ms step_avg:92.96ms
step:66/1750 train_time:6136ms step_avg:92.97ms
step:67/1750 train_time:6230ms step_avg:92.98ms
step:68/1750 train_time:6324ms step_avg:93.00ms
step:69/1750 train_time:6418ms step_avg:93.01ms
step:70/1750 train_time:6512ms step_avg:93.03ms
step:71/1750 train_time:6606ms step_avg:93.04ms
step:72/1750 train_time:6700ms step_avg:93.05ms
step:73/1750 train_time:6793ms step_avg:93.06ms
step:74/1750 train_time:6887ms step_avg:93.07ms
step:75/1750 train_time:6980ms step_avg:93.06ms
step:76/1750 train_time:7072ms step_avg:93.06ms
step:77/1750 train_time:7166ms step_avg:93.07ms
step:78/1750 train_time:7259ms step_avg:93.07ms
step:79/1750 train_time:7353ms step_avg:93.07ms
step:80/1750 train_time:7446ms step_avg:93.07ms
step:81/1750 train_time:7539ms step_avg:93.07ms
step:82/1750 train_time:7632ms step_avg:93.08ms
step:83/1750 train_time:7726ms step_avg:93.09ms
step:84/1750 train_time:7820ms step_avg:93.09ms
step:85/1750 train_time:7913ms step_avg:93.09ms
step:86/1750 train_time:8007ms step_avg:93.10ms
step:87/1750 train_time:8100ms step_avg:93.11ms
step:88/1750 train_time:8194ms step_avg:93.11ms
step:89/1750 train_time:8287ms step_avg:93.11ms
step:90/1750 train_time:8380ms step_avg:93.11ms
step:91/1750 train_time:8473ms step_avg:93.12ms
step:92/1750 train_time:8567ms step_avg:93.12ms
step:93/1750 train_time:8661ms step_avg:93.13ms
step:94/1750 train_time:8755ms step_avg:93.13ms
step:95/1750 train_time:8847ms step_avg:93.13ms
step:96/1750 train_time:8940ms step_avg:93.13ms
step:97/1750 train_time:9034ms step_avg:93.13ms
step:98/1750 train_time:9128ms step_avg:93.14ms
step:99/1750 train_time:9222ms step_avg:93.15ms
step:100/1750 train_time:9315ms step_avg:93.15ms
step:101/1750 train_time:9409ms step_avg:93.16ms
step:102/1750 train_time:9502ms step_avg:93.16ms
step:103/1750 train_time:9596ms step_avg:93.16ms
step:104/1750 train_time:9689ms step_avg:93.17ms
step:105/1750 train_time:9783ms step_avg:93.17ms
step:106/1750 train_time:9876ms step_avg:93.17ms
step:107/1750 train_time:9969ms step_avg:93.17ms
step:108/1750 train_time:10062ms step_avg:93.17ms
step:109/1750 train_time:10156ms step_avg:93.18ms
step:110/1750 train_time:10250ms step_avg:93.18ms
step:111/1750 train_time:10343ms step_avg:93.18ms
step:112/1750 train_time:10437ms step_avg:93.18ms
step:113/1750 train_time:10530ms step_avg:93.19ms
step:114/1750 train_time:10624ms step_avg:93.19ms
step:115/1750 train_time:10717ms step_avg:93.19ms
step:116/1750 train_time:10810ms step_avg:93.19ms
step:117/1750 train_time:10904ms step_avg:93.19ms
step:118/1750 train_time:10997ms step_avg:93.20ms
step:119/1750 train_time:11091ms step_avg:93.20ms
step:120/1750 train_time:11184ms step_avg:93.20ms
step:121/1750 train_time:11277ms step_avg:93.20ms
step:122/1750 train_time:11371ms step_avg:93.20ms
step:123/1750 train_time:11464ms step_avg:93.21ms
step:124/1750 train_time:11558ms step_avg:93.21ms
step:125/1750 train_time:11651ms step_avg:93.21ms
step:125/1750 val_loss:4.6660 train_time:11735ms step_avg:93.88ms
step:126/1750 train_time:11757ms step_avg:93.31ms
step:127/1750 train_time:11844ms step_avg:93.26ms
step:128/1750 train_time:11944ms step_avg:93.31ms
step:129/1750 train_time:12040ms step_avg:93.33ms
step:130/1750 train_time:12134ms step_avg:93.34ms
step:131/1750 train_time:12226ms step_avg:93.33ms
step:132/1750 train_time:12320ms step_avg:93.33ms
step:133/1750 train_time:12412ms step_avg:93.33ms
step:134/1750 train_time:12505ms step_avg:93.32ms
step:135/1750 train_time:12598ms step_avg:93.32ms
step:136/1750 train_time:12693ms step_avg:93.33ms
step:137/1750 train_time:12787ms step_avg:93.34ms
step:138/1750 train_time:12882ms step_avg:93.35ms
step:139/1750 train_time:12978ms step_avg:93.37ms
step:140/1750 train_time:13074ms step_avg:93.38ms
step:141/1750 train_time:13168ms step_avg:93.39ms
step:142/1750 train_time:13262ms step_avg:93.39ms
step:143/1750 train_time:13356ms step_avg:93.40ms
step:144/1750 train_time:13448ms step_avg:93.39ms
step:145/1750 train_time:13541ms step_avg:93.39ms
step:146/1750 train_time:13635ms step_avg:93.39ms
step:147/1750 train_time:13728ms step_avg:93.39ms
step:148/1750 train_time:13823ms step_avg:93.40ms
step:149/1750 train_time:13918ms step_avg:93.41ms
step:150/1750 train_time:14012ms step_avg:93.41ms
step:151/1750 train_time:14106ms step_avg:93.42ms
step:152/1750 train_time:14201ms step_avg:93.42ms
step:153/1750 train_time:14295ms step_avg:93.43ms
step:154/1750 train_time:14388ms step_avg:93.43ms
step:155/1750 train_time:14481ms step_avg:93.43ms
step:156/1750 train_time:14574ms step_avg:93.43ms
step:157/1750 train_time:14667ms step_avg:93.42ms
step:158/1750 train_time:14761ms step_avg:93.42ms
step:159/1750 train_time:14855ms step_avg:93.43ms
step:160/1750 train_time:14950ms step_avg:93.44ms
step:161/1750 train_time:15045ms step_avg:93.45ms
step:162/1750 train_time:15139ms step_avg:93.45ms
step:163/1750 train_time:15233ms step_avg:93.46ms
step:164/1750 train_time:15327ms step_avg:93.45ms
step:165/1750 train_time:15420ms step_avg:93.45ms
step:166/1750 train_time:15514ms step_avg:93.46ms
step:167/1750 train_time:15607ms step_avg:93.46ms
step:168/1750 train_time:15701ms step_avg:93.46ms
step:169/1750 train_time:15795ms step_avg:93.46ms
step:170/1750 train_time:15889ms step_avg:93.46ms
step:171/1750 train_time:15983ms step_avg:93.47ms
step:172/1750 train_time:16077ms step_avg:93.47ms
step:173/1750 train_time:16170ms step_avg:93.47ms
step:174/1750 train_time:16263ms step_avg:93.47ms
step:175/1750 train_time:16358ms step_avg:93.47ms
step:176/1750 train_time:16451ms step_avg:93.47ms
step:177/1750 train_time:16544ms step_avg:93.47ms
step:178/1750 train_time:16638ms step_avg:93.47ms
step:179/1750 train_time:16732ms step_avg:93.47ms
step:180/1750 train_time:16825ms step_avg:93.47ms
step:181/1750 train_time:16919ms step_avg:93.48ms
step:182/1750 train_time:17014ms step_avg:93.48ms
step:183/1750 train_time:17108ms step_avg:93.48ms
step:184/1750 train_time:17201ms step_avg:93.49ms
step:185/1750 train_time:17295ms step_avg:93.49ms
step:186/1750 train_time:17389ms step_avg:93.49ms
step:187/1750 train_time:17482ms step_avg:93.49ms
step:188/1750 train_time:17576ms step_avg:93.49ms
step:189/1750 train_time:17670ms step_avg:93.49ms
step:190/1750 train_time:17764ms step_avg:93.49ms
step:191/1750 train_time:17858ms step_avg:93.50ms
step:192/1750 train_time:17952ms step_avg:93.50ms
step:193/1750 train_time:18045ms step_avg:93.50ms
step:194/1750 train_time:18139ms step_avg:93.50ms
step:195/1750 train_time:18233ms step_avg:93.50ms
step:196/1750 train_time:18327ms step_avg:93.51ms
step:197/1750 train_time:18421ms step_avg:93.51ms
step:198/1750 train_time:18515ms step_avg:93.51ms
step:199/1750 train_time:18608ms step_avg:93.51ms
step:200/1750 train_time:18702ms step_avg:93.51ms
step:201/1750 train_time:18796ms step_avg:93.51ms
step:202/1750 train_time:18890ms step_avg:93.51ms
step:203/1750 train_time:18983ms step_avg:93.51ms
step:204/1750 train_time:19076ms step_avg:93.51ms
step:205/1750 train_time:19169ms step_avg:93.51ms
step:206/1750 train_time:19263ms step_avg:93.51ms
step:207/1750 train_time:19357ms step_avg:93.51ms
step:208/1750 train_time:19452ms step_avg:93.52ms
step:209/1750 train_time:19546ms step_avg:93.52ms
step:210/1750 train_time:19639ms step_avg:93.52ms
step:211/1750 train_time:19734ms step_avg:93.53ms
step:212/1750 train_time:19828ms step_avg:93.53ms
step:213/1750 train_time:19922ms step_avg:93.53ms
step:214/1750 train_time:20016ms step_avg:93.53ms
step:215/1750 train_time:20109ms step_avg:93.53ms
step:216/1750 train_time:20202ms step_avg:93.53ms
step:217/1750 train_time:20296ms step_avg:93.53ms
step:218/1750 train_time:20390ms step_avg:93.53ms
step:219/1750 train_time:20484ms step_avg:93.53ms
step:220/1750 train_time:20577ms step_avg:93.53ms
step:221/1750 train_time:20670ms step_avg:93.53ms
step:222/1750 train_time:20764ms step_avg:93.53ms
step:223/1750 train_time:20859ms step_avg:93.54ms
step:224/1750 train_time:20954ms step_avg:93.54ms
step:225/1750 train_time:21047ms step_avg:93.54ms
step:226/1750 train_time:21140ms step_avg:93.54ms
step:227/1750 train_time:21234ms step_avg:93.54ms
step:228/1750 train_time:21328ms step_avg:93.55ms
step:229/1750 train_time:21422ms step_avg:93.55ms
step:230/1750 train_time:21516ms step_avg:93.55ms
step:231/1750 train_time:21609ms step_avg:93.55ms
step:232/1750 train_time:21703ms step_avg:93.55ms
step:233/1750 train_time:21797ms step_avg:93.55ms
step:234/1750 train_time:21890ms step_avg:93.55ms
step:235/1750 train_time:21983ms step_avg:93.55ms
step:236/1750 train_time:22077ms step_avg:93.55ms
step:237/1750 train_time:22171ms step_avg:93.55ms
step:238/1750 train_time:22265ms step_avg:93.55ms
step:239/1750 train_time:22358ms step_avg:93.55ms
step:240/1750 train_time:22452ms step_avg:93.55ms
step:241/1750 train_time:22546ms step_avg:93.55ms
step:242/1750 train_time:22640ms step_avg:93.55ms
step:243/1750 train_time:22732ms step_avg:93.55ms
step:244/1750 train_time:22826ms step_avg:93.55ms
step:245/1750 train_time:22920ms step_avg:93.55ms
step:246/1750 train_time:23013ms step_avg:93.55ms
step:247/1750 train_time:23107ms step_avg:93.55ms
step:248/1750 train_time:23201ms step_avg:93.55ms
step:249/1750 train_time:23295ms step_avg:93.55ms
step:250/1750 train_time:23389ms step_avg:93.56ms
step:250/1750 val_loss:4.1078 train_time:23471ms step_avg:93.89ms
step:251/1750 train_time:23493ms step_avg:93.60ms
step:252/1750 train_time:23582ms step_avg:93.58ms
step:253/1750 train_time:23678ms step_avg:93.59ms
step:254/1750 train_time:23773ms step_avg:93.59ms
step:255/1750 train_time:23866ms step_avg:93.59ms
step:256/1750 train_time:23959ms step_avg:93.59ms
step:257/1750 train_time:24051ms step_avg:93.58ms
step:258/1750 train_time:24144ms step_avg:93.58ms
step:259/1750 train_time:24237ms step_avg:93.58ms
step:260/1750 train_time:24329ms step_avg:93.57ms
step:261/1750 train_time:24424ms step_avg:93.58ms
step:262/1750 train_time:24520ms step_avg:93.59ms
step:263/1750 train_time:24616ms step_avg:93.60ms
step:264/1750 train_time:24710ms step_avg:93.60ms
step:265/1750 train_time:24804ms step_avg:93.60ms
step:266/1750 train_time:24897ms step_avg:93.60ms
step:267/1750 train_time:24991ms step_avg:93.60ms
step:268/1750 train_time:25085ms step_avg:93.60ms
step:269/1750 train_time:25178ms step_avg:93.60ms
step:270/1750 train_time:25271ms step_avg:93.60ms
step:271/1750 train_time:25365ms step_avg:93.60ms
step:272/1750 train_time:25460ms step_avg:93.60ms
step:273/1750 train_time:25555ms step_avg:93.61ms
step:274/1750 train_time:25650ms step_avg:93.61ms
step:275/1750 train_time:25745ms step_avg:93.62ms
step:276/1750 train_time:25838ms step_avg:93.62ms
step:277/1750 train_time:25933ms step_avg:93.62ms
step:278/1750 train_time:26026ms step_avg:93.62ms
step:279/1750 train_time:26120ms step_avg:93.62ms
step:280/1750 train_time:26214ms step_avg:93.62ms
step:281/1750 train_time:26308ms step_avg:93.62ms
step:282/1750 train_time:26402ms step_avg:93.62ms
step:283/1750 train_time:26496ms step_avg:93.63ms
step:284/1750 train_time:26591ms step_avg:93.63ms
step:285/1750 train_time:26685ms step_avg:93.63ms
step:286/1750 train_time:26779ms step_avg:93.63ms
step:287/1750 train_time:26873ms step_avg:93.64ms
step:288/1750 train_time:26968ms step_avg:93.64ms
step:289/1750 train_time:27062ms step_avg:93.64ms
step:290/1750 train_time:27156ms step_avg:93.64ms
step:291/1750 train_time:27250ms step_avg:93.64ms
step:292/1750 train_time:27344ms step_avg:93.64ms
step:293/1750 train_time:27438ms step_avg:93.65ms
step:294/1750 train_time:27533ms step_avg:93.65ms
step:295/1750 train_time:27628ms step_avg:93.65ms
step:296/1750 train_time:27721ms step_avg:93.65ms
step:297/1750 train_time:27815ms step_avg:93.65ms
step:298/1750 train_time:27910ms step_avg:93.66ms
step:299/1750 train_time:28004ms step_avg:93.66ms
step:300/1750 train_time:28097ms step_avg:93.66ms
step:301/1750 train_time:28191ms step_avg:93.66ms
step:302/1750 train_time:28285ms step_avg:93.66ms
step:303/1750 train_time:28379ms step_avg:93.66ms
step:304/1750 train_time:28473ms step_avg:93.66ms
step:305/1750 train_time:28567ms step_avg:93.66ms
step:306/1750 train_time:28662ms step_avg:93.67ms
step:307/1750 train_time:28757ms step_avg:93.67ms
step:308/1750 train_time:28851ms step_avg:93.67ms
step:309/1750 train_time:28945ms step_avg:93.67ms
step:310/1750 train_time:29039ms step_avg:93.67ms
step:311/1750 train_time:29133ms step_avg:93.67ms
step:312/1750 train_time:29227ms step_avg:93.68ms
step:313/1750 train_time:29321ms step_avg:93.68ms
step:314/1750 train_time:29414ms step_avg:93.68ms
step:315/1750 train_time:29509ms step_avg:93.68ms
step:316/1750 train_time:29603ms step_avg:93.68ms
step:317/1750 train_time:29697ms step_avg:93.68ms
step:318/1750 train_time:29792ms step_avg:93.68ms
step:319/1750 train_time:29887ms step_avg:93.69ms
step:320/1750 train_time:29981ms step_avg:93.69ms
step:321/1750 train_time:30075ms step_avg:93.69ms
step:322/1750 train_time:30169ms step_avg:93.69ms
step:323/1750 train_time:30263ms step_avg:93.69ms
step:324/1750 train_time:30358ms step_avg:93.70ms
step:325/1750 train_time:30452ms step_avg:93.70ms
step:326/1750 train_time:30547ms step_avg:93.70ms
step:327/1750 train_time:30984ms step_avg:94.75ms
step:328/1750 train_time:31041ms step_avg:94.64ms
step:329/1750 train_time:31134ms step_avg:94.63ms
step:330/1750 train_time:31227ms step_avg:94.63ms
step:331/1750 train_time:31320ms step_avg:94.62ms
step:332/1750 train_time:31413ms step_avg:94.62ms
step:333/1750 train_time:31506ms step_avg:94.61ms
step:334/1750 train_time:31599ms step_avg:94.61ms
step:335/1750 train_time:31693ms step_avg:94.60ms
step:336/1750 train_time:31786ms step_avg:94.60ms
step:337/1750 train_time:31879ms step_avg:94.60ms
step:338/1750 train_time:31977ms step_avg:94.61ms
step:339/1750 train_time:32075ms step_avg:94.62ms
step:340/1750 train_time:32171ms step_avg:94.62ms
step:341/1750 train_time:32265ms step_avg:94.62ms
step:342/1750 train_time:32359ms step_avg:94.62ms
step:343/1750 train_time:32453ms step_avg:94.61ms
step:344/1750 train_time:32546ms step_avg:94.61ms
step:345/1750 train_time:32639ms step_avg:94.61ms
step:346/1750 train_time:32732ms step_avg:94.60ms
step:347/1750 train_time:32826ms step_avg:94.60ms
step:348/1750 train_time:32922ms step_avg:94.60ms
step:349/1750 train_time:33018ms step_avg:94.61ms
step:350/1750 train_time:33114ms step_avg:94.61ms
step:351/1750 train_time:33209ms step_avg:94.61ms
step:352/1750 train_time:33304ms step_avg:94.61ms
step:353/1750 train_time:33397ms step_avg:94.61ms
step:354/1750 train_time:33491ms step_avg:94.61ms
step:355/1750 train_time:33586ms step_avg:94.61ms
step:356/1750 train_time:33679ms step_avg:94.60ms
step:357/1750 train_time:33773ms step_avg:94.60ms
step:358/1750 train_time:33867ms step_avg:94.60ms
step:359/1750 train_time:33962ms step_avg:94.60ms
step:360/1750 train_time:34057ms step_avg:94.60ms
step:361/1750 train_time:34152ms step_avg:94.60ms
step:362/1750 train_time:34246ms step_avg:94.60ms
step:363/1750 train_time:34340ms step_avg:94.60ms
step:364/1750 train_time:34435ms step_avg:94.60ms
step:365/1750 train_time:34529ms step_avg:94.60ms
step:366/1750 train_time:34623ms step_avg:94.60ms
step:367/1750 train_time:34717ms step_avg:94.60ms
step:368/1750 train_time:34812ms step_avg:94.60ms
step:369/1750 train_time:34906ms step_avg:94.59ms
step:370/1750 train_time:35000ms step_avg:94.59ms
step:371/1750 train_time:35095ms step_avg:94.60ms
step:372/1750 train_time:35190ms step_avg:94.60ms
step:373/1750 train_time:35284ms step_avg:94.60ms
step:374/1750 train_time:35378ms step_avg:94.59ms
step:375/1750 train_time:35473ms step_avg:94.60ms
step:375/1750 val_loss:3.9069 train_time:35558ms step_avg:94.82ms
step:376/1750 train_time:35581ms step_avg:94.63ms
step:377/1750 train_time:35671ms step_avg:94.62ms
step:378/1750 train_time:35773ms step_avg:94.64ms
step:379/1750 train_time:35868ms step_avg:94.64ms
step:380/1750 train_time:35962ms step_avg:94.64ms
step:381/1750 train_time:36055ms step_avg:94.63ms
step:382/1750 train_time:36148ms step_avg:94.63ms
step:383/1750 train_time:36241ms step_avg:94.62ms
step:384/1750 train_time:36335ms step_avg:94.62ms
step:385/1750 train_time:36429ms step_avg:94.62ms
step:386/1750 train_time:36523ms step_avg:94.62ms
step:387/1750 train_time:36620ms step_avg:94.62ms
step:388/1750 train_time:36716ms step_avg:94.63ms
step:389/1750 train_time:36810ms step_avg:94.63ms
step:390/1750 train_time:36905ms step_avg:94.63ms
step:391/1750 train_time:37001ms step_avg:94.63ms
step:392/1750 train_time:37096ms step_avg:94.63ms
step:393/1750 train_time:37192ms step_avg:94.64ms
step:394/1750 train_time:37288ms step_avg:94.64ms
step:395/1750 train_time:37383ms step_avg:94.64ms
step:396/1750 train_time:37479ms step_avg:94.64ms
step:397/1750 train_time:37575ms step_avg:94.65ms
step:398/1750 train_time:37672ms step_avg:94.65ms
step:399/1750 train_time:37769ms step_avg:94.66ms
step:400/1750 train_time:37867ms step_avg:94.67ms
step:401/1750 train_time:37963ms step_avg:94.67ms
step:402/1750 train_time:38060ms step_avg:94.68ms
step:403/1750 train_time:38154ms step_avg:94.68ms
step:404/1750 train_time:38250ms step_avg:94.68ms
step:405/1750 train_time:38346ms step_avg:94.68ms
step:406/1750 train_time:38441ms step_avg:94.68ms
step:407/1750 train_time:38537ms step_avg:94.69ms
step:408/1750 train_time:38633ms step_avg:94.69ms
step:409/1750 train_time:38730ms step_avg:94.69ms
step:410/1750 train_time:38826ms step_avg:94.70ms
step:411/1750 train_time:38922ms step_avg:94.70ms
step:412/1750 train_time:39019ms step_avg:94.71ms
step:413/1750 train_time:39114ms step_avg:94.71ms
step:414/1750 train_time:39210ms step_avg:94.71ms
step:415/1750 train_time:39306ms step_avg:94.71ms
step:416/1750 train_time:39401ms step_avg:94.71ms
step:417/1750 train_time:39497ms step_avg:94.72ms
step:418/1750 train_time:39593ms step_avg:94.72ms
step:419/1750 train_time:39689ms step_avg:94.72ms
step:420/1750 train_time:39785ms step_avg:94.73ms
step:421/1750 train_time:39882ms step_avg:94.73ms
step:422/1750 train_time:39978ms step_avg:94.73ms
step:423/1750 train_time:40074ms step_avg:94.74ms
step:424/1750 train_time:40170ms step_avg:94.74ms
step:425/1750 train_time:40266ms step_avg:94.74ms
step:426/1750 train_time:40362ms step_avg:94.75ms
step:427/1750 train_time:40458ms step_avg:94.75ms
step:428/1750 train_time:40554ms step_avg:94.75ms
step:429/1750 train_time:40650ms step_avg:94.75ms
step:430/1750 train_time:40745ms step_avg:94.76ms
step:431/1750 train_time:40842ms step_avg:94.76ms
step:432/1750 train_time:40938ms step_avg:94.76ms
step:433/1750 train_time:41034ms step_avg:94.77ms
step:434/1750 train_time:41130ms step_avg:94.77ms
step:435/1750 train_time:41226ms step_avg:94.77ms
step:436/1750 train_time:41323ms step_avg:94.78ms
step:437/1750 train_time:41419ms step_avg:94.78ms
step:438/1750 train_time:41514ms step_avg:94.78ms
step:439/1750 train_time:41611ms step_avg:94.79ms
step:440/1750 train_time:41708ms step_avg:94.79ms
step:441/1750 train_time:41804ms step_avg:94.79ms
step:442/1750 train_time:41900ms step_avg:94.80ms
step:443/1750 train_time:41996ms step_avg:94.80ms
step:444/1750 train_time:42093ms step_avg:94.80ms
step:445/1750 train_time:42189ms step_avg:94.81ms
step:446/1750 train_time:42285ms step_avg:94.81ms
step:447/1750 train_time:42381ms step_avg:94.81ms
step:448/1750 train_time:42477ms step_avg:94.81ms
step:449/1750 train_time:42573ms step_avg:94.82ms
step:450/1750 train_time:42669ms step_avg:94.82ms
step:451/1750 train_time:42765ms step_avg:94.82ms
step:452/1750 train_time:42860ms step_avg:94.82ms
step:453/1750 train_time:42956ms step_avg:94.83ms
step:454/1750 train_time:43052ms step_avg:94.83ms
step:455/1750 train_time:43149ms step_avg:94.83ms
step:456/1750 train_time:43245ms step_avg:94.84ms
step:457/1750 train_time:43341ms step_avg:94.84ms
step:458/1750 train_time:43437ms step_avg:94.84ms
step:459/1750 train_time:43534ms step_avg:94.84ms
step:460/1750 train_time:43630ms step_avg:94.85ms
step:461/1750 train_time:43725ms step_avg:94.85ms
step:462/1750 train_time:43821ms step_avg:94.85ms
step:463/1750 train_time:43917ms step_avg:94.85ms
step:464/1750 train_time:44013ms step_avg:94.85ms
step:465/1750 train_time:44109ms step_avg:94.86ms
step:466/1750 train_time:44205ms step_avg:94.86ms
step:467/1750 train_time:44301ms step_avg:94.86ms
step:468/1750 train_time:44397ms step_avg:94.87ms
step:469/1750 train_time:44493ms step_avg:94.87ms
step:470/1750 train_time:44589ms step_avg:94.87ms
step:471/1750 train_time:44685ms step_avg:94.87ms
step:472/1750 train_time:44782ms step_avg:94.88ms
step:473/1750 train_time:44878ms step_avg:94.88ms
step:474/1750 train_time:44974ms step_avg:94.88ms
step:475/1750 train_time:45070ms step_avg:94.88ms
step:476/1750 train_time:45166ms step_avg:94.89ms
step:477/1750 train_time:45262ms step_avg:94.89ms
step:478/1750 train_time:45358ms step_avg:94.89ms
step:479/1750 train_time:45454ms step_avg:94.89ms
step:480/1750 train_time:45550ms step_avg:94.90ms
step:481/1750 train_time:45646ms step_avg:94.90ms
step:482/1750 train_time:45742ms step_avg:94.90ms
step:483/1750 train_time:45838ms step_avg:94.90ms
step:484/1750 train_time:45934ms step_avg:94.90ms
step:485/1750 train_time:46030ms step_avg:94.91ms
step:486/1750 train_time:46126ms step_avg:94.91ms
step:487/1750 train_time:46222ms step_avg:94.91ms
step:488/1750 train_time:46318ms step_avg:94.91ms
step:489/1750 train_time:46414ms step_avg:94.92ms
step:490/1750 train_time:46510ms step_avg:94.92ms
step:491/1750 train_time:46607ms step_avg:94.92ms
step:492/1750 train_time:46703ms step_avg:94.92ms
step:493/1750 train_time:46799ms step_avg:94.93ms
step:494/1750 train_time:46894ms step_avg:94.93ms
step:495/1750 train_time:46991ms step_avg:94.93ms
step:496/1750 train_time:47087ms step_avg:94.93ms
step:497/1750 train_time:47183ms step_avg:94.94ms
step:498/1750 train_time:47279ms step_avg:94.94ms
step:499/1750 train_time:47374ms step_avg:94.94ms
step:500/1750 train_time:47471ms step_avg:94.94ms
step:500/1750 val_loss:3.7575 train_time:47556ms step_avg:95.11ms
step:501/1750 train_time:47578ms step_avg:94.97ms
step:502/1750 train_time:47671ms step_avg:94.96ms
step:503/1750 train_time:47771ms step_avg:94.97ms
step:504/1750 train_time:47867ms step_avg:94.98ms
step:505/1750 train_time:47963ms step_avg:94.98ms
step:506/1750 train_time:48059ms step_avg:94.98ms
step:507/1750 train_time:48154ms step_avg:94.98ms
step:508/1750 train_time:48249ms step_avg:94.98ms
step:509/1750 train_time:48343ms step_avg:94.98ms
step:510/1750 train_time:48440ms step_avg:94.98ms
step:511/1750 train_time:48535ms step_avg:94.98ms
step:512/1750 train_time:48634ms step_avg:94.99ms
step:513/1750 train_time:48732ms step_avg:94.99ms
step:514/1750 train_time:48829ms step_avg:95.00ms
step:515/1750 train_time:48925ms step_avg:95.00ms
step:516/1750 train_time:49022ms step_avg:95.00ms
step:517/1750 train_time:49117ms step_avg:95.00ms
step:518/1750 train_time:49213ms step_avg:95.01ms
step:519/1750 train_time:49309ms step_avg:95.01ms
step:520/1750 train_time:49404ms step_avg:95.01ms
step:521/1750 train_time:49500ms step_avg:95.01ms
step:522/1750 train_time:49597ms step_avg:95.01ms
step:523/1750 train_time:49694ms step_avg:95.02ms
step:524/1750 train_time:49792ms step_avg:95.02ms
step:525/1750 train_time:49888ms step_avg:95.03ms
step:526/1750 train_time:49985ms step_avg:95.03ms
step:527/1750 train_time:50081ms step_avg:95.03ms
step:528/1750 train_time:50177ms step_avg:95.03ms
step:529/1750 train_time:50273ms step_avg:95.03ms
step:530/1750 train_time:50369ms step_avg:95.04ms
step:531/1750 train_time:50465ms step_avg:95.04ms
step:532/1750 train_time:50561ms step_avg:95.04ms
step:533/1750 train_time:50658ms step_avg:95.04ms
step:534/1750 train_time:50755ms step_avg:95.05ms
step:535/1750 train_time:50852ms step_avg:95.05ms
step:536/1750 train_time:50950ms step_avg:95.06ms
step:537/1750 train_time:51046ms step_avg:95.06ms
step:538/1750 train_time:51142ms step_avg:95.06ms
step:539/1750 train_time:51240ms step_avg:95.06ms
step:540/1750 train_time:51336ms step_avg:95.07ms
step:541/1750 train_time:51432ms step_avg:95.07ms
step:542/1750 train_time:51529ms step_avg:95.07ms
step:543/1750 train_time:51625ms step_avg:95.07ms
step:544/1750 train_time:51723ms step_avg:95.08ms
step:545/1750 train_time:51820ms step_avg:95.08ms
step:546/1750 train_time:51916ms step_avg:95.09ms
step:547/1750 train_time:52013ms step_avg:95.09ms
step:548/1750 train_time:52110ms step_avg:95.09ms
step:549/1750 train_time:52207ms step_avg:95.09ms
step:550/1750 train_time:52303ms step_avg:95.10ms
step:551/1750 train_time:52401ms step_avg:95.10ms
step:552/1750 train_time:52497ms step_avg:95.10ms
step:553/1750 train_time:52594ms step_avg:95.11ms
step:554/1750 train_time:52690ms step_avg:95.11ms
step:555/1750 train_time:52786ms step_avg:95.11ms
step:556/1750 train_time:52883ms step_avg:95.11ms
step:557/1750 train_time:52981ms step_avg:95.12ms
step:558/1750 train_time:53078ms step_avg:95.12ms
step:559/1750 train_time:53175ms step_avg:95.12ms
step:560/1750 train_time:53271ms step_avg:95.13ms
step:561/1750 train_time:53366ms step_avg:95.13ms
step:562/1750 train_time:53463ms step_avg:95.13ms
step:563/1750 train_time:53560ms step_avg:95.13ms
step:564/1750 train_time:53656ms step_avg:95.14ms
step:565/1750 train_time:53753ms step_avg:95.14ms
step:566/1750 train_time:53849ms step_avg:95.14ms
step:567/1750 train_time:53946ms step_avg:95.14ms
step:568/1750 train_time:54043ms step_avg:95.15ms
step:569/1750 train_time:54140ms step_avg:95.15ms
step:570/1750 train_time:54236ms step_avg:95.15ms
step:571/1750 train_time:54332ms step_avg:95.15ms
step:572/1750 train_time:54429ms step_avg:95.16ms
step:573/1750 train_time:54525ms step_avg:95.16ms
step:574/1750 train_time:54622ms step_avg:95.16ms
step:575/1750 train_time:54718ms step_avg:95.16ms
step:576/1750 train_time:54815ms step_avg:95.16ms
step:577/1750 train_time:54911ms step_avg:95.17ms
step:578/1750 train_time:55008ms step_avg:95.17ms
step:579/1750 train_time:55105ms step_avg:95.17ms
step:580/1750 train_time:55202ms step_avg:95.18ms
step:581/1750 train_time:55299ms step_avg:95.18ms
step:582/1750 train_time:55396ms step_avg:95.18ms
step:583/1750 train_time:55492ms step_avg:95.18ms
step:584/1750 train_time:55589ms step_avg:95.19ms
step:585/1750 train_time:55684ms step_avg:95.19ms
step:586/1750 train_time:55781ms step_avg:95.19ms
step:587/1750 train_time:55878ms step_avg:95.19ms
step:588/1750 train_time:55975ms step_avg:95.20ms
step:589/1750 train_time:56073ms step_avg:95.20ms
step:590/1750 train_time:56170ms step_avg:95.20ms
step:591/1750 train_time:56266ms step_avg:95.20ms
step:592/1750 train_time:56362ms step_avg:95.21ms
step:593/1750 train_time:56459ms step_avg:95.21ms
step:594/1750 train_time:56555ms step_avg:95.21ms
step:595/1750 train_time:56651ms step_avg:95.21ms
step:596/1750 train_time:56747ms step_avg:95.21ms
step:597/1750 train_time:56843ms step_avg:95.21ms
step:598/1750 train_time:56940ms step_avg:95.22ms
step:599/1750 train_time:57038ms step_avg:95.22ms
step:600/1750 train_time:57135ms step_avg:95.22ms
step:601/1750 train_time:57232ms step_avg:95.23ms
step:602/1750 train_time:57327ms step_avg:95.23ms
step:603/1750 train_time:57423ms step_avg:95.23ms
step:604/1750 train_time:57520ms step_avg:95.23ms
step:605/1750 train_time:57616ms step_avg:95.23ms
step:606/1750 train_time:57713ms step_avg:95.24ms
step:607/1750 train_time:57809ms step_avg:95.24ms
step:608/1750 train_time:57906ms step_avg:95.24ms
step:609/1750 train_time:58002ms step_avg:95.24ms
step:610/1750 train_time:58098ms step_avg:95.24ms
step:611/1750 train_time:58195ms step_avg:95.25ms
step:612/1750 train_time:58291ms step_avg:95.25ms
step:613/1750 train_time:58387ms step_avg:95.25ms
step:614/1750 train_time:58484ms step_avg:95.25ms
step:615/1750 train_time:58580ms step_avg:95.25ms
step:616/1750 train_time:58677ms step_avg:95.25ms
step:617/1750 train_time:58773ms step_avg:95.26ms
step:618/1750 train_time:58869ms step_avg:95.26ms
step:619/1750 train_time:58965ms step_avg:95.26ms
step:620/1750 train_time:59062ms step_avg:95.26ms
step:621/1750 train_time:59159ms step_avg:95.26ms
step:622/1750 train_time:59255ms step_avg:95.27ms
step:623/1750 train_time:59351ms step_avg:95.27ms
step:624/1750 train_time:59447ms step_avg:95.27ms
step:625/1750 train_time:59545ms step_avg:95.27ms
step:625/1750 val_loss:3.6701 train_time:59631ms step_avg:95.41ms
step:626/1750 train_time:59652ms step_avg:95.29ms
step:627/1750 train_time:59748ms step_avg:95.29ms
step:628/1750 train_time:59847ms step_avg:95.30ms
step:629/1750 train_time:59943ms step_avg:95.30ms
step:630/1750 train_time:60038ms step_avg:95.30ms
step:631/1750 train_time:60134ms step_avg:95.30ms
step:632/1750 train_time:60229ms step_avg:95.30ms
step:633/1750 train_time:60324ms step_avg:95.30ms
step:634/1750 train_time:60420ms step_avg:95.30ms
step:635/1750 train_time:60516ms step_avg:95.30ms
step:636/1750 train_time:60614ms step_avg:95.30ms
step:637/1750 train_time:60712ms step_avg:95.31ms
step:638/1750 train_time:60809ms step_avg:95.31ms
step:639/1750 train_time:60906ms step_avg:95.32ms
step:640/1750 train_time:61002ms step_avg:95.32ms
step:641/1750 train_time:61098ms step_avg:95.32ms
step:642/1750 train_time:61194ms step_avg:95.32ms
step:643/1750 train_time:61290ms step_avg:95.32ms
step:644/1750 train_time:61386ms step_avg:95.32ms
step:645/1750 train_time:61481ms step_avg:95.32ms
step:646/1750 train_time:61578ms step_avg:95.32ms
step:647/1750 train_time:61675ms step_avg:95.32ms
step:648/1750 train_time:61773ms step_avg:95.33ms
step:649/1750 train_time:61869ms step_avg:95.33ms
step:650/1750 train_time:61966ms step_avg:95.33ms
step:651/1750 train_time:62064ms step_avg:95.34ms
step:652/1750 train_time:62161ms step_avg:95.34ms
step:653/1750 train_time:62259ms step_avg:95.34ms
step:654/1750 train_time:62357ms step_avg:95.35ms
step:655/1750 train_time:62455ms step_avg:95.35ms
step:656/1750 train_time:62552ms step_avg:95.35ms
step:657/1750 train_time:62650ms step_avg:95.36ms
step:658/1750 train_time:62748ms step_avg:95.36ms
step:659/1750 train_time:62847ms step_avg:95.37ms
step:660/1750 train_time:62944ms step_avg:95.37ms
step:661/1750 train_time:63042ms step_avg:95.37ms
step:662/1750 train_time:63140ms step_avg:95.38ms
step:663/1750 train_time:63237ms step_avg:95.38ms
step:664/1750 train_time:63335ms step_avg:95.38ms
step:665/1750 train_time:63433ms step_avg:95.39ms
step:666/1750 train_time:63530ms step_avg:95.39ms
step:667/1750 train_time:63627ms step_avg:95.39ms
step:668/1750 train_time:63725ms step_avg:95.40ms
step:669/1750 train_time:63823ms step_avg:95.40ms
step:670/1750 train_time:63921ms step_avg:95.41ms
step:671/1750 train_time:64019ms step_avg:95.41ms
step:672/1750 train_time:64118ms step_avg:95.41ms
step:673/1750 train_time:64214ms step_avg:95.42ms
step:674/1750 train_time:64312ms step_avg:95.42ms
step:675/1750 train_time:64410ms step_avg:95.42ms
step:676/1750 train_time:64508ms step_avg:95.43ms
step:677/1750 train_time:64605ms step_avg:95.43ms
step:678/1750 train_time:64703ms step_avg:95.43ms
step:679/1750 train_time:64801ms step_avg:95.44ms
step:680/1750 train_time:64900ms step_avg:95.44ms
step:681/1750 train_time:64998ms step_avg:95.44ms
step:682/1750 train_time:65096ms step_avg:95.45ms
step:683/1750 train_time:65193ms step_avg:95.45ms
step:684/1750 train_time:65291ms step_avg:95.45ms
step:685/1750 train_time:65388ms step_avg:95.46ms
step:686/1750 train_time:65486ms step_avg:95.46ms
step:687/1750 train_time:65584ms step_avg:95.46ms
step:688/1750 train_time:65681ms step_avg:95.47ms
step:689/1750 train_time:65778ms step_avg:95.47ms
step:690/1750 train_time:65877ms step_avg:95.47ms
step:691/1750 train_time:65975ms step_avg:95.48ms
step:692/1750 train_time:66071ms step_avg:95.48ms
step:693/1750 train_time:66169ms step_avg:95.48ms
step:694/1750 train_time:66267ms step_avg:95.49ms
step:695/1750 train_time:66364ms step_avg:95.49ms
step:696/1750 train_time:66463ms step_avg:95.49ms
step:697/1750 train_time:66562ms step_avg:95.50ms
step:698/1750 train_time:66660ms step_avg:95.50ms
step:699/1750 train_time:66758ms step_avg:95.51ms
step:700/1750 train_time:66857ms step_avg:95.51ms
step:701/1750 train_time:66956ms step_avg:95.52ms
step:702/1750 train_time:67054ms step_avg:95.52ms
step:703/1750 train_time:67152ms step_avg:95.52ms
step:704/1750 train_time:67249ms step_avg:95.52ms
step:705/1750 train_time:67348ms step_avg:95.53ms
step:706/1750 train_time:67446ms step_avg:95.53ms
step:707/1750 train_time:67543ms step_avg:95.53ms
step:708/1750 train_time:67640ms step_avg:95.54ms
step:709/1750 train_time:67738ms step_avg:95.54ms
step:710/1750 train_time:67837ms step_avg:95.55ms
step:711/1750 train_time:67936ms step_avg:95.55ms
step:712/1750 train_time:68034ms step_avg:95.55ms
step:713/1750 train_time:68132ms step_avg:95.56ms
step:714/1750 train_time:68231ms step_avg:95.56ms
step:715/1750 train_time:68330ms step_avg:95.57ms
step:716/1750 train_time:68428ms step_avg:95.57ms
step:717/1750 train_time:68525ms step_avg:95.57ms
step:718/1750 train_time:68623ms step_avg:95.58ms
step:719/1750 train_time:68722ms step_avg:95.58ms
step:720/1750 train_time:68820ms step_avg:95.58ms
step:721/1750 train_time:68918ms step_avg:95.59ms
step:722/1750 train_time:69016ms step_avg:95.59ms
step:723/1750 train_time:69114ms step_avg:95.59ms
step:724/1750 train_time:69211ms step_avg:95.60ms
step:725/1750 train_time:69310ms step_avg:95.60ms
step:726/1750 train_time:69407ms step_avg:95.60ms
step:727/1750 train_time:69504ms step_avg:95.60ms
step:728/1750 train_time:69601ms step_avg:95.61ms
step:729/1750 train_time:69699ms step_avg:95.61ms
step:730/1750 train_time:69796ms step_avg:95.61ms
step:731/1750 train_time:69894ms step_avg:95.61ms
step:732/1750 train_time:69992ms step_avg:95.62ms
step:733/1750 train_time:70090ms step_avg:95.62ms
step:734/1750 train_time:70188ms step_avg:95.62ms
step:735/1750 train_time:70285ms step_avg:95.63ms
step:736/1750 train_time:70383ms step_avg:95.63ms
step:737/1750 train_time:70481ms step_avg:95.63ms
step:738/1750 train_time:70579ms step_avg:95.64ms
step:739/1750 train_time:70678ms step_avg:95.64ms
step:740/1750 train_time:70775ms step_avg:95.64ms
step:741/1750 train_time:70873ms step_avg:95.64ms
step:742/1750 train_time:70970ms step_avg:95.65ms
step:743/1750 train_time:71068ms step_avg:95.65ms
step:744/1750 train_time:71166ms step_avg:95.65ms
step:745/1750 train_time:71263ms step_avg:95.65ms
step:746/1750 train_time:71361ms step_avg:95.66ms
step:747/1750 train_time:71459ms step_avg:95.66ms
step:748/1750 train_time:71557ms step_avg:95.66ms
step:749/1750 train_time:71656ms step_avg:95.67ms
step:750/1750 train_time:71753ms step_avg:95.67ms
step:750/1750 val_loss:3.6025 train_time:71840ms step_avg:95.79ms
step:751/1750 train_time:71862ms step_avg:95.69ms
step:752/1750 train_time:71954ms step_avg:95.68ms
step:753/1750 train_time:72053ms step_avg:95.69ms
step:754/1750 train_time:72150ms step_avg:95.69ms
step:755/1750 train_time:72249ms step_avg:95.69ms
step:756/1750 train_time:72346ms step_avg:95.70ms
step:757/1750 train_time:72443ms step_avg:95.70ms
step:758/1750 train_time:72540ms step_avg:95.70ms
step:759/1750 train_time:72637ms step_avg:95.70ms
step:760/1750 train_time:72735ms step_avg:95.70ms
step:761/1750 train_time:72835ms step_avg:95.71ms
step:762/1750 train_time:72935ms step_avg:95.71ms
step:763/1750 train_time:73033ms step_avg:95.72ms
step:764/1750 train_time:73130ms step_avg:95.72ms
step:765/1750 train_time:73228ms step_avg:95.72ms
step:766/1750 train_time:73325ms step_avg:95.72ms
step:767/1750 train_time:73422ms step_avg:95.73ms
step:768/1750 train_time:73520ms step_avg:95.73ms
step:769/1750 train_time:73618ms step_avg:95.73ms
step:770/1750 train_time:73715ms step_avg:95.73ms
step:771/1750 train_time:73813ms step_avg:95.74ms
step:772/1750 train_time:73911ms step_avg:95.74ms
step:773/1750 train_time:74010ms step_avg:95.74ms
step:774/1750 train_time:74109ms step_avg:95.75ms
step:775/1750 train_time:74207ms step_avg:95.75ms
step:776/1750 train_time:74304ms step_avg:95.75ms
step:777/1750 train_time:74402ms step_avg:95.76ms
step:778/1750 train_time:74499ms step_avg:95.76ms
step:779/1750 train_time:74598ms step_avg:95.76ms
step:780/1750 train_time:74696ms step_avg:95.76ms
step:781/1750 train_time:74795ms step_avg:95.77ms
step:782/1750 train_time:74894ms step_avg:95.77ms
step:783/1750 train_time:74993ms step_avg:95.78ms
step:784/1750 train_time:75091ms step_avg:95.78ms
step:785/1750 train_time:75189ms step_avg:95.78ms
step:786/1750 train_time:75287ms step_avg:95.79ms
step:787/1750 train_time:75384ms step_avg:95.79ms
step:788/1750 train_time:75482ms step_avg:95.79ms
step:789/1750 train_time:75580ms step_avg:95.79ms
step:790/1750 train_time:75678ms step_avg:95.79ms
step:791/1750 train_time:75776ms step_avg:95.80ms
step:792/1750 train_time:75874ms step_avg:95.80ms
step:793/1750 train_time:75972ms step_avg:95.80ms
step:794/1750 train_time:76070ms step_avg:95.81ms
step:795/1750 train_time:76169ms step_avg:95.81ms
step:796/1750 train_time:76267ms step_avg:95.81ms
step:797/1750 train_time:76365ms step_avg:95.82ms
step:798/1750 train_time:76462ms step_avg:95.82ms
step:799/1750 train_time:76560ms step_avg:95.82ms
step:800/1750 train_time:76659ms step_avg:95.82ms
step:801/1750 train_time:76757ms step_avg:95.83ms
step:802/1750 train_time:76855ms step_avg:95.83ms
step:803/1750 train_time:76953ms step_avg:95.83ms
step:804/1750 train_time:77052ms step_avg:95.84ms
step:805/1750 train_time:77151ms step_avg:95.84ms
step:806/1750 train_time:77249ms step_avg:95.84ms
step:807/1750 train_time:77347ms step_avg:95.85ms
step:808/1750 train_time:77445ms step_avg:95.85ms
step:809/1750 train_time:77544ms step_avg:95.85ms
step:810/1750 train_time:77642ms step_avg:95.85ms
step:811/1750 train_time:77740ms step_avg:95.86ms
step:812/1750 train_time:77839ms step_avg:95.86ms
step:813/1750 train_time:77938ms step_avg:95.87ms
step:814/1750 train_time:78037ms step_avg:95.87ms
step:815/1750 train_time:78136ms step_avg:95.87ms
step:816/1750 train_time:78234ms step_avg:95.88ms
step:817/1750 train_time:78334ms step_avg:95.88ms
step:818/1750 train_time:78433ms step_avg:95.88ms
step:819/1750 train_time:78530ms step_avg:95.89ms
step:820/1750 train_time:78629ms step_avg:95.89ms
step:821/1750 train_time:78728ms step_avg:95.89ms
step:822/1750 train_time:78827ms step_avg:95.90ms
step:823/1750 train_time:78926ms step_avg:95.90ms
step:824/1750 train_time:79023ms step_avg:95.90ms
step:825/1750 train_time:79122ms step_avg:95.91ms
step:826/1750 train_time:79222ms step_avg:95.91ms
step:827/1750 train_time:79320ms step_avg:95.91ms
step:828/1750 train_time:79418ms step_avg:95.92ms
step:829/1750 train_time:79517ms step_avg:95.92ms
step:830/1750 train_time:79615ms step_avg:95.92ms
step:831/1750 train_time:79714ms step_avg:95.93ms
step:832/1750 train_time:79812ms step_avg:95.93ms
step:833/1750 train_time:79910ms step_avg:95.93ms
step:834/1750 train_time:80008ms step_avg:95.93ms
step:835/1750 train_time:80106ms step_avg:95.94ms
step:836/1750 train_time:80205ms step_avg:95.94ms
step:837/1750 train_time:80304ms step_avg:95.94ms
step:838/1750 train_time:80402ms step_avg:95.94ms
step:839/1750 train_time:80501ms step_avg:95.95ms
step:840/1750 train_time:80600ms step_avg:95.95ms
step:841/1750 train_time:80700ms step_avg:95.96ms
step:842/1750 train_time:80799ms step_avg:95.96ms
step:843/1750 train_time:80898ms step_avg:95.96ms
step:844/1750 train_time:80997ms step_avg:95.97ms
step:845/1750 train_time:81096ms step_avg:95.97ms
step:846/1750 train_time:81195ms step_avg:95.97ms
step:847/1750 train_time:81292ms step_avg:95.98ms
step:848/1750 train_time:81391ms step_avg:95.98ms
step:849/1750 train_time:81489ms step_avg:95.98ms
step:850/1750 train_time:81587ms step_avg:95.98ms
step:851/1750 train_time:81686ms step_avg:95.99ms
step:852/1750 train_time:81784ms step_avg:95.99ms
step:853/1750 train_time:81882ms step_avg:95.99ms
step:854/1750 train_time:81981ms step_avg:96.00ms
step:855/1750 train_time:82079ms step_avg:96.00ms
step:856/1750 train_time:82177ms step_avg:96.00ms
step:857/1750 train_time:82276ms step_avg:96.00ms
step:858/1750 train_time:82375ms step_avg:96.01ms
step:859/1750 train_time:82473ms step_avg:96.01ms
step:860/1750 train_time:82571ms step_avg:96.01ms
step:861/1750 train_time:82670ms step_avg:96.02ms
step:862/1750 train_time:82768ms step_avg:96.02ms
step:863/1750 train_time:82865ms step_avg:96.02ms
step:864/1750 train_time:82963ms step_avg:96.02ms
step:865/1750 train_time:83061ms step_avg:96.02ms
step:866/1750 train_time:83160ms step_avg:96.03ms
step:867/1750 train_time:83259ms step_avg:96.03ms
step:868/1750 train_time:83358ms step_avg:96.03ms
step:869/1750 train_time:83456ms step_avg:96.04ms
step:870/1750 train_time:83555ms step_avg:96.04ms
step:871/1750 train_time:83653ms step_avg:96.04ms
step:872/1750 train_time:83750ms step_avg:96.04ms
step:873/1750 train_time:83850ms step_avg:96.05ms
step:874/1750 train_time:83949ms step_avg:96.05ms
step:875/1750 train_time:84047ms step_avg:96.05ms
step:875/1750 val_loss:3.5525 train_time:84135ms step_avg:96.15ms
step:876/1750 train_time:84158ms step_avg:96.07ms
step:877/1750 train_time:84250ms step_avg:96.07ms
step:878/1750 train_time:84349ms step_avg:96.07ms
step:879/1750 train_time:84447ms step_avg:96.07ms
step:880/1750 train_time:84545ms step_avg:96.07ms
step:881/1750 train_time:84643ms step_avg:96.08ms
step:882/1750 train_time:84741ms step_avg:96.08ms
step:883/1750 train_time:84839ms step_avg:96.08ms
step:884/1750 train_time:84935ms step_avg:96.08ms
step:885/1750 train_time:85032ms step_avg:96.08ms
step:886/1750 train_time:85132ms step_avg:96.09ms
step:887/1750 train_time:85234ms step_avg:96.09ms
step:888/1750 train_time:85332ms step_avg:96.09ms
step:889/1750 train_time:85431ms step_avg:96.10ms
step:890/1750 train_time:85530ms step_avg:96.10ms
step:891/1750 train_time:85629ms step_avg:96.10ms
step:892/1750 train_time:85727ms step_avg:96.11ms
step:893/1750 train_time:85825ms step_avg:96.11ms
step:894/1750 train_time:85923ms step_avg:96.11ms
step:895/1750 train_time:86021ms step_avg:96.11ms
step:896/1750 train_time:86120ms step_avg:96.12ms
step:897/1750 train_time:86219ms step_avg:96.12ms
step:898/1750 train_time:86319ms step_avg:96.12ms
step:899/1750 train_time:86418ms step_avg:96.13ms
step:900/1750 train_time:86516ms step_avg:96.13ms
step:901/1750 train_time:86613ms step_avg:96.13ms
step:902/1750 train_time:86712ms step_avg:96.13ms
step:903/1750 train_time:86811ms step_avg:96.14ms
step:904/1750 train_time:86910ms step_avg:96.14ms
step:905/1750 train_time:87007ms step_avg:96.14ms
step:906/1750 train_time:87104ms step_avg:96.14ms
step:907/1750 train_time:87203ms step_avg:96.14ms
step:908/1750 train_time:87302ms step_avg:96.15ms
step:909/1750 train_time:87403ms step_avg:96.15ms
step:910/1750 train_time:87503ms step_avg:96.16ms
step:911/1750 train_time:87603ms step_avg:96.16ms
step:912/1750 train_time:87703ms step_avg:96.17ms
step:913/1750 train_time:87803ms step_avg:96.17ms
step:914/1750 train_time:87904ms step_avg:96.17ms
step:915/1750 train_time:88003ms step_avg:96.18ms
step:916/1750 train_time:88102ms step_avg:96.18ms
step:917/1750 train_time:88201ms step_avg:96.18ms
step:918/1750 train_time:88301ms step_avg:96.19ms
step:919/1750 train_time:88401ms step_avg:96.19ms
step:920/1750 train_time:88502ms step_avg:96.20ms
step:921/1750 train_time:88602ms step_avg:96.20ms
step:922/1750 train_time:88703ms step_avg:96.21ms
step:923/1750 train_time:88804ms step_avg:96.21ms
step:924/1750 train_time:88904ms step_avg:96.22ms
step:925/1750 train_time:89002ms step_avg:96.22ms
step:926/1750 train_time:89103ms step_avg:96.22ms
step:927/1750 train_time:89202ms step_avg:96.23ms
step:928/1750 train_time:89302ms step_avg:96.23ms
step:929/1750 train_time:89402ms step_avg:96.23ms
step:930/1750 train_time:89502ms step_avg:96.24ms
step:931/1750 train_time:89602ms step_avg:96.24ms
step:932/1750 train_time:89703ms step_avg:96.25ms
step:933/1750 train_time:89804ms step_avg:96.25ms
step:934/1750 train_time:89904ms step_avg:96.26ms
step:935/1750 train_time:90004ms step_avg:96.26ms
step:936/1750 train_time:90103ms step_avg:96.26ms
step:937/1750 train_time:90204ms step_avg:96.27ms
step:938/1750 train_time:90304ms step_avg:96.27ms
step:939/1750 train_time:90405ms step_avg:96.28ms
step:940/1750 train_time:90505ms step_avg:96.28ms
step:941/1750 train_time:90606ms step_avg:96.29ms
step:942/1750 train_time:90705ms step_avg:96.29ms
step:943/1750 train_time:90805ms step_avg:96.29ms
step:944/1750 train_time:90905ms step_avg:96.30ms
step:945/1750 train_time:91005ms step_avg:96.30ms
step:946/1750 train_time:91104ms step_avg:96.30ms
step:947/1750 train_time:91204ms step_avg:96.31ms
step:948/1750 train_time:91304ms step_avg:96.31ms
step:949/1750 train_time:91404ms step_avg:96.32ms
step:950/1750 train_time:91504ms step_avg:96.32ms
step:951/1750 train_time:91604ms step_avg:96.32ms
step:952/1750 train_time:91704ms step_avg:96.33ms
step:953/1750 train_time:91804ms step_avg:96.33ms
step:954/1750 train_time:91904ms step_avg:96.34ms
step:955/1750 train_time:92005ms step_avg:96.34ms
step:956/1750 train_time:92104ms step_avg:96.34ms
step:957/1750 train_time:92204ms step_avg:96.35ms
step:958/1750 train_time:92304ms step_avg:96.35ms
step:959/1750 train_time:92404ms step_avg:96.35ms
step:960/1750 train_time:92505ms step_avg:96.36ms
step:961/1750 train_time:92605ms step_avg:96.36ms
step:962/1750 train_time:92705ms step_avg:96.37ms
step:963/1750 train_time:92805ms step_avg:96.37ms
step:964/1750 train_time:92905ms step_avg:96.37ms
step:965/1750 train_time:93004ms step_avg:96.38ms
step:966/1750 train_time:93103ms step_avg:96.38ms
step:967/1750 train_time:93203ms step_avg:96.38ms
step:968/1750 train_time:93304ms step_avg:96.39ms
step:969/1750 train_time:93405ms step_avg:96.39ms
step:970/1750 train_time:93505ms step_avg:96.40ms
step:971/1750 train_time:93605ms step_avg:96.40ms
step:972/1750 train_time:93704ms step_avg:96.40ms
step:973/1750 train_time:93804ms step_avg:96.41ms
step:974/1750 train_time:93904ms step_avg:96.41ms
step:975/1750 train_time:94004ms step_avg:96.41ms
step:976/1750 train_time:94104ms step_avg:96.42ms
step:977/1750 train_time:94204ms step_avg:96.42ms
step:978/1750 train_time:94304ms step_avg:96.43ms
step:979/1750 train_time:94404ms step_avg:96.43ms
step:980/1750 train_time:94504ms step_avg:96.43ms
step:981/1750 train_time:94604ms step_avg:96.44ms
step:982/1750 train_time:94704ms step_avg:96.44ms
step:983/1750 train_time:94805ms step_avg:96.44ms
step:984/1750 train_time:94906ms step_avg:96.45ms
step:985/1750 train_time:95006ms step_avg:96.45ms
step:986/1750 train_time:95105ms step_avg:96.46ms
step:987/1750 train_time:95205ms step_avg:96.46ms
step:988/1750 train_time:95304ms step_avg:96.46ms
step:989/1750 train_time:95404ms step_avg:96.46ms
step:990/1750 train_time:95503ms step_avg:96.47ms
step:991/1750 train_time:95603ms step_avg:96.47ms
step:992/1750 train_time:95704ms step_avg:96.48ms
step:993/1750 train_time:95804ms step_avg:96.48ms
step:994/1750 train_time:95904ms step_avg:96.48ms
step:995/1750 train_time:96005ms step_avg:96.49ms
step:996/1750 train_time:96104ms step_avg:96.49ms
step:997/1750 train_time:96204ms step_avg:96.49ms
step:998/1750 train_time:96304ms step_avg:96.50ms
step:999/1750 train_time:96403ms step_avg:96.50ms
step:1000/1750 train_time:96503ms step_avg:96.50ms
step:1000/1750 val_loss:3.5111 train_time:96592ms step_avg:96.59ms
step:1001/1750 train_time:96614ms step_avg:96.52ms
step:1002/1750 train_time:96710ms step_avg:96.52ms
step:1003/1750 train_time:96814ms step_avg:96.52ms
step:1004/1750 train_time:96912ms step_avg:96.53ms
step:1005/1750 train_time:97010ms step_avg:96.53ms
step:1006/1750 train_time:97109ms step_avg:96.53ms
step:1007/1750 train_time:97207ms step_avg:96.53ms
step:1008/1750 train_time:97305ms step_avg:96.53ms
step:1009/1750 train_time:97404ms step_avg:96.53ms
step:1010/1750 train_time:97503ms step_avg:96.54ms
step:1011/1750 train_time:97604ms step_avg:96.54ms
step:1012/1750 train_time:97706ms step_avg:96.55ms
step:1013/1750 train_time:97808ms step_avg:96.55ms
step:1014/1750 train_time:97907ms step_avg:96.56ms
step:1015/1750 train_time:98007ms step_avg:96.56ms
step:1016/1750 train_time:98106ms step_avg:96.56ms
step:1017/1750 train_time:98205ms step_avg:96.56ms
step:1018/1750 train_time:98304ms step_avg:96.57ms
step:1019/1750 train_time:98402ms step_avg:96.57ms
step:1020/1750 train_time:98502ms step_avg:96.57ms
step:1021/1750 train_time:98602ms step_avg:96.57ms
step:1022/1750 train_time:98703ms step_avg:96.58ms
step:1023/1750 train_time:98803ms step_avg:96.58ms
step:1024/1750 train_time:98905ms step_avg:96.59ms
step:1025/1750 train_time:99006ms step_avg:96.59ms
step:1026/1750 train_time:99105ms step_avg:96.59ms
step:1027/1750 train_time:99204ms step_avg:96.60ms
step:1028/1750 train_time:99303ms step_avg:96.60ms
step:1029/1750 train_time:99404ms step_avg:96.60ms
step:1030/1750 train_time:99503ms step_avg:96.61ms
step:1031/1750 train_time:99603ms step_avg:96.61ms
step:1032/1750 train_time:99703ms step_avg:96.61ms
step:1033/1750 train_time:99803ms step_avg:96.62ms
step:1034/1750 train_time:99903ms step_avg:96.62ms
step:1035/1750 train_time:100003ms step_avg:96.62ms
step:1036/1750 train_time:100103ms step_avg:96.62ms
step:1037/1750 train_time:100203ms step_avg:96.63ms
step:1038/1750 train_time:100302ms step_avg:96.63ms
step:1039/1750 train_time:100402ms step_avg:96.63ms
step:1040/1750 train_time:100503ms step_avg:96.64ms
step:1041/1750 train_time:100602ms step_avg:96.64ms
step:1042/1750 train_time:100964ms step_avg:96.89ms
step:1043/1750 train_time:101062ms step_avg:96.90ms
step:1044/1750 train_time:101161ms step_avg:96.90ms
step:1045/1750 train_time:101259ms step_avg:96.90ms
step:1046/1750 train_time:101359ms step_avg:96.90ms
step:1047/1750 train_time:101456ms step_avg:96.90ms
step:1048/1750 train_time:101554ms step_avg:96.90ms
step:1049/1750 train_time:101909ms step_avg:97.15ms
step:1050/1750 train_time:102007ms step_avg:97.15ms
step:1051/1750 train_time:102105ms step_avg:97.15ms
step:1052/1750 train_time:102204ms step_avg:97.15ms
step:1053/1750 train_time:102303ms step_avg:97.15ms
step:1054/1750 train_time:102401ms step_avg:97.15ms
step:1055/1750 train_time:102500ms step_avg:97.16ms
step:1056/1750 train_time:102599ms step_avg:97.16ms
step:1057/1750 train_time:102698ms step_avg:97.16ms
step:1058/1750 train_time:102804ms step_avg:97.17ms
step:1059/1750 train_time:102907ms step_avg:97.17ms
step:1060/1750 train_time:103007ms step_avg:97.18ms
step:1061/1750 train_time:103106ms step_avg:97.18ms
step:1062/1750 train_time:103205ms step_avg:97.18ms
step:1063/1750 train_time:103304ms step_avg:97.18ms
step:1064/1750 train_time:103403ms step_avg:97.18ms
step:1065/1750 train_time:103502ms step_avg:97.18ms
step:1066/1750 train_time:103601ms step_avg:97.19ms
step:1067/1750 train_time:103701ms step_avg:97.19ms
step:1068/1750 train_time:103801ms step_avg:97.19ms
step:1069/1750 train_time:103903ms step_avg:97.20ms
step:1070/1750 train_time:104005ms step_avg:97.20ms
step:1071/1750 train_time:104105ms step_avg:97.20ms
step:1072/1750 train_time:104206ms step_avg:97.21ms
step:1073/1750 train_time:104304ms step_avg:97.21ms
step:1074/1750 train_time:104403ms step_avg:97.21ms
step:1075/1750 train_time:104502ms step_avg:97.21ms
step:1076/1750 train_time:104601ms step_avg:97.21ms
step:1077/1750 train_time:104700ms step_avg:97.21ms
step:1078/1750 train_time:104801ms step_avg:97.22ms
step:1079/1750 train_time:105187ms step_avg:97.49ms
step:1080/1750 train_time:105285ms step_avg:97.49ms
step:1081/1750 train_time:105385ms step_avg:97.49ms
step:1082/1750 train_time:105483ms step_avg:97.49ms
step:1083/1750 train_time:105582ms step_avg:97.49ms
step:1084/1750 train_time:105680ms step_avg:97.49ms
step:1085/1750 train_time:105778ms step_avg:97.49ms
step:1086/1750 train_time:105877ms step_avg:97.49ms
step:1087/1750 train_time:105975ms step_avg:97.49ms
step:1088/1750 train_time:106077ms step_avg:97.50ms
step:1089/1750 train_time:106184ms step_avg:97.51ms
step:1090/1750 train_time:106286ms step_avg:97.51ms
step:1091/1750 train_time:106385ms step_avg:97.51ms
step:1092/1750 train_time:106484ms step_avg:97.51ms
step:1093/1750 train_time:106584ms step_avg:97.51ms
step:1094/1750 train_time:106683ms step_avg:97.52ms
step:1095/1750 train_time:106782ms step_avg:97.52ms
step:1096/1750 train_time:106881ms step_avg:97.52ms
step:1097/1750 train_time:106981ms step_avg:97.52ms
step:1098/1750 train_time:107083ms step_avg:97.53ms
step:1099/1750 train_time:107184ms step_avg:97.53ms
step:1100/1750 train_time:107284ms step_avg:97.53ms
step:1101/1750 train_time:107384ms step_avg:97.53ms
step:1102/1750 train_time:107484ms step_avg:97.54ms
step:1103/1750 train_time:107583ms step_avg:97.54ms
step:1104/1750 train_time:107683ms step_avg:97.54ms
step:1105/1750 train_time:108078ms step_avg:97.81ms
step:1106/1750 train_time:108176ms step_avg:97.81ms
step:1107/1750 train_time:108275ms step_avg:97.81ms
step:1108/1750 train_time:108373ms step_avg:97.81ms
step:1109/1750 train_time:108773ms step_avg:98.08ms
step:1110/1750 train_time:108871ms step_avg:98.08ms
step:1111/1750 train_time:108969ms step_avg:98.08ms
step:1112/1750 train_time:109067ms step_avg:98.08ms
step:1113/1750 train_time:109166ms step_avg:98.08ms
step:1114/1750 train_time:109264ms step_avg:98.08ms
step:1115/1750 train_time:109362ms step_avg:98.08ms
step:1116/1750 train_time:109462ms step_avg:98.08ms
step:1117/1750 train_time:109561ms step_avg:98.09ms
step:1118/1750 train_time:109663ms step_avg:98.09ms
step:1119/1750 train_time:109766ms step_avg:98.09ms
step:1120/1750 train_time:109867ms step_avg:98.10ms
step:1121/1750 train_time:109968ms step_avg:98.10ms
step:1122/1750 train_time:110067ms step_avg:98.10ms
step:1123/1750 train_time:110165ms step_avg:98.10ms
step:1124/1750 train_time:110264ms step_avg:98.10ms
step:1125/1750 train_time:110363ms step_avg:98.10ms
step:1125/1750 val_loss:3.4599 train_time:110450ms step_avg:98.18ms
step:1126/1750 train_time:110472ms step_avg:98.11ms
step:1127/1750 train_time:110569ms step_avg:98.11ms
step:1128/1750 train_time:110672ms step_avg:98.11ms
step:1129/1750 train_time:110772ms step_avg:98.12ms
step:1130/1750 train_time:110871ms step_avg:98.12ms
step:1131/1750 train_time:110970ms step_avg:98.12ms
step:1132/1750 train_time:111069ms step_avg:98.12ms
step:1133/1750 train_time:111167ms step_avg:98.12ms
step:1134/1750 train_time:111266ms step_avg:98.12ms
step:1135/1750 train_time:111365ms step_avg:98.12ms
step:1136/1750 train_time:111466ms step_avg:98.12ms
step:1137/1750 train_time:111566ms step_avg:98.12ms
step:1138/1750 train_time:111665ms step_avg:98.12ms
step:1139/1750 train_time:111766ms step_avg:98.13ms
step:1140/1750 train_time:111865ms step_avg:98.13ms
step:1141/1750 train_time:111965ms step_avg:98.13ms
step:1142/1750 train_time:112064ms step_avg:98.13ms
step:1143/1750 train_time:112163ms step_avg:98.13ms
step:1144/1750 train_time:112262ms step_avg:98.13ms
step:1145/1750 train_time:112361ms step_avg:98.13ms
step:1146/1750 train_time:112460ms step_avg:98.13ms
step:1147/1750 train_time:112561ms step_avg:98.13ms
step:1148/1750 train_time:112660ms step_avg:98.14ms
step:1149/1750 train_time:112760ms step_avg:98.14ms
step:1150/1750 train_time:112859ms step_avg:98.14ms
step:1151/1750 train_time:112959ms step_avg:98.14ms
step:1152/1750 train_time:113058ms step_avg:98.14ms
step:1153/1750 train_time:113158ms step_avg:98.14ms
step:1154/1750 train_time:113257ms step_avg:98.14ms
step:1155/1750 train_time:113357ms step_avg:98.14ms
step:1156/1750 train_time:113457ms step_avg:98.15ms
step:1157/1750 train_time:113558ms step_avg:98.15ms
step:1158/1750 train_time:113918ms step_avg:98.37ms
step:1159/1750 train_time:114015ms step_avg:98.37ms
step:1160/1750 train_time:114114ms step_avg:98.37ms
step:1161/1750 train_time:114212ms step_avg:98.37ms
step:1162/1750 train_time:114311ms step_avg:98.37ms
step:1163/1750 train_time:114411ms step_avg:98.38ms
step:1164/1750 train_time:114510ms step_avg:98.38ms
step:1165/1750 train_time:114860ms step_avg:98.59ms
step:1166/1750 train_time:114958ms step_avg:98.59ms
step:1167/1750 train_time:115057ms step_avg:98.59ms
step:1168/1750 train_time:115156ms step_avg:98.59ms
step:1169/1750 train_time:115255ms step_avg:98.59ms
step:1170/1750 train_time:115355ms step_avg:98.59ms
step:1171/1750 train_time:115455ms step_avg:98.60ms
step:1172/1750 train_time:115556ms step_avg:98.60ms
step:1173/1750 train_time:115656ms step_avg:98.60ms
step:1174/1750 train_time:115760ms step_avg:98.60ms
step:1175/1750 train_time:115862ms step_avg:98.61ms
step:1176/1750 train_time:115963ms step_avg:98.61ms
step:1177/1750 train_time:116063ms step_avg:98.61ms
step:1178/1750 train_time:116163ms step_avg:98.61ms
step:1179/1750 train_time:116263ms step_avg:98.61ms
step:1180/1750 train_time:116364ms step_avg:98.61ms
step:1181/1750 train_time:116464ms step_avg:98.61ms
step:1182/1750 train_time:116566ms step_avg:98.62ms
step:1183/1750 train_time:116667ms step_avg:98.62ms
step:1184/1750 train_time:116768ms step_avg:98.62ms
step:1185/1750 train_time:116870ms step_avg:98.62ms
step:1186/1750 train_time:116970ms step_avg:98.63ms
step:1187/1750 train_time:117071ms step_avg:98.63ms
step:1188/1750 train_time:117171ms step_avg:98.63ms
step:1189/1750 train_time:117272ms step_avg:98.63ms
step:1190/1750 train_time:117373ms step_avg:98.63ms
step:1191/1750 train_time:117475ms step_avg:98.64ms
step:1192/1750 train_time:117576ms step_avg:98.64ms
step:1193/1750 train_time:117677ms step_avg:98.64ms
step:1194/1750 train_time:117778ms step_avg:98.64ms
step:1195/1750 train_time:117878ms step_avg:98.64ms
step:1196/1750 train_time:117979ms step_avg:98.64ms
step:1197/1750 train_time:118080ms step_avg:98.65ms
step:1198/1750 train_time:118182ms step_avg:98.65ms
step:1199/1750 train_time:118282ms step_avg:98.65ms
step:1200/1750 train_time:118384ms step_avg:98.65ms
step:1201/1750 train_time:118485ms step_avg:98.66ms
step:1202/1750 train_time:118586ms step_avg:98.66ms
step:1203/1750 train_time:118686ms step_avg:98.66ms
step:1204/1750 train_time:118787ms step_avg:98.66ms
step:1205/1750 train_time:118888ms step_avg:98.66ms
step:1206/1750 train_time:118989ms step_avg:98.66ms
step:1207/1750 train_time:119089ms step_avg:98.67ms
step:1208/1750 train_time:119189ms step_avg:98.67ms
step:1209/1750 train_time:119290ms step_avg:98.67ms
step:1210/1750 train_time:119391ms step_avg:98.67ms
step:1211/1750 train_time:119494ms step_avg:98.67ms
step:1212/1750 train_time:119595ms step_avg:98.68ms
step:1213/1750 train_time:119696ms step_avg:98.68ms
step:1214/1750 train_time:119796ms step_avg:98.68ms
step:1215/1750 train_time:119896ms step_avg:98.68ms
step:1216/1750 train_time:119998ms step_avg:98.68ms
step:1217/1750 train_time:120099ms step_avg:98.68ms
step:1218/1750 train_time:120199ms step_avg:98.69ms
step:1219/1750 train_time:120300ms step_avg:98.69ms
step:1220/1750 train_time:120401ms step_avg:98.69ms
step:1221/1750 train_time:120502ms step_avg:98.69ms
step:1222/1750 train_time:120603ms step_avg:98.69ms
step:1223/1750 train_time:120705ms step_avg:98.70ms
step:1224/1750 train_time:120805ms step_avg:98.70ms
step:1225/1750 train_time:120906ms step_avg:98.70ms
step:1226/1750 train_time:121006ms step_avg:98.70ms
step:1227/1750 train_time:121107ms step_avg:98.70ms
step:1228/1750 train_time:121208ms step_avg:98.70ms
step:1229/1750 train_time:121308ms step_avg:98.70ms
step:1230/1750 train_time:121408ms step_avg:98.71ms
step:1231/1750 train_time:121509ms step_avg:98.71ms
step:1232/1750 train_time:121610ms step_avg:98.71ms
step:1233/1750 train_time:121710ms step_avg:98.71ms
step:1234/1750 train_time:121812ms step_avg:98.71ms
step:1235/1750 train_time:121913ms step_avg:98.71ms
step:1236/1750 train_time:122014ms step_avg:98.72ms
step:1237/1750 train_time:122116ms step_avg:98.72ms
step:1238/1750 train_time:122217ms step_avg:98.72ms
step:1239/1750 train_time:122318ms step_avg:98.72ms
step:1240/1750 train_time:122419ms step_avg:98.73ms
step:1241/1750 train_time:122522ms step_avg:98.73ms
step:1242/1750 train_time:122624ms step_avg:98.73ms
step:1243/1750 train_time:122724ms step_avg:98.73ms
step:1244/1750 train_time:122825ms step_avg:98.73ms
step:1245/1750 train_time:122926ms step_avg:98.74ms
step:1246/1750 train_time:123028ms step_avg:98.74ms
step:1247/1750 train_time:123128ms step_avg:98.74ms
step:1248/1750 train_time:123229ms step_avg:98.74ms
step:1249/1750 train_time:123330ms step_avg:98.74ms
step:1250/1750 train_time:123430ms step_avg:98.74ms
step:1250/1750 val_loss:3.4150 train_time:123520ms step_avg:98.82ms
step:1251/1750 train_time:123542ms step_avg:98.75ms
step:1252/1750 train_time:123641ms step_avg:98.75ms
step:1253/1750 train_time:123742ms step_avg:98.76ms
step:1254/1750 train_time:123842ms step_avg:98.76ms
step:1255/1750 train_time:123943ms step_avg:98.76ms
step:1256/1750 train_time:124044ms step_avg:98.76ms
step:1257/1750 train_time:124145ms step_avg:98.76ms
step:1258/1750 train_time:124244ms step_avg:98.76ms
step:1259/1750 train_time:124343ms step_avg:98.76ms
step:1260/1750 train_time:124444ms step_avg:98.76ms
step:1261/1750 train_time:124548ms step_avg:98.77ms
step:1262/1750 train_time:124652ms step_avg:98.77ms
step:1263/1750 train_time:124752ms step_avg:98.77ms
step:1264/1750 train_time:124852ms step_avg:98.78ms
step:1265/1750 train_time:124953ms step_avg:98.78ms
step:1266/1750 train_time:125053ms step_avg:98.78ms
step:1267/1750 train_time:125154ms step_avg:98.78ms
step:1268/1750 train_time:125253ms step_avg:98.78ms
step:1269/1750 train_time:125352ms step_avg:98.78ms
step:1270/1750 train_time:125452ms step_avg:98.78ms
step:1271/1750 train_time:125554ms step_avg:98.78ms
step:1272/1750 train_time:125654ms step_avg:98.78ms
step:1273/1750 train_time:125755ms step_avg:98.79ms
step:1274/1750 train_time:125857ms step_avg:98.79ms
step:1275/1750 train_time:125958ms step_avg:98.79ms
step:1276/1750 train_time:126061ms step_avg:98.79ms
step:1277/1750 train_time:126161ms step_avg:98.80ms
step:1278/1750 train_time:126262ms step_avg:98.80ms
step:1279/1750 train_time:126362ms step_avg:98.80ms
step:1280/1750 train_time:126462ms step_avg:98.80ms
step:1281/1750 train_time:126563ms step_avg:98.80ms
step:1282/1750 train_time:126663ms step_avg:98.80ms
step:1283/1750 train_time:126765ms step_avg:98.80ms
step:1284/1750 train_time:126865ms step_avg:98.80ms
step:1285/1750 train_time:126966ms step_avg:98.81ms
step:1286/1750 train_time:127067ms step_avg:98.81ms
step:1287/1750 train_time:127166ms step_avg:98.81ms
step:1288/1750 train_time:127267ms step_avg:98.81ms
step:1289/1750 train_time:127369ms step_avg:98.81ms
step:1290/1750 train_time:127469ms step_avg:98.81ms
step:1291/1750 train_time:127570ms step_avg:98.81ms
step:1292/1750 train_time:127671ms step_avg:98.82ms
step:1293/1750 train_time:127771ms step_avg:98.82ms
step:1294/1750 train_time:127874ms step_avg:98.82ms
step:1295/1750 train_time:127975ms step_avg:98.82ms
step:1296/1750 train_time:128076ms step_avg:98.82ms
step:1297/1750 train_time:128178ms step_avg:98.83ms
step:1298/1750 train_time:128279ms step_avg:98.83ms
step:1299/1750 train_time:128380ms step_avg:98.83ms
step:1300/1750 train_time:128481ms step_avg:98.83ms
step:1301/1750 train_time:128583ms step_avg:98.83ms
step:1302/1750 train_time:128684ms step_avg:98.84ms
step:1303/1750 train_time:128785ms step_avg:98.84ms
step:1304/1750 train_time:128886ms step_avg:98.84ms
step:1305/1750 train_time:128988ms step_avg:98.84ms
step:1306/1750 train_time:129089ms step_avg:98.84ms
step:1307/1750 train_time:129190ms step_avg:98.84ms
step:1308/1750 train_time:129291ms step_avg:98.85ms
step:1309/1750 train_time:129392ms step_avg:98.85ms
step:1310/1750 train_time:129493ms step_avg:98.85ms
step:1311/1750 train_time:129593ms step_avg:98.85ms
step:1312/1750 train_time:129693ms step_avg:98.85ms
step:1313/1750 train_time:129795ms step_avg:98.85ms
step:1314/1750 train_time:129897ms step_avg:98.86ms
step:1315/1750 train_time:129999ms step_avg:98.86ms
step:1316/1750 train_time:130101ms step_avg:98.86ms
step:1317/1750 train_time:130203ms step_avg:98.86ms
step:1318/1750 train_time:130304ms step_avg:98.86ms
step:1319/1750 train_time:130405ms step_avg:98.87ms
step:1320/1750 train_time:130506ms step_avg:98.87ms
step:1321/1750 train_time:130607ms step_avg:98.87ms
step:1322/1750 train_time:130708ms step_avg:98.87ms
step:1323/1750 train_time:130809ms step_avg:98.87ms
step:1324/1750 train_time:130912ms step_avg:98.88ms
step:1325/1750 train_time:131013ms step_avg:98.88ms
step:1326/1750 train_time:131115ms step_avg:98.88ms
step:1327/1750 train_time:131216ms step_avg:98.88ms
step:1328/1750 train_time:131318ms step_avg:98.88ms
step:1329/1750 train_time:131422ms step_avg:98.89ms
step:1330/1750 train_time:131522ms step_avg:98.89ms
step:1331/1750 train_time:131623ms step_avg:98.89ms
step:1332/1750 train_time:131723ms step_avg:98.89ms
step:1333/1750 train_time:131826ms step_avg:98.89ms
step:1334/1750 train_time:131926ms step_avg:98.90ms
step:1335/1750 train_time:132027ms step_avg:98.90ms
step:1336/1750 train_time:132129ms step_avg:98.90ms
step:1337/1750 train_time:132231ms step_avg:98.90ms
step:1338/1750 train_time:132331ms step_avg:98.90ms
step:1339/1750 train_time:132433ms step_avg:98.90ms
step:1340/1750 train_time:132534ms step_avg:98.91ms
step:1341/1750 train_time:132634ms step_avg:98.91ms
step:1342/1750 train_time:132735ms step_avg:98.91ms
step:1343/1750 train_time:132838ms step_avg:98.91ms
step:1344/1750 train_time:132939ms step_avg:98.91ms
step:1345/1750 train_time:133040ms step_avg:98.91ms
step:1346/1750 train_time:133142ms step_avg:98.92ms
step:1347/1750 train_time:133244ms step_avg:98.92ms
step:1348/1750 train_time:133345ms step_avg:98.92ms
step:1349/1750 train_time:133445ms step_avg:98.92ms
step:1350/1750 train_time:133547ms step_avg:98.92ms
step:1351/1750 train_time:133648ms step_avg:98.93ms
step:1352/1750 train_time:133750ms step_avg:98.93ms
step:1353/1750 train_time:133850ms step_avg:98.93ms
step:1354/1750 train_time:133951ms step_avg:98.93ms
step:1355/1750 train_time:134053ms step_avg:98.93ms
step:1356/1750 train_time:134153ms step_avg:98.93ms
step:1357/1750 train_time:134253ms step_avg:98.93ms
step:1358/1750 train_time:134354ms step_avg:98.94ms
step:1359/1750 train_time:134455ms step_avg:98.94ms
step:1360/1750 train_time:134557ms step_avg:98.94ms
step:1361/1750 train_time:134659ms step_avg:98.94ms
step:1362/1750 train_time:134759ms step_avg:98.94ms
step:1363/1750 train_time:134862ms step_avg:98.94ms
step:1364/1750 train_time:134962ms step_avg:98.95ms
step:1365/1750 train_time:135064ms step_avg:98.95ms
step:1366/1750 train_time:135164ms step_avg:98.95ms
step:1367/1750 train_time:135264ms step_avg:98.95ms
step:1368/1750 train_time:135366ms step_avg:98.95ms
step:1369/1750 train_time:135467ms step_avg:98.95ms
step:1370/1750 train_time:135567ms step_avg:98.95ms
step:1371/1750 train_time:135667ms step_avg:98.95ms
step:1372/1750 train_time:135768ms step_avg:98.96ms
step:1373/1750 train_time:135870ms step_avg:98.96ms
step:1374/1750 train_time:135970ms step_avg:98.96ms
step:1375/1750 train_time:136072ms step_avg:98.96ms
step:1375/1750 val_loss:3.3760 train_time:136160ms step_avg:99.03ms
step:1376/1750 train_time:136182ms step_avg:98.97ms
step:1377/1750 train_time:136281ms step_avg:98.97ms
step:1378/1750 train_time:136382ms step_avg:98.97ms
step:1379/1750 train_time:136482ms step_avg:98.97ms
step:1380/1750 train_time:136583ms step_avg:98.97ms
step:1381/1750 train_time:136683ms step_avg:98.97ms
step:1382/1750 train_time:136782ms step_avg:98.97ms
step:1383/1750 train_time:136882ms step_avg:98.97ms
step:1384/1750 train_time:136981ms step_avg:98.97ms
step:1385/1750 train_time:137082ms step_avg:98.98ms
step:1386/1750 train_time:137185ms step_avg:98.98ms
step:1387/1750 train_time:137287ms step_avg:98.98ms
step:1388/1750 train_time:137388ms step_avg:98.98ms
step:1389/1750 train_time:137490ms step_avg:98.98ms
step:1390/1750 train_time:137590ms step_avg:98.99ms
step:1391/1750 train_time:137690ms step_avg:98.99ms
step:1392/1750 train_time:137791ms step_avg:98.99ms
step:1393/1750 train_time:137892ms step_avg:98.99ms
step:1394/1750 train_time:137993ms step_avg:98.99ms
step:1395/1750 train_time:138095ms step_avg:98.99ms
step:1396/1750 train_time:138197ms step_avg:99.00ms
step:1397/1750 train_time:138299ms step_avg:99.00ms
step:1398/1750 train_time:138400ms step_avg:99.00ms
step:1399/1750 train_time:138501ms step_avg:99.00ms
step:1400/1750 train_time:138603ms step_avg:99.00ms
step:1401/1750 train_time:138703ms step_avg:99.00ms
step:1402/1750 train_time:138804ms step_avg:99.00ms
step:1403/1750 train_time:138905ms step_avg:99.01ms
step:1404/1750 train_time:139006ms step_avg:99.01ms
step:1405/1750 train_time:139106ms step_avg:99.01ms
step:1406/1750 train_time:139208ms step_avg:99.01ms
step:1407/1750 train_time:139311ms step_avg:99.01ms
step:1408/1750 train_time:139411ms step_avg:99.01ms
step:1409/1750 train_time:139513ms step_avg:99.02ms
step:1410/1750 train_time:139613ms step_avg:99.02ms
step:1411/1750 train_time:139714ms step_avg:99.02ms
step:1412/1750 train_time:139818ms step_avg:99.02ms
step:1413/1750 train_time:139918ms step_avg:99.02ms
step:1414/1750 train_time:140019ms step_avg:99.02ms
step:1415/1750 train_time:140122ms step_avg:99.03ms
step:1416/1750 train_time:140223ms step_avg:99.03ms
step:1417/1750 train_time:140323ms step_avg:99.03ms
step:1418/1750 train_time:140423ms step_avg:99.03ms
step:1419/1750 train_time:140526ms step_avg:99.03ms
step:1420/1750 train_time:140627ms step_avg:99.03ms
step:1421/1750 train_time:140727ms step_avg:99.03ms
step:1422/1750 train_time:140828ms step_avg:99.03ms
step:1423/1750 train_time:140930ms step_avg:99.04ms
step:1424/1750 train_time:141032ms step_avg:99.04ms
step:1425/1750 train_time:141133ms step_avg:99.04ms
step:1426/1750 train_time:141236ms step_avg:99.04ms
step:1427/1750 train_time:141340ms step_avg:99.05ms
step:1428/1750 train_time:141442ms step_avg:99.05ms
step:1429/1750 train_time:141543ms step_avg:99.05ms
step:1430/1750 train_time:141645ms step_avg:99.05ms
step:1431/1750 train_time:141746ms step_avg:99.05ms
step:1432/1750 train_time:141849ms step_avg:99.06ms
step:1433/1750 train_time:141950ms step_avg:99.06ms
step:1434/1750 train_time:142051ms step_avg:99.06ms
step:1435/1750 train_time:142154ms step_avg:99.06ms
step:1436/1750 train_time:142256ms step_avg:99.06ms
step:1437/1750 train_time:142358ms step_avg:99.07ms
step:1438/1750 train_time:142460ms step_avg:99.07ms
step:1439/1750 train_time:142564ms step_avg:99.07ms
step:1440/1750 train_time:142668ms step_avg:99.08ms
step:1441/1750 train_time:142770ms step_avg:99.08ms
step:1442/1750 train_time:142870ms step_avg:99.08ms
step:1443/1750 train_time:142971ms step_avg:99.08ms
step:1444/1750 train_time:143072ms step_avg:99.08ms
step:1445/1750 train_time:143173ms step_avg:99.08ms
step:1446/1750 train_time:143274ms step_avg:99.08ms
step:1447/1750 train_time:143376ms step_avg:99.08ms
step:1448/1750 train_time:143481ms step_avg:99.09ms
step:1449/1750 train_time:143583ms step_avg:99.09ms
step:1450/1750 train_time:143685ms step_avg:99.09ms
step:1451/1750 train_time:143786ms step_avg:99.09ms
step:1452/1750 train_time:143887ms step_avg:99.10ms
step:1453/1750 train_time:143989ms step_avg:99.10ms
step:1454/1750 train_time:144092ms step_avg:99.10ms
step:1455/1750 train_time:144194ms step_avg:99.10ms
step:1456/1750 train_time:144296ms step_avg:99.10ms
step:1457/1750 train_time:144399ms step_avg:99.11ms
step:1458/1750 train_time:144503ms step_avg:99.11ms
step:1459/1750 train_time:144605ms step_avg:99.11ms
step:1460/1750 train_time:144706ms step_avg:99.11ms
step:1461/1750 train_time:144809ms step_avg:99.12ms
step:1462/1750 train_time:144911ms step_avg:99.12ms
step:1463/1750 train_time:145013ms step_avg:99.12ms
step:1464/1750 train_time:145114ms step_avg:99.12ms
step:1465/1750 train_time:145217ms step_avg:99.12ms
step:1466/1750 train_time:145319ms step_avg:99.13ms
step:1467/1750 train_time:145420ms step_avg:99.13ms
step:1468/1750 train_time:145523ms step_avg:99.13ms
step:1469/1750 train_time:145625ms step_avg:99.13ms
step:1470/1750 train_time:145726ms step_avg:99.13ms
step:1471/1750 train_time:145828ms step_avg:99.14ms
step:1472/1750 train_time:145929ms step_avg:99.14ms
step:1473/1750 train_time:146031ms step_avg:99.14ms
step:1474/1750 train_time:146132ms step_avg:99.14ms
step:1475/1750 train_time:146234ms step_avg:99.14ms
step:1476/1750 train_time:146336ms step_avg:99.14ms
step:1477/1750 train_time:146439ms step_avg:99.15ms
step:1478/1750 train_time:146541ms step_avg:99.15ms
step:1479/1750 train_time:146643ms step_avg:99.15ms
step:1480/1750 train_time:146745ms step_avg:99.15ms
step:1481/1750 train_time:146847ms step_avg:99.15ms
step:1482/1750 train_time:146950ms step_avg:99.16ms
step:1483/1750 train_time:147050ms step_avg:99.16ms
step:1484/1750 train_time:147153ms step_avg:99.16ms
step:1485/1750 train_time:147255ms step_avg:99.16ms
step:1486/1750 train_time:147357ms step_avg:99.16ms
step:1487/1750 train_time:147459ms step_avg:99.17ms
step:1488/1750 train_time:147562ms step_avg:99.17ms
step:1489/1750 train_time:147665ms step_avg:99.17ms
step:1490/1750 train_time:147767ms step_avg:99.17ms
step:1491/1750 train_time:147869ms step_avg:99.17ms
step:1492/1750 train_time:147969ms step_avg:99.18ms
step:1493/1750 train_time:148071ms step_avg:99.18ms
step:1494/1750 train_time:148173ms step_avg:99.18ms
step:1495/1750 train_time:148275ms step_avg:99.18ms
step:1496/1750 train_time:148376ms step_avg:99.18ms
step:1497/1750 train_time:148478ms step_avg:99.18ms
step:1498/1750 train_time:148581ms step_avg:99.19ms
step:1499/1750 train_time:148682ms step_avg:99.19ms
step:1500/1750 train_time:148784ms step_avg:99.19ms
step:1500/1750 val_loss:3.3394 train_time:148874ms step_avg:99.25ms
step:1501/1750 train_time:148896ms step_avg:99.20ms
step:1502/1750 train_time:148999ms step_avg:99.20ms
step:1503/1750 train_time:149102ms step_avg:99.20ms
step:1504/1750 train_time:149202ms step_avg:99.20ms
step:1505/1750 train_time:149302ms step_avg:99.20ms
step:1506/1750 train_time:149403ms step_avg:99.20ms
step:1507/1750 train_time:149504ms step_avg:99.21ms
step:1508/1750 train_time:149605ms step_avg:99.21ms
step:1509/1750 train_time:149706ms step_avg:99.21ms
step:1510/1750 train_time:149809ms step_avg:99.21ms
step:1511/1750 train_time:149913ms step_avg:99.21ms
step:1512/1750 train_time:150017ms step_avg:99.22ms
step:1513/1750 train_time:150119ms step_avg:99.22ms
step:1514/1750 train_time:150219ms step_avg:99.22ms
step:1515/1750 train_time:150323ms step_avg:99.22ms
step:1516/1750 train_time:150424ms step_avg:99.22ms
step:1517/1750 train_time:150525ms step_avg:99.23ms
step:1518/1750 train_time:150626ms step_avg:99.23ms
step:1519/1750 train_time:150728ms step_avg:99.23ms
step:1520/1750 train_time:150829ms step_avg:99.23ms
step:1521/1750 train_time:150931ms step_avg:99.23ms
step:1522/1750 train_time:151034ms step_avg:99.23ms
step:1523/1750 train_time:151138ms step_avg:99.24ms
step:1524/1750 train_time:151242ms step_avg:99.24ms
step:1525/1750 train_time:151344ms step_avg:99.24ms
step:1526/1750 train_time:151446ms step_avg:99.24ms
step:1527/1750 train_time:151547ms step_avg:99.25ms
step:1528/1750 train_time:151653ms step_avg:99.25ms
step:1529/1750 train_time:151754ms step_avg:99.25ms
step:1530/1750 train_time:151857ms step_avg:99.25ms
step:1531/1750 train_time:151959ms step_avg:99.25ms
step:1532/1750 train_time:152060ms step_avg:99.26ms
step:1533/1750 train_time:152162ms step_avg:99.26ms
step:1534/1750 train_time:152264ms step_avg:99.26ms
step:1535/1750 train_time:152365ms step_avg:99.26ms
step:1536/1750 train_time:152465ms step_avg:99.26ms
step:1537/1750 train_time:152567ms step_avg:99.26ms
step:1538/1750 train_time:152668ms step_avg:99.26ms
step:1539/1750 train_time:152769ms step_avg:99.26ms
step:1540/1750 train_time:152871ms step_avg:99.27ms
step:1541/1750 train_time:152977ms step_avg:99.27ms
step:1542/1750 train_time:153081ms step_avg:99.27ms
step:1543/1750 train_time:153183ms step_avg:99.28ms
step:1544/1750 train_time:153284ms step_avg:99.28ms
step:1545/1750 train_time:153385ms step_avg:99.28ms
step:1546/1750 train_time:153487ms step_avg:99.28ms
step:1547/1750 train_time:153589ms step_avg:99.28ms
step:1548/1750 train_time:153691ms step_avg:99.28ms
step:1549/1750 train_time:153793ms step_avg:99.29ms
step:1550/1750 train_time:153894ms step_avg:99.29ms
step:1551/1750 train_time:153998ms step_avg:99.29ms
step:1552/1750 train_time:154100ms step_avg:99.29ms
step:1553/1750 train_time:154202ms step_avg:99.29ms
step:1554/1750 train_time:154303ms step_avg:99.29ms
step:1555/1750 train_time:154405ms step_avg:99.30ms
step:1556/1750 train_time:154507ms step_avg:99.30ms
step:1557/1750 train_time:154609ms step_avg:99.30ms
step:1558/1750 train_time:154710ms step_avg:99.30ms
step:1559/1750 train_time:154814ms step_avg:99.30ms
step:1560/1750 train_time:154917ms step_avg:99.31ms
step:1561/1750 train_time:155019ms step_avg:99.31ms
step:1562/1750 train_time:155122ms step_avg:99.31ms
step:1563/1750 train_time:155227ms step_avg:99.31ms
step:1564/1750 train_time:155330ms step_avg:99.32ms
step:1565/1750 train_time:155430ms step_avg:99.32ms
step:1566/1750 train_time:155532ms step_avg:99.32ms
step:1567/1750 train_time:155633ms step_avg:99.32ms
step:1568/1750 train_time:155734ms step_avg:99.32ms
step:1569/1750 train_time:155836ms step_avg:99.32ms
step:1570/1750 train_time:155940ms step_avg:99.32ms
step:1571/1750 train_time:156041ms step_avg:99.33ms
step:1572/1750 train_time:156143ms step_avg:99.33ms
step:1573/1750 train_time:156246ms step_avg:99.33ms
step:1574/1750 train_time:156348ms step_avg:99.33ms
step:1575/1750 train_time:156449ms step_avg:99.33ms
step:1576/1750 train_time:156552ms step_avg:99.33ms
step:1577/1750 train_time:156655ms step_avg:99.34ms
step:1578/1750 train_time:156757ms step_avg:99.34ms
step:1579/1750 train_time:156858ms step_avg:99.34ms
step:1580/1750 train_time:156960ms step_avg:99.34ms
step:1581/1750 train_time:157062ms step_avg:99.34ms
step:1582/1750 train_time:157163ms step_avg:99.34ms
step:1583/1750 train_time:157267ms step_avg:99.35ms
step:1584/1750 train_time:157369ms step_avg:99.35ms
step:1585/1750 train_time:157470ms step_avg:99.35ms
step:1586/1750 train_time:157572ms step_avg:99.35ms
step:1587/1750 train_time:157674ms step_avg:99.35ms
step:1588/1750 train_time:157777ms step_avg:99.36ms
step:1589/1750 train_time:157879ms step_avg:99.36ms
step:1590/1750 train_time:157980ms step_avg:99.36ms
step:1591/1750 train_time:158083ms step_avg:99.36ms
step:1592/1750 train_time:158184ms step_avg:99.36ms
step:1593/1750 train_time:158286ms step_avg:99.36ms
step:1594/1750 train_time:158391ms step_avg:99.37ms
step:1595/1750 train_time:158492ms step_avg:99.37ms
step:1596/1750 train_time:158594ms step_avg:99.37ms
step:1597/1750 train_time:158696ms step_avg:99.37ms
step:1598/1750 train_time:158798ms step_avg:99.37ms
step:1599/1750 train_time:158899ms step_avg:99.37ms
step:1600/1750 train_time:159000ms step_avg:99.38ms
step:1601/1750 train_time:159102ms step_avg:99.38ms
step:1602/1750 train_time:159204ms step_avg:99.38ms
step:1603/1750 train_time:159307ms step_avg:99.38ms
step:1604/1750 train_time:159407ms step_avg:99.38ms
step:1605/1750 train_time:159510ms step_avg:99.38ms
step:1606/1750 train_time:159613ms step_avg:99.39ms
step:1607/1750 train_time:159714ms step_avg:99.39ms
step:1608/1750 train_time:159816ms step_avg:99.39ms
step:1609/1750 train_time:159919ms step_avg:99.39ms
step:1610/1750 train_time:160021ms step_avg:99.39ms
step:1611/1750 train_time:160123ms step_avg:99.39ms
step:1612/1750 train_time:160225ms step_avg:99.40ms
step:1613/1750 train_time:160327ms step_avg:99.40ms
step:1614/1750 train_time:160427ms step_avg:99.40ms
step:1615/1750 train_time:160528ms step_avg:99.40ms
step:1616/1750 train_time:160629ms step_avg:99.40ms
step:1617/1750 train_time:160732ms step_avg:99.40ms
step:1618/1750 train_time:160835ms step_avg:99.40ms
step:1619/1750 train_time:160938ms step_avg:99.41ms
step:1620/1750 train_time:161041ms step_avg:99.41ms
step:1621/1750 train_time:161142ms step_avg:99.41ms
step:1622/1750 train_time:161243ms step_avg:99.41ms
step:1623/1750 train_time:161344ms step_avg:99.41ms
step:1624/1750 train_time:161448ms step_avg:99.41ms
step:1625/1750 train_time:161551ms step_avg:99.42ms
step:1625/1750 val_loss:3.3094 train_time:161642ms step_avg:99.47ms
step:1626/1750 train_time:161664ms step_avg:99.42ms
step:1627/1750 train_time:161765ms step_avg:99.43ms
step:1628/1750 train_time:161866ms step_avg:99.43ms
step:1629/1750 train_time:161968ms step_avg:99.43ms
step:1630/1750 train_time:162068ms step_avg:99.43ms
step:1631/1750 train_time:162169ms step_avg:99.43ms
step:1632/1750 train_time:162269ms step_avg:99.43ms
step:1633/1750 train_time:162371ms step_avg:99.43ms
step:1634/1750 train_time:162474ms step_avg:99.43ms
step:1635/1750 train_time:162577ms step_avg:99.44ms
step:1636/1750 train_time:162681ms step_avg:99.44ms
step:1637/1750 train_time:162784ms step_avg:99.44ms
step:1638/1750 train_time:162886ms step_avg:99.44ms
step:1639/1750 train_time:162987ms step_avg:99.44ms
step:1640/1750 train_time:163090ms step_avg:99.44ms
step:1641/1750 train_time:163190ms step_avg:99.45ms
step:1642/1750 train_time:163291ms step_avg:99.45ms
step:1643/1750 train_time:163392ms step_avg:99.45ms
step:1644/1750 train_time:163495ms step_avg:99.45ms
step:1645/1750 train_time:163596ms step_avg:99.45ms
step:1646/1750 train_time:163699ms step_avg:99.45ms
step:1647/1750 train_time:163803ms step_avg:99.46ms
step:1648/1750 train_time:163907ms step_avg:99.46ms
step:1649/1750 train_time:164008ms step_avg:99.46ms
step:1650/1750 train_time:164109ms step_avg:99.46ms
step:1651/1750 train_time:164211ms step_avg:99.46ms
step:1652/1750 train_time:164313ms step_avg:99.46ms
step:1653/1750 train_time:164415ms step_avg:99.46ms
step:1654/1750 train_time:164516ms step_avg:99.47ms
step:1655/1750 train_time:164618ms step_avg:99.47ms
step:1656/1750 train_time:164722ms step_avg:99.47ms
step:1657/1750 train_time:164824ms step_avg:99.47ms
step:1658/1750 train_time:164926ms step_avg:99.47ms
step:1659/1750 train_time:165029ms step_avg:99.48ms
step:1660/1750 train_time:165130ms step_avg:99.48ms
step:1661/1750 train_time:165233ms step_avg:99.48ms
step:1662/1750 train_time:165337ms step_avg:99.48ms
step:1663/1750 train_time:165439ms step_avg:99.48ms
step:1664/1750 train_time:165541ms step_avg:99.48ms
step:1665/1750 train_time:165646ms step_avg:99.49ms
step:1666/1750 train_time:165749ms step_avg:99.49ms
step:1667/1750 train_time:165851ms step_avg:99.49ms
step:1668/1750 train_time:165953ms step_avg:99.49ms
step:1669/1750 train_time:166055ms step_avg:99.49ms
step:1670/1750 train_time:166156ms step_avg:99.49ms
step:1671/1750 train_time:166257ms step_avg:99.50ms
step:1672/1750 train_time:166359ms step_avg:99.50ms
step:1673/1750 train_time:166461ms step_avg:99.50ms
step:1674/1750 train_time:166563ms step_avg:99.50ms
step:1675/1750 train_time:166665ms step_avg:99.50ms
step:1676/1750 train_time:166769ms step_avg:99.50ms
step:1677/1750 train_time:166870ms step_avg:99.50ms
step:1678/1750 train_time:166972ms step_avg:99.51ms
step:1679/1750 train_time:167075ms step_avg:99.51ms
step:1680/1750 train_time:167176ms step_avg:99.51ms
step:1681/1750 train_time:167278ms step_avg:99.51ms
step:1682/1750 train_time:167382ms step_avg:99.51ms
step:1683/1750 train_time:167484ms step_avg:99.52ms
step:1684/1750 train_time:167586ms step_avg:99.52ms
step:1685/1750 train_time:167688ms step_avg:99.52ms
step:1686/1750 train_time:167791ms step_avg:99.52ms
step:1687/1750 train_time:167892ms step_avg:99.52ms
step:1688/1750 train_time:167994ms step_avg:99.52ms
step:1689/1750 train_time:168096ms step_avg:99.52ms
step:1690/1750 train_time:168198ms step_avg:99.53ms
step:1691/1750 train_time:168300ms step_avg:99.53ms
step:1692/1750 train_time:168403ms step_avg:99.53ms
step:1693/1750 train_time:168508ms step_avg:99.53ms
step:1694/1750 train_time:168611ms step_avg:99.53ms
step:1695/1750 train_time:168716ms step_avg:99.54ms
step:1696/1750 train_time:168818ms step_avg:99.54ms
step:1697/1750 train_time:168922ms step_avg:99.54ms
step:1698/1750 train_time:169025ms step_avg:99.54ms
step:1699/1750 train_time:169127ms step_avg:99.55ms
step:1700/1750 train_time:169230ms step_avg:99.55ms
step:1701/1750 train_time:169332ms step_avg:99.55ms
step:1702/1750 train_time:169436ms step_avg:99.55ms
step:1703/1750 train_time:169537ms step_avg:99.55ms
step:1704/1750 train_time:169640ms step_avg:99.55ms
step:1705/1750 train_time:169742ms step_avg:99.56ms
step:1706/1750 train_time:169845ms step_avg:99.56ms
step:1707/1750 train_time:169949ms step_avg:99.56ms
step:1708/1750 train_time:170051ms step_avg:99.56ms
step:1709/1750 train_time:170154ms step_avg:99.56ms
step:1710/1750 train_time:170256ms step_avg:99.56ms
step:1711/1750 train_time:170360ms step_avg:99.57ms
step:1712/1750 train_time:170463ms step_avg:99.57ms
step:1713/1750 train_time:170566ms step_avg:99.57ms
step:1714/1750 train_time:170670ms step_avg:99.57ms
step:1715/1750 train_time:170774ms step_avg:99.58ms
step:1716/1750 train_time:170877ms step_avg:99.58ms
step:1717/1750 train_time:170981ms step_avg:99.58ms
step:1718/1750 train_time:171084ms step_avg:99.58ms
step:1719/1750 train_time:171191ms step_avg:99.59ms
step:1720/1750 train_time:171292ms step_avg:99.59ms
step:1721/1750 train_time:171395ms step_avg:99.59ms
step:1722/1750 train_time:171497ms step_avg:99.59ms
step:1723/1750 train_time:171600ms step_avg:99.59ms
step:1724/1750 train_time:171704ms step_avg:99.60ms
step:1725/1750 train_time:171808ms step_avg:99.60ms
step:1726/1750 train_time:171910ms step_avg:99.60ms
step:1727/1750 train_time:172013ms step_avg:99.60ms
step:1728/1750 train_time:172117ms step_avg:99.60ms
step:1729/1750 train_time:172220ms step_avg:99.61ms
step:1730/1750 train_time:172323ms step_avg:99.61ms
step:1731/1750 train_time:172428ms step_avg:99.61ms
step:1732/1750 train_time:172530ms step_avg:99.61ms
step:1733/1750 train_time:172634ms step_avg:99.62ms
step:1734/1750 train_time:172739ms step_avg:99.62ms
step:1735/1750 train_time:172841ms step_avg:99.62ms
step:1736/1750 train_time:172944ms step_avg:99.62ms
step:1737/1750 train_time:173047ms step_avg:99.62ms
step:1738/1750 train_time:173150ms step_avg:99.63ms
step:1739/1750 train_time:173252ms step_avg:99.63ms
step:1740/1750 train_time:173354ms step_avg:99.63ms
step:1741/1750 train_time:173461ms step_avg:99.63ms
step:1742/1750 train_time:173564ms step_avg:99.63ms
step:1743/1750 train_time:173667ms step_avg:99.64ms
step:1744/1750 train_time:173769ms step_avg:99.64ms
step:1745/1750 train_time:173871ms step_avg:99.64ms
step:1746/1750 train_time:173973ms step_avg:99.64ms
step:1747/1750 train_time:174075ms step_avg:99.64ms
step:1748/1750 train_time:174179ms step_avg:99.64ms
step:1749/1750 train_time:174282ms step_avg:99.65ms
step:1750/1750 train_time:174386ms step_avg:99.65ms
step:1750/1750 val_loss:3.2859 train_time:174476ms step_avg:99.70ms
peak memory allocated: 33278 MiB reserved: 48754 MiB
