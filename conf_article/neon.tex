\documentclass{article} % For LaTeX2e
\usepackage{icomp2024_conference,times}

% Optional math command from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath,amssymb}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{changes}
\definechangesauthor[name={Ivan Kozyrev}, color=orange]{IK}
\newtheorem{lemma}{Lemma}
\newcommand{\norm}[1]{\lVert #1\rVert}
\newcommand{\abs}[1]{\lvert #1 \rvert}


%\title{From Muon to Neon: Introducing Nuclear Norm to Large Matrices}

\title{The Ky Fan Norms and Beyond: Dual Norms and Combinations for Matrix Optimization}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \icompfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Alexey Kravatskiy \\
Moscow Institute of Physics and Technology (MIPT) \\
\texttt{kravtskii.aiu@phystech.edu} \\
\And
Ivan Kozyrev \\
Moscow Institute of Physics and Technology (MIPT) \\
\texttt{kozyrev.in@phystech.edu} \\
\And
Nikolai Kozlov \\
Moscow Institute of Physics and Technology (MIPT) \\
\texttt{kozlov.na@phystech.edu} \\
\And
Alexander Vinogradov \\
Moscow Institute of Physics and Technology (MIPT) \\
\texttt{vinogradov.am@phystech.edu} \\
\And
Daniil Merkulov \\
Moscow Institute of Physics and Technology (MIPT), Skoltech, HSE, Sber \\
\texttt{daniil.merkulov@phystech.edu} \\
\And
Ivan Oseledets \\
AIRI, Skoltech \\
\texttt{i.oseledets@skoltech.ru}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\icompfinalcopy % Uncomment for camera-ready version, but NOT for submission.

%------------------------------------------------------------------------------------
% \epsilon is predefined; override to use \varepsilon
\renewcommand{\epsilon}{\varepsilon}
\newcommand{\Rmn}{\R^{m\times n}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\Ed}[2]{\mathbb{E}_{#1}\left[#2\right]}
\usepackage{mathtools}
\DeclarePairedDelimiter{\sqne}{\|}{\|_2^2}
%\DeclarePairedDelimiter{\norme}{\|}{\|_2}
\DeclarePairedDelimiter{\normf}{\|}{\|_\mathrm{F}}
\DeclarePairedDelimiter{\normkfk}{\|}{\|_\mathrm{KF-k}}
\DeclarePairedDelimiter{\normfstar}{\|}{\|_\mathrm{F*}}
\DeclarePairedDelimiter{\normftwo}{\|}{\|_\mathrm{F2}}
\DeclarePairedDelimiter{\sqns}{\|}{\|_{\mathrm{op}}^2}
\DeclarePairedDelimiter{\norms}{\|}{\|_{\mathrm{op}}}
\DeclarePairedDelimiter{\sqnn}{\|}{\|_{\mathrm{nuc}}^2}
\DeclarePairedDelimiter{\sqnf}{\|}{\|_{\mathrm{F}}^2}
\DeclarePairedDelimiter{\normn}{\|}{\|_{\mathrm{nuc}}}
\def\<#1,#2>{\langle #1,#2\rangle}
\DeclarePairedDelimiter{\dotprod}{\langle}{\rangle}

\usepackage{cleveref}
\usepackage{environ}
\newcounter{aequation}
\NewEnviron{aequation}{\refstepcounter{aequation}$$\BODY\eqno{\text{(A\theaequation)}}$$}
\crefname{aequation}{assumption}{assumptions}
\creflabelformat{aequation}{(#2A#1#3)}

\usepackage{amsmath}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\diag}{diag}

\usepackage{arydshln} % dashed lines
\usepackage[right=5cm, marginparwidth=4.6cm, marginparsep=0.2cm]{geometry} % For comments!

%------------------------------------------------------------------------------------

\begin{document}


\maketitle

\begin{abstract}
	% In this article, we explore the use of matrix norms for optimizing functions of weight matrices, a crucial problem in training large language models. Moving beyond the spectral norm that underlies the Muon update, we leverage the Ky Fan k-norms, their affine combinations with the Frobenius norms, and corresponding duals to develop a new family of Muon-like algorithms we name {\it Fanions}. We complement our theoretical analysis with an extensive empirical study of the algorithms across a wide range of tasks and settings.

	In this article, we explore the use of matrix norms for optimizing functions of weight matrices, a crucial problem in training large language models. Moving beyond the spectral norm that underlies the Muon update, we leverage the duals to the Ky Fan k-norms to introduce a \deleted[id=IK, comment={Redundant}]{new} family of Muon-like algorithms we name {\it Fanions}, which happen to be similar to Dion. \replaced[id=IK, comment={This rewrite clarifies the sentence structure, making it clear that the norms (not the algorithms) are being combined.}]{Subsequently, we construct a second family of algorithms, dubbed {\it F-Fanions}, based on the duals of convex combinations of Ky Fan k-norms and the Frobenius norm. One prominent member of this family is {\it F-Muon}.}{Then, we consider their convex combinations with the Frobenius norm and corresponding duals to develop a new family of algorithms we name {\it F-Fanions}, one of them being {\it F-Muon}.} We complement our theoretical analysis with an extensive empirical study of the algorithms across a wide range of tasks and settings, from which it follows that properly-tuned F-Muon is almost on par with Muon, \replaced[id=IK, comment={Alex, clarify this please. It's a very non-obvious and non-clear conclusion, especially when knowing that Muon itself only provides an approximate answer to the LMO. I provided an alternative phrase.}]{which questions the necessity of finding the exact solution to the problem in linear optimization oracle framework.}{which shows the flexibility in the choice of the norm and opens possibilities for easier-to-compute variants.}

\end{abstract}

\section{Introduction}

Minimizing loss functions in unprecedentedly high-dimensional spaces has recently become an integral and crucial part in training large language models. Hence, new scalable, time- and memory-efficient algorithms have been demanded. Besides well-known Adam \citep{kingma2014adam} and AdamW \citep{loshchilov2017decoupled}, recently proposed Muon \citep{jordan2024muon} has shown promising results on training very large models \citep{liu2025muon}. Its key difference from Adam and AdamW is that it has been constructed specifically for optimizing functions of weight matrices, which are common in deep learning.

That is what can be said from a practical point of view. \replaced[id=IK, comment={Rewrote this to be more precise. The innovation wasn't just 'using' a norm, but deriving the update from a principled, constrained optimization problem. I think this better reflects the contribution of the Muon paper.}]{From a theoretical perspective, a key innovation of Muon was its principled derivation of the update rule, which emerges as the solution to an optimization problem constrained by the RMS-to-RMS norm (scaled version of spectral norm) \citep{bernstein2025deriving}}{From the perspective of theory, the main innovation of Muon has been an intentional usage of matrix norms, i.e. the spectral norm, to derive the algorithm's update}.

\replaced[id = IK, comment={Subsequent paragraphs in the introduction are very heavily modified.}]{}{}

Motivated by the success of Muon, many generalizations and variations of it were proposed. Among the notable ones are Scion \citep{pethick2025training}, Dion \citep{ahn2025dioncommunicationefficientoptimizerlarge} and Gluon \citep{riabinin2025gluon}. Those researches target Muon's efficiency and establish convergence bounds. One central question, however, remains unanswered:

\textit{In deriving Muon's update step, why constrain by the spectral norm? How would alternative norms affect performance and computational cost?}

In this article, we tackle this question by first showing the connection between matrix norms and corresponding existing algorithms and discuss the theoretic bounds derived for those algorithms. We then leverage the family of norms dual to Ky Fan k-norms to derive a new class of algorithms with low-rank updates, which we call Fanions. Subsequently, we create a second, hybrid family named F-Fanions by constructing convex combinations of the Ky Fan norms with the Frobenius norm and taking dual of that composite norm. Working within the linear minimization oracle (\replaced[id = IK, comment={I think it's better this way.}]{LMO}{lmo}) framework we derive the explicit update formulas for both algorithm families. As it was done with Muon, we stipulate our algorithms to be fast to approximate, for which we utilize the Lanczos algorithm as described in \cref{sec:matrix-side}.

We then compare the performance of the algorithm families across various benchmarks (\cref{sec:experiments}):
\begin{itemize}
	\item Synthetic least squares experiment,
	\item CIFAR-10 airbench,
	\item Pre-training NanoGPT on FineWeb dataset.
\end{itemize}

Our experiments reveal important insights about the role of matrix norms in optimization. On a synthetic least squares problem, we observe a striking discrepancy: while some algorithms converge slowly in terms of loss, they may converge quickly in terms of gradient norm, and vice versa. This suggests that existing theoretical guarantees may not fully explain practical algorithm performance.

Most notably, our experiments on real-world tasks demonstrate that the choice of underlying matrix norm is remarkably flexible. On CIFAR-10 airbench, properly-tuned F-Muon achieves $94.01 \pm 0.14\%$ accuracy, essentially matching Muon's $94.00 \pm 0.13\%$ performance. On NanoGPT pre-training, F-Muon achieves 3.281 cross-entropy loss, only marginally worse than Muon's 3.279. These results show that Muon-like algorithms can maintain competitive performance even when the underlying norm constraint is significantly modified, answering affirmatively the central question posed above. This flexibility suggests potential for developing easier-to-compute variants of successful algorithms like Muon.

\section{\added[id = IK, comment={I think it goes well into one big section.}]{Update Step as Constrained Optimization}}

\subsection{\replaced[id = IK, comment={Merged what previously had been section with two subsections into this one and improved the notation. Also removed the thing about trust region, it'll be referenced in succeeding sections.}]{Linear Minimization Oracle Framework}{Problem Statement}}

Training a neural network involves optimizing a function of weight matrices. For simplicity, we consider the problem of minimizing a differentiable function of a single matrix:
\begin{equation}\label{eq:mat}
	F(\cdot)\colon \Rmn \to \R\,,\qquad F(\mX) \to \min_{\mX \in \Rmn}\,.
\end{equation}
We equip the matrix space $\Rmn$ with a standard dot product $\<\cdot, \cdot> \to \R$ and a norm $\norm{\cdot}\colon \Rmn \to \R_+$, which possibly does not coincide with the Frobenius norm $\normf{\cdot}$. The dual norm $\norm{\cdot}^\dagger\colon \Rmn \to \R_+$ associated with $\norm{\cdot}$ is defined as
\begin{equation}\label{eq:dual_norm}
	\norm{\mG}^\dagger = \sup_{\mD \in \Rmn\,,\norm{\mD}\leq 1} \<\mG,\mD>\,.
\end{equation}

One approach to optimizing such functions is to employ an iterative algorithm based on Linear Minimization Oracle (LMO):
\begin{equation}\label{eq:lmo}
	\mathrm{LMO}(\mG) \in \argmin_{\mD \in \cS} \<\mG, \mD>\,,
\end{equation}
where $\mG$ is typically a gradient (possibly with momentum) of $F$ and $\cS \subset \Rmn$ is some set. The simple update formula then can look like
\begin{equation}\label{eq:simple_update}
	\mX^{k + 1} = \mX^k + \mathrm{LMO}\left(\nabla F\left(\mX^k\right)\right)\,.
\end{equation}

In a typical neural network, the objective function $F$ depends on a set of weight matrices $\{ \mX_1, \mX_2, \ldots \}$. The optimization framework we have described is applied in a layer-wise fashion. At each iteration $k$, a stochastic gradient $g(\mX^k, \xi^k)$ is computed using a mini-batch of data $\xi^k$ via backpropagation. This yields a separate gradient component, $\mG_i^k$, for each matrix $\mX_i$. The LMO-based update rule is then applied to each matrix $\mX_i$ using its corresponding gradient component $\mG_i^k$.

The update rule used in Muon optimizer is uSCG \citep{pethick2025training}. In the most general case, which involves momentum, it can be written as
\begin{equation}\label{eq:usgd_update}
	\begin{aligned}
		\mM^{k}     & = \alpha_{k} g(\mX^k, \xi^k) + (1 - \alpha_k)\mM^{k - 1}\,, \\
		\mX^{k + 1} & = \mX^k + \gamma_k\mathrm{LMO}(\mM^{k})\,.
	\end{aligned}
\end{equation}

Here we are particularly interested in the case when $\cS$ is a ball in the $\norm{\cdot}$ norm:
\begin{equation}\label{eq:ball}
	\cS = \cB_\eta = \{ \mD \in \Rmn \mid \norm{\mD} \leq \eta \}\,.
\end{equation}
In this case it can be easily shown that
\begin{equation} \label{eq:lmo_in_norm_ball}
	\argmin_{\mD \in \cS} \<\mG, \mD> = - \eta \{ \mD \in \cB_1 \mid \<\mG, \mD> = \norm{\mG}^\dagger\}\,.
\end{equation}
And update for $\mX$ in \cref{eq:usgd_update} becomes
\begin{equation}\label{eq:our_update}
	\mX^{k+1} = \mX^{k} - \gamma_k\eta \{\mD \in \cB_1 \mid \<\mM^k, \mD> = \norm{\mM^k}^\dagger\}\,.
\end{equation}
This formula will later allow us to easily compute updates for various norms.

\subsection{Different Norms $\norm{\cdot}$ imply different updates}

In this subsection we will derive and discuss updates induced by the choice of the norm in \cref{eq:our_update}. To describe those updates, the singular value decomposition (SVD) of $\mM^k$ is required. We denote SVD of $\mM^k$ as $\mM^k = \mU \mSigma \mV^\top$, where $\mU = [u_1, u_2, \dots, u_r]$, $\mSigma = \diag(\sigma_1, \sigma_2, \dots, \sigma_r)$, and  $\mV = [v_1, v_2, \dots, v_r]$.

\subsection{\texorpdfstring{$\normf{\mM^{k}}$ and Normalized SGD}{NSGD}}

\begin{lemma}\label{lemma:nsgd_update}
	When $\norm{\cdot} = \normf{\cdot}$, \cref{eq:our_update} turns into:

	\begin{equation}\label{eq:nsgd_update}
		\mX^{k+1} = \mX^{k} - \eta \frac{\mM^k}{\normf{\mM^k}}
	\end{equation}

\end{lemma}

The result is the same as in (Table 1, \citet{pethick2025training}), but here we use the Euclidean norm as $\normf{\mM^k} = \sqrt{\sum_{i=1}^{\min\{m,n\}}\sigma_i^2}$, which clearly is a matrix norm.

\subsection{\texorpdfstring{$\norms{\mM^{k}}$ and Muon}{Muon}}
\begin{lemma}\label{lemma:muon_update}
	When $\norm{\cdot} = \norms{\cdot}$, \cref{eq:our_update} turns into:

	\begin{equation}\label{eq:muon_update}
		\mX^{k+1} = \mX^{k} - \eta U V^\top
	\end{equation}
\end{lemma}

Though the update is well-known, we provide a much simpler proof in the appendix, when compared to \citet{bernstein2024oldoptimizernewnorm}.

\subsection{\texorpdfstring{$\normn{\mM^{k}}$ and Neon}{Neon}}
\begin{lemma}\label{lemma:neon_update}
	When $\norm{\cdot} = \normn{\cdot}$, \cref{eq:our_update} turns into:

	\begin{equation}\label{eq:neon_update}
		\mX^{k+1} = \mX^{k} - \eta u_1 v_1^\top
	\end{equation}
\end{lemma}
We name the derived algorithm \emph{Neon}. In the section {\it The Matrix side of updates}, we will discuss how to compute the update efficiently.

\subsection{\texorpdfstring{$\normfstar{\mM^{k}}^\dagger$ and F-Muon}{F-Muon}}
We define $\normfstar{\cdot}$ as a convex combination of $\normn{\cdot}$ and $\normf{\cdot}$:

\begin{equation}\label{eq:fstar_norm}
	\normfstar{\mX} = \alpha \normn{\mX} + (1-\alpha)\normf{\mX},
\end{equation}

where $\alpha \in [0, 1]$ defines a specific norm from the $F*$-family. $\normfstar{\cdot}^\dagger$ can be expressed as in \cref{eq:normfstardual}. However, its norm ball is a simple Minkowski sum of $\alpha \normn{\cdot}$ and $(1-\alpha)\normf{\cdot}$ balls \cref{fig:fstardual}.

\begin{lemma}\label{lemma:nsgd_muon_update}
	When $\norm{\cdot} = \normfstar{\cdot}^\dagger$, \cref{eq:our_update} turns into:

	\begin{equation}\label{eq:nsgd_muon_update}
		\mX^{k+1} = \mX^{k} - \eta (\alpha U V^\top + (1-\alpha)\frac{\mM^k}{\normf{\mM^k}})
	\end{equation}
\end{lemma}

We name the derived algorithm \emph{F-Muon}. It turns out that F-Muon is a convex combination of Normalized SGD and Muon. The implications are significant and discussed in the following sections.

\subsection{\texorpdfstring{$\normftwo{\mM^{k}}^\dagger$ and F-Neon}{F-Neon}}
We define $\normftwo{\cdot}$ as a convex combination of $\norms{\cdot}$ and $\normf{\cdot}$:

\begin{equation}\label{eq:ftwo_norm}
	\normftwo{\mX} = \alpha \norms{\mX} + (1-\alpha)\normf{\mX},
\end{equation}

where $\alpha \in [0, 1]$ defines a specific norm from the F2-family.$\normftwo{\cdot}^\dagger$ can be expressed as in \cref{eq:normftwodual}. However, its norm ball is a simple Minkowski sum of $\alpha \norms{\cdot}$ and $(1-\alpha)\normf{\cdot}$ balls \cref{fig:ftwodual}.

\begin{lemma}\label{lemma:nsgd_neon_update}
	When $\norm{\cdot} = \normftwo{\cdot}^\dagger$, \cref{eq:our_update} turns into:

	\begin{equation}\label{eq:nsgd_neon_update}
		\mX^{k+1} = \mX^{k} - \eta (\alpha u_1 v_1^\top + (1-\alpha)\frac{\mM^k}{\normf{\mM^k}})
	\end{equation}
\end{lemma}

We name the derived algorithm \emph{F-Neon}. It turns out that F-Neon is a convex combination of Normalized SGD and Neon. The implications are significant and discussed in the following sections.


\subsection{\texorpdfstring{$\normkfk{\mM^k}^\dagger$ and Muon, Neon, and Centralized Dion without error feedback}{Muon, Neon, and Centralized Dion without error feedback}}
We remind the reader that the Ky Fan k-norm (\citet{bhatia2013matrix}, p. 92), which we denote as $\normkfk{\cdot}$, is the sum of the k largest singular values of the matrix. It can be proved that $\normkfk{\cdot}^\dagger = \max\{\frac{1}{k} \normn{\cdot}, \norms{\cdot}\}$ (see \citet{bhatia2013matrix}, p. 96). Special cases of the Ky Fan k-norm are the Ky Fan 1-norm, which is the sprectral norm, and the Ky Fan $\min\{m, n\}$-norm, which is the nuclear norm.

\begin{lemma}\label{lemma:ky_fan_update}
	When $\norm{\cdot} = \normkfk{\mM^t}^\dagger$, \cref{eq:our_update} turns into:

	\begin{equation}\label{eq:ky_fan_update}
		\mX^{t+1} = \mX^{t} - \eta \sum_{i=1}^{k}u_i v_i^\top
	\end{equation}
\end{lemma}
In the section {\it The Matrix side of updates}, we will discuss how to compute the updates efficiently.

We introduce the family of {\it Fanions}, which consists of {\it Fanion-$k$}, lmo-based algorithms under the $\normkfk{\mM^t}^\dagger$ norms. By this terminology and the lemma, Muon is a Fanion-$\min\{n,m\}$, while Neon is an Fanion-1. Moreover, the centralized version of rank-$r$ Dion \citep{ahn2025dioncommunicationefficientoptimizerlarge} without the error feedback and without scaling of the update, from the perspective of lmo, is actually a Fanion-$r$.

In addition, one can consider F-KF-k-norm: $\norm{\cdot}_{\mathrm{F-KF-k}}$ = $\alpha \normkfk{\cdot} + (1-\alpha)\normf{\cdot}$, the dual to which will produce algorithms like F-Dion without the error feedback. The resulting family can be named {\it F-Fanions}.

\subsection{Algorithms for Matrices $\leftrightarrow$ Algorithms for Vectors}

lmo optimizers in Schatten $S_p$ norms and in $l_p$ norms with common $p$ may be analogous to each other, as is illustrated by \cref{tbl:mat_vs_vec_lmo}. The analogies go beyond similarities in the updates. SignSGD is very close to Adam, as noted in \citet{bernstein2024oldoptimizernewnorm}, and both Adam and Muon perform well in training large models. NSGD is the same for both matrix and vector cases. Greedy Coordinate Descent methods are not applied to high-dimensional problems, from this perspective, it is not surprising that Neon underperforms on such problems.

\deleted[id = IK, comment={This is essentially like saying uh oh the optimization in multy-dimensional space is so hard, why there is no algorithm to reduce this problem to 1-d case.}]{Despite such similarities, no theory has been yet proposed that would reduce the matrix case to the optimization of a function of a singular values vector. If such a theory is developed, analysis of matrix algorithms like Muon will be greatly simplified.}

\begin{table*}[t]
	\caption{lmo optimizers in Schatten $S_p$ norms and in $l_p$ norms. $g$ is the gradient. When it is a matrix, $g = \mU \mSigma \mV^\top$}
	\label{tbl:mat_vs_vec_lmo}
	\bgroup
	\def\arraystretch{1.2}
	\resizebox{\textwidth}{!}{
		\begin{tabular}{|l|c|c|c|}
			\hline
			Method                             & lmo constraint set $\mathcal D$ & lmo                                                         & Reference                            \\
			\hline
			\hline
			Normalized SGD                     & $l_2$-ball, $S_2$-ball          & $-\eta \tfrac{g}{\norm{g}_2} = -\eta \tfrac{g}{\norm{g}_F}$ & \citep{hazan2015beyond}              \\
			Momentum Normalized SGD            & Ball in $l_2$, or Ball in $S_2$ & $-\eta \tfrac{g}{\norm{g}_2} = -\eta\tfrac{g}{\norm{g}_F}$  & \citep{cutkosky2020momentum}         \\
			\hline
			SignSGD                            & Ball in Max-norm $l_\infty$     & $-\eta \sign(g)$                                            & \citep[Thm. 1]{bernstein2018signsgd} \\
			Signum                             & Ball in Max-norm $l_\infty$     & $-\eta \sign(g)$                                            & \citep[Thm. 3]{bernstein2018signsgd} \\
			\hdashline
			Muon                               & Ball in Spectral $S_\infty$     & $-\eta UV^\top$                                             & \citep{jordan2024muon}               \\
			\hline
			Gauss-Southwell Coordinate Descent & Ball in $l_1$                   & $-\eta \sum_{i \in \argmax|g_i^t|} \sign(g_i^t)e_i$                       & \citep[p. 19]{shi2016primer}         \\
			\hdashline
			Neon                               & Ball in Nuclear $S_1$           & $-\eta u_1 v_1^\top$                                        & This work                            \\
			\hline
		\end{tabular}
	}
	\egroup
\end{table*}


\section{Matrix side of updates}\label{sec:matrix-side}
To compute the algorithms' updates, we use thick-restarted Lanczos method for singular value problem (TRLan) on $\mM^{k\top}\mM^k$ or $\mM^k \mM^{k\top}$ matrices (the one with less size is picked), implemented in CuPy lSibrary \citep{cupy_svds_ref} and described in \citet{simonz1998thick}.

This method is designed for efficiently approximating the largest singular values and vectors of large matrices. Its thick-restart strategy retains the most informative Ritz vectors at each cycle, which accelerates convergence while avoiding excessive memory growth. We are specifically interested in this algorithm because it allows us to extract several largest singular values and related singular vectors of the matrix to make a Neon step. Moreover, TRLan is stable GPU-friendly algorithm because it mainly operates with matrix-vector multiplications (MVs), which are higly-parallel, and does not require full reorthogonalization against the whole Krylov basis by managing  short recurrent formulas and incorparating thick restarting.

Per-cycle complexity is $O(mn^2 + n^2k + nk^2)$, where m and n are dimensions of the target matrix and n is the smaller one, k is retained subspace's size.

In \cref{tbl:matrix_methods}, we compare performance of TRLan, RSVD, and power iterations on calculating k-rank update, which is used in k-Fanion. The results highlight that TrLan is much faster that its competitors.

During the research it was noted that RSVD can give good and fast approximation for singular values, but the matrix of approximation is far from the one given by truncated SVD, while TRLan gives good and fast approximation of a matrix, but not so good approximation for singular values. That means that TRLan may be not a perfect choice for algorithms like Dion, where $\sigma_i$ are required for error feedback.

The practical drawback of the TRLAn is the absence of its implementation in PyTorch.
\begin{table}[h!]
	\centering
	\begin{tabular}{c|c|c|c}
		\hline
		Method                              & rtol & k   & time,s \\
		\hline
		Power Iterations                    & 0.01 & 1   & 7.7    \\
		SVDS (thick-restart Lanczos method) & 0.01 & 1   & 0.18   \\
		PCA Low Rank (RSVD)                 & 0.01 & 1   & 1.15   \\
		SVDS (thick-restart Lanczos method) & 0.01 & 10  & 0.47   \\
		PCA Low Rank (RSVD)                 & 0.01 & 10  & 19.4   \\
		SVDS (thick-restart Lanczos method) & 0.01 & 100 & 1.96   \\
		PCA Low Rank (RSVD)                 & 0.01 & 100 & 170    \\
	\end{tabular}
	\caption{k-rank updated comparison}
	Comparison of different numerical methods to calculate k-rank update on $5000\times5000$ matrix of real numbers, rtol is an error in Frobenius norm relative to the k-rank approximation.
	\label{tbl:matrix_methods}
\end{table}

\section{Experiments}\label{sec:experiments}
\subsection{Randomized Linear Least Squares}
We first compare F-Fanions on the following L-smooth problem:

\begin{equation}\label{eq:lls}
	F(\mX) = \frac{1}{2} \<(\mX-\mS), \mM (\mX - \mS) \mN> \to \min_{\mX \in \Rmn}\
\end{equation}

where $\mX \in \Rmn$, $m=500$ and $n=500$ are typical dimensions of a neural network weight matrix, $S \in \Rmn$, $\mM \in \mathbb{S}^m_{+}$ and $\mN \in \mathbb{S}^n_{+}$ are positive-semidefinite matrices. The spectra of $\mM$ and $\mN$ are uniformly distributed in the (0, 1) interval. We set $\mS=0$, and $\mX^0 \sim 0.1 \gN(0, \mI)$.

We run different Fanions and their respective F-Fanions with $\alpha=1/2$: Neon (Fanion-1), Fanion-2, Fanion-5, Fanion-100, and Muon (Fanion-500). We test them against Normalized SGD, which is also a F-Fanion with $\alpha = 0$ and an arbitrary $k$.

Since theoretical bounds \citep{kovalev2025understanding, riabinin2025gluon} rely on a very loose norm bound $\norm{\cdot}\le \rho \normf{\cdot}$, we do not derive learning rate from smoothness. Rather, we set the learning rate of $1/\sqrt{T}$ for each optimizer and the momentum of 0.6. 12 000 iterations for each algorithm ensure that the rate at some point gets low enough for each algorithm to converge. The results are presented in \cref{fig:lls} and ~\cref{sec:lls_plots_section}.

In terms of the loss minimization, F-Muon had the fastest convergence. In terms of the Frobenius norm of the gradient, the lowest was observed for NSGD. Then go F-Neon (F-Fanion-1), F-Fanion-2, F-Fanion-5, F-Fanion-100, and F-Muon (Fanion-500). It is noticeable that all F-Fanions have lower Frobenius norms of the gradients than their respective Fanioins. The same behavior will be recorded in the next section.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[t]{0.8\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/simple_lls/loss_vs_iteration_500x500.pdf}
        \caption{The loss}
        \label{fig:lls_loss}
    \end{subfigure}
    \vspace{1em} % optional: add vertical spacing between subfigures
    \begin{subfigure}[t]{0.8\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/simple_lls/gradient_frobenius_norm_vs_iteration_500x500.pdf}
        \caption{The Frobenius norm of the gradient}
        \label{fig:lls_fro_grad_norm}
    \end{subfigure}
    \caption{Linear least squares problem for a 500x500 matrix}
    \label{fig:lls}
\end{figure}



\subsection{CIFAR-10 airbench}
We adapt Keller Jordan's code to test F-Muon, Neon, and F-Neon on the CIFAR-10 airbench \citep{cifar2023airbench}.
First, we run F-Muon for different $\alpha$ with the same {\tt lr=0.24(1 - step/total\_steps), momentum=0.6, nesterov=True}, as have been finetuned by Jordan, 10 repetitions for each $\alpha$. We record the accuracy after 8 epochs of training. The results are in \cref{fig:muon_alphas}.

Then we tune F-Muon with $\alpha = 0.5$. Tuned parameters are {\tt lr=0.4(1 - step/total\_steps), momentum=0.65, nesterov = True}. While Muon reaches $94.00 \pm 0.13$\% accuracy after 8 epochs, tuned F-Muon reaches $94.01 \pm 0.14\%$ after 8 epochs (average was done by 200 iterations).

Finally, we take this set of tuned parameters and test on different $\alpha$, 5 times for each $\alpha$. The results are in \cref{fig:fmuon_alphas}. We notice that even when $\alpha=0.1$, the accuracy is much higher than in case of the pure NSGD.

\begin{figure}[h!]
	\centering
	\begin{subfigure}[b]{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figs/muon_tuned_diff_alpha.pdf}
		\caption{With parameters tuned for Muon}
		\label{fig:muon_alphas}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figs/fmuon_tuned_diff_alpha.pdf}
		\caption{With parameters tuned for F-Muon}
		\label{fig:fmuon_alphas}
	\end{subfigure}
	\caption{Mean accuracies for different $\alpha$ of F-Muon.}
	\label{fig:diff_alphas}
\end{figure}

The results are curious and could be represented by \cref{fig:cifar_ball}: lmo ball, which we plotted in a 2D space of singular values, has drastically changed, but the observed convergence after tuning has not degraded. These observations raise the question of how much lmo-based algorithms are sensitive to the constraint area, i.e. what will happen if the ball is corrupted. In this particular example, we have had a blurred ball, which proved as robust as the original ball.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\textwidth]{figs/norms/fstardual_cifar.pdf} % adjust width as needed
	\caption{Visualization of lmo balls for Muon and F-Muon.}
	\label{fig:cifar_ball}
\end{figure}

The most pathological case is \replaced[id=IK, comment={In the pictures there are no experiments with $\alpha < 0$, but there is a one with $\alpha $}]{$\alpha > 1$}{$\alpha < 0$}, which corresponds to the lmo with an area that is not a norm ball! Despite this violation, the mixture of algorithms reaches almost the same accuracy as vanilla Muon.

\subsection{NanoGPT speedrun}
We test F-Muon on NanoGPT speedrun \citep{modded_nanogpt_2024}. For $\alpha = 0.5$, the optimal parameters are {\tt lr=0.07, momentum=0.95}, while for Muon they were  {\tt lr=0.05, momentum=0.95}. After testing for 1750 steps, as it has been done on the speedruns, we get 3.281 cross-entropy loss, while on Muon, it falls below the target threshold 3.28 reaching 3.279. However, this difference is negligible, if one looks at \cref{fig:gpt_loss}. It is even more striking, considering the fact that F-Muon is an average of Muon and NSGD, and the later performed quite poorly.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{figs/nanogpt/val_loss_vs_step.pdf}
	\caption{The validation loss for NanoGPT}
	\label{fig:gpt_loss}
\end{figure}

Again, as on the CIFAR airbench, if we set $\alpha=-0.1$, F* to which is not a norm, we get a 3.2818 loss, which is not higher than for $\alpha=0.3$, for example.

\section{Related Work}
As Muon \citep{jordan2024muon} is a very successful and popular optimizer for functions of weight matrices, a lot of reseach has been put into, first, further improving its performance, and, second, in explaining its success.

	{\bf Improvements of Muon.} Regarding the first point, in less than a year, a large number of applications and improvements of Muon has been proposed. \citet{liu2025muon} adapted the algorthm for training language models larger than NanoGPT. \citet{shah2025practical} organized efficient hyperparameter transfer by combining Muon with maximal update parametrization. To construct their COSMOS optimizer, \citet{chen2025cosmoshybridadaptive} applied computationally intensive updates of SOAP optimizer to a low-dimensional "leading eigensubspace" while using memory-efficient methods like Muon for the remaining parameters. \citet{amsel2025polar} proposed a more efficient alternative to Newton-Schulz operations. \citet{si2025adamuon} introduced AdaMuon which combines element-wise adaptivity with orthogonal updates. We suppose that the described above or similar techniques can be applied to our optimizers as well. For example, F-Muon also benefits from faster alternatives to Newton-Schulz iterations, and Neon may be a great substitute to Muon in COSMOS, because, as we have shown in {\it the Matrix side of the updates}, Lanczos algorithms is much faster than Newton-Schulz iterations on large matrices.

	{\bf Theory behind Muon.} Regarding the second point, there has been a prolonged gap in theory behind Muon, simplistic derivation of \citet{bernstein2025deriving} based on \citet{bernstein2024oldoptimizernewnorm} excluded. This gap, as it seems to us, is not even now completely closed. For example, \citet{kovalev2025understanding} has provided convergence guarantees of Muon in various settings, from which, however, Muon's supremacy cannot be recovered. Indeed, although the obtained bounds depend on the norm choice, the asymptotics of the convergence remain the same as for NSGD and other optimizers, $K = \cO(\epsilon^{-4})$ in a L-smooth stochastic case.

Similar drawback has a recent article \citet{riabinin2025gluon}, where L-smoothness assumption is replaced with a more practical $(L_0, L_1)$-smoothness. The authors derived from their theorems optimal stepsizes for Muon and Scion that match fine-tuned stepsizes reported by \citet{pethick2025training}. But still, they did not explain why, for instance, NSGD is inferior to Muon in training large-language models.

We suppose that the reason for the recorded by us discrepancy between Neon and Muon performance, both of which are described by Scion or Gluon frameworks, lies in the structure of the norm ball, which must be an object of further research.

	{\bf The nuclear norm in lmo.} As we found out only when writing this article, the nuclear norm has been already explored in the context of the linear minimization oracle. \citet{pethick2025sam} applied it to create $\nu$SAM, a new sharpness-aware minimization technique. The update from their Lemma 3.1 coincides with the update of Neon, but is used for completely different purposes. In addition, \citet{pethick2025sam} use power iterations to find $u_1$ and $v_1^\top$, while we suggest utilizing much more efficient and precise Lanczos algorithm.

	{\bf The Ky Fan Norm and Dion.} Rank-$k$ Centralized Dion, Algorithm 1 from \citet{ahn2025dioncommunicationefficientoptimizerlarge}, without an error feedback and scaling of the update, turns out to be an lmo-based algorithm under the $\normkfk{\cdot}^\dagger$ norm, which we described in {\it Different norms imply different updates}. Reported by the authors necessity of using error feedback to obtain satisfactory accuracy may take place in the cases of our algorithms as well, for example, F-Neon. This we leave to future research.

\section{Conclusion}
In this article, we have generalized several successful algorithms, like Muon and Dion, to the lmo-based algorithms in the $\normkfk{\cdot}^\dagger$ norm. Also we have proposed the technique of ``regularizing'' the updates with NSGD, a trick to increase the robustness of the algorithms, which is motivated by the consideration of the $\normfstar{\cdot}$, $\normftwo{\cdot}$, and general $\norm{\cdot}_{\mathrm{F-KF-k}}$ norms. Generalizations of well-known norms and subsequent combinations of them may further improve performance of lmo-based algorithms. If a theory is developed that explains the discrepancy between performance of different algorithms based on the matrix norms, one will probably be able to intentionally construct norms optimal to given architectures and probably even their parameters.

\section{Author Contributions}
IO suggested using the nuclear norm in the \citet{bernstein2024oldoptimizernewnorm} framework. DM supervised the project and helped with editing the article. IK, NK, and AV participated mainly on the first stage of research, when it has been a project in the optimization course at MIPT. IK suggested using composite norms (though not F2 and F*) and KKT conditions to find the resulting updates. NK suggested Lanczos algorithm as the fastest tool to compute Neon's updates and conducted experiments to prove it. AV tested Neon on the finetuning of NanoGPT. All other work was done by AK.

\bibliography{icomp2024_conference}
\bibliographystyle{icomp2024_conference}

\appendix


\section{Norms $\normfstar{\cdot}^\dagger$ and $\normftwo{\cdot}^\dagger$}
First, we need a well-known fact mentioned, for example, in \citet[Table 1]{yu2012arithmetic}. For the sake of completeness, we provide a proof of the fact.

\begin{lemma}\label{lemma:dual_to_conv_comb}
	Let $\norm{\cdot}_{(1)}$ and $\norm{\cdot}_{(2)}$ be norms on a finite-dimensional Euclidean space, and let $\alpha,\beta \geq 0$. Define
	$$
		\norm{x} := \alpha \norm{x}_{(1)} + \beta \norm{x}_{(2)}.
	$$
	Then the dual unit ball of $\norm{\cdot}$ satisfies
	$$
		B_{\norm{\cdot}^\dagger}
		= \alpha B_{\norm{\cdot}_{(1)}^\dagger} + \beta B_{\norm{\cdot}_{(2)}^\dagger},
	$$
	where $+$ denotes the Minkowski sum and $B_{\norm{\cdot}_{(i)}^\dagger}$ is the unit ball of the dual norm $\norm{\cdot}_{(i)}^\dagger$.
\end{lemma}

\begin{proof}
	Write $f(x) = \alpha \norm{x}_{(1)}$ and $g(x) = \beta \norm{x}_{(2)}$, so
	$$
		\norm{x} = f(x)+g(x).
	$$
	Recall two standard facts:
	\begin{enumerate}
		\item For any norm $\norm{\cdot}$ and $\lambda>0$,
		      $$
			      (\lambda \norm{\cdot})^*(y) =
			      \sup_{x}\bigl( \langle y,x \rangle - \lambda \norm{x}\bigr)
			      = \delta_{\lambda B_{\norm{\cdot}^\dagger}}(y),
		      $$
		      i.e., the indicator function of the scaled dual ball.
		\item The Fenchel conjugate of a sum satisfies
		      $$
			      (f+g)^*(y) = \inf_{u+v=y} \bigl(f^*(u) + g^*(v)\bigr).
		      $$
	\end{enumerate}
	Applying these to $f$ and $g$, we have
	$$
		f^*(u) = \delta_{\alpha B_{\norm{\cdot}_{(1)}^\dagger}}(u),
		\quad
		g^*(v) = \delta_{\beta B_{\norm{\cdot}_{(2)}^\dagger}}(v).
	$$
	Thus
	$$
		\norm{\cdot}^*(y)
		= (f+g)^*(y)
		= \inf_{u+v=y}
		\bigl(
		\delta_{\alpha B_{\norm{\cdot}_{(1)}^\dagger}}(u)
		+
		\delta_{\beta B_{\norm{\cdot}_{(2)}^\dagger}}(v)
		\bigr)
		=
		\delta_{\alpha B_{\norm{\cdot}_{(1)}^\dagger}+\beta B_{\norm{\cdot}_{(2)}^\dagger}}(y).
	$$
	But by definition, the conjugate of a norm is exactly the indicator of its dual unit ball:
	$$
		\norm{\cdot}^*(y) = \delta_{B_{\norm{\cdot}^\dagger}}(y).
	$$
	Therefore,
	$$
		B_{\norm{\cdot}^\dagger} = \alpha B_{\norm{\cdot}_{(1)}^\dagger} + \beta B_{\norm{\cdot}_{(2)}^\dagger}.
	$$
\end{proof}

Consequently,
\begin{equation}\label{eq:dual_to_conv_comb}
	\norm{y}^\dagger
	= \inf\bigl\{ t \ge 0 : y \in t\bigl(\alpha B_{\norm{\cdot}_{(1)}^\dagger} + \beta B_{\norm{\cdot}_{(2)}^\dagger}\bigr) \bigr\}
	= \inf_{z,t}\bigl\{t \ge 0: z \in t \alpha B_{\norm{\cdot}_{(1)}^\dagger}, y - z \in t \beta B_{\norm{\cdot}_{(1)}^\dagger}\bigr\}
\end{equation}

Thus, we immediately find $\normfstar{\cdot}^\dagger$, which is related to F-Muon update. Indeed, after setting $\beta=1-\alpha$ and remembering that for smooth and bounded cases we can use $\min$ instead of $\inf$, we get
\begin{equation}\label{eq:normfstardual}
	\normfstar{\mY}^\dagger = \min_{\mZ} \min_t\{t, s.t. \norms{\mZ}\leq \alpha t, \normf{\mY-\mZ}\leq (1-\alpha) t\}
\end{equation}

If $\alpha = 1$, then $\mZ = \mY$, and we get $\normfstar{\mY}^\dagger = \norms{\mY}$. If $\alpha = 0$, then $\mZ = 0$, and we get $\normfstar{\mY}^\dagger = \normf{\mY}$.

Similarly, we find $\normftwo{\cdot}^\dagger$, which is related to F-Neon update:
\begin{equation}\label{eq:normftwodual}
	\normftwo{\mY}^\dagger = \min_{\mZ} \min_t\{t, s.t. \normn{\mZ}\leq \alpha t, \normf{\mY-\mZ}\leq (1-\alpha) t\}
\end{equation}

If $\alpha = 1$, then $\mZ = \mY$, and we get $\normftwo{\mY}^\dagger = \normn{\mY}$. If $\alpha = 0$, then $\mZ = 0$, and we get $\normftwo{\mY}^\dagger = \normf{\mY}$.


\section{Visualization of different matrix norms}
\subsection{Duals to F* and F2 norms}

It follows from \cref{lemma:dual_to_conv_comb} that the norm ball in $\normfstar{\cdot}^\dagger$ is the Minkowski sum of the norm ball in $\alpha\normn{\cdot}$ and $(1-\alpha)\normf{\cdot}$ and the norm ball in $\normftwo{\cdot}^\dagger$ is the Minkowski sum of the norm ball in $\alpha\norms{\cdot}$ and $(1-\alpha)\normf{\cdot}$.

In Fig. \cref{fig:fduals} we plot these norms. On x-axis and y-axis, there are singular values $\sigma_1$, $\sigma_2$ respectively of a matrix from $\Rmn$ with $\min\{m,n\}=2$.

\begin{figure}[h!]
	\centering
	\begin{subfigure}[b]{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figs/norms/fstardualball.pdf}
		\caption{lmo balls for F-Muon for different $\alpha$}
		\label{fig:fstardual}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figs/norms/ftwodualball.pdf}
		\caption{lmo ball for F-Neon for different $\alpha$}
		\label{fig:ftwodual}
	\end{subfigure}
	\caption{Balls in the duals to F* and F2 norms for different $\alpha$}
	\label{fig:fduals}
\end{figure}

\subsection{The Ky Fan norm and its dual}
1-balls in $l_\infty$, $l_1$ and $l_2$ norms are well-known from textbooks. But what about the Ky Fan $k$-norm? How can it be represented?

To showcase the complex structure of the Ky Fan $k$-norm and its dual, we suggest the illustrations \cref{fig:kyfan_combined} with the ball in the Ky Fan $2$-norm in \cref{fig:kyfan} and its dual in \cref{fig:kyfandual}. On x-, y-, and z-axes, there are singular values $\sigma_1$, $\sigma_2$, and $\sigma_3$ respectively of a matrix from $\Rmn$ with $\min\{m,n\}=3$. In this particular case, we do not sort the singular values. In the proposed representation, we actually plot balls in the Top-$2$ norm $\max\{\abs{x}+\abs{y}, \abs{x}+\abs{z}, \abs{y}+\abs{z}\}$ and its dual norm $\max\{\max(\abs(x),\abs{y},\abs{z}), \frac{1}{2} (\abs{x}+\abs{y}+\abs{z})\}$. The resulting balls are much more complex than balls in $l_\infty$, $l_1$ and $l_2$ norms.

In fact, those balls can be described easier if we use the results from \citet{yu2012arithmetic}. The Ky Fan $2$-norm ball is an intersection of three $l_1$ balls in $(x,y)$, $(x, z)$, and $(y,z)$ spaces. The 1-ball in the dual Ky Fan $2$-norm is an intersection of 1-ball the in $l_\infty$ norm and $\frac{1}{2}$-ball in the $l_1$ norm.

\begin{figure}[h!]
	\centering
	\begin{subfigure}[b]{0.45\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figs/norms/KyFan.pdf}
		\caption{Ky Fan 2-norm}
		\label{fig:kyfan}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.45\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figs/norms/KyFanDual.pdf}
		\caption{Dual to Ky Fan 2-norm}
		\label{fig:kyfandual}
	\end{subfigure}
	\caption{Ky Fan 2-norm and its dual}
	\label{fig:kyfan_combined}
\end{figure}


\section{Updates derivations}

Proof of \cref{lemma:muon_update} follows from \cref{eq:nsgd_muon_update} with $\alpha=1$. Indeed, $\norms{\cdot}^\dagger=1\cdot\normn{\cdot} + 0 \cdot \normf{\cdot} $.

Proof of \cref{lemma:nsgd_muon_update}:
Since $\norm{\cdot}^\dagger = \normfstar{\cdot}^{\dagger\dagger} = \normfstar{\cdot}$, the goal is to reach $\normfstar{\mM^k} = \alpha \tr{\mSigma} + (1-\alpha) \normf{\mM^k}$.

Let us note that $\Delta = \alpha \mU \mV^\top + (1-\alpha) \frac{\mM^k}{\normf{\mM^k}}$ delivers this value. Indeed, by the trace property, $\<\mM^k, \Delta> =\<\mU \mSigma \mV^\top, \alpha \mU \mV^\top + (1-\alpha) \frac{\mU \mSigma \mV^\top}{\normf{\mM^k}}> = \alpha \tr \mSigma + (1-\alpha) \normf{\mM^k} = \normfstar{\mM^k}$, which completes the proof.

Proof of \cref{lemma:neon_update} follows from \cref{eq:nsgd_neon_update} with $\alpha=1$. Indeed, $\normn{\cdot}^\dagger=1\cdot\norms{\cdot} + 0 \cdot \normf{\cdot} $.

Proof of \cref{lemma:nsgd_neon_update}:
Since $\norm{\cdot}^\dagger = \normftwo{\cdot}^{\dagger\dagger} = \normftwo{\cdot}$, the goal is to reach $\normftwo{\mM^k} = \alpha \sigma_1 + (1-\alpha) \normf{\mM^k}$.

Let us note that $\Delta = \alpha (u_1 v_1^\top) + (1-\alpha) \frac{\mM^k}{\normf{\mM^k}}$ delivers this value. Indeed, by the trace property and singular vectors orthogonality, $\<\mM^k, \Delta> =\<\mU \mSigma \mV^\top, \alpha u_1 v_1^\top + (1-\alpha) \frac{\mU \mSigma \mV^\top}{\normf{\mM^k}}> = \alpha \tr \diag({\sigma_1, 0, \dots, 0}) + (1-\alpha) \normf{\mM^k} = \normftwo{\mM^k}$, which completes the proof.


Proof of \cref{lemma:ky_fan_update}:
Since $\norm{\cdot}^\dagger = \normkfk{\cdot}^{\dagger\dagger} = \normkfk{\cdot}$, the goal to reach $\normkfk{\mM^t}$.

Let us note that $\Delta = \sum_{i=1}^{k} u_i v_i^\top$ delivers the value. Indeed, $\<\mM^t, \Delta> =\<\mU \mSigma \mV^\top, \sum_{i=1}^{k} u_i v_i^\top> = \sum_{i,j=1}^{r, k}\<u_i \sigma_i v_i^\top, u_j v_j^\top> = \sum_{i=1}^{k}\sigma_i = \normkfk{\mM^t}$, which completes the proof.

\section{More plots for Linear Least Squares}
\label{sec:lls_plots_section}

\begin{figure}[h!]
    \centering
    \begin{subfigure}[t]{0.6\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/simple_lls/gradient_spectral_norm_vs_iteration_500x500.pdf}
        \caption{The spectral norm of the gradient}
        \label{fig:lls_op_grad_norm}
    \end{subfigure}
    \vspace{1em} % optional: add vertical spacing between subfigures
    \begin{subfigure}[t]{0.6\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/simple_lls/gradient_nuclear_norm_vs_iteration_500x500.pdf}
        \caption{The nuclear norm of the gradient}
        \label{fig:lls_nuclear_grad_norm}
    \end{subfigure}
    \vspace{1em} % optional: add vertical spacing between subfigures
    \begin{subfigure}[t]{0.6\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/simple_lls/loss_vs_time_500x500.pdf}
        \caption{The loss over time}
        \label{fig:lls_loss_time}
    \end{subfigure}
    \caption{More images for Linear least squares problem for a 500x500 matrix}
    \label{fig:app_lls}
\end{figure}

\section{Technical details of the experiments}
We compared TRLand with other methods in Google Colab. We used RTX A4000 for CIFAR airbench.


\end{document}