\documentclass{article} % For LaTeX2e
\usepackage{icomp2024_conference,times}

% Optional math command from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath,amssymb}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{changes}
\definechangesauthor[name={Ivan Kozyrev}, color=orange]{IK}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newcommand{\norm}[1]{\lVert #1\rVert}
\newcommand{\abs}[1]{\lvert #1 \rvert}


%\title{From Muon to Neon: Introducing Nuclear Norm to Large Matrices}

\title{The Ky Fan Norms and Beyond: Dual Norms and Combinations for Matrix Optimization}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \icompfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Alexey Kravatskiy \\
Moscow Institute of Physics and Technology (MIPT) \\
\texttt{kravtskii.aiu@phystech.edu} \\
\And
Ivan Kozyrev \\
Moscow Institute of Physics and Technology (MIPT), \\
Marchuk Institute of Numerical Mathematics \\
\texttt{kozyrev.in@phystech.edu} \\
\And
Nikolai Kozlov \\
Moscow Institute of Physics and Technology (MIPT) \\
\texttt{kozlov.na@phystech.edu} \\
\And
Alexander Vinogradov \\
Moscow Institute of Physics and Technology (MIPT) \\
\texttt{vinogradov.am@phystech.edu} \\
\And
Daniil Merkulov \\
Moscow Institute of Physics and Technology (MIPT), Skoltech, HSE, AI4Science \\
\texttt{daniil.merkulov@phystech.edu} \\
\And
Ivan Oseledets \\
AIRI, Skoltech \\
\texttt{i.oseledets@skoltech.ru}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\icompfinalcopy % Uncomment for camera-ready version, but NOT for submission.

%------------------------------------------------------------------------------------
% \epsilon is predefined; override to use \varepsilon
\renewcommand{\epsilon}{\varepsilon}
\newcommand{\Rmn}{\R^{m\times n}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\Ed}[2]{\mathbb{E}_{#1}\left[#2\right]}
\usepackage{mathtools}
\DeclarePairedDelimiter{\sqne}{\|}{\|_2^2}
%\DeclarePairedDelimiter{\norme}{\|}{\|_2}
\DeclarePairedDelimiter{\normf}{\|}{\|_\mathrm{F}}
\DeclarePairedDelimiter{\normkfk}{\|}{\|_{\text{KF-}k}}
\DeclarePairedDelimiter{\normfstar}{\|}{\|_\mathrm{F*}}
\DeclarePairedDelimiter{\normftwo}{\|}{\|_\mathrm{F2}}
\DeclarePairedDelimiter{\sqns}{\|}{\|_{\mathrm{2}}^2}
\DeclarePairedDelimiter{\norms}{\|}{\|_{\mathrm{2}}}
\DeclarePairedDelimiter{\sqnn}{\|}{\|_{\mathrm{nuc}}^2}
\DeclarePairedDelimiter{\sqnf}{\|}{\|_{\mathrm{F}}^2}
\DeclarePairedDelimiter{\normn}{\|}{\|_{\mathrm{nuc}}}
\DeclarePairedDelimiter{\normc}{\|}{\|_{\mathrm{C}}}
\def\<#1,#2>{\langle #1,#2\rangle}
\DeclarePairedDelimiter{\dotprod}{\langle}{\rangle}

\usepackage{cleveref}
\usepackage{environ}
\newcounter{aequation}
\NewEnviron{aequation}{\refstepcounter{aequation}$$\BODY\eqno{\text{(A\theaequation)}}$$}
\crefname{aequation}{assumption}{assumptions}
\creflabelformat{aequation}{(#2A#1#3)}

\usepackage{amsmath}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\diag}{diag}

\usepackage{arydshln} % dashed lines
\usepackage[right=5cm, marginparwidth=4.6cm, marginparsep=0.2cm]{geometry} % For comments!

%------------------------------------------------------------------------------------

\begin{document}


\maketitle

\begin{abstract}
	In this article, we explore the use of various matrix norms for optimizing functions of weight matrices, a crucial problem in training large language models. Moving beyond the spectral norm that underlies the Muon update, we leverage the duals to the Ky Fan $k$-norms to introduce a family of Muon-like algorithms we name {\it Fanions}, which happen to be similar to Dion. Then working with the duals of convex combinations of the Ky Fan $k$-norms and the Frobenius norm or the $l_\infty$ norm, we construct the families of {\it F-Fanions} and {\it S-Fanions} respectively. Their most prominent members are {\it F-Muon} and {\it S-Muon}. We complement our theoretical analysis with an extensive empirical study of the algorithms across a wide range of tasks and settings, from which it follows that F-Muon and S-Muon are always on par with Muon and S-Muon, while on fine-tuning of NanoGPT and synthetic linear least squares even better than vanilla Muon.

\end{abstract}

\section{Introduction}

Minimizing loss functions in unprecedentedly high-dimensional spaces has recently become an integral and crucial part in training large language models. Hence, new scalable, time- and memory-efficient algorithms have been demanded. Besides well-known Adam \citep{kingma2014adam} and AdamW \citep{loshchilov2017decoupled}, recently proposed Muon \citep{jordan2024muon} has shown promising results on training very large models \citep{liu2025muon}. Its key difference from Adam and AdamW is that it has been constructed specifically for optimizing functions of weight matrices, which are common in deep learning.

That is what can be said from a practical point of view. From a theoretical perspective, a key innovation of Muon was its principled derivation of the update rule, which emerges as the solution to an optimization problem constrained by the RMS-to-RMS norm (scaled version of the spectral norm) \citep{bernstein2025deriving}

Motivated by the success of Muon, many generalizations and variations of it were proposed. Among the notable ones are Scion \citep{pethick2025training}, Dion \citep{ahn2025dion} and Gluon \citep{riabinin2025gluon}. Those works try to explain Muon's efficiency and establish convergence bounds. One central question, however, remains unanswered:

\textit{In deriving Muon's update step, why should one constrain by the spectral or any operator norm? How would alternative norms affect performance and computational cost?}

In this article, we tackle this question by actually showing that there are many viable non-operator norms. We leverage the family of norms dual to Ky Fan $k$-norms to derive a new family of Fanions, algorithms with low-rank updates. This approach theoretically explains the backbone of Dion's update \citep{ahn2025dion} and generalizes the memory-motivated application of the nuclear norm to Sharpness-Aware Minimization \citep{pethick2025sam}. As it was done with Muon, we come up with an effective procedure for computing Fanions' updates, Lanczos algorithm as described in~\cref{sec:matrix-side} is the most effective algorithm, which, however, lacks an effective GPU- and PyTorch-friendly implementation.

Working with duals to conic combinations of dual norms, we consturct the families of F-Fanions and S-Fanions, which are hybrids between Muon and NormalizedSGD and SignSGD, respectively.

We then compare the performance of the algorithm families across various benchmarks (\cref{sec:experiments}):
\begin{itemize}
	\item Synthetic least squares experiment,
	\item CIFAR-10 airbench,
	\item Pre-training NanoGPT on FineWeb dataset.
	\item Fine-tuning NanoGPT on Tiny Stories
\end{itemize}

Our experiments reveal important insights into the role of matrix norms in optimization. First, we show on the example of Neon, a one-rank Fanion, that not every LMO-based algorithm is effective, despite the general bounds of \citet{kovalev2025understanding} and \citet{riabinin2025gluon}.

\textcolor{red}{On a synthetic least squares problem, we observe a striking discrepancy: while some algorithms converge slowly in terms of loss, they may converge quickly in terms of gradient norm, and vice versa}. This suggests that existing theoretical guarantees may not fully explain practical algorithm performance.

Most notably, our experiments on real-world tasks demonstrate that the choice of underlying matrix norm is remarkably flexible. On CIFAR-10 airbench, properly-tuned F-Muon and S-Muon achieve $94.01 \pm 0.14\%$ and $94.02 \pm 0.13\%$ accuracy, essentially matching Muon's $94.01 \pm 0.13\%$ performance. On NanoGPT pre-training, F-Muon achieves 3.281 cross-entropy loss, while fully-tuned Muon achieves 3.279. Finally, S-Muon beats both Muon and F-Muon on fine-tuning of NanoGPT on Tiny Stories. These results show that Muon-like algorithms can maintain competitive performance even when the underlying norm constraint is significantly modified, answering affirmatively the central question posed above. This flexibility suggests potential for developing easier-to-compute variants of successful algorithms like Muon.


\section{Preliminaries: Linear Minimization Oracle Framework}

Training a neural network is essentially an optimization of a function of several weight matrices and a few vectors. Let us start by considering the problem of minimizing a differentiable function of a {\it single} matrix:
\begin{equation}\label{eq:matrix_problem}
	F(\cdot)\colon \Rmn \to \R\,,\qquad F(\mX) \to \min_{\mX \in \Rmn}\,
\end{equation}
We equip the matrix space $\Rmn$ with a standard dot product $\<\cdot, \cdot> \to \R$ and a norm $\norm{\cdot}\colon \Rmn \to \R_+$, which does not have to coincide with the Frobenius norm $\normf{\cdot}$. The dual norm $\norm{\cdot}^\dagger\colon \Rmn \to \R_+$ that is associated with $\norm{\cdot}$ is defined as
\begin{equation}\label{eq:dual_norm}
	\norm{\mG}^\dagger = \sup_{\mD \in \Rmn\,:\norm{\mD}\leq 1} \<\mG,\mD>\,.
\end{equation}

Such problems can be solved with an iterative algorithm based on the Linear Minimization Oracle (LMO):
\begin{equation}\label{eq:lmo}
	\mathrm{LMO}(\mM^k) \in \argmin_{\mD \in \cS} \<\mM^k, \mD>\,,
\end{equation}
where $\mM^k$ is a gradient (or a momentum) of $F$ in $\mX^k$ and $\cS \subset \Rmn$ is some set. The update of the algorithm is defined as follows:
\begin{equation}\label{eq:simple_update}
	\mX^{k + 1} = \mX^k + \gamma_k\mathrm{LMO}\left(\mM^k\right)\,.
\end{equation}

We are particularly interested in the case when $\cS$ is a unit ball in the norm $\norm{\cdot}$:

$$
\cS = \cB_{\norm{\cdot}} = \{ \mD \in \Rmn \mid \norm{\mD} \leq 1 \}\,.
$$
In this case,
$$
\argmin_{\mD \in \cS} \<\mM^k, \mD> = - \{ \mD \in \cB_1 \mid \<\mM^k, \mD> = \norm{\mM^k}^\dagger\}\,,
$$
and update for $\mX^k$ in \cref{eq:simple_update} simplifies to
\begin{equation}\label{eq:our_update}
	\mX^{k+1} = \mX^{k} - \gamma_k \{\mD \in \cB_1 \mid \<\mM^k, \mD> = \norm{\mM^k}^\dagger\}\,.
\end{equation}

Using this formula it is easy to compute updates for algorithms induced by various norms $\norm{\cdot}$. 

For example, when the norm $\norm{\cdot}$ is the Frobenius norm $\normf{\cdot}$, \cref{eq:our_update} turns into
\begin{equation}\label{eq:nsgd_update}
	\mX^{k+1} = \mX^{k} - \eta_k \frac{\mM^k}{\normf{\mM^k}}\,,
\end{equation} 
which recovers Normalized SGD (NSGD). 

And when the norm is the spectral norm $\norms{\cdot}$, we get
\begin{equation}\label{eq:muon_update}
	\mX^{k+1} = \mX^{k} - \eta_k U V^\top\,,
\end{equation} 
which is Muon without the $\sqrt{m/n}$ factor.
Here, $\mM^k = \mU \mSigma \mV^\top$ is the Singular Value Decomposition (SVD) of $\mM^k$ ($\mU = [u_1, u_2, \dots, u_r]$, $\mSigma = \diag(\sigma_1, \space \sigma_2, \dots, \sigma_r)$, and $\mV = [v_1, v_2, \dots, v_r]$). Muon can be recovered by taking the RMS-to-RMS operator norm $\sqrt{\frac{n}{m}}\norms{\cdot}$.

When the norm is the Chebyshev norm $\normc{\cdot}$, we get
\begin{equation}\label{eq:signsgd_update}
	\mX^{k+1} = \mX^{k} - \eta_k \sign(\mM^k)\,,
\end{equation}
which recovers SignSGD~\citep{bernstein2018signsgd}. Here, $\sign(\mM^k)$ denotes the element-wise sign function. SignSGD is particularly notable for its communication efficiency in distributed training, as it compresses gradients to 1-bit per parameter.

\section{Duals to Ky Fan Norms Instead of the Spectral Norm}

\subsection{\texorpdfstring{$\normn{\mM^{k}}$ and Neon}{Neon}}
After considering $\normf{\mM^k}$ and $\norms{\mM^k}$, it is natural to look at the nuclear norm $\normn{\mM^k}$.
\begin{lemma}\label{lemma:neon_update}
	When $\norm{\cdot} = \normn{\cdot}$, \cref{eq:our_update} turns into:

	\begin{equation}\label{eq:neon_update}
		\mX^{k+1} = \mX^{k} - \eta_k u_1 v_1^\top
	\end{equation}
\end{lemma}
\begin{proof}
Since $\norm{\cdot}^\dagger = \norms{\cdot}$, the goal is to reach $\norms{\mM^k} = \sigma_1$.

Let us note that $\Delta = u_1 v_1^\top$ delivers this value. Indeed, $\normn{\Delta}=1$ and by the trace property and orthogonality of the singular vectors, $\<\mM^k, \Delta> =\<\mU \mSigma \mV^\top, u_1 v_1^\top> = \tr \diag({\sigma_1, 0, \dots, 0}) = \norms{\mM^k}$, which completes the proof.
\end{proof}

We name the derived algorithm \emph{Neon}. In the~\cref{sec:matrix-side}, we will discuss how to compute the Neon's update.

\subsection{\texorpdfstring{$\normkfk{\mM^k}^\dagger$ and Muon, Neon, and Centralized Dion without error feedback}{Muon, Neon, and Centralized Dion without error feedback}}

Neon's and Muon's updates seem to be complete opposites: one has rank one, while the other is full-rank. It would be interesting to derive algorithms with updates of various ranks.

\textbf{Schatten norms.}
\citet{cesista2025schattenp} considered Schatten-$p$ norms $\norm{\mM^k}_{S_p} = \left(\sum_{i=1}^{\min(m, n)}\sigma_i^p\right)^{1/p}$, which produce the update:
$$X^{k+1} = X^k - \eta_k \mU \frac{\diag\left(\sigma_1^{q-1},\dots, \sigma_{\min(m,n)}^{q-1}\right)}{\left(\sum_{i=1}^{\min(m,n)}{\sigma_i^q}\right)^{\frac{q-1}{q}}}\mV^\top$$
for $q$: $\frac{1}{p} + \frac{1}{q} = 1$. This formula recovers Neon when $p\rightarrow1$ provided that $\sigma_1 > \sigma_2$, which is true on real data; NSGD when $p=2$, and Muon when $p\rightarrow\infty$.

However, Schatten norms do not fill the gap rank: indeed, when $p > 1$, the rank of the update is full, while when $p=1$ ($p \rightarrow 1$) it is one. Moreover, it is not clear how to calculate the update for $p \neq 1, 2, \infty$: it seems one has to know all $\sigma_i$ to compute the update, which makes the problem as hard as the full SVD.

\textbf{Ky Fan norms.}
There is another family of matrix norms, which might help us: Ky Fan norms. For $k \in \{1, \dots, \min(m, n)\}$, the Ky Fan $k$-norm $\normkfk{\cdot}$ is $\sum_{i=1}^{k}\sigma_i$, i.e. the sum of the $k$ largest singular values of the matrix. Special cases of the Ky Fan $k$-norm are the Ky Fan $1$-norm, which is the spectral norm, and the Ky Fan $\min\{m, n\}$-norm, which is the nuclear norm.

Let us derive the update for an arbitrary $k$. Since $\normkfk{\cdot}^\dagger = \max\{\frac{1}{k} \normn{\cdot}, \norms{\cdot}\}$ (see \citet{bhatia2013matrix}, p. 96), the goal is to reach $\max\{\frac{1}{k}\sum_{i=1}^{\min(m,n)}\sigma_i, \sigma_1\}$. $\Delta = u_1 v_1^\top$ from Neon delivers $\sigma_1$, while $\Delta = \frac{1}{k}\sum_{i=1}^{\min(m,n)}u_i v_i^\top$ from Muon (with an extra factor of $1/k$) delivers $\frac{1}{k}\sum_{i=1}^{\min(m,n)}\sigma_i$. Thus, the update is either
$$
\mX^{t+1} = \mX^{t} - \eta_t u_1 v_1^\top\text{ or }\mX^{t+1} = \mX^{t} - \frac{\eta_t}{k}\mU \mV^\top\,,
$$
depending on which one better minimizes the linear function $L(\mX) \equiv F(\mX^t) + \mM^t(\mX - \mX^t)$.

The rank gap is not closed by the Ky Fan norms because the obtained update is either one-rank or full-rank.

\textbf{Duals to Ky Fan norms.}
While Schatten norms are closed under dualization and $\norm{\mM^k}_{S_p}^\dagger = \norm{\mM^k}_{S_q}$, for Ky Fan norms it is not generally the case. $k = 1$ and $k=\min(m, n)$ are exceptional: for $k=1$ the dual norm is $\normn{\cdot}$ with $k = \min(m, n)$ and for $k = \min(m, n)$ the dual norm is $\norms{\cdot}$ with $k=1$. For each other $k$, $\normkfk{\cdot} \neq \norm{\cdot}_{\text{KF-}k'}$ for any $k'$: $k'=\min(m, n)$ and $k'= 1$ correspond to the already discussed cases, while for other $k'$ one can change $\norm{\mM^k}_{\text{KF-}k'}$ by moving a small value between $\sigma_{k'}$ and $\sigma_{k'+1}$. During this change, $\norms{\mM^k}$ and $\normn{\mM^k}$ will remain constant, hence $\normkfk{\mM^k}$ will not change as well.

\begin{lemma}\label{lemma:ky_fan_update}
	When $\norm{\cdot} = \normkfk{\mM^t}^\dagger$, \cref{eq:our_update} turns into:

	\begin{equation}\label{eq:ky_fan_update}
		\mX^{t+1} = \mX^{t} - \eta \sum_{i=1}^{k}u_i v_i^\top
	\end{equation}
\end{lemma}

\begin{proof}
Since $\norm{\cdot}^\dagger = \normkfk{\cdot}^{\dagger\dagger} = \normkfk{\cdot}$, the goal to reach $\normkfk{\mM^t}$.

Let us note that $\Delta = \sum_{i=1}^{k} u_i v_i^\top$ delivers the value. Indeed, $\<\mM^t, \Delta> =\<\mU \mSigma \mV^\top, \sum_{i=1}^{k} u_i v_i^\top> = \sum_{i,j=1}^{r, k}\<u_i \sigma_i v_i^\top, u_j v_j^\top> = \sum_{i=1}^{k}\sigma_i = \normkfk{\mM^t}$, which completes the proof.
\end{proof}
These updates constitute a family {\it Fanions}, LMO-based algorithms under the $\normkfk{\mM^t}^\dagger$ norms. The algorithm for a particular $k$ we will call {\it Fanion-$k$}. Thus, Muon is a Fanion-$\min\{m,n\}$, while Neon is a Fanion-1. Moreover, the unsharded version of the rank-$r$ Dion (Algorithm 1 from \citet{ahn2025dion}) without the error feedback and without scaling of the update is actually a Fanion-$r$ (see \citet{pethick2025understandingdion}, where Dion is written down in a notation more similar to ours).

\section{Conic Combination of LMO-algorithms is an LMO-algorithm}
\label{sec:conic_combo}

Simply applying norm dual to the Ky Fank $k$-norm however, is not enough to provide family of algorithms diverse enough for our cause. Next we consider linear combinations of algorithms from LMO framework, which happen to be LMO-algorithms too, as we show in succeeding paragraphs.

\subsection{General Case}
First, we need a well-known fact mentioned, for example, in \citet[Table 1]{yu2012arithmetic}. For the sake of completeness, we provide a proof here.

\begin{lemma}\label{lemma:dual_to_conv_comb}
	Let $\norm{\cdot}_{(1)},\dots,\norm{\cdot}_{(n)}$ be norms on a finite-dimensional Euclidean space, and let $\alpha_1,\dots,\alpha_n$ be non-negative reals. Define
	$$
		\norm{\cdot} := \sum_{i = 1}^n \alpha_i \norm{\cdot}_{(i)}.
	$$
	Then the dual unit ball of $\norm{\cdot}$ satisfies
	$$
		\cB_{\norm{\cdot}^\dagger}
		= \sum_{i = 1}^n \alpha_i \cB_{\norm{\cdot}_{(i)}^\dagger}
	$$
	where $\sum$ denotes the Minkowski sum and $\cB_{\norm{\cdot}_{(i)}^\dagger}$ is the unit ball of the dual norm $\norm{\cdot}_{(i)}^\dagger$.
\end{lemma}

\begin{proof}
	Let us first prove the lemma for the case $n = 2$. Denote $f(x) = \alpha_1 \norm{x}_{(1)}$ and $g(x) = \alpha_2 \norm{x}_{(2)}$, such that $\norm{x} = f(x) + g(x)$. Recall two standard facts:
	\begin{enumerate}
		\item For any norm $\norm{\cdot}$ and $\lambda>0$,
		      $$
			      (\lambda \norm{\cdot})^*(y) =
			      \sup_{x}\bigl( \langle y,x \rangle - \lambda \norm{x}\bigr)
			      = \delta_{\lambda \cB_{\norm{\cdot}^\dagger}}(y)\,,
		      $$
		      i.e., the indicator function of the scaled dual ball.
		\item The Fenchel conjugate of a sum satisfies
		      $$
			      (f+g)^*(y) = \inf_{u+v=y} \bigl(f^*(u) + g^*(v)\bigr)\,.
		      $$
	\end{enumerate}
	Applying these to $f$ and $g$, we have
	$$
		f^*(u) = \delta_{\alpha_1 \cB_{\norm{\cdot}_{(1)}^\dagger}}(u)\,,
		\quad
		g^*(v) = \delta_{\alpha_2 \cB_{\norm{\cdot}_{(2)}^\dagger}}(v)\,.
	$$
	Thus,
	$$
		\norm{\cdot}^*(y)
		= (f+g)^*(y)
		= \inf_{u+v=y}
		\bigl(
		\delta_{\alpha_1 \cB_{\norm{\cdot}_{(1)}^\dagger}}(u)
		+
		\delta_{\alpha_2 \cB_{\norm{\cdot}_{(2)}^\dagger}}(v)
		\bigr)
		=
		\delta_{\alpha_1 \cB_{\norm{\cdot}_{(1)}^\dagger}+\alpha_2 \cB_{\norm{\cdot}_{(2)}^\dagger}}(y).
	$$
	By definition, the conjugate of a norm is exactly the indicator of its dual unit ball: 
	$$
	\norm{\cdot}^*(y) = \delta_{\cB_{\norm{\cdot}^\dagger}}(y)\,.
	$$
	Therefore, $\cB_{\norm{\cdot}^\dagger} = \alpha_1 \cB_{\norm{\cdot}_{(1)}^\dagger} + \alpha_2 \cB_{\norm{\cdot}_{(2)}^\dagger}$.

	Now we prove the general case by induction. The base case ($n=2$) is already proven. Suppose that the assumption of the lemma holds for $n = k$. Then, for $n = k + 1$, 
	$$
	\norm{x} = \sum_{i = 1}^k \alpha_i \norm{x}_{(i)} + \alpha_{k + 1}\norm{x}_{(k + 1)} = \norm{x}_{(1:k)} + \alpha_{k + 1}\norm{x}_{(k + 1)}\,.
	$$
	Applying the result for $n=2$ combined with the induction assumption, we obtain
	$$
	\cB_{\norm{\cdot}^\dagger} = \cB_{\norm{\cdot}_{(1:k)}^\dagger} + \alpha_{k + 1} \cB_{\norm{\cdot}_{(k + 1)}^\dagger} = \sum_{i = 1}^{k + 1} \alpha_i \cB_{\norm{\cdot}_{(i)}^\dagger}\,,
	$$
	which proves the lemma.
\end{proof}

\begin{lemma}\label{lemma:lin_comb_lmo}
	Let $\norm{\cdot}_{(1)}, \dots, \norm{\cdot}_{(n)}$ be norms on a finite-dimensional Euclidean space, and let $\alpha_1, \dots, \alpha_n$ be non-negative reals. Consider Linear Minimization Oracles $\mathrm{LMO}_{1}, \dots, \mathrm{LMO}_{n}$, corresponding to the unit balls of these norms. Then, $\sum_{i = 1}^n \alpha_i \mathrm{LMO}_{i}$ is the LMO corresponding to the norm $\norm{\cdot}$ dual to the $\sum_{i = 1}^n \alpha_i \norm{\cdot}_{(i)}^\dagger$.
\end{lemma}
	
\begin{proof}
	Using \cref{lemma:dual_to_conv_comb} and the fact $\norm{\cdot}^{\dagger \dagger} = \norm{\cdot}$, we can obtain general form of the unit ball in the $\norm{\cdot}$ norm: $\cB_{\norm{\cdot}} = \sum_{i = 1}^n \alpha_i \cB_{\norm{\cdot}_{(i)}}$. Thus, the linear function minimization objective on a $\norm{\cdot}$ norm ball can be transformed as follows:
	$$
	\argmin_{\mD \in \cB_{\norm{\cdot}}}\<\mM, \mD> = \argmin_{\mD_1 \in \alpha_i \cB_{\norm{\cdot}_{(1)}},\dots,\mD_n \in \alpha_n\cB_{\norm{\cdot}_{(n)}}}\<\mM, \sum_{i = 1}^n\mD_i> = \sum_{i = 1}^n \argmin_{D_i \in \cB_{\norm{\cdot}_{(i)}}}\<\mM, \mD_i>\,,
	$$
	where the last summation denotes the Minkowski sum. This immediately implies $\sum_{i = 1}^n \alpha_i \mathrm{LMO}_{i} \in \argmin_{\mD \in \cB_{\norm{\cdot}}}\<\mM, \mD>$, which proves the claim of the lemma.
\end{proof}

Applying it to optimization algorithms, we obtain the following corollary.

\begin{corollary}\label{corollary:lin_comb_alg}
	Let there be a finite family of LMO based algorithms indexed by $i = 1,\dots,n$, where the update of the $i$-th algorithm is defined by 
	$$
	\mX^{k + 1} - \mX^{k} = \gamma_k \mathrm{LMO}_{i}(\mM^{k})\,,
	$$
	and $\mathrm{LMO}_i$ corresponds to the unit ball of norm $\norm{\cdot}_i$. For arbitrary non-negative $\alpha_1,\dots,\alpha_n$, the algorithm with the update given by 
	$$
	\mX^{k + 1} - \mX^{k} = \gamma_k \sum_{i = 1}^n \alpha_i \mathrm{LMO}_{\norm{\cdot}_{(i)}}(\mM^k)
	$$
	is an LMO-algorithm itself, with LMO corresponding to the unit ball of the norm $\norm{\cdot}$ dual to the norm given by $\sum_{i = 1}^n \alpha_i \norm{\cdot}_{(i)}^\dagger$.
\end{corollary}

\subsection{Frobeniusize Them: F-Muon and F-Neon}
Let us construct the concrete examples of algorithms given by linear combinations of LMO-algorithms. It follows from \cref{corollary:lin_comb_alg} that those algorithms can also be viewed as LMO-algorithms.

Combining Fanions for various $k$ with another excellent LMO-algorithm, NSGD, we obtain the family of algorithms with updates defined by
\begin{equation}\label{eq:nsgd_muon_update}
		\mX^{t+1} = \mX^{t} - \gamma_k \left(\alpha \sum_{i = 1}^k u_i v_i^\top + (1-\alpha)\frac{\mM^k}{\normf{\mM^k}}\right)\,.
\end{equation}
Recall that Fanion-$k$ update is governed by the norm dual to the Ky Fan-$k$ norm, and NSGD corresponds to the Frobenius norm, which is dual to itself. Thus, by \cref{corollary:lin_comb_alg}, our new algorithm is an LMO-algorithm corresponding to the unit ball of the norm $\norm{\cdot}_{\mathrm{F-KF-k}}$:
\begin{equation}\label{eq:fkfk_norm}
	\norm{\cdot}_{\mathrm{F-KF-k}}^\dagger = \alpha \normkfk{\cdot} + (1-\alpha)\normf{\cdot}\,.
\end{equation}
We name the derived family of algorithm \emph{F-Fanions}. 

Two edge members of this family, with $k = 1$ and $k = \min\{m, n\}$ correspondingly, are \emph{F-Neon} and \emph{F-Muon}, are of particular interest to us. A bit of information and visualizations related to the $\normfstar{\cdot} = \norm{\cdot}_{\mathrm{F-KF-1}}$ norm is presented in the appendix (see \cref{eq:normfstardual}, \cref{fig:fstardual}).

\subsection{Add SignSGD: S-Muon and S-Neon}

Another algorithm we consider is the SignSGD. Mixing its update with that of Fanion-$k$, we obtain the update of \emph{S-Fanion-$k$}:
\begin{equation}\label{eq:sign_muon_update}
		\mX^{t+1} = \mX^{t} - \gamma_k \left(\alpha \sum_{i = 1}^k u_i v_i^\top + (1-\alpha)\eta\sign(\mM^k)\right)\,,
\end{equation}
where $\eta$ is the special SignSGD learning rate coefficient.

The norm $\norm{\cdot}_{\mathrm{C-KF-k}}$ which produces this update in the LMO framework is again given by \cref{corollary:lin_comb_alg}:
\begin{equation}\label{eq:ckfk_norm}
	\norm{\cdot}_{\mathrm{C-KF-k}}^\dagger = \alpha \normkfk{\cdot} + \frac{1-\alpha}{\eta}\normc{\cdot}^\dagger\,.
\end{equation}


\section{Computing the Updates}\label{sec:matrix-side}
We employ the thick-restarted Lanczos method for the symmetric eigenvalue problem (thick-restart Lanczos, TRLan) to compute the low-rank updates of Fanions. We apply TRLan to either $\mM^{k\top}\mM^k$ or $\mM^k \mM^{k\top}$ (whichever matrix is smaller). We use the CuPy implementation of \texttt{cupy.sparse.linalg.svds} \citep{cupy_svds_ref} which internally relies on TRLan \citep{simonz1998thick}. 

TRLan is specifically designed for efficiently approximating the largest singular values and corresponding singular vectors of very large matrices. The thick-restart strategy retains the most informative Ritz vectors across restarts, which dramatically accelerates convergence while keeping memory consumption moderate. TRLan is particularly attractive in our GPU setting because its dominant cost is a modest number of highly parallelizable matrix-vector multiplications (matvecs) and it avoids full reorthogonalization against the entire Krylov basis by using short recurrence relations combined with thick restarting.

The per-cycle complexity is $\mathcal{O}(mn^2 + n^2k + nk^2)$, where $m \geq n$ are the dimensions of the target matrix and $k$ is the size of the retained subspace (typically $k \ll n$).

In Table~\ref{tbl:matrix_methods}, we compare TRLan against randomized SVD (RSVD) and simple power iterations when computing the rank-$k$ update used in Fanion-$k$ and related algorithms. Experiments are performed on dense random matrices with i.i.d. $\mathcal{N}(0,1)$ entries and use CPU implementations for fair comparison. 
We report:
\begin{itemize}
  \item $err_1$: relative error in the Frobenius norm of $\sum_i^k u_i v_i^T$,
  \item $err_2$: relative error in the Frobenius norm of $\sum_i^k \sigma_i u_i v_i^T$.\end{itemize} 

On $500\times500$ matrices, TRLan and RSVD require comparable wall-clock time, but TRLan delivers orders-of-magnitude lower error and far fewer matvecs. On larger $5000\times5000$ matrices the advantage becomes even more pronounced: TRLan is 3-4 times faster than RSVD while using $\sim$30 times fewer matvecs at comparable or better accuracy.

An interesting empirical observation is that RSVD tends to approximate the \emph{singular values} themselves reasonably well but the reconstructed low-rank matrix noticeably deviates from the truncated SVD, whereas TRLan provides an excellent approximation to the truncated SVD matrix itself (low $err_2$) at the cost of occasionally less accurate individual singular values. This makes TRLan the preferred choice for algorithms like Neon/Fanion-$k$ that only need the low-rank term $\sum \sigma_i u_i v_i^\top$, but less ideal for methods (e.g., Dion) that explicitly require accurate $\sigma_i$ for error feedback or step-size control.

A current practical limitation is the lack of a native PyTorch implementation of thick-restart Lanczos; existing PyTorch-based randomized SVD routines cannot match TRLan's accuracy/efficiency combination for the matrix reconstruction task.

For reference, Table~\ref{tbl:newton_schulz_reference} shows results for the Newton-Schulz polar decomposition iteration on the same matrices, $err_1$ is the relative error of $UV^T$ (29-30 iterations to converge, significantly higher matvec count than TRLan).



\begin{table}[ht]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|c|c|}
	  \hline
	  Matrix sizes & $k$  & Method           & Time (s) & Matvecs & Iterations & $err_1$         & $err_2$       \\
	  \hline
	  $500 \times 500$      & 5  & Power Iterations           & 0.062   & 2005    & 200        & 9.2$\cdot 10^{-3}$ & 9.1$\cdot10^{-3}$\\
	  $500 \times 500$      & 5  & RSVD             & 0.017   & 1170    & 38         & 9.8$\cdot 10^{-3}$ & 9.6$\cdot10^{-3}$\\
	  $500 \times 500$      & 5  & TRLan            & 0.018   & 131     & 65         & 9.6$\cdot 10^{-5}$ & 9.4$\cdot10^{-5}$\\
	  \hdashline
	  $500 \times 500$      & 50 & Power Iterations           & 0.44    & 43750   & 437      & 9.9$\cdot 10^{-3}$ & 9.0$\cdot10^{-3}$\\
	  $500 \times 500$      & 50 & RSVD             & 0.61    & 6120    & 50      & 9.9$\cdot 10^{-3}$ & 9.1$\cdot10^{-3}$\\
	  $500 \times 500$      & 50 & TRLan            & 0.16    & 462     & 231      & 3.3$\cdot 10^{-7}$ & 3.0$\cdot10^{-7}$\\
	  \hdashline 
	  $5000 \times 5000$    & 5  & Power Iterations           & 9.6     & 9065    & 906      & 8.6$\cdot 10^{-3}$ & 8.6$\cdot10^{-3}$\\
	  $5000 \times 5000$    & 5  & RSVD             & 2.1     & 5640    & 187      & 9.7$\cdot 10^{-3}$ & 9.7$\cdot10^{-3}$\\
	  $5000 \times 5000$    & 5  & TRLan            & 0.70    & 205     & 102      & 7.7$\cdot 10^{-3}$ & 7.7$\cdot10^{-3}$\\
	  \hline
	\end{tabular}
  \caption{Comparison of methods for computing rank-$k$ updates on dense random matrices (CPU, double precision). Lower is better in all columns.}
  \label{tbl:matrix_methods}
  \end{table}
  
  \begin{table}[ht]
  \centering
  \begin{tabular}{|c|c|c|c|c|}
  \hline
  Matrix size    & Time (s) & Matvecs & Iterations & $err_1$ \\
  \hline
  $500 \times 500$     & 0.041    & 27 000  & 27         & 4.8e-3  \\
  $5000 \times 5000$   & 26.4     & 290 000 & 29         & 6.5e-3  \\
  \hline
  \end{tabular}
  \caption{Newtonâ€“Schulz iteration on the same matrices (for reference).}
  \label{tbl:newton_schulz_reference}
  \end{table}
  
\section{Experiments}\label{sec:experiments}
\subsection{Randomized Linear Least Squares}
First, we compare F-Fanions on the following convex $L$-smooth problem:

\begin{equation}\label{eq:lls}
	F(\mX) = \frac{1}{2} \<(\mX-\mS), \mM (\mX - \mS) \mN> \to \min_{\mX \in \Rmn}\
\end{equation}

where $\mX \in \Rmn$, $m=500$ and $n=500$ are typical dimensions of a neural network weight matrix, $\mS \in \Rmn$, $\mM \in \mathbb{S}^m_{+}$ and $\mN \in \mathbb{S}^n_{+}$ are positive-semidefinite matrices. The spectra of $\mM$ and $\mN$ are uniformly distributed in the (0, 1) interval. We set $\mS=0$, and $\mX^0 \sim 0.1 \gN(0, \mI)$.

We run different Fanions and their respective F-Fanions with $\alpha=1/2$: Neon (Fanion-1), Fanion-2, Fanion-5, Fanion-100, and Muon (Fanion-500). Also, we test S-Fanions with $\alpha=1/2$ and \texttt{sign\_lr\_coeff=0.01}. We test them against Normalized SGD and SignSGD, which is are also F-Fanion and S-Fanion respectively with $\alpha = 0$ and an arbitrary $k$.

Since theoretical bounds \citep{kovalev2025understanding, riabinin2025gluon} rely on a very loose norm bound $\norm{\cdot}\le \rho \normf{\cdot}$, we do not derive learning rate or momentum from the smoothness constants that also depend on the norm choice. Rather, we find such \texttt{(lr, momentum)} pair that the loss threshold $0.01$ is reached in a minimal number of iterations. This setting is the most realistic, and at the same time aligns with the corollaries of the convergence theorems where constant learning rate and momentum coefficients are proposed.

The results are presented in \cref{fig:lls} and ~\cref{sec:lls_plots_section}.

Both F-Muon and S-Muon converge faster and to a lower loss and a lower Frobenius norm of the gradient than NSGD, Muon, or SignSGD.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[t]{0.8\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/simple_lls/loss_vs_iteration_500x500.pdf}
        \caption{The loss}
        \label{fig:lls_loss}
    \end{subfigure}
    \vspace{1em} % optional: add vertical spacing between subfigures
    \begin{subfigure}[t]{0.8\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/simple_lls/gradient_frobenius_norm_vs_iteration_500x500.pdf}
        \caption{The Frobenius norm of the gradient}
        \label{fig:lls_fro_grad_norm}
    \end{subfigure}
    \caption{Linear least squares problem for a 500x500 matrix}
    \label{fig:lls}
\end{figure}



\subsection{CIFAR-10 airbench}

We compare the algorithms on the CIFAR-10 airbench \citep{cifar2023airbench}. First, to test the impact of $\alpha$, we run F-Muon for different $\alpha$ with the same {\tt lr=0.24(1 - step/total\_steps), momentum=0.6, nesterov=True} and weight normalization, as have been finetuned by Keller Jordan, 10 repetitions for each $\alpha$. We record the accuracy after 8 epochs of training (\cref{fig:muon_alphas}).

Then we tune F-Muon with $\alpha = 0.5$. Tuned parameters are {\tt lr=0.4(1 - step/total\_steps), momentum=0.65, nesterov = True}. Tuned F-Muon reaches $94.01 \pm 0.14\%$ after 8 epochs (averaged by 200 iterations), matching Muon's accuracy and variance. Once again we measure \texttt{val\_accuracy}($\alpha$) curve (\cref{fig:fmuon_alphas}). We notice that even when $\alpha=0.1$, the accuracy is much higher than in the case of vanilla NSGD.

We tune S-Muon with $\alpha=0.5$ for the best \texttt{(lr, momentum, sign\_lr\_coeff)} and by setting {\tt lr=0.42(1 - step/total\_steps), momentum=0.65, nesterov = True, sign\_lr\_coeff = 0.003} get $\bf{94.02} \pm 0.13\%$, which is even higher than for vanilla Muon.

\begin{figure}[h!]
	\centering
	\begin{subfigure}[b]{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figs/muon_tuned_diff_alpha.pdf}
		\caption{With parameters tuned for Muon}
		\label{fig:muon_alphas}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figs/fmuon_tuned_diff_alpha.pdf}
		\caption{With parameters tuned for F-Muon}
		\label{fig:fmuon_alphas}
	\end{subfigure}
	\caption{Mean validation accuracies for F-Muon with different $\alpha$}
	\label{fig:diff_alphas}
\end{figure}

The Muon-level performace is surprising for the contorted geometry of F-Muon and S-Muon. F-Muon can be visualized by \cref{fig:cifar_ball}, where the LMO balls of Muon and F-Muon for real \texttt{lr} are plotted in a 2D space of singular values. S-Muon cannot be visualized because the $l_\infty$ norm is not a function of singular values, while $\norms{\cdot}$, when plotted for the $\Rmn$ space with $m, n > 2$, requires at least a 4D space. However, it is plain that the LMO ball of S-Muon significantly differs from the ball of Muon because the $\tt{lr}(1-\alpha)\tt{sign\_lr\_coeff} = 6.3 \times 10^{-4}$, which is comparable to the usual \texttt{lr} of SignSGD for deep learning problems.

The most pathological case is $\alpha > 1$, which corresponds to the LMO with an area that is not a norm ball! Despite this violation, the mixture of algorithms reaches almost the same accuracy as vanilla Muon.

 These observations raise the question of how much LMO-based algorithms are sensitive to the constraint area.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\textwidth]{figs/norms/fstardual_cifar.pdf} % adjust width as needed
	\caption{Visualization of the LMO balls for Muon and F-Muon.}
	\label{fig:cifar_ball}
\end{figure}


\subsection{NanoGPT speedrun}
We test F-Muon on the NanoGPT speedrun \citep{modded_nanogpt_2024}. For $\alpha = 0.5$, the optimal parameters are {\tt lr=0.07, momentum=0.95}, while for Muon they were {\tt lr=0.05, momentum=0.95}. After testing for 1750 steps we get 3.281 cross-entropy loss. For Muon, it falls below the target threshold 3.28 and reaches 3.279. However, this difference is negligible if one looks at \cref{fig:gpt_loss}. It is even more striking, considering the fact that this F-Muon is an average of Muon and NSGD, and the latter performed quite poorly.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{figs/nanogpt/val_loss_vs_step.pdf}
	\caption{The validation loss for NanoGPT}
	\label{fig:gpt_loss}
\end{figure}

Again, as on the CIFAR airbench, if we set $\alpha=-0.1$ that corresponds to no ball, we get a 3.2818 loss, which is a small difference from the baseline.

\section{Algorithms for Vectors $\rightarrow$ Algorithms for Matrices}

One cannot but notice the similarity between the updates LMO optimizers in the vector $l_p$ and the matrix Schatten $S_p$ norms, which is illustrated by \cref{tbl:mat_vs_vec_lmo}. However, the parallels are not limited to this similarity and can be traced even in the empirical observations. SignSGD is very close to Adam, as noted in \citet{bernstein2024oldoptimizernewnorm}, and both Adam and Muon perform well in training large models. NSGD is the same for both matrix and vector cases. Greedy Coordinate Descent is not applied to high-dimensional problems, so from this perspective it is not surprising that one-rank Neon underperforms on such problems.


\begin{table*}[t]
	\caption{lmo optimizers in Schatten $S_p$ norms and in $l_p$ norms. $g$ is the gradient. When it is a matrix, $g = \mU \mSigma \mV^\top$}
	\label{tbl:mat_vs_vec_lmo}
	\bgroup
	\def\arraystretch{1.2}
	\resizebox{\textwidth}{!}{
		\begin{tabular}{|l|c|c|c|}
			\hline
			Method                             & lmo constraint set $\mathcal D$ & lmo                                                         & Reference                            \\
			\hline
			\hline
			Normalized SGD                     & $l_2$-ball, $S_2$-ball          & $-\eta \tfrac{g}{\norm{g}_2} = -\eta \tfrac{g}{\norm{g}_F}$ & \citep{hazan2015beyond}              \\
			Momentum Normalized SGD            & Ball in $l_2$, or Ball in $S_2$ & $-\eta \tfrac{g}{\norm{g}_2} = -\eta\tfrac{g}{\norm{g}_F}$  & \citep{cutkosky2020momentum}         \\
			\hline
			SignSGD                            & Ball in Max-norm $l_\infty$     & $-\eta \sign(g)$                                            & \citep[Thm. 1]{bernstein2018signsgd} \\
			Signum                             & Ball in Max-norm $l_\infty$     & $-\eta \sign(g)$                                            & \citep[Thm. 3]{bernstein2018signsgd} \\
			\hdashline
			Muon                               & Ball in Spectral $S_\infty$     & $-\eta UV^\top$                                             & \citep{jordan2024muon}               \\
			\hline
			Gauss-Southwell Coordinate Descent & Ball in $l_1$                   & $-\eta \sum_{i \in \argmax|g_i^t|} \sign(g_i^t)e_i$                       & \citep[p. 19]{shi2016primer}         \\
			\hdashline
			Neon                               & Ball in Nuclear $S_1$           & $-\eta u_1 v_1^\top$                                        & This work                            \\
			\hline
		\end{tabular}
	}
	\egroup
\end{table*}

\section{Related Work and Discussion}
Since Muon \citep{jordan2024muon} is a very successful and popular optimizer for functions of weight matrices, a lot of reseach has been put into, first, further improving its performance and, second, explaining its success.

	{\bf Improvements of Muon.} Regarding the first point, a large number of applications and improvements of Muon has been proposed in less than a year. \citet{liu2025muon} adapted the algorthm for training language models larger than NanoGPT. \citet{shah2025practical} organized efficient hyperparameter transfer by combining Muon with maximal update parametrization. To construct their COSMOS optimizer, \citet{chen2025cosmoshybridadaptive} applied computationally intensive updates of SOAP optimizer \citep{vyas2025soap} to a low-dimensional "leading eigensubspace" while using memory-efficient methods like Muon for the remaining parameters. \citet{amsel2025polar, grishina2025chebyshev} proposed more efficient alternatives to Newton-Schulz algorithm. \citet{si2025adamuon} introduced AdaMuon which combines element-wise adaptivity with orthogonal updates. We suppose that the described above or similar techniques can be applied to our optimizers as well. For example, F-Muon and S-Muon also benefit from faster alternatives to Newton-Schulz iterations, and Fanions may be a great substitute to Muon in COSMOS, because, as we have shown in Section~\ref{sec:matrix-side}, Lanczos algorithm is much faster than Newton-Schulz iterations on very large matrices.

	{\bf Theory behind Muon.} Regarding the second point, there has been a prolonged gap in theory behind Muon, simplistic derivation of \citet{bernstein2025deriving} based on \citet{bernstein2024oldoptimizernewnorm} excluded. This gap, as it seems to us, is not even now completely closed. For example, \citet{kovalev2025understanding} has provided convergence guarantees of Muon and in various settings, from which, however, Muon's supremacy cannot be recovered. Indeed, although the obtained bounds depend on the norm choice, the asymptotics of the convergence remain the same as for NSGD and other optimizers, $K = \cO(\epsilon^{-4})$ in an $L$-smooth stochastic case.

	A similar drawback has the article \citet{riabinin2025gluon}, where $L$-smoothness assumption is replaced with a more practical $(L_0, L_1)$-smoothness. By estimating smoothness and substituting it into their Theorem 1, the authors recovered the optimal fine-tuned stepsizes reported by \citet{pethick2025training}. However, they have not showed the optimality of the spectral or the RMS-to-RMS norm, which is observed in practice, as our comparison with NSGD highlights.

	Common important drawback of the analyses is the consideration of the convergence by the norm of the gradient. As we showed in our experiments on CIFAR, the gradient norm may decrease only by a factor of ten, when the accuracy reaches 100\%.

	We suppose that the reason for the recorded stark discrepancy between Neon and Muon performance, both of which are described by Stochastic Conditional Gradient \citep{pethick2025training} or Gluon frameworks, lies in the structure of the norm ball or in the preconditioner interpretation \citep{pethick2025trainingneuralnetworksscale}, which must be an object of further research.

	{\bf The LMO and Error Feedback.} We already mentioned that rank-$k$ unsharded Dion without error feedback and scaling of the update is Fanion-$k$. Since error feedback for Dion is very important as discovered by the ablation study in \citet{ahn2025dion}, F-Fanions and S-Fanions will benefit from it as well. In the context of federated learning, error feedback is effective even for compressed Muon \citep{gruntkowska2025efmuon}. The advantage of Fanions and S-Fanions is that less bits are required for transmission: $\sum_{i=1}^k u_i v_i^\top$ is easily transmitted as $\{u_1, v_1, \dots, u_k, v_k\}$ ($(m+n)\times k$ floats), while the sign part is as usual coded in $m \times n$ bits. Thus, the compression is "built-in". Moreover, there is an intriguing possibility to construct differentially private Fanions and S-Fanions with the specific more optimal non-Gaussian noise, as was done with DP-signSGD \citep{jang2024dplogsign}. This we leave for future research.

	{\bf The nuclear norm in the LMO.} As we found out only when transforming our results into an article, the nuclear norm has already been explored in the context of the linear minimization oracle. \citet{pethick2025sam} applied it to create $\nu$SAM, a new sharpness-aware minimization technique. It would be interesting to substitute $\normn{\cdot}$ with $\normkfk{\cdot}^\dagger$ in their approach. Since the SAM-neighborhood becomes more diverse, $k > 1$ might add to the accuracy boost, while preserving a small memory footprint and a small time overhead, if Dion-style power iterations are used.

	
	{\bf Orthogonal research.} Fanions, F-Fanions, and S-Fanions benefit from the general theoretical description of \citet{riabinin2025gluon}, and better learning rates can be predicted by calculating the trajectory smoothness. They could be transformed to Drop-Fanions by updating only some layers as in \citet{gruntkowska2025dropmuon}. They could be viewed as approximations of the Non-Euclidean Broximal Point Method for the corresponding norms \citep{gruntkowska2025noneuclideanbroximal}. One can clip them to produce ClippedScion-like algorithms \citep{pethick2025generalizedgradientnormclipping}. They could be made more memory-effective by the zero-order techniques that worked for Muon \citep{petrov2025zeromuon}. Finally, the results from \citet{shulgin2025inexactmuon} can be used to explain the robustness of Muon to the norm change that occured in our experiments and to theoretically derive faster yet working approximate schemes to calculate the LMO; power iterations with a very limited number of iterations is a good aglorithm to consider for the analysis.

\section{Conclusion}
In this article, we have generalized several successful algorithms, like Muon and Dion, to the lmo-based algorithms in the $\normkfk{\cdot}^\dagger$ norm. Also we have proposed the technique of ``regularizing'' the updates with NSGD, a trick to increase the robustness of the algorithms, which is motivated by the consideration of the $\normfstar{\cdot}$, $\normftwo{\cdot}$, and general $\norm{\cdot}_{\mathrm{F-KF-k}}$ norms. Generalizations of well-known norms and subsequent combinations of them may further improve performance of lmo-based algorithms. If a theory is developed that explains the discrepancy between performance of different algorithms based on the matrix norms, one will probably be able to intentionally construct norms optimal to given architectures and probably even their parameters.

We suggest that future works that deal with the non-Euclidean LMO explain in corollaries to their convergence theorems the empirical superiority of Muon to other Fanions. Without that, it is hard to believe that the bounds are relevant for practitioners.

\section{Author Contributions}
IO suggested using the nuclear norm in the \citet{bernstein2024oldoptimizernewnorm} framework. DM supervised the project and helped with editing the article. IK suggested using composite norms (though not the ones that induce F-Fanions and S-Fanions) and refactored \cref{sec:conic_combo}. NK suggested Lanczos algorithm as the most precise means to compute Fanions' updates, conducted experiments to prove it, and wrote ~\cref{sec:matrix-side}. AV tested Neon on the finetuning of NanoGPT on Tiny Stories. All other work was done by AK.

\bibliography{icomp2024_conference}
\bibliographystyle{icomp2024_conference}

\appendix
\section{LMO for Neural Networks}
In a typical neural network, the objective function $F$ depends on a set of weight matrices $\{ \mW_1, \mW_2, \ldots \}$. The optimization framework we have described is applied in a layer-wise fashion. At each iteration $k$, a stochastic gradient $g(\mX^k, \xi^k)$ is computed using a mini-batch of data with the $\xi^k$ noise via backpropagation. This yields a separate gradient component, $\mG_i^k$, for each matrix $\mX_i$. The LMO-based update rule is then applied to each matrix $\mX_i$ using its corresponding gradient component $\mG_i^k$.

The update rule used in Muon optimizer is uSCG \citep{pethick2025training}. In the most general case, which involves momentum, it can be written as
\begin{equation}\label{eq:usgd_update}
	\begin{aligned}
		\mM^{k}     & = \alpha_{k} g(\mX^k, \xi^k) + (1 - \alpha_k)\mM^{k - 1}\,, \\
		\mX^{k + 1} & = \mX^k + \gamma_k\mathrm{LMO}(\mM^{k})\,.
	\end{aligned}
\end{equation}


\section{Norms $\normfstar{\cdot}^\dagger$ and $\normftwo{\cdot}^\dagger$}

Based on the lemma, we immediately find $\normfstar{\cdot}^\dagger$, which is related to F-Muon update. Indeed, after setting $\beta=1-\alpha$ and remembering that for smooth and bounded cases we can use $\min$ instead of $\inf$, we get
\begin{equation}\label{eq:normfstardual}
	\normfstar{\mY}^\dagger = \min_{\mZ} \min_t\{t, s.t. \norms{\mZ}\leq \alpha t, \normf{\mY-\mZ}\leq (1-\alpha) t\}
\end{equation}

If $\alpha = 1$, then $\mZ = \mY$, and we get $\normfstar{\mY}^\dagger = \norms{\mY}$. If $\alpha = 0$, then $\mZ = 0$, and we get $\normfstar{\mY}^\dagger = \normf{\mY}$.

Similarly, we find $\normftwo{\cdot}^\dagger$, which is related to F-Neon update:
\begin{equation}\label{eq:normftwodual}
	\normftwo{\mY}^\dagger = \min_{\mZ} \min_t\{t, s.t. \normn{\mZ}\leq \alpha t, \normf{\mY-\mZ}\leq (1-\alpha) t\}
\end{equation}

If $\alpha = 1$, then $\mZ = \mY$, and we get $\normftwo{\mY}^\dagger = \normn{\mY}$. If $\alpha = 0$, then $\mZ = 0$, and we get $\normftwo{\mY}^\dagger = \normf{\mY}$.

Unfortunately, $\normfstar{\cdot}^\dagger$ and $\normftwo{\cdot}^\dagger$ do not have a closed-form expression.

\section{Visualization of different matrix norms}
\subsection{Duals to F* and F2 norms}

It follows from \cref{lemma:dual_to_conv_comb} that the norm ball in $\normfstar{\cdot}^\dagger$ is the Minkowski sum of the norm ball in $\alpha\normn{\cdot}$ and $(1-\alpha)\normf{\cdot}$ and the norm ball in $\normftwo{\cdot}^\dagger$ is the Minkowski sum of the norm ball in $\alpha\norms{\cdot}$ and $(1-\alpha)\normf{\cdot}$.

In Fig. \cref{fig:fduals} we plot these norms. On x-axis and y-axis, there are singular values $\sigma_1$, $\sigma_2$ respectively of a matrix from $\Rmn$ with $\min\{m,n\}=2$.

\begin{figure}[h!]
	\centering
	\begin{subfigure}[b]{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figs/norms/fstardualball.pdf}
		\caption{lmo balls for F-Muon for different $\alpha$}
		\label{fig:fstardual}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figs/norms/ftwodualball.pdf}
		\caption{lmo ball for F-Neon for different $\alpha$}
		\label{fig:ftwodual}
	\end{subfigure}
	\caption{Balls in the duals to F* and F2 norms for different $\alpha$}
	\label{fig:fduals}
\end{figure}

\subsection{The Ky Fan norm and its dual}
1-balls in $l_\infty$, $l_1$ and $l_2$ norms are well-known from textbooks. But what about the Ky Fan $k$-norm? How can it be represented?

To showcase the complex structure of the Ky Fan $k$-norm and its dual, we suggest the illustrations \cref{fig:kyfan_combined} with the ball in the Ky Fan $2$-norm in \cref{fig:kyfan} and its dual in \cref{fig:kyfandual}. On x-, y-, and z-axes, there are singular values $\sigma_1$, $\sigma_2$, and $\sigma_3$ respectively of a matrix from $\Rmn$ with $\min\{m,n\}=3$. In this particular case, we do not sort the singular values. In the proposed representation, we actually plot balls in the Top-$2$ norm $\max\{\abs{x}+\abs{y}, \abs{x}+\abs{z}, \abs{y}+\abs{z}\}$ and its dual norm $\max\{\max(\abs(x),\abs{y},\abs{z}), \frac{1}{2} (\abs{x}+\abs{y}+\abs{z})\}$. The resulting balls are much more complex than balls in $l_\infty$, $l_1$ and $l_2$ norms.

In fact, those balls can be described easier if we use the results from \citet{yu2012arithmetic}. The Ky Fan $2$-norm ball is an intersection of three $l_1$ balls in $(x,y)$, $(x, z)$, and $(y,z)$ spaces. The 1-ball in the dual Ky Fan $2$-norm is an intersection of 1-ball the in $l_\infty$ norm and $\frac{1}{2}$-ball in the $l_1$ norm.

\begin{figure}[h!]
	\centering
	\begin{subfigure}[b]{0.45\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figs/norms/KyFan.pdf}
		\caption{Ky Fan 2-norm}
		\label{fig:kyfan}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.45\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figs/norms/KyFanDual.pdf}
		\caption{Dual to Ky Fan 2-norm}
		\label{fig:kyfandual}
	\end{subfigure}
	\caption{Ky Fan 2-norm and its dual}
	\label{fig:kyfan_combined}
\end{figure}


%\section{Updates derivations}
% we prove a general lemma instead
%Proof of \cref{lemma:nsgd_muon_update}:
%Since $\norm{\cdot}^\dagger = \normfstar{\cdot}^{\dagger\dagger} = \normfstar{\cdot}$, the goal is to reach $\normfstar{\mM^k} = \alpha \tr{\mSigma} + (1-\alpha) \normf{\mM^k}$.

%Let us note that $\Delta = \alpha \mU \mV^\top + (1-\alpha) \frac{\mM^k}{\normf{\mM^k}}$ delivers this value. Indeed, by the trace property, $\<\mM^k, \Delta> =\<\mU \mSigma \mV^\top, \alpha \mU \mV^\top + (1-\alpha) \frac{\mU \mSigma \mV^\top}{\normf{\mM^k}}> = \alpha \tr \mSigma + (1-\alpha) \normf{\mM^k} = \normfstar{\mM^k}$, which completes the proof.



% we prove a general lemma instead
%Proof of \cref{lemma:nsgd_neon_update}:
%Since $\norm{\cdot}^\dagger = \normftwo{\cdot}^{\dagger\dagger} = \normftwo{\cdot}$, the goal is to reach $\normftwo{\mM^k} = \alpha \sigma_1 + (1-\alpha) \normf{\mM^k}$.

% Let us note that $\Delta = \alpha (u_1 v_1^\top) + (1-\alpha) \frac{\mM^k}{\normf{\mM^k}}$ delivers this value. Indeed, by the trace property and singular vectors orthogonality, $\<\mM^k, \Delta> =\<\mU \mSigma \mV^\top, \alpha u_1 v_1^\top + (1-\alpha) \frac{\mU \mSigma \mV^\top}{\normf{\mM^k}}> = \alpha \tr \diag({\sigma_1, 0, \dots, 0}) + (1-\alpha) \normf{\mM^k} = \normftwo{\mM^k}$, which completes the proof.



\section{More data for Linear Least Squares}
\label{sec:lls_plots_section}

\begin{table}[ht]
	\centering
	\begin{tabular}{|c|c|c|c|}
	  \hline
	  Method & lr  & momentum           & Iterations to 0.01 loss \\
	  \hline
	  NSGD & 0.25 & 0.9 & 350 \\
	  \hdashline
	  SignSGD & 0.055 $\cdot$ 0.01 & 0.95 & 700 \\
	  \hdashline
	  Muon & 0.025 & 0.5 & 290 \\
	  \hdashline
	  F-Muon & 0.035 & 0.5 & 320 \\
	  \hdashline
	  S-Muon & 0.035 & 0.8 & 270 \\
	  \hline
	\end{tabular}
  \caption{Tuning lrs and momentum coefficients}
  \label{tbl:lls_lrs}
  \end{table}


\begin{figure}[h!]
    \centering
    \begin{subfigure}[t]{0.6\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/simple_lls/gradient_spectral_norm_vs_iteration_500x500.pdf}
        \caption{The spectral norm of the gradient}
        \label{fig:lls_op_grad_norm}
    \end{subfigure}
    \vspace{1em} % optional: add vertical spacing between subfigures
    \begin{subfigure}[t]{0.6\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/simple_lls/gradient_nuclear_norm_vs_iteration_500x500.pdf}
        \caption{The nuclear norm of the gradient}
        \label{fig:lls_nuclear_grad_norm}
    \end{subfigure}
    \vspace{1em} % optional: add vertical spacing between subfigures
    \begin{subfigure}[t]{0.6\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/simple_lls/loss_vs_time_500x500.pdf}
        \caption{The loss over time}
        \label{fig:lls_loss_time}
    \end{subfigure}
    \caption{More images for Linear least squares problem for a 500x500 matrix}
    \label{fig:app_lls}
\end{figure}

\section{Technical details of the experiments}
We compared TRLand with other methods in Google Colab. We used RTX A4000 for CIFAR airbench.


\end{document}