\documentclass{article} % For LaTeX2e
\usepackage{icomp2024_conference,times}

% Optional math command from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath,amssymb}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{changes}
\definechangesauthor[name={Ivan Kozyrev}, color=orange]{IK}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newcommand{\norm}[1]{\lVert #1\rVert}
\newcommand{\abs}[1]{\lvert #1 \rvert}


%\title{From Muon to Neon: Introducing Nuclear Norm to Large Matrices}

\title{The Ky Fan Norms and Beyond: Dual Norms and Combinations for Matrix Optimization}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \icompfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Alexey Kravatskiy \\
Moscow Institute of Physics and Technology (MIPT) \\
\texttt{kravtskii.aiu@phystech.edu} \\
\And
Ivan Kozyrev \\
Moscow Institute of Physics and Technology (MIPT) \\
\texttt{kozyrev.in@phystech.edu} \\
\And
Nikolai Kozlov \\
Moscow Institute of Physics and Technology (MIPT) \\
\texttt{kozlov.na@phystech.edu} \\
\And
Alexander Vinogradov \\
Moscow Institute of Physics and Technology (MIPT) \\
\texttt{vinogradov.am@phystech.edu} \\
\And
Daniil Merkulov \\
Moscow Institute of Physics and Technology (MIPT), Skoltech, HSE, AI4Science \\
\texttt{daniil.merkulov@phystech.edu} \\
\And
Ivan Oseledets \\
AIRI, Skoltech \\
\texttt{i.oseledets@skoltech.ru}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\icompfinalcopy % Uncomment for camera-ready version, but NOT for submission.

%------------------------------------------------------------------------------------
% \epsilon is predefined; override to use \varepsilon
\renewcommand{\epsilon}{\varepsilon}
\newcommand{\Rmn}{\R^{m\times n}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\Ed}[2]{\mathbb{E}_{#1}\left[#2\right]}
\usepackage{mathtools}
\DeclarePairedDelimiter{\sqne}{\|}{\|_2^2}
%\DeclarePairedDelimiter{\norme}{\|}{\|_2}
\DeclarePairedDelimiter{\normf}{\|}{\|_\mathrm{F}}
\DeclarePairedDelimiter{\normkfk}{\|}{\|_{\text{KF-}k}}
\DeclarePairedDelimiter{\normfstar}{\|}{\|_\mathrm{F*}}
\DeclarePairedDelimiter{\normftwo}{\|}{\|_\mathrm{F2}}
\DeclarePairedDelimiter{\sqns}{\|}{\|_{\mathrm{2}}^2}
\DeclarePairedDelimiter{\norms}{\|}{\|_{\mathrm{2}}}
\DeclarePairedDelimiter{\sqnn}{\|}{\|_{\mathrm{nuc}}^2}
\DeclarePairedDelimiter{\sqnf}{\|}{\|_{\mathrm{F}}^2}
\DeclarePairedDelimiter{\normn}{\|}{\|_{\mathrm{nuc}}}
\def\<#1,#2>{\langle #1,#2\rangle}
\DeclarePairedDelimiter{\dotprod}{\langle}{\rangle}

\usepackage{cleveref}
\usepackage{environ}
\newcounter{aequation}
\NewEnviron{aequation}{\refstepcounter{aequation}$$\BODY\eqno{\text{(A\theaequation)}}$$}
\crefname{aequation}{assumption}{assumptions}
\creflabelformat{aequation}{(#2A#1#3)}

\usepackage{amsmath}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\diag}{diag}

\usepackage{arydshln} % dashed lines
\usepackage[right=5cm, marginparwidth=4.6cm, marginparsep=0.2cm]{geometry} % For comments!

%------------------------------------------------------------------------------------

\begin{document}


\maketitle

\begin{abstract}
	In this article, we explore the use of various matrix norms for optimizing functions of weight matrices, a crucial problem in training large language models. Moving beyond the spectral norm that underlies the Muon update, we leverage the duals to the Ky Fan k-norms to introduce a family of Muon-like algorithms we name {\it Fanions}, which happen to be similar to Dion. Subsequently, we construct a second family of {\it F-Fanions}, which are based on the duals of convex combinations of the Ky Fan k-norms and the Frobenius norm. One prominent member of this family is {\it F-Muon}. We complement our theoretical analysis with an extensive empirical study of the algorithms across a wide range of tasks and settings, from which it follows that F-Muon is on par with Muon, which questions the exclusivity of a spectral norm.

\end{abstract}

\section{Introduction}

Minimizing loss functions in unprecedentedly high-dimensional spaces has recently become an integral and crucial part in training large language models. Hence, new scalable, time- and memory-efficient algorithms have been demanded. Besides well-known Adam \citep{kingma2014adam} and AdamW \citep{loshchilov2017decoupled}, recently proposed Muon \citep{jordan2024muon} has shown promising results on training very large models \citep{liu2025muon}. Its key difference from Adam and AdamW is that it has been constructed specifically for optimizing functions of weight matrices, which are common in deep learning.

That is what can be said from a practical point of view. From a theoretical perspective, a key innovation of Muon was its principled derivation of the update rule, which emerges as the solution to an optimization problem constrained by the RMS-to-RMS norm (scaled version of spectral norm) \citep{bernstein2025deriving}


Motivated by the success of Muon, many generalizations and variations of it were proposed. Among the notable ones are Scion \citep{pethick2025training}, Dion \citep{ahn2025dion} and Gluon \citep{riabinin2025gluon}. Those researches target Muon's efficiency and establish convergence bounds. One central question, however, remains unanswered:

\textit{In deriving Muon's update step, why constrain by the spectral norm? How would alternative norms affect performance and computational cost?}

In this article, we tackle this question by first showing the connection between matrix norms and corresponding existing algorithms and discuss the theoretic bounds derived for those algorithms. We then leverage the family of norms dual to Ky Fan k-norms to derive a new class of algorithms with low-rank updates, which we call Fanions. Subsequently, we create a second, hybrid family named F-Fanions by constructing convex combinations of the Ky Fan norms with the Frobenius norm and taking dual of that composite norm. Working within the linear minimization oracle (LMO) framework we derive the explicit update formulas for both algorithm families. As it was done with Muon, we stipulate our algorithms to be fast to approximate, for which we utilize the Lanczos algorithm as described in ~\cref{sec:matrix-side}.

We then compare the performance of the algorithm families across various benchmarks (\cref{sec:experiments}):
\begin{itemize}
	\item Synthetic least squares experiment,
	\item CIFAR-10 airbench,
	\item Pre-training NanoGPT on FineWeb dataset.
\end{itemize}

Our experiments reveal important insights about the role of matrix norms in optimization. \textcolor{red}{On a synthetic least squares problem, we observe a striking discrepancy: while some algorithms converge slowly in terms of loss, they may converge quickly in terms of gradient norm, and vice versa}. This suggests that existing theoretical guarantees may not fully explain practical algorithm performance.

Most notably, our experiments on real-world tasks demonstrate that the choice of underlying matrix norm is remarkably flexible. On CIFAR-10 airbench, properly-tuned F-Muon achieves $94.01 \pm 0.14\%$ accuracy, essentially matching Muon's $94.00 \pm 0.13\%$ performance. On NanoGPT pre-training, F-Muon achieves 3.281 cross-entropy loss, only marginally worse than Muon's 3.279. These results show that Muon-like algorithms can maintain competitive performance even when the underlying norm constraint is significantly modified, answering affirmatively the central question posed above. This flexibility suggests potential for developing easier-to-compute variants of successful algorithms like Muon.


\section{Preliminaries: Linear Minimization Oracle Framework}

Training a neural network is essentially an optimization of a function of several weight matrices and a few vectors. Let us start by considering the problem of minimizing a differentiable function of a {\it single} matrix:
\begin{equation}\label{eq:matrix_problem}
	F(\cdot)\colon \Rmn \to \R\,,\qquad F(\mX) \to \min_{\mX \in \Rmn}\,
\end{equation}
We equip the matrix space $\Rmn$ with a standard dot product $\<\cdot, \cdot> \to \R$ and a norm $\norm{\cdot}\colon \Rmn \to \R_+$, which does not have to coincide with the Frobenius norm $\normf{\cdot}$. The dual norm $\norm{\cdot}^\dagger\colon \Rmn \to \R_+$ that is associated with $\norm{\cdot}$ is defined as
\begin{equation}\label{eq:dual_norm}
	\norm{\mG}^\dagger = \sup_{\mD \in \Rmn\,:\norm{\mD}\leq 1} \<\mG,\mD>\,.
\end{equation}

Such problems can be solved with an iterative algorithm based on the Linear Minimization Oracle (LMO):
\begin{equation}\label{eq:lmo}
	\mathrm{LMO}(\mM^k) \in \argmin_{\mD \in \cS} \<\mM^k, \mD>\,,
\end{equation}
where $\mM^k$ is a gradient (or a momentum) of $F$ in $\mX^k$ and $\cS \subset \Rmn$ is some set. The update of the algorithm is defined as follows:
\begin{equation}\label{eq:simple_update}
	\mX^{k + 1} = \mX^k + \gamma_k\mathrm{LMO}\left(\mM^k\right)\,.
\end{equation}

We are particularly interested in the case when $\cS$ is a unit ball in the norm $\norm{\cdot}$:

$$
\cS = \cB_{\norm{\cdot}} = \{ \mD \in \Rmn \mid \norm{\mD} \leq 1 \}\,.
$$
In this case,
$$
\argmin_{\mD \in \cS} \<\mM^k, \mD> = - \{ \mD \in \cB_1 \mid \<\mM^k, \mD> = \norm{\mM^k}^\dagger\}\,,
$$
and update for $\mX^k$ in \cref{eq:simple_update} simplifies to
\begin{equation}\label{eq:our_update}
	\mX^{k+1} = \mX^{k} - \gamma_k \{\mD \in \cB_1 \mid \<\mM^k, \mD> = \norm{\mM^k}^\dagger\}\,.
\end{equation}

Using this formula it is easy to compute updates for algorithms induced by various norms $\norm{\cdot}$. For example, when the norm $\norm{\cdot}$ is the Frobenius norm $\normf{\cdot}$, \cref{eq:our_update} turns into
\begin{equation}\label{eq:nsgd_update}
	\mX^{k+1} = \mX^{k} - \eta_k \frac{\mM^k}{\normf{\mM^k}}\,,
\end{equation} 
which recovers Normalized SGD (NSGD). When one has the spectral norm $\norms{\cdot}$, one gets
\begin{equation}\label{eq:muon_update}
	\mX^{k+1} = \mX^{k} - \eta_k U V^\top\,,
\end{equation} 
which is Muon without the $\sqrt{m/n}$ factor.
Here, $\mM^k = \mU \mSigma \mV^\top$ is the Singular Value Decomposition (SVD) of $\mM^k$ ($\mU = [u_1, u_2, \dots, u_r]$, $\mSigma = \diag(\sigma_1, \space \sigma_2, \dots, \sigma_r)$, and $\mV = [v_1, v_2, \dots, v_r]$). Muon can be recovered by taking the RMS-to-RMS operator norm $\sqrt{\frac{n}{m}}\norms{\cdot}$.

\section{Duals to Ky Fan Norms Instead of the Spectral Norm}

\subsection{\texorpdfstring{$\normn{\mM^{k}}$ and Neon}{Neon}}
After considering $\normf{\mM^k}$ and $\norms{\mM^k}$, it is natural to look at the nuclear norm $\normn{\mM^k}$.
\begin{lemma}\label{lemma:neon_update}
	When $\norm{\cdot} = \normn{\cdot}$, \cref{eq:our_update} turns into:

	\begin{equation}\label{eq:neon_update}
		\mX^{k+1} = \mX^{k} - \eta_k u_1 v_1^\top
	\end{equation}
\end{lemma}
\begin{proof}
Since $\norm{\cdot}^\dagger = \norms{\cdot}$, the goal is to reach $\norms{\mM^k} = \sigma_1$.

Let us note that $\Delta = u_1 v_1^\top$ delivers this value. Indeed, $\normn{\Delta}=1$ and by the trace property and orthogonality of the singular vectors, $\<\mM^k, \Delta> =\<\mU \mSigma \mV^\top, u_1 v_1^\top> = \tr \diag({\sigma_1, 0, \dots, 0}) = \norms{\mM^k}$, which completes the proof.
\end{proof}

We name the derived algorithm \emph{Neon}. In the~\cref{sec:matrix-side}, we will discuss how to compute the Neon's update.


\subsection{\texorpdfstring{$\normkfk{\mM^k}^\dagger$ and Muon, Neon, and Centralized Dion without error feedback}{Muon, Neon, and Centralized Dion without error feedback}}

Neon's and Muon's updates seem to be complete opposites: one has rank one, while the other is full-rank. It would be interesting to derive algorithms with updates of various ranks.

\textbf{Schatten norms.}
\citet{cesista2025schattenp} considered Schatten-$p$ norms $\norm{\mM^k}_{S_p} = \left(\sum_{i=1}^{\min(m, n)}\sigma_i^p\right)^{1/p}$, which produce the update:
$$X^{k+1} = X^k - \eta_k \mU \frac{\diag\left(\sigma_1^{q-1},\dots, \sigma_{\min(m,n)}^{q-1}\right)}{\left(\sum_{i=1}^{\min(m,n)}{\sigma_i^q}\right)^{\frac{q-1}{q}}}\mV^\top$$
for $q$: $\frac{1}{p} + \frac{1}{q} = 1$. This formula recovers Neon when $p\rightarrow1$ provided that $\sigma_1 > \sigma_2$, which is true on real data; NSGD when $p=2$, and Muon when $p\rightarrow\infty$.

However, Schatten norms do not fill the gap rank: indeed, when $p > 1$, the rank of the update is full, while when $p=1$ ($p \rightarrow 1$) it is one. Moreover, it is not clear how to calculate the update for $p \neq 1, 2, \infty$: it seems one has to know all $\sigma_i$ to compute the update, which makes the problem as hard as the full SVD.

\textbf{Ky Fan norms.}
There is another family of matrix norms, which might help us: Ky Fan norms. For $k \in \{1, \dots, \min(m, n)\}$, the Ky Fan $k$-norm $\normkfk{\cdot}$ is $\sum_{i=1}^{k}\sigma_i$, i.e. the sum of the $k$ largest singular values of the matrix. Special cases of the Ky Fan $k$-norm are the Ky Fan $1$-norm, which is the sprectral norm, and the Ky Fan $\min\{m, n\}$-norm, which is the nuclear norm.

Let us derive the update for an arbitrary $k$. Since $\normkfk{\cdot}^\dagger = \max\{\frac{1}{k} \normn{\cdot}, \norms{\cdot}\}$ (see \citet{bhatia2013matrix}, p. 96), the goal is to reach $\max\{\frac{1}{k}\sum_{i=1}^{\min(m,n)}\sigma_i, \sigma_1\}$. $\Delta = u_1 v_1^\top$ from Neon delivers $\sigma_1$, while $\Delta = \frac{1}{k}\sum_{i=1}^{\min(m,n)}u_i v_i^\top$ from Muon (with an extra factor of $1/k$) delivers $\frac{1}{k}\sum_{i=1}^{\min(m,n)}\sigma_i$. Thus, the update is either
$$
\mX^{t+1} = \mX^{t} - \eta_t u_1 v_1^\top\text{ or }\mX^{t+1} = \mX^{t} - \frac{\eta_t}{k}\mU \mV^\top\,,
$$
depending on which one better minimizes the function $F(\mX^t) + \mM^t(\mX^{t+1} - \mX^t)$.

The rank gap is not closed by the Ky Fan norms because the obtained update is either one-rank or full-rank.

\textbf{Duals to Ky Fan norms.}
While Schatten norms are closed under dualization and $\norm{\mM^k}_{S_p}^\dagger = \norm{\mM^k}_{S_q}$, for Ky Fan norms it is not generally the case. $k = 1$ and $k=\min(m, n)$ are exceptional: for $k=1$ the dual norm is $\normn{\cdot}$ with $k = \min(m, n)$ and for $k = \min(m, n)$ the dual norm is $\norms{\cdot}$ with $k=1$. For each other $k$, $\normkfk{\cdot} \neq \norm{\cdot}_{\text{KF-}k'}$ for any $k'$: $k'=\min(m, n)$ and $k'= 1$ correspond to the already discussed cases, while for other $k'$ one can change $\norm{\mM^k}_{\text{KF-}k'}$ by moving a small value between $\sigma_{k'}$ and $\sigma_{k'+1}$. During this change, $\norms{\mM^k}$ and $\normn{\mM^k}$ will remain constant, hence $\normkfk{\mM^k}$ will not change as well.

\begin{lemma}\label{lemma:ky_fan_update}
	When $\norm{\cdot} = \normkfk{\mM^t}^\dagger$, \cref{eq:our_update} turns into:

	\begin{equation}\label{eq:ky_fan_update}
		\mX^{t+1} = \mX^{t} - \eta \sum_{i=1}^{k}u_i v_i^\top
	\end{equation}
\end{lemma}

\begin{proof}
Since $\norm{\cdot}^\dagger = \normkfk{\cdot}^{\dagger\dagger} = \normkfk{\cdot}$, the goal to reach $\normkfk{\mM^t}$.

Let us note that $\Delta = \sum_{i=1}^{k} u_i v_i^\top$ delivers the value. Indeed, $\<\mM^t, \Delta> =\<\mU \mSigma \mV^\top, \sum_{i=1}^{k} u_i v_i^\top> = \sum_{i,j=1}^{r, k}\<u_i \sigma_i v_i^\top, u_j v_j^\top> = \sum_{i=1}^{k}\sigma_i = \normkfk{\mM^t}$, which completes the proof.
\end{proof}
These updates constitute a family {\it Fanions}, LMO-based algorithms under the $\normkfk{\mM^t}^\dagger$ norms. The algorithm for a particular $k$ we will call {\it Fanion-$k$}. Thus, Muon is a Fanion-$\min\{m,n\}$, while Neon is a Fanion-1. Moreover, the unsharded version of the rank-$r$ Dion (Algorithm 1 from \citet{ahn2025dion}) without the error feedback and without scaling of the update is actually a Fanion-$r$ (see \citet{pethick2025understandingdion}, where Dion is written down in a notation more similar to ours).

\section{Conic Combination of LMO-algorithms is an LMO-algorithm}

Simply applying norm dual to the Ky Fank $k$-norm however, is not enough to provide family of algorithms diverse enough for our cause. Next we consider linear combinations of algorithms from LMO framework, which happen to be LMO-algorithms too, as we show in succeeding paragraphs.

\subsection{General Case}
First, we need a well-known fact mentioned, for example, in \citet[Table 1]{yu2012arithmetic}. For the sake of completeness, we provide a proof here.

\begin{lemma}\label{lemma:dual_to_conv_comb}
	Let $\norm{\cdot}_{(1)},\dots,\norm{\cdot}_{(n)}$ be norms on a finite-dimensional Euclidean space, and let $\alpha_1,\dots,\alpha_n$ be non-negative reals. Define
	$$
		\norm{x} := \sum_{i = 1}^n \alpha_i \norm{x}_{(i)}.
	$$
	Then the dual unit ball of $\norm{\cdot}$ satisfies
	$$
		\cB_{\norm{\cdot}^\dagger}
		= \sum_{i = 1}^n \alpha_i \cB_{\norm{\cdot}_{(i)}^\dagger}
	$$
	where $\sum$ denotes the Minkowski sum and $\cB_{\norm{\cdot}_{(i)}^\dagger}$ is the unit ball of the dual norm $\norm{\cdot}_{(i)}^\dagger$.
\end{lemma}

\begin{proof}
	Let us first prove the lemma for the case $n = 2$. Denote $f(x) = \alpha_1 \norm{x}_{(1)}$ and $g(x) = \alpha_2 \norm{x}_{(2)}$, such that $\norm{x} = f(x) + g(x)$. Recall two standard facts:
	\begin{enumerate}
		\item For any norm $\norm{\cdot}$ and $\lambda>0$,
		      $$
			      (\lambda \norm{\cdot})^*(y) =
			      \sup_{x}\bigl( \langle y,x \rangle - \lambda \norm{x}\bigr)
			      = \delta_{\lambda \cB_{\norm{\cdot}^\dagger}}(y)\,,
		      $$
		      i.e., the indicator function of the scaled dual ball.
		\item The Fenchel conjugate of a sum satisfies
		      $$
			      (f+g)^*(y) = \inf_{u+v=y} \bigl(f^*(u) + g^*(v)\bigr)\,.
		      $$
	\end{enumerate}
	Applying these to $f$ and $g$, we have
	$$
		f^*(u) = \delta_{\alpha_1 \cB_{\norm{\cdot}_{(1)}^\dagger}}(u)\,,
		\quad
		g^*(v) = \delta_{\alpha_2 \cB_{\norm{\cdot}_{(2)}^\dagger}}(v)\,.
	$$
	Thus,
	$$
		\norm{\cdot}^*(y)
		= (f+g)^*(y)
		= \inf_{u+v=y}
		\bigl(
		\delta_{\alpha_1 \cB_{\norm{\cdot}_{(1)}^\dagger}}(u)
		+
		\delta_{\alpha_2 \cB_{\norm{\cdot}_{(2)}^\dagger}}(v)
		\bigr)
		=
		\delta_{\alpha_1 \cB_{\norm{\cdot}_{(1)}^\dagger}+\alpha_2 \cB_{\norm{\cdot}_{(2)}^\dagger}}(y).
	$$
	By definition, the conjugate of a norm is exactly the indicator of its dual unit ball: 
	$$
	\norm{\cdot}^*(y) = \delta_{\cB_{\norm{\cdot}^\dagger}}(y)\,.
	$$
	Therefore, $\cB_{\norm{\cdot}^\dagger} = \alpha_1 \cB_{\norm{\cdot}_{(1)}^\dagger} + \alpha_2 \cB_{\norm{\cdot}_{(2)}^\dagger}$.

	Now we prove the general case by induction. The base case ($n=2$) is already proven. Suppose that the assumption of the lemma holds for $n = k$. Then, for $n = k + 1$, 
	$$
	\norm{x} = \sum_{i = 1}^k \alpha_i \norm{x}_{(i)} + \alpha_{k + 1}\norm{x}_{(k + 1)} = \norm{x}_{(1:k)} + \alpha_{k + 1}\norm{x}_{(k + 1)}\,.
	$$
	Applying the result for $n=2$ combined with the induction assumption, we obtain
	$$
	\cB_{\norm{\cdot}^\dagger} = \cB_{\norm{\cdot}_{(1:k)}^\dagger} + \alpha_{k + 1} \cB_{\norm{\cdot}_{(k + 1)}^\dagger} = \sum_{i = 1}^{k + 1} \alpha_i \cB_{\norm{\cdot}_{(i)}^\dagger}\,,
	$$
	which proves the lemma.
\end{proof}

\begin{lemma}\label{lemma:lin_comb_lmo}
	Let $\norm{\cdot}_{(1)}, \dots, \norm{\cdot}_{(n)}$ be norms on a finite-dimensional Euclidean space, and let $\alpha_1, \dots, \alpha_n$ be non-negative reals. Consider Linear Minimization Oracles $\mathrm{LMO}_{1}, \dots, \mathrm{LMO}_{n}$, corresponding to the unit balls of these norms. Then, $\sum_{i = 1}^n \alpha_i \mathrm{LMO}_{i}$ is the LMO corresponding to the norm $\norm{\cdot}$ dual to the $\sum_{i = 1}^n \alpha_i \norm{\cdot}_{(i)}^\dagger$.
\end{lemma}
	
\begin{proof}
	Using \cref{lemma:dual_to_conv_comb} and the fact $\norm{\cdot}^{\dagger \dagger} = \norm{\cdot}$, we can obtain general form of the unit ball in the $\norm{\cdot}$ norm: $\cB_{\norm{\cdot}} = \sum_{i = 1}^n \alpha_i \cB_{\norm{\cdot}_{(i)}}$. Thus, the linear function minimization objective on a $\norm{\cdot}$ norm ball can be transformed as follows:
	$$
	\argmin_{\mD \in \cB_{\norm{\cdot}}}\<\mM, \mD> = \argmin_{\mD_1 \in \alpha_i \cB_{\norm{\cdot}_{(1)}},\dots,\mD_n \in \alpha_n\cB_{\norm{\cdot}_{(n)}}}\<\mM, \sum_{i = 1}^n\mD_i> = \sum_{i = 1}^n \argmin_{D_i \in \cB_{\norm{\cdot}_{(i)}}}\<\mM, \mD_i>\,,
	$$
	where the last summation denotes the Minkowski sum. This immediately implies $\sum_{i = 1}^n \alpha_i \mathrm{LMO}_{i} \in \argmin_{\mD \in \cB_{\norm{\cdot}}}\<\mM, \mD>$, which proves the claim of the lemma.
\end{proof}

Applying it to optimization algorithms we obtain the following corollary.

\begin{corollary}\label{corollary:lin_comb_alg}
	Let there be a finite family of LMO based algorithms indexed by $i = 1,\dots,n$, where the update of the $i$-th algorithm is defined by 
	$$
	\mX^{k + 1} - \mX^{k} = \gamma_k \mathrm{LMO}_{i}(\mM^{k})\,,
	$$
	and $\mathrm{LMO}_i$ corresponds to the unit ball of norm $\norm{\cdot}_i$. For arbitrary non-negative $\alpha_1,\dots,\alpha_n$, the algorithm with the update given by 
	$$
	\mX^{k + 1} - \mX^{k} = \gamma_k \sum_{i = 1}^n \alpha_k \mathrm{LMO}_{\norm{\cdot}_{(i)}}(\mM^k)
	$$
	is an LMO-algorithm itself, with LMO corresponding to the unit ball of the norm $\norm{\cdot}$ dual to the norm given by $\sum_{i = 1}^n \alpha_i \norm{\cdot}_{(i)}^\dagger$.
\end{corollary}

\subsection{Frobeniusize Them: F-Muon and F-Neon}
Let us construct the concrete examples of algorithms given by linear combinations of LMO-algorithms. It follows from \cref{corollary:lin_comb_alg} that those algorithms can also be viewed as LMO-algorithms.

Combining Fanions-$k$ with another excellent LMO-algorithm, NSGD, we obtain the family of algorithms with updates defined by
\begin{equation}\label{eq:nsgd_muon_update}
		\mX^{t+1} = \mX^{t} - \gamma \left(\alpha \sum_{i = 1}^k u_i v_i^\top + (1-\alpha)\frac{\mM^k}{\normf{\mM^k}}\right)\,,
\end{equation}
where $u_1, \dots, u_k$ ($v_1, \dots, v_k$) are first $k$ left (right) sighular vectors of $\mM^k$. By \cref{corollary:lin_comb_alg}, this is an LMO-algorithm corresponding to the unit ball of the norm $\norm{\cdot}_{\mathrm{F-KF-k}}$, given by
\begin{equation}\label{eq:fstar_norm}
	\norm{\cdot}_{\mathrm{F-KF-k}}^\dagger = \alpha \normkfk{\cdot}^\dagger + (1-\alpha)\normf{\cdot}^\dagger\,.
\end{equation}
We name the derived algorithms \emph{F-Fanions}. A bit of information and visualizations related to the $\normfstar{\cdot}$ norm is presented in the appendix (see \cref{eq:normfstardual}, \cref{fig:fstardual}).

Two edge members of this family, with $k = 1$ and $k = \min\{m, n\}$ correspondingly, are \emph{F-Neon} and \emph{F-Muon}.

\subsection{Add SignSGD: S-Muon and S-Neon}
Trivial formulas, but we need more experiments.

\section{Computing the Updates}\label{sec:matrix-side}
We employ the thick-restarted Lanczos method for the symmetric eigenvalue problem (thick-restart Lanczos, TRLan) to compute the low-rank updates of Fanions. We apply TRLan to either $\mM^{k\top}\mM^k$ or $\mM^k \mM^{k\top}$ (whichever matrix is smaller). We use the CuPy implementation of \texttt{cupy.sparse.linalg.svds} \citep{cupy_svds_ref} which internally relies on TRLan \citep{simonz1998thick}. 

TRLan is specifically designed for efficiently approximating the largest singular values and corresponding singular vectors of very large matrices. The thick-restart strategy retains the most informative Ritz vectors across restarts, which dramatically accelerates convergence while keeping memory consumption moderate. TRLan is particularly attractive in our GPU setting because its dominant cost is a modest number of highly parallelizable matrix-vector multiplications (matvecs) and it avoids full reorthogonalization against the entire Krylov basis by using short recurrence relations combined with thick restarting.

The per-restart complexity is $\mathcal{O}(mn^2 + n^2k + nk^2)$, where $m \geq n$ are the dimensions of the target matrix and $k$ is the size of the retained subspace (typically $k \ll n$).

In Table~\ref{tbl:matrix_methods}, we compare TRLan against randomized SVD (RSVD) and simple power iterations when computing the rank-$k$ update used in Fanion-$k$ and related algorithms. Experiments are performed on dense random matrices with i.i.d. $\mathcal{N}(0,1)$ entries and use CPU implementations for fair comparison. 
We report:
\begin{itemize}
  \item $err_1$: relative error in the Frobenius norm of $\sum_i^k u_i v_i^T$,
  \item $err_2$: relative error in the Frobenius norm of $\sum_i^k \sigma_i u_i v_i^T$.\end{itemize} 

On $500\times500$ matrices, TRLan and RSVD require comparable wall-clock time, but TRLan delivers orders-of-magnitude lower error and far fewer matvecs. On larger $5000\times5000$ matrices the advantage becomes even more pronounced: TRLan is 3-4 times faster than RSVD while using $\sim$30 times fewer matvecs at comparable or better accuracy.

An interesting empirical observation is that RSVD tends to approximate the \emph{singular values} themselves reasonably well but the reconstructed low-rank matrix noticeably deviates from the truncated SVD, whereas TRLan provides an excellent approximation to the truncated SVD matrix itself (low $err_2$) at the cost of occasionally less accurate individual singular values. This makes TRLan the preferred choice for algorithms like Neon/Fanion-$k$ that only need the low-rank term $\sum \sigma_i u_i v_i^\top$, but less ideal for methods (e.g., Dion) that explicitly require accurate $\sigma_i$ for error feedback or step-size control.

A current practical limitation is the lack of a native PyTorch implementation of thick-restart Lanczos; existing PyTorch-based randomized SVD routines cannot match TRLan's accuracy/efficiency combination for the matrix reconstruction task.

For reference, Table~\ref{tbl:newton_schulz_reference} shows results for the Newton-Schulz polar decomposition iteration on the same matrices, $err_1$ is the relative error of $UV^T$ (29-30 iterations to converge, significantly higher matvec count than TRLan).



\begin{table}[ht]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|c|c|}
	  \hline
	  Matrix sizes & $k$  & Method           & Time (s) & Matvecs & Iterations & $err_1$         & $err_2$       \\
	  \hline
	  $500 \times 500$      & 5  & Power Iterations           & 0.062   & 2005    & 200        & 9.2$\cdot 10^{-3}$ & 9.1$\cdot10^{-3}$\\
	  $500 \times 500$      & 5  & RSVD             & 0.017   & 1170    & 38         & 9.8$\cdot 10^{-3}$ & 9.6$\cdot10^{-3}$\\
	  $500 \times 500$      & 5  & TRLan            & 0.018   & 131     & 65         & 9.6$\cdot 10^{-5}$ & 9.4$\cdot10^{-5}$\\
	  \hdashline
	  $500 \times 500$      & 50 & Power Iterations           & 0.44    & 43750   & 437      & 9.9$\cdot 10^{-3}$ & 9.0$\cdot10^{-3}$\\
	  $500 \times 500$      & 50 & RSVD             & 0.61    & 6120    & 50      & 9.9$\cdot 10^{-3}$ & 9.1$\cdot10^{-3}$\\
	  $500 \times 500$      & 50 & TRLan            & 0.16    & 462     & 231      & 3.3$\cdot 10^{-7}$ & 3.0$\cdot10^{-7}$\\
	  \hdashline 
	  $5000 \times 5000$    & 5  & Power Iterations           & 9.6     & 9065    & 906      & 8.6$\cdot 10^{-3}$ & 8.6$\cdot10^{-3}$\\
	  $5000 \times 5000$    & 5  & RSVD             & 2.1     & 5640    & 187      & 9.7$\cdot 10^{-3}$ & 9.7$\cdot10^{-3}$\\
	  $5000 \times 5000$    & 5  & TRLan            & 0.70    & 205     & 102      & 7.7$\cdot 10^{-3}$ & 7.7$\cdot10^{-3}$\\
	  \hline
	\end{tabular}
  \caption{Comparison of methods for computing rank-$k$ updates on dense random matrices (CPU, double precision). Lower is better in all columns.}
  \label{tbl:matrix_methods}
  \end{table}
  
  \begin{table}[ht]
  \centering
  \begin{tabular}{|c|c|c|c|c|}
  \hline
  Matrix size    & Time (s) & Matvecs & Iterations & $err_1$ \\
  \hline
  $500 \times 500$     & 0.041    & 27 000  & 27         & 4.8e-3  \\
  $5000 \times 5000$   & 26.4     & 290 000 & 29         & 6.5e-3  \\
  \hline
  \end{tabular}
  \caption{Newtonâ€“Schulz iteration on the same matrices (for reference).}
  \label{tbl:newton_schulz_reference}
  \end{table}
  
\section{Experiments}\label{sec:experiments}
\subsection{Randomized Linear Least Squares}
First, we compare F-Fanions on the following convex L-smooth problem:

\begin{equation}\label{eq:lls}
	F(\mX) = \frac{1}{2} \<(\mX-\mS), \mM (\mX - \mS) \mN> \to \min_{\mX \in \Rmn}\
\end{equation}

where $\mX \in \Rmn$, $m=500$ and $n=500$ are typical dimensions of a neural network weight matrix, $\mS \in \Rmn$, $\mM \in \mathbb{S}^m_{+}$ and $\mN \in \mathbb{S}^n_{+}$ are positive-semidefinite matrices. The spectra of $\mM$ and $\mN$ are uniformly distributed in the (0, 1) interval. We set $\mS=0$, and $\mX^0 \sim 0.1 \gN(0, \mI)$.

We run different Fanions and their respective F-Fanions with $\alpha=1/2$: Neon (Fanion-1), Fanion-2, Fanion-5, Fanion-100, and Muon (Fanion-500). We test them against Normalized SGD, which is also a F-Fanion with $\alpha = 0$ and an arbitrary $k$.

Since theoretical bounds \citep{kovalev2025understanding, riabinin2025gluon} rely on a very loose norm bound $\norm{\cdot}\le \rho \normf{\cdot}$, we do not derive learning rate or momentum from the smoothness constants. Rather, we find such \texttt{lr, momentum} pair that the loss threshold $0.01$ is reached in a minimal number of iterations. This setting is the most realistic, and at the same time aligns with the corollaries of the convergence theorems where constant learning rate and momentum coefficients are proposed.

12 000 iterations for each algorithm ensure that the rate at some point gets low enough for each algorithm to converge. The results are presented in \cref{fig:lls} and ~\cref{sec:lls_plots_section}.

In terms of the loss minimization, F-Muon had the fastest convergence. In terms of the Frobenius norm of the gradient, the lowest was observed for NSGD. Then go F-Neon (F-Fanion-1), F-Fanion-2, F-Fanion-5, F-Fanion-100, and F-Muon (Fanion-500). It is noticeable that all F-Fanions have lower Frobenius norms of the gradients than their respective Fanioins. The same behavior will be recorded in the next section.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[t]{0.8\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/simple_lls/loss_vs_iteration_500x500.pdf}
        \caption{The loss}
        \label{fig:lls_loss}
    \end{subfigure}
    \vspace{1em} % optional: add vertical spacing between subfigures
    \begin{subfigure}[t]{0.8\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/simple_lls/gradient_frobenius_norm_vs_iteration_500x500.pdf}
        \caption{The Frobenius norm of the gradient}
        \label{fig:lls_fro_grad_norm}
    \end{subfigure}
    \caption{Linear least squares problem for a 500x500 matrix}
    \label{fig:lls}
\end{figure}



\subsection{CIFAR-10 airbench}
We adapt Keller Jordan's code to test F-Muon, Neon, and F-Neon on the CIFAR-10 airbench \citep{cifar2023airbench}.
First, we run F-Muon for different $\alpha$ with the same {\tt lr=0.24(1 - step/total\_steps), momentum=0.6, nesterov=True}, as have been finetuned by Jordan, 10 repetitions for each $\alpha$. We record the accuracy after 8 epochs of training. The results are in \cref{fig:muon_alphas}.

Then we tune F-Muon with $\alpha = 0.5$. Tuned parameters are {\tt lr=0.4(1 - step/total\_steps), momentum=0.65, nesterov = True}. While Muon reaches $94.00 \pm 0.13$\% accuracy after 8 epochs, tuned F-Muon reaches $94.01 \pm 0.14\%$ after 8 epochs (average was done by 200 iterations).

Finally, we take this set of tuned parameters and test on different $\alpha$, 5 times for each $\alpha$. The results are in \cref{fig:fmuon_alphas}. We notice that even when $\alpha=0.1$, the accuracy is much higher than in case of the pure NSGD.

\begin{figure}[h!]
	\centering
	\begin{subfigure}[b]{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figs/muon_tuned_diff_alpha.pdf}
		\caption{With parameters tuned for Muon}
		\label{fig:muon_alphas}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figs/fmuon_tuned_diff_alpha.pdf}
		\caption{With parameters tuned for F-Muon}
		\label{fig:fmuon_alphas}
	\end{subfigure}
	\caption{Mean accuracies for different $\alpha$ of F-Muon.}
	\label{fig:diff_alphas}
\end{figure}

The results are curious and could be represented by \cref{fig:cifar_ball}: the LMO ball that we plotted in a 2D space of singular values has drastically changed, but the observed convergence after tuning has not degraded. These observations raise the question of how much lmo-based algorithms are sensitive to the constraint area, i.e. what will happen if the ball is corrupted. In this particular example, we have had a blurred ball, which proved as robust as the original ball.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\textwidth]{figs/norms/fstardual_cifar.pdf} % adjust width as needed
	\caption{Visualization of the LMO balls for Muon and F-Muon.}
	\label{fig:cifar_ball}
\end{figure}

The most pathological case is $\alpha > 1$, which corresponds to the lmo with an area that is not a norm ball! Despite this violation, the mixture of algorithms reaches almost the same accuracy as vanilla Muon.

\subsection{NanoGPT speedrun}
We test F-Muon on the NanoGPT speedrun \citep{modded_nanogpt_2024}. For $\alpha = 0.5$, the optimal parameters are {\tt lr=0.07, momentum=0.95}, while for Muon they were {\tt lr=0.05, momentum=0.95}. After testing for 1750 steps we get 3.281 cross-entropy loss. For Muon, it falls below the target threshold 3.28 and reaches 3.279. However, this difference is negligible, if one looks at \cref{fig:gpt_loss}. It is even more striking, considering the fact that F-Muon is an average of Muon and NSGD, and the latter performed quite poorly.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{figs/nanogpt/val_loss_vs_step.pdf}
	\caption{The validation loss for NanoGPT}
	\label{fig:gpt_loss}
\end{figure}

Again, as on the CIFAR airbench, if we set $\alpha=-0.1$, F* to which is not a norm, we get a 3.2818 loss, which is not higher than for $\alpha=0.3$, for example.

\section{Algorithms for Vectors $\rightarrow$ Algorithms for Matrices}

One cannot but notice the similarity between the updates LMO optimizers in the vector $l_p$ and the matrix Schatten $S_p$ norms, which is illustrated by \cref{tbl:mat_vs_vec_lmo}. However, the parallels are not limited to this similarity and can be traced even in the empirical observations. SignSGD is very close to Adam, as noted in \citet{bernstein2024oldoptimizernewnorm}, and both Adam and Muon perform well in training large models. NSGD is the same for both matrix and vector cases. Greedy Coordinate Descent is not applied to high-dimensional problems, so from this perspective it is not surprising that one-rank Neon underperforms on such problems.


\begin{table*}[t]
	\caption{lmo optimizers in Schatten $S_p$ norms and in $l_p$ norms. $g$ is the gradient. When it is a matrix, $g = \mU \mSigma \mV^\top$}
	\label{tbl:mat_vs_vec_lmo}
	\bgroup
	\def\arraystretch{1.2}
	\resizebox{\textwidth}{!}{
		\begin{tabular}{|l|c|c|c|}
			\hline
			Method                             & lmo constraint set $\mathcal D$ & lmo                                                         & Reference                            \\
			\hline
			\hline
			Normalized SGD                     & $l_2$-ball, $S_2$-ball          & $-\eta \tfrac{g}{\norm{g}_2} = -\eta \tfrac{g}{\norm{g}_F}$ & \citep{hazan2015beyond}              \\
			Momentum Normalized SGD            & Ball in $l_2$, or Ball in $S_2$ & $-\eta \tfrac{g}{\norm{g}_2} = -\eta\tfrac{g}{\norm{g}_F}$  & \citep{cutkosky2020momentum}         \\
			\hline
			SignSGD                            & Ball in Max-norm $l_\infty$     & $-\eta \sign(g)$                                            & \citep[Thm. 1]{bernstein2018signsgd} \\
			Signum                             & Ball in Max-norm $l_\infty$     & $-\eta \sign(g)$                                            & \citep[Thm. 3]{bernstein2018signsgd} \\
			\hdashline
			Muon                               & Ball in Spectral $S_\infty$     & $-\eta UV^\top$                                             & \citep{jordan2024muon}               \\
			\hline
			Gauss-Southwell Coordinate Descent & Ball in $l_1$                   & $-\eta \sum_{i \in \argmax|g_i^t|} \sign(g_i^t)e_i$                       & \citep[p. 19]{shi2016primer}         \\
			\hdashline
			Neon                               & Ball in Nuclear $S_1$           & $-\eta u_1 v_1^\top$                                        & This work                            \\
			\hline
		\end{tabular}
	}
	\egroup
\end{table*}

\section{Related Work and Discussion}
As Muon \citep{jordan2024muon} is a very successful and popular optimizer for functions of weight matrices, a lot of reseach has been put into, first, further improving its performance, and, second, in explaining its success.

	{\bf Improvements of Muon.} Regarding the first point, in less than a year, a large number of applications and improvements of Muon has been proposed. \citet{liu2025muon} adapted the algorthm for training language models larger than NanoGPT. \citet{shah2025practical} organized efficient hyperparameter transfer by combining Muon with maximal update parametrization. To construct their COSMOS optimizer, \citet{chen2025cosmoshybridadaptive} applied computationally intensive updates of SOAP optimizer to a low-dimensional "leading eigensubspace" while using memory-efficient methods like Muon for the remaining parameters. \citet{amsel2025polar} proposed a more efficient alternative to Newton-Schulz operations. \citet{si2025adamuon} introduced AdaMuon which combines element-wise adaptivity with orthogonal updates. We suppose that the described above or similar techniques can be applied to our optimizers as well. For example, F-Muon also benefits from faster alternatives to Newton-Schulz iterations, and Neon may be a great substitute to Muon in COSMOS, because, as we have shown in {\it the Matrix side of the updates}, Lanczos algorithms is much faster than Newton-Schulz iterations on large matrices.

	{\bf Theory behind Muon.} Regarding the second point, there has been a prolonged gap in theory behind Muon, simplistic derivation of \citet{bernstein2025deriving} based on \citet{bernstein2024oldoptimizernewnorm} excluded. This gap, as it seems to us, is not even now completely closed. For example, \citet{kovalev2025understanding} has provided convergence guarantees of Muon in various settings, from which, however, Muon's supremacy cannot be recovered. Indeed, although the obtained bounds depend on the norm choice, the asymptotics of the convergence remain the same as for NSGD and other optimizers, $K = \cO(\epsilon^{-4})$ in a L-smooth stochastic case.

Similar drawback has a recent article \citet{riabinin2025gluon}, where L-smoothness assumption is replaced with a more practical $(L_0, L_1)$-smoothness. The authors derived from their theorems optimal stepsizes for Muon and Scion that match fine-tuned stepsizes reported by \citet{pethick2025training}. But still, they did not explain why, for instance, NSGD is inferior to Muon in training large-language models.

We suppose that the reason for the recorded by us discrepancy between Neon and Muon performance, both of which are described by Scion or Gluon frameworks, lies in the structure of the norm ball, which must be an object of further research.

	{\bf The LMO and Error Feedback.} We already mentioned that rank-$k$ unsharded Dion without error feedback and scaling of the update is Fanion-$k$. Since error feedback for Dion is very important as discovered by the ablation study in \citet{ahn2025dion}, F-Fanions and S-Fanions will benefit from it as well. In the context of federated learning, error feedback is effective even for compressed Muon \citep{gruntkowska2025efmuon}. The advantage of Fanions and S-Fanions is that less bits are required to transmit the updates: $\sum_{i=1}^k u_i v_i^\top$ is easily transmitted as $\{u_1, v_1, \dots, u_k, v_k\}$ ($(m+n)\times k$ floats), while the sign part is as usual coded in $m \times n$ bits. Thus, the compression is "built-in". Moreover, there is an intriguing possibility to construct differentially private Fanions and S-Fanions with the specific more optimal non-Gaussian noise, as was done with DP-signSGD \citep{jang2024dplogsign}. This we leave for future research.

	{\bf The nuclear norm in the LMO.} As we found out only when transforming our results into an article, the nuclear norm has already been explored in the context of the linear minimization oracle. \citet{pethick2025sam} applied it to create $\nu$SAM, a new sharpness-aware minimization technique. It would be interesting to substitute $\normn{\cdot}$ with $\normkfk{\cdot}^\dagger$ in their approach: $k > 1$ might increase accuracy boost, while preserving a small memory footprint and a small time overhead, if Dion-style power iterations are used.

	
	{\bf Orthogonal research} Fanions, F-Fanions, and S-Fanions benefit from the general theoretical description of \citet{riabinin2025gluon}. They could be transformed to Drop-Fanions by updating only some layers as in \citet{gruntkowska2025dropmuon}. They could be viewed as approximations of the Non-Euclidean Broximal Point Method for the corresponding norms \citep{gruntkowska2025noneuclideanbroximal}. One can clip them to produce ClippedScion-like algorithms \citep{pethick2025generalizedgradientnormclipping}. Finally, the results from \citet{shulgin2025inexactmuon} can be used to theoretically derive faster yet working approximate schemes to calculate the LMO; power iterations with a very limited number of iterations is a good thing to start with.

\section{Conclusion}
In this article, we have generalized several successful algorithms, like Muon and Dion, to the lmo-based algorithms in the $\normkfk{\cdot}^\dagger$ norm. Also we have proposed the technique of ``regularizing'' the updates with NSGD, a trick to increase the robustness of the algorithms, which is motivated by the consideration of the $\normfstar{\cdot}$, $\normftwo{\cdot}$, and general $\norm{\cdot}_{\mathrm{F-KF-k}}$ norms. Generalizations of well-known norms and subsequent combinations of them may further improve performance of lmo-based algorithms. If a theory is developed that explains the discrepancy between performance of different algorithms based on the matrix norms, one will probably be able to intentionally construct norms optimal to given architectures and probably even their parameters.

We suggest that future works that deal with the non-Euclidean LMO explain in corollaries to their convergence theorems the empirical superiority of Muon to other Fanions. Without that, it is hard to believe that the bounds are relevant for practitioners.

\section{Author Contributions}
IO suggested using the nuclear norm in the \citet{bernstein2024oldoptimizernewnorm} framework. DM supervised the project and helped with editing the article. IK, NK, and AV participated mainly on the first stage of research, when it has been a project in the optimization course at MIPT. IK suggested using composite norms (though not F2 and F*) and KKT conditions to find the resulting updates. NK suggested Lanczos algorithm as the fastest tool to compute Neon's updates and conducted experiments to prove it. AV tested Neon on the finetuning of NanoGPT. All other work was done by AK.

\bibliography{icomp2024_conference}
\bibliographystyle{icomp2024_conference}

\appendix
\section{LMO for Neural Networks}
In a typical neural network, the objective function $F$ depends on a set of weight matrices $\{ \mW_1, \mW_2, \ldots \}$. The optimization framework we have described is applied in a layer-wise fashion. At each iteration $k$, a stochastic gradient $g(\mX^k, \xi^k)$ is computed using a mini-batch of data with the $\xi^k$ noise via backpropagation. This yields a separate gradient component, $\mG_i^k$, for each matrix $\mX_i$. The LMO-based update rule is then applied to each matrix $\mX_i$ using its corresponding gradient component $\mG_i^k$.

The update rule used in Muon optimizer is uSCG \citep{pethick2025training}. In the most general case, which involves momentum, it can be written as
\begin{equation}\label{eq:usgd_update}
	\begin{aligned}
		\mM^{k}     & = \alpha_{k} g(\mX^k, \xi^k) + (1 - \alpha_k)\mM^{k - 1}\,, \\
		\mX^{k + 1} & = \mX^k + \gamma_k\mathrm{LMO}(\mM^{k})\,.
	\end{aligned}
\end{equation}


\section{Norms $\normfstar{\cdot}^\dagger$ and $\normftwo{\cdot}^\dagger$}

Based on the lemma, we immediately find $\normfstar{\cdot}^\dagger$, which is related to F-Muon update. Indeed, after setting $\beta=1-\alpha$ and remembering that for smooth and bounded cases we can use $\min$ instead of $\inf$, we get
\begin{equation}\label{eq:normfstardual}
	\normfstar{\mY}^\dagger = \min_{\mZ} \min_t\{t, s.t. \norms{\mZ}\leq \alpha t, \normf{\mY-\mZ}\leq (1-\alpha) t\}
\end{equation}

If $\alpha = 1$, then $\mZ = \mY$, and we get $\normfstar{\mY}^\dagger = \norms{\mY}$. If $\alpha = 0$, then $\mZ = 0$, and we get $\normfstar{\mY}^\dagger = \normf{\mY}$.

Similarly, we find $\normftwo{\cdot}^\dagger$, which is related to F-Neon update:
\begin{equation}\label{eq:normftwodual}
	\normftwo{\mY}^\dagger = \min_{\mZ} \min_t\{t, s.t. \normn{\mZ}\leq \alpha t, \normf{\mY-\mZ}\leq (1-\alpha) t\}
\end{equation}

If $\alpha = 1$, then $\mZ = \mY$, and we get $\normftwo{\mY}^\dagger = \normn{\mY}$. If $\alpha = 0$, then $\mZ = 0$, and we get $\normftwo{\mY}^\dagger = \normf{\mY}$.

Unfortunately, $\normfstar{\cdot}^\dagger$ and $\normftwo{\cdot}^\dagger$ do not have a closed-form expression.

\section{Visualization of different matrix norms}
\subsection{Duals to F* and F2 norms}

It follows from \cref{lemma:dual_to_conv_comb} that the norm ball in $\normfstar{\cdot}^\dagger$ is the Minkowski sum of the norm ball in $\alpha\normn{\cdot}$ and $(1-\alpha)\normf{\cdot}$ and the norm ball in $\normftwo{\cdot}^\dagger$ is the Minkowski sum of the norm ball in $\alpha\norms{\cdot}$ and $(1-\alpha)\normf{\cdot}$.

In Fig. \cref{fig:fduals} we plot these norms. On x-axis and y-axis, there are singular values $\sigma_1$, $\sigma_2$ respectively of a matrix from $\Rmn$ with $\min\{m,n\}=2$.

\begin{figure}[h!]
	\centering
	\begin{subfigure}[b]{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figs/norms/fstardualball.pdf}
		\caption{lmo balls for F-Muon for different $\alpha$}
		\label{fig:fstardual}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figs/norms/ftwodualball.pdf}
		\caption{lmo ball for F-Neon for different $\alpha$}
		\label{fig:ftwodual}
	\end{subfigure}
	\caption{Balls in the duals to F* and F2 norms for different $\alpha$}
	\label{fig:fduals}
\end{figure}

\subsection{The Ky Fan norm and its dual}
1-balls in $l_\infty$, $l_1$ and $l_2$ norms are well-known from textbooks. But what about the Ky Fan $k$-norm? How can it be represented?

To showcase the complex structure of the Ky Fan $k$-norm and its dual, we suggest the illustrations \cref{fig:kyfan_combined} with the ball in the Ky Fan $2$-norm in \cref{fig:kyfan} and its dual in \cref{fig:kyfandual}. On x-, y-, and z-axes, there are singular values $\sigma_1$, $\sigma_2$, and $\sigma_3$ respectively of a matrix from $\Rmn$ with $\min\{m,n\}=3$. In this particular case, we do not sort the singular values. In the proposed representation, we actually plot balls in the Top-$2$ norm $\max\{\abs{x}+\abs{y}, \abs{x}+\abs{z}, \abs{y}+\abs{z}\}$ and its dual norm $\max\{\max(\abs(x),\abs{y},\abs{z}), \frac{1}{2} (\abs{x}+\abs{y}+\abs{z})\}$. The resulting balls are much more complex than balls in $l_\infty$, $l_1$ and $l_2$ norms.

In fact, those balls can be described easier if we use the results from \citet{yu2012arithmetic}. The Ky Fan $2$-norm ball is an intersection of three $l_1$ balls in $(x,y)$, $(x, z)$, and $(y,z)$ spaces. The 1-ball in the dual Ky Fan $2$-norm is an intersection of 1-ball the in $l_\infty$ norm and $\frac{1}{2}$-ball in the $l_1$ norm.

\begin{figure}[h!]
	\centering
	\begin{subfigure}[b]{0.45\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figs/norms/KyFan.pdf}
		\caption{Ky Fan 2-norm}
		\label{fig:kyfan}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.45\linewidth}
		\centering
		\includegraphics[width=\linewidth]{figs/norms/KyFanDual.pdf}
		\caption{Dual to Ky Fan 2-norm}
		\label{fig:kyfandual}
	\end{subfigure}
	\caption{Ky Fan 2-norm and its dual}
	\label{fig:kyfan_combined}
\end{figure}


%\section{Updates derivations}
% we prove a general lemma instead
%Proof of \cref{lemma:nsgd_muon_update}:
%Since $\norm{\cdot}^\dagger = \normfstar{\cdot}^{\dagger\dagger} = \normfstar{\cdot}$, the goal is to reach $\normfstar{\mM^k} = \alpha \tr{\mSigma} + (1-\alpha) \normf{\mM^k}$.

%Let us note that $\Delta = \alpha \mU \mV^\top + (1-\alpha) \frac{\mM^k}{\normf{\mM^k}}$ delivers this value. Indeed, by the trace property, $\<\mM^k, \Delta> =\<\mU \mSigma \mV^\top, \alpha \mU \mV^\top + (1-\alpha) \frac{\mU \mSigma \mV^\top}{\normf{\mM^k}}> = \alpha \tr \mSigma + (1-\alpha) \normf{\mM^k} = \normfstar{\mM^k}$, which completes the proof.



% we prove a general lemma instead
%Proof of \cref{lemma:nsgd_neon_update}:
%Since $\norm{\cdot}^\dagger = \normftwo{\cdot}^{\dagger\dagger} = \normftwo{\cdot}$, the goal is to reach $\normftwo{\mM^k} = \alpha \sigma_1 + (1-\alpha) \normf{\mM^k}$.

% Let us note that $\Delta = \alpha (u_1 v_1^\top) + (1-\alpha) \frac{\mM^k}{\normf{\mM^k}}$ delivers this value. Indeed, by the trace property and singular vectors orthogonality, $\<\mM^k, \Delta> =\<\mU \mSigma \mV^\top, \alpha u_1 v_1^\top + (1-\alpha) \frac{\mU \mSigma \mV^\top}{\normf{\mM^k}}> = \alpha \tr \diag({\sigma_1, 0, \dots, 0}) + (1-\alpha) \normf{\mM^k} = \normftwo{\mM^k}$, which completes the proof.



\section{More plots for Linear Least Squares}
\label{sec:lls_plots_section}

\begin{figure}[h!]
    \centering
    \begin{subfigure}[t]{0.6\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/simple_lls/gradient_spectral_norm_vs_iteration_500x500.pdf}
        \caption{The spectral norm of the gradient}
        \label{fig:lls_op_grad_norm}
    \end{subfigure}
    \vspace{1em} % optional: add vertical spacing between subfigures
    \begin{subfigure}[t]{0.6\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/simple_lls/gradient_nuclear_norm_vs_iteration_500x500.pdf}
        \caption{The nuclear norm of the gradient}
        \label{fig:lls_nuclear_grad_norm}
    \end{subfigure}
    \vspace{1em} % optional: add vertical spacing between subfigures
    \begin{subfigure}[t]{0.6\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/simple_lls/loss_vs_time_500x500.pdf}
        \caption{The loss over time}
        \label{fig:lls_loss_time}
    \end{subfigure}
    \caption{More images for Linear least squares problem for a 500x500 matrix}
    \label{fig:app_lls}
\end{figure}

\section{Technical details of the experiments}
We compared TRLand with other methods in Google Colab. We used RTX A4000 for CIFAR airbench.


\end{document}