\documentclass[fontsize=10pt]{beamer}
%% Possible paper sizes: a0, a0b, a1, a2, a3, a4.
%% Possible orientations: portrait, landscape
%% Font sizes can be changed using the scale option.
\usepackage[size=custom, width=120, height=90, orientation=landscape, scale=1]{beamerposter}
\usetheme{LLT-poster}
\usecolortheme{ComingClean}
%\usecolortheme{Entrepreneur}
%\usecolortheme{ConspiciousCreep}  %% VERY garish.

%\usepackage[utf8]{inputenc}
\usepackage{libertine}
\usepackage[scaled=1]{inconsolata}
\usepackage{svg}
\usepackage{tabularx}
\usepackage{calc}
\usepackage{makecell}
\usepackage{csquotes}
\usepackage{caption}
\usepackage{mathtools}
\usepackage{arydshln}  % For \hdashline in tables
\usepackage{booktabs}
\usepackage{multirow}

% 1. Load the unicode-math package. This is the modern engine for math fonts.
\usepackage{unicode-math}
\usepackage{mwe}

% Custom operatore and contractions
\input{defs.tex}

% Graphics from the article repo
\graphicspath{{../conf_article/figs/}{../figures/13may_muon_neon/}}

% Define eqspace command (for text mode spacing)
\newcommand{\eqspace}{\par\vspace{0.2em}}

\defineauthorblock{%
  \renewcommand\cellalign{l}
  % We use tabular* spanning slightly less than full width to leave room for margins
  % Then center it to distribute margins equally
  \hspace*{\fill}%
  \begin{tabular*}{\dimexpr\linewidth-4em\relax}{
    @{}
    l
    @{\extracolsep{\fill}}
    l
    l
    l
    l
    l
    l
    @{}
  }
    % --- Column 1: First Author ---
    \makecell{
    \textbf{\Large Alexey Kravatskiy$^{1}$} \\[2ex]
    {\Large kravtskii.aiu@phystech.edu}
    } &

    % --- Column 2: Second Author ---
    \makecell{
    \textbf{\Large Ivan Kozyrev$^{1}$} \\[2ex]
    {\Large kozyrev.in@phystech.edu}
    } &

    % --- Column 3: Third Author ---
    \makecell{
    \textbf{\Large Nikolai Kozlov$^{1}$} \\[2ex]
    {\Large kozlov.na@phystech.edu}
    } &

    % --- Column 4: Fourth Author ---
    \makecell{
    \textbf{\Large Alexander Vinogradov$^{1}$} \\[2ex]
    {\Large vinogradov.am@phystech.edu}
    } &

    % --- Column 5: Fifth Author ---
    \makecell{
    \textbf{\Large Daniil Merkulov$^{1, 2, 3, 4}$} \\[2ex]
    {\Large daniil.merkulov@phystech.edu}
    } &

    % --- Column 6: Sixth Author ---
    \makecell{
    \textbf{\Large Ivan Oseledets$^{5, 2}$} \\[2ex]
    {\Large i.oseledets@skoltech.ru}
    } &

    % --- Column 7: All Institutions ---
    \makecell{
    \Large $^{1}$MIPT\; \Large $^{2}$Skoltech\\
    \Large $^{3}$HSE\; \Large $^{4}$AI4Science\\
    \Large $^{5}$AIRI
    }
  \end{tabular*}%
  \hspace*{\fill}%
}

\title{The Ky Fan Norms and Beyond: Dual Norms and Combinations for Matrix Optimization}

\begin{document}
\begin{frame}[fragile]
\begin{columns}[T]
\hspace{0.02\textwidth}% Left margin
%%%% First Column
\begin{column}{0.30\textwidth}
% Apply paragraph spacing inside column
\setlength{\parskip}{0.7em}
\setlength{\parindent}{0pt}
\Large

\textbf{\Huge\color{Zen}Introduction}\\[0.3em]

LLM training requires more scalable and efficient optimizers. The recent Muon~[2] sometimes doubles AdamW's efficiency due to Muon's unique design :
\begin{itemize}
  \item \textbf{\color{HazySummerEve}Matrix-Aware Design:} Muon is constructed specifically for weight matrices rather than for generic vectors.
  \item \textbf{\color{HazySummerEve}Theory-Based Update Rule:} Muon's update is not a mere heuristic. It is derived as the solution to a linear minimization problem constrained by the operator (spectral) norm, giving it a solid mathematical foundation.
\end{itemize}

Muon's success has inspired variants like Dion~[1] algorithm and frameworks of Scion and Gluon~[4]. We build on this work by asking a fundamental question:
\begin{attention}
Why should the choice of a matrix norm in the Muon’s update derivation be restricted to the spectral norm? How might other matrix norms influence the algorithm’s performance and computational cost?
\end{attention}
To investigate this, we first analyze the link between Muon-like algorithms and their defining norms. We then propose \textbf{\color{HazySummerEve}F-Fanions}, a novel family of optimizers derived from exotic, mixed norms. Finally, we present an empirical comparison between Muon and F-Fanions.

\vspace{0.5em}
\textbf{\Huge\color{Zen}How Norms Shape the Update}\\[0.3em]

Many optimizers are defined by a norm-constrained Linear Minimization Oracle (LMO)~[3], which computes the update $\Delta\mX^t$ based on the gradient (possibly stochastic / with momentum) $\mG^t$:
\begin{equation*}
    \Delta X^t = \mX^{t+1} - \mX^t \in \eta \arg\min_{\norm{\mX} \leq 1} \inner{\mG^t}{\mX}\,.
\end{equation*}
The choice of norm dictates the algorithm. For matrix methods, the update often uses the Singular Value Decomposition (SVD) of the gradient, $\mG^t = \mU \mSigma \mV^\top$.

{
\centering
% Using 'tabular' now for better width control based on content
\begin{tabular}{clcc}
\toprule
\textbf{Case} & \textbf{Method} & \textbf{Norm} & \textbf{Update Formula} \\
\midrule
\multirow{3}{*}{\rotatebox[origin=c]{90}{\parbox{1.5cm}{\centering Vec.}}}
& Normalized SGD    & $\ell_2$          & $-\eta g^t / \norm{g^t}_2$ \\
& SignSGD           & $\ell_\infty$     & $-\eta\sign(g^t)$ \\
& Gauss-Southwell CD    & $\ell_1$      & $-\eta \sum_{i \in \argmax|g_i^t|} \sign(g_i^t)e_i$ \\
\midrule
\multirow{4}{*}{\rotatebox[origin=c]{90}{\parbox{1.5cm}{\centering Mat.}}}
& Normalized SGD    & $\normf{\cdot}$   & $-\eta\mG^t / \normf{\mG^t}$ \\
& Muon     & $\norms{\cdot}$   & $-\eta\mU \mV^\top$ \\
& Neon (ours)   & $\norm{\cdot}_{nuc}$ $ & $-\eta u_1 v_1^\top$ \\
& Dion (without EF)   & $\norm{\cdot}_{KF-k}^\dag$ & $-\eta\mU_k \mV_k^\top$ \\

\bottomrule
\end{tabular}\eqspace
}
Here $\norm{\cdot}_{KF-k}$ denotes Ky Fan $k$ norm, $\norm{\cdot}^\dag$ is the dual norm.

\end{column}
\hspace{0.02\textwidth}% Space between columns
%%%% Second Column
\begin{column}{0.30\textwidth}
\setlength{\parskip}{0.7em}
\setlength{\parindent}{0pt}
\Large

We introduce a generalized norm that unifies and interpolates between these methods:
\begin{equation*}
    \norm{\mX}_{\text{gen}} = \left( \alpha \normkfk{\mX} + (1 - \alpha) \normf{\mX} \right)^\dag\,.
\end{equation*}

This defines the \textbf{\color{HazySummerEve}F-Fanions}, a new family of optimizers parameterized by $\alpha \in [0, 1]$ and $k$, with the update rule:
\begin{equation*}
    \Delta \mX^t = -\eta \alpha \mU_k \mV_k^\top - \eta(1 - \alpha) \frac{\mG^t}{\normf{\mG^t}}\,.
\end{equation*}

This framework recovers existing algorithms as special cases:
\begin{itemize}
    \item \textbf{\color{HazySummerEve}Normalized SGD:} $\alpha = 0$.
    \item \textbf{\color{HazySummerEve}Dion} (without error feedback): $\alpha = 1$, $k < \min\{m, n\}$.
    \item \textbf{\color{HazySummerEve}Muon:} $\alpha = 1$, $k = \min\{m, n\}$.
\end{itemize}
We also identify and analyze \textbf{\color{HazySummerEve}F-Neon}, a computationally efficient member of this family with $k=1$.

\vspace{0.5em}
\textbf{\Huge\color{Zen}Efficiently Computing the Update}\\[0.3em]

Computing the F-Fanion update is efficient at the extremes of the rank parameter $k$. We propose using the GPU-friendly \textbf{\color{HazySummerEve}Thick-Restarted Lanczos (TRLan)} method in two distinct ways:
\begin{itemize}
    \item \textbf{\color{HazySummerEve}For small $k$ (e.g., F-Neon):} TRLan is used directly to find the few leading singular vectors.
    \item \textbf{\color{HazySummerEve}For large $k$ (e.g., Muon):} TRLan is used to compute trailing singular values, they are substracted from a full-rank term, obtained through Newton-Shulz iterations.
\end{itemize}

The table below demonstrates the speed of the direct approach for finding $k$ leading singular vectors, from a $5000 \times 5000$ matrix. 

{
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{k=1} & \textbf{k=10} & \textbf{k=100} \\
\midrule
TRLan (our choice) & 0.18s & 0.47s & 1.96s \\
Randomized SVD (RSVD)       & 1.15s          & 19.4s          & 170.0s \\
Power Iterations            & 7.70s          & ---            & --- \\
\bottomrule
\end{tabular}
\par
\vspace{0.5em}
}

TRLan's dramatic speed advantage in this direct computation makes low-rank F-Fanions, like F-Neon, highly efficient.

\vspace{0.5em}
\textbf{\Huge\color{Zen}Modded NanoGPT Speedrun}\\[0.3em]

We benchmarked our method against Muon and NSGD on the \texttt{modded nanogpt 2024} speedrun, which is pretraining of a real LLM. Below are the results after 1,750 steps. The objective was to achieve a loss lower than 3.28.

{
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{LR} & \textbf{Momentum} & \textbf{Final Loss} \\
\midrule
Muon (Baseline)         & 0.05 & 0.95 & 3.279  \\
F-Muon ($\alpha=0.5$)   & 0.07 & 0.95 & 3.281 \\
NSGD                    & 0.07 & 0.96 & 3.4651 \\
F-Muon ($\alpha=0.5$)   & 0.07 & 0.96 & 3.2824 \\
\bottomrule
\end{tabular}
\par
}

\end{column}
\hspace{0.02\textwidth}% Space between columns

%%%% Third column
\begin{column}{0.30\textwidth}
\setlength{\parskip}{0.7em}
\setlength{\parindent}{0pt}
\Large

\textbf{\Huge\color{Zen}CIFAR-10 Airbench Experiments}\\[0.3em]

We evaluate F-Muon against Muon by training a Convolutional Neural Network (CNN) on the CIFAR-10 Airbench. All results are averaged over multiple runs and reported after 8 epochs with batch size = 2,000.

\begin{columns}[T,totalwidth=\textwidth]
      \begin{column}{0.48\textwidth}
        \includegraphics[width=\linewidth]{muon_tuned_diff_alpha.pdf}

        {\centering With parameters from tuned version of Muon\par}
      \end{column}
      \begin{column}{0.48\textwidth}
        \includegraphics[width=\linewidth]{fmuon_tuned_diff_alpha.pdf}

        {\centering With parameters tuned for F-Muon, $\alpha = 0.5$\par}
      \end{column}
    \end{columns}\vspace{1em}

\begin{columns}[T,totalwidth=\textwidth]
  \begin{column}{0.47\textwidth}
    \setlength{\parskip}{0.7em}
\setlength{\parindent}{0pt}
\Large
    We observe that a properly tuned F-Muon performs on par with Muon and achieves the same 94.01\% accuracy. 
    
    Moreover, the tuned F-Muon has a significantly larger trust region than Muon, and the learning rate is so high that Muon yields lower test accuracy when trained with it (see the figure on the right).
  \end{column}
  \begin{column}{0.49\textwidth}
    \includegraphics[width=\linewidth]{fstardual_cifar.pdf}
    \centering
  \end{column}
\end{columns}

\vspace{0.5em}
\textbf{\Huge\color{Zen}Conclusion \& Outlook}\\[0.6em]

\begin{itemize}
    \item We introduced the \textbf{\color{HazySummerEve}F-Fanions} that unify LMO-based optimizers like Muon and Normalized SGD via mixed norms. 

    \item The choice of a norm is a flexible design parameter rather than a fixed rule. Our results show that other norms may be as efficient as the spectral norm.

    \item Our work poses a crucial question of how to theoretically guide norm selection. Future work could also explore adapting the norm (via $\alpha$ and $k$) dynamically during training.
\end{itemize}

\vspace{0.5em}
\textbf{\Huge\color{Zen}Key References}\\[0.6em]

\begin{itemize}
   \large
    \setlength{\itemsep}{0pt} % Remove spacing between items
    \item[{[1]}] Ahn, K. \& Xu, B. \textit{Dion: A Communication-Efficient Optimizer for Large Models}. arXiv:2504.05295, 2025.
    \item[{[2]}] Bernstein, J. \textit{Deriving Muon}. 2025. URL: jeremybernste.in/writing/deriving-muon
    \item[{[3]}] Bernstein, J. \& Newhouse, L. \textit{Old Optimizer, New Norm: An Anthology}. arXiv:2409.20325, 2024.
    \item[{[4]}] Riabinin, A. et al. \textit{Gluon: Making Muon \& Scion Great Again!} arXiv:2505.13416, 2025.
\end{itemize}

\end{column}
\hspace*{0.02\textwidth}% Right margin (using \hspace* to prevent it from being dropped)
\end{columns}

\end{frame}
\end{document}